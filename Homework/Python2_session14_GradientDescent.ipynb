{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 14: Gradient Descent for Multivariate Linear Regression\n",
    "\n",
    "In this session, we will:\n",
    "1. Learn the basics of **NumPy** for numerical computing\n",
    "2. Understand **gradient descent** optimization\n",
    "3. Implement **multivariate linear regression** from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Introduction to NumPy\n",
    "\n",
    "NumPy is the fundamental package for numerical computing in Python. It provides:\n",
    "- N-dimensional arrays (`ndarray`)\n",
    "- Broadcasting for element-wise operations\n",
    "- Linear algebra operations\n",
    "- Mathematical functions optimized for arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Creating Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D array: [-1  2  3  4  5]\n",
      "Shape: (5,)\n",
      "Dtype: int8\n"
     ]
    }
   ],
   "source": [
    "# From Python lists\n",
    "arr1 = np.array([-1, 2, 3, 4, 5], dtype=np.int8)\n",
    "print(f\"1D array: {arr1}\")\n",
    "print(f\"Shape: {arr1.shape}\")\n",
    "print(f\"Dtype: {arr1.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D array:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "Shape: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "# 2D array (matrix)\n",
    "arr2 = np.array([[1, 2, 3], \n",
    "                 [4, 5, 6]])\n",
    "print(f\"2D array:\\n{arr2}\")\n",
    "print(f\"Shape: {arr2.shape}\")  # (rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeros:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "ones:\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "\n",
      "random:\n",
      "[[ 0.28012272  0.5106498  -0.69741541]\n",
      " [-1.88922666 -0.04131382  0.85451728]\n",
      " [-0.68920583  0.3280622   0.90273135]]\n",
      "\n",
      "arange: [0 2 4 6 8]\n",
      "linspace: [0.   0.25 0.5  0.75 1.  ]\n"
     ]
    }
   ],
   "source": [
    "# Common array creation functions\n",
    "zeros = np.zeros((3, 4))       # 3x4 matrix of zeros\n",
    "ones = np.ones((2, 3))         # 2x3 matrix of ones\n",
    "random = np.random.randn(3, 3) # 3x3 matrix of random values (normal distribution)\n",
    "arange = np.arange(0, 10, 2)   # [0, 2, 4, 6, 8]\n",
    "linspace = np.linspace(0, 1, 5) # 5 evenly spaced values from 0 to 1\n",
    "\n",
    "print(f\"zeros:\\n{zeros}\\n\")\n",
    "print(f\"ones:\\n{ones}\\n\")\n",
    "print(f\"random:\\n{random}\\n\")\n",
    "print(f\"arange: {arange}\")\n",
    "print(f\"linspace: {linspace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Array Operations and Broadcasting\n",
    "\n",
    "NumPy operations are **element-wise** by default. Broadcasting allows operations between arrays of different shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b = [5 7 9]\n",
      "a * b = [ 4 10 18]\n",
      "a ** 2 = [1 4 9]\n",
      "a * 10 = [10 20 30]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "print(f\"a + b = {a + b}\")      # Element-wise addition\n",
    "print(f\"a * b = {a * b}\")      # Element-wise multiplication\n",
    "print(f\"a ** 2 = {a ** 2}\")    # Element-wise power\n",
    "print(f\"a * 10 = {a * 10}\")    # Broadcasting: scalar applied to all elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix + row_vector:\n",
      "[[11 22 33]\n",
      " [14 25 36]]\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting with 2D arrays\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6]])\n",
    "row_vector = np.array([10, 20, 30])\n",
    "\n",
    "\"\"\"\n",
    "[[10, 20, 30],\n",
    "[10, 20 ,30]]\n",
    "\"\"\"\n",
    "\n",
    "# The row vector is \"broadcast\" to each row of the matrix\n",
    "result = matrix + row_vector\n",
    "print(f\"Matrix + row_vector:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Matrix Operations\n",
    "\n",
    "For linear regression, we need matrix multiplication and transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A @ B (matrix multiplication):\n",
      "[[19 22]\n",
      " [43 50]]\n",
      "\n",
      "np.dot(A, B):\n",
      "[[19 22]\n",
      " [43 50]]\n",
      "\n",
      "A.T (transpose):\n",
      "[[1 3]\n",
      " [2 4]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2], \n",
    "              [3, 4]])\n",
    "B = np.array([[5, 6], \n",
    "              [7, 8]])\n",
    "\n",
    "# Matrix multiplication (dot product)\n",
    "print(f\"A @ B (matrix multiplication):\\n{A @ B}\")\n",
    "print(f\"\\nnp.dot(A, B):\\n{np.dot(A, B)}\")\n",
    "\n",
    "# Transpose\n",
    "print(f\"\\nA.T (transpose):\\n{A.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Useful Aggregation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum all: 21\n",
      "Sum along rows (axis=1): [ 6 15]\n",
      "Sum along columns (axis=0): [5 7 9]\n",
      "Mean: 3.5\n",
      "Std: 1.707825127659933\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2, 3],\n",
    "                [4, 5, 6]])\n",
    "\n",
    "print(f\"Sum all: {np.sum(arr)}\")\n",
    "print(f\"Sum along rows (axis=1): {np.sum(arr, axis=1)}\")\n",
    "print(f\"Sum along columns (axis=0): {np.sum(arr, axis=0)}\")\n",
    "print(f\"Mean: {np.mean(arr)}\")\n",
    "print(f\"Std: {np.std(arr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Linear Regression Review\n",
    "\n",
    "### What is Linear Regression?\n",
    "\n",
    "Linear regression models the relationship between features $X$ and target $y$ as:\n",
    "\n",
    "$$\\hat{y} = X \\cdot w + b$$\n",
    "\n",
    "Where:\n",
    "- $X$ is the feature matrix of shape `(n_samples, n_features)`\n",
    "- $w$ is the weight vector of shape `(n_features,)`\n",
    "- $b$ is the bias (intercept) scalar\n",
    "- $\\hat{y}$ is the predicted values\n",
    "\n",
    "### The Goal\n",
    "\n",
    "Find $w$ and $b$ that minimize the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\\mathcal{L} = MSE = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients for Linear Regression\n",
    "\n",
    "For MSE loss with linear regression, the gradients are:\n",
    "\n",
    "$$\\nabla_w \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{\\partial MSE}{\\partial w} = \\frac{2}{n}  (\\hat{y} - y) X$$\n",
    "\n",
    "$$\\nabla_b \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{\\partial MSE}{\\partial b} = \\frac{2}{n} \\sum(\\hat{y} - y)$$\n",
    "\n",
    "Let's derive this step by step:\n",
    "\n",
    "1. $MSE = \\frac{1}{n}\\sum(\\hat{y} - y)^2 = \\frac{1}{n}\\sum(Xw + b - y)^2$\n",
    "\n",
    "2. Using chain rule: $\\frac{\\partial MSE}{\\partial w} = \\frac{2}{n} \\cdot (Xw + b - y) \\cdot X$\n",
    "\n",
    "3. In matrix form: $\\frac{\\partial MSE}{\\partial w} = \\frac{2}{n} (\\hat{y} - y) X$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Gradient Descent\n",
    "\n",
    "### What is Gradient Descent?\n",
    "\n",
    "Gradient descent is an optimization algorithm that iteratively updates parameters to minimize a loss function.\n",
    "\n",
    "Think of it like descending a mountain in fog - you can only feel the slope at your current position, so you take small steps in the steepest downward direction.\n",
    "\n",
    "### The Update Rule\n",
    "\n",
    "$$w_{new} = w_{old} - \\alpha \\cdot \\nabla_w \\mathcal{L} = w_{old} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial w} $$\n",
    "\n",
    "Where $\\alpha$ is the **learning rate** - how big of a step we take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Implementation from Scratch\n",
    "\n",
    "Let's build our linear regression step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1000, 3)\n",
      "y shape: (1000,)\n",
      "True weights: [ 2.  -3.5  1.5]\n",
      "True bias: 5.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.49671415, -0.1382643 ,  0.64768854],\n",
       "       [ 1.52302986, -0.23415337, -0.23413696],\n",
       "       [ 1.57921282,  0.76743473, -0.46947439],\n",
       "       ...,\n",
       "       [ 0.60211832,  0.07203686, -0.21220897],\n",
       "       [-0.95191846,  0.07748052,  0.25775254],\n",
       "       [-1.24176058,  0.33417642, -0.15525905]], shape=(1000, 3))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's create some synthetic data\n",
    "np.random.seed(42)\n",
    "\n",
    "# True parameters we want to learn\n",
    "true_weights = np.array([2.0, -3.5, 1.5])\n",
    "true_bias = 5.0\n",
    "\n",
    "# Generate random features\n",
    "n_samples = 1000\n",
    "n_features = 3\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Generate target with some noise\n",
    "noise = np.random.randn(n_samples) * 0.5\n",
    "y = X @ true_weights + true_bias + noise\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"True weights: {true_weights}\")\n",
    "print(f\"True bias: {true_bias}\")\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Core Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write `predict()`, `compute_mse()`, `compute_gradients()`, which perform neccessary operations for gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: np.ndarray, w: np.ndarray, b: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute predictions for linear regression.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        w: Weight vector of shape (n_features,)\n",
    "        b: Bias scalar\n",
    "    \n",
    "    Returns:\n",
    "        Predictions of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    return (X @ w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Mean Squared Error.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Actual target values\n",
    "        y_pred: Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        MSE loss value\n",
    "    \"\"\"\n",
    "   # Your code here\n",
    "    return np.mean((y_pred - y_true) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X: np.ndarray, y: np.ndarray, y_pred: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute gradients for weights and bias.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: True target values\n",
    "        y_pred: Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (gradient_w, gradient_b)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    return (2*((y_pred - y) @ X)/y.shape[0], 2*(np.mean(y_pred - y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute first gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 1.]), 0.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.ones((3))\n",
    "b = 0.0\n",
    "w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 3), (3,), 0.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, w.shape, b # Should be ((1000, 3), (3,), (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-2.85186442,  9.29304439, -1.0522734 ]), np.float64(-10.19197986236907))\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X, w, b) # Initial predictions\n",
    "print(compute_gradients(X, y, y_pred)) # Should print gradients (grad_w, grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you observe what happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    learning_rate: float = 0.01, \n",
    "    n_iterations: int = 1000,\n",
    "    verbose: bool = True,\n",
    "    log_every_n_step = 25,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression using gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: Target values of shape (n_samples,)\n",
    "        learning_rate: Step size for gradient descent\n",
    "        n_iterations: Number of training iterations\n",
    "        verbose: Whether to print progress\n",
    "        log_every_n_step: Number of steps to log the result\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (final_weights, final_bias, loss_history)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize parameters randomly\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Your code here\n",
    "        y_pred = predict(X, w, b)\n",
    "        gd_w, gd_b = compute_gradients(X, y, y_pred)\n",
    "        \n",
    "        w = w - (learning_rate * gd_w)\n",
    "        b = b - (learning_rate * gd_b)\n",
    "        if verbose and (i % log_every_n_step == 0 or i == n_iterations - 1):\n",
    "            loss = compute_mse(y, y_pred)\n",
    "            loss_history.append(loss)\n",
    "            print(f\"Iteration {i:4d} | Loss: {loss:.6f}\")\n",
    "    \n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 | Loss: 45.871205\n",
      "Iteration   25 | Loss: 45.823379\n",
      "Iteration   50 | Loss: 45.775604\n",
      "Iteration   75 | Loss: 45.727879\n",
      "Iteration  100 | Loss: 45.680204\n",
      "Iteration  125 | Loss: 45.632579\n",
      "Iteration  150 | Loss: 45.585004\n",
      "Iteration  175 | Loss: 45.537479\n",
      "Iteration  200 | Loss: 45.490004\n",
      "Iteration  225 | Loss: 45.442579\n",
      "Iteration  250 | Loss: 45.395203\n",
      "Iteration  275 | Loss: 45.347877\n",
      "Iteration  300 | Loss: 45.300601\n",
      "Iteration  325 | Loss: 45.253375\n",
      "Iteration  350 | Loss: 45.206198\n",
      "Iteration  375 | Loss: 45.159070\n",
      "Iteration  400 | Loss: 45.111992\n",
      "Iteration  425 | Loss: 45.064964\n",
      "Iteration  450 | Loss: 45.017984\n",
      "Iteration  475 | Loss: 44.971054\n",
      "Iteration  500 | Loss: 44.924174\n",
      "Iteration  525 | Loss: 44.877342\n",
      "Iteration  550 | Loss: 44.830560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  575 | Loss: 44.783827\n",
      "Iteration  600 | Loss: 44.737142\n",
      "Iteration  625 | Loss: 44.690507\n",
      "Iteration  650 | Loss: 44.643921\n",
      "Iteration  675 | Loss: 44.597383\n",
      "Iteration  700 | Loss: 44.550895\n",
      "Iteration  725 | Loss: 44.504455\n",
      "Iteration  750 | Loss: 44.458064\n",
      "Iteration  775 | Loss: 44.411722\n",
      "Iteration  800 | Loss: 44.365428\n",
      "Iteration  825 | Loss: 44.319183\n",
      "Iteration  850 | Loss: 44.272986\n",
      "Iteration  875 | Loss: 44.226838\n",
      "Iteration  900 | Loss: 44.180739\n",
      "Iteration  925 | Loss: 44.134687\n",
      "Iteration  950 | Loss: 44.088684\n",
      "Iteration  975 | Loss: 44.042730\n",
      "Iteration 1000 | Loss: 43.996823\n",
      "Iteration 1025 | Loss: 43.950965\n",
      "Iteration 1050 | Loss: 43.905155\n",
      "Iteration 1075 | Loss: 43.859393\n",
      "Iteration 1100 | Loss: 43.813678\n",
      "Iteration 1125 | Loss: 43.768012\n",
      "Iteration 1150 | Loss: 43.722394\n",
      "Iteration 1175 | Loss: 43.676824\n",
      "Iteration 1200 | Loss: 43.631301\n",
      "Iteration 1225 | Loss: 43.585827\n",
      "Iteration 1250 | Loss: 43.540400\n",
      "Iteration 1275 | Loss: 43.495020\n",
      "Iteration 1300 | Loss: 43.449689\n",
      "Iteration 1325 | Loss: 43.404405\n",
      "Iteration 1350 | Loss: 43.359168\n",
      "Iteration 1375 | Loss: 43.313979\n",
      "Iteration 1400 | Loss: 43.268837\n",
      "Iteration 1425 | Loss: 43.223743\n",
      "Iteration 1450 | Loss: 43.178696\n",
      "Iteration 1475 | Loss: 43.133696\n",
      "Iteration 1500 | Loss: 43.088744\n",
      "Iteration 1525 | Loss: 43.043838\n",
      "Iteration 1550 | Loss: 42.998980\n",
      "Iteration 1575 | Loss: 42.954169\n",
      "Iteration 1600 | Loss: 42.909405\n",
      "Iteration 1625 | Loss: 42.864688\n",
      "Iteration 1650 | Loss: 42.820018\n",
      "Iteration 1675 | Loss: 42.775394\n",
      "Iteration 1700 | Loss: 42.730818\n",
      "Iteration 1725 | Loss: 42.686288\n",
      "Iteration 1750 | Loss: 42.641805\n",
      "Iteration 1775 | Loss: 42.597369\n",
      "Iteration 1800 | Loss: 42.552979\n",
      "Iteration 1825 | Loss: 42.508636\n",
      "Iteration 1850 | Loss: 42.464339\n",
      "Iteration 1875 | Loss: 42.420089\n",
      "Iteration 1900 | Loss: 42.375886\n",
      "Iteration 1925 | Loss: 42.331729\n",
      "Iteration 1950 | Loss: 42.287618\n",
      "Iteration 1975 | Loss: 42.243553\n",
      "Iteration 2000 | Loss: 42.199535\n",
      "Iteration 2025 | Loss: 42.155562\n",
      "Iteration 2050 | Loss: 42.111636\n",
      "Iteration 2075 | Loss: 42.067757\n",
      "Iteration 2100 | Loss: 42.023923\n",
      "Iteration 2125 | Loss: 41.980135\n",
      "Iteration 2150 | Loss: 41.936393\n",
      "Iteration 2175 | Loss: 41.892697\n",
      "Iteration 2200 | Loss: 41.849047\n",
      "Iteration 2225 | Loss: 41.805442\n",
      "Iteration 2250 | Loss: 41.761884\n",
      "Iteration 2275 | Loss: 41.718371\n",
      "Iteration 2300 | Loss: 41.674903\n",
      "Iteration 2325 | Loss: 41.631482\n",
      "Iteration 2350 | Loss: 41.588106\n",
      "Iteration 2375 | Loss: 41.544775\n",
      "Iteration 2400 | Loss: 41.501490\n",
      "Iteration 2425 | Loss: 41.458251\n",
      "Iteration 2450 | Loss: 41.415056\n",
      "Iteration 2475 | Loss: 41.371907\n",
      "Iteration 2500 | Loss: 41.328804\n",
      "Iteration 2525 | Loss: 41.285745\n",
      "Iteration 2550 | Loss: 41.242732\n",
      "Iteration 2575 | Loss: 41.199764\n",
      "Iteration 2600 | Loss: 41.156841\n",
      "Iteration 2625 | Loss: 41.113963\n",
      "Iteration 2650 | Loss: 41.071130\n",
      "Iteration 2675 | Loss: 41.028342\n",
      "Iteration 2700 | Loss: 40.985599\n",
      "Iteration 2725 | Loss: 40.942901\n",
      "Iteration 2750 | Loss: 40.900247\n",
      "Iteration 2775 | Loss: 40.857638\n",
      "Iteration 2800 | Loss: 40.815074\n",
      "Iteration 2825 | Loss: 40.772555\n",
      "Iteration 2850 | Loss: 40.730080\n",
      "Iteration 2875 | Loss: 40.687650\n",
      "Iteration 2900 | Loss: 40.645265\n",
      "Iteration 2925 | Loss: 40.602923\n",
      "Iteration 2950 | Loss: 40.560627\n",
      "Iteration 2975 | Loss: 40.518374\n",
      "Iteration 3000 | Loss: 40.476166\n",
      "Iteration 3025 | Loss: 40.434003\n",
      "Iteration 3050 | Loss: 40.391883\n",
      "Iteration 3075 | Loss: 40.349808\n",
      "Iteration 3100 | Loss: 40.307777\n",
      "Iteration 3125 | Loss: 40.265790\n",
      "Iteration 3150 | Loss: 40.223847\n",
      "Iteration 3175 | Loss: 40.181948\n",
      "Iteration 3200 | Loss: 40.140093\n",
      "Iteration 3225 | Loss: 40.098282\n",
      "Iteration 3250 | Loss: 40.056515\n",
      "Iteration 3275 | Loss: 40.014792\n",
      "Iteration 3300 | Loss: 39.973112\n",
      "Iteration 3325 | Loss: 39.931476\n",
      "Iteration 3350 | Loss: 39.889884\n",
      "Iteration 3375 | Loss: 39.848336\n",
      "Iteration 3400 | Loss: 39.806831\n",
      "Iteration 3425 | Loss: 39.765370\n",
      "Iteration 3450 | Loss: 39.723952\n",
      "Iteration 3475 | Loss: 39.682578\n",
      "Iteration 3500 | Loss: 39.641247\n",
      "Iteration 3525 | Loss: 39.599959\n",
      "Iteration 3550 | Loss: 39.558715\n",
      "Iteration 3575 | Loss: 39.517514\n",
      "Iteration 3600 | Loss: 39.476356\n",
      "Iteration 3625 | Loss: 39.435242\n",
      "Iteration 3650 | Loss: 39.394170\n",
      "Iteration 3675 | Loss: 39.353142\n",
      "Iteration 3700 | Loss: 39.312157\n",
      "Iteration 3725 | Loss: 39.271215\n",
      "Iteration 3750 | Loss: 39.230316\n",
      "Iteration 3775 | Loss: 39.189459\n",
      "Iteration 3800 | Loss: 39.148646\n",
      "Iteration 3825 | Loss: 39.107875\n",
      "Iteration 3850 | Loss: 39.067147\n",
      "Iteration 3875 | Loss: 39.026462\n",
      "Iteration 3900 | Loss: 38.985820\n",
      "Iteration 3925 | Loss: 38.945220\n",
      "Iteration 3950 | Loss: 38.904663\n",
      "Iteration 3975 | Loss: 38.864148\n",
      "Iteration 4000 | Loss: 38.823676\n",
      "Iteration 4025 | Loss: 38.783246\n",
      "Iteration 4050 | Loss: 38.742859\n",
      "Iteration 4075 | Loss: 38.702514\n",
      "Iteration 4100 | Loss: 38.662212\n",
      "Iteration 4125 | Loss: 38.621952\n",
      "Iteration 4150 | Loss: 38.581734\n",
      "Iteration 4175 | Loss: 38.541558\n",
      "Iteration 4200 | Loss: 38.501425\n",
      "Iteration 4225 | Loss: 38.461333\n",
      "Iteration 4250 | Loss: 38.421284\n",
      "Iteration 4275 | Loss: 38.381276\n",
      "Iteration 4300 | Loss: 38.341311\n",
      "Iteration 4325 | Loss: 38.301387\n",
      "Iteration 4350 | Loss: 38.261506\n",
      "Iteration 4375 | Loss: 38.221666\n",
      "Iteration 4400 | Loss: 38.181868\n",
      "Iteration 4425 | Loss: 38.142112\n",
      "Iteration 4450 | Loss: 38.102398\n",
      "Iteration 4475 | Loss: 38.062725\n",
      "Iteration 4500 | Loss: 38.023094\n",
      "Iteration 4525 | Loss: 37.983504\n",
      "Iteration 4550 | Loss: 37.943956\n",
      "Iteration 4575 | Loss: 37.904450\n",
      "Iteration 4600 | Loss: 37.864985\n",
      "Iteration 4625 | Loss: 37.825561\n",
      "Iteration 4650 | Loss: 37.786179\n",
      "Iteration 4675 | Loss: 37.746838\n",
      "Iteration 4700 | Loss: 37.707538\n",
      "Iteration 4725 | Loss: 37.668280\n",
      "Iteration 4750 | Loss: 37.629063\n",
      "Iteration 4775 | Loss: 37.589886\n",
      "Iteration 4800 | Loss: 37.550751\n",
      "Iteration 4825 | Loss: 37.511658\n",
      "Iteration 4850 | Loss: 37.472605\n",
      "Iteration 4875 | Loss: 37.433593\n",
      "Iteration 4900 | Loss: 37.394622\n",
      "Iteration 4925 | Loss: 37.355692\n",
      "Iteration 4950 | Loss: 37.316803\n",
      "Iteration 4975 | Loss: 37.277954\n",
      "Iteration 5000 | Loss: 37.239146\n",
      "Iteration 5025 | Loss: 37.200380\n",
      "Iteration 5050 | Loss: 37.161653\n",
      "Iteration 5075 | Loss: 37.122968\n",
      "Iteration 5100 | Loss: 37.084323\n",
      "Iteration 5125 | Loss: 37.045718\n",
      "Iteration 5150 | Loss: 37.007154\n",
      "Iteration 5175 | Loss: 36.968631\n",
      "Iteration 5200 | Loss: 36.930148\n",
      "Iteration 5225 | Loss: 36.891705\n",
      "Iteration 5250 | Loss: 36.853303\n",
      "Iteration 5275 | Loss: 36.814941\n",
      "Iteration 5300 | Loss: 36.776619\n",
      "Iteration 5325 | Loss: 36.738337\n",
      "Iteration 5350 | Loss: 36.700096\n",
      "Iteration 5375 | Loss: 36.661894\n",
      "Iteration 5400 | Loss: 36.623733\n",
      "Iteration 5425 | Loss: 36.585612\n",
      "Iteration 5450 | Loss: 36.547531\n",
      "Iteration 5475 | Loss: 36.509490\n",
      "Iteration 5500 | Loss: 36.471488\n",
      "Iteration 5525 | Loss: 36.433527\n",
      "Iteration 5550 | Loss: 36.395605\n",
      "Iteration 5575 | Loss: 36.357724\n",
      "Iteration 5600 | Loss: 36.319882\n",
      "Iteration 5625 | Loss: 36.282079\n",
      "Iteration 5650 | Loss: 36.244317\n",
      "Iteration 5675 | Loss: 36.206593\n",
      "Iteration 5700 | Loss: 36.168910\n",
      "Iteration 5725 | Loss: 36.131266\n",
      "Iteration 5750 | Loss: 36.093662\n",
      "Iteration 5775 | Loss: 36.056097\n",
      "Iteration 5800 | Loss: 36.018571\n",
      "Iteration 5825 | Loss: 35.981085\n",
      "Iteration 5850 | Loss: 35.943638\n",
      "Iteration 5875 | Loss: 35.906231\n",
      "Iteration 5900 | Loss: 35.868862\n",
      "Iteration 5925 | Loss: 35.831533\n",
      "Iteration 5950 | Loss: 35.794243\n",
      "Iteration 5975 | Loss: 35.756992\n",
      "Iteration 6000 | Loss: 35.719781\n",
      "Iteration 6025 | Loss: 35.682608\n",
      "Iteration 6050 | Loss: 35.645474\n",
      "Iteration 6075 | Loss: 35.608380\n",
      "Iteration 6100 | Loss: 35.571324\n",
      "Iteration 6125 | Loss: 35.534307\n",
      "Iteration 6150 | Loss: 35.497329\n",
      "Iteration 6175 | Loss: 35.460390\n",
      "Iteration 6200 | Loss: 35.423489\n",
      "Iteration 6225 | Loss: 35.386627\n",
      "Iteration 6250 | Loss: 35.349804\n",
      "Iteration 6275 | Loss: 35.313020\n",
      "Iteration 6300 | Loss: 35.276274\n",
      "Iteration 6325 | Loss: 35.239567\n",
      "Iteration 6350 | Loss: 35.202898\n",
      "Iteration 6375 | Loss: 35.166268\n",
      "Iteration 6400 | Loss: 35.129676\n",
      "Iteration 6425 | Loss: 35.093122\n",
      "Iteration 6450 | Loss: 35.056607\n",
      "Iteration 6475 | Loss: 35.020130\n",
      "Iteration 6500 | Loss: 34.983692\n",
      "Iteration 6525 | Loss: 34.947292\n",
      "Iteration 6550 | Loss: 34.910929\n",
      "Iteration 6575 | Loss: 34.874605\n",
      "Iteration 6600 | Loss: 34.838320\n",
      "Iteration 6625 | Loss: 34.802072\n",
      "Iteration 6650 | Loss: 34.765862\n",
      "Iteration 6675 | Loss: 34.729690\n",
      "Iteration 6700 | Loss: 34.693557\n",
      "Iteration 6725 | Loss: 34.657461\n",
      "Iteration 6750 | Loss: 34.621403\n",
      "Iteration 6775 | Loss: 34.585382\n",
      "Iteration 6800 | Loss: 34.549400\n",
      "Iteration 6825 | Loss: 34.513455\n",
      "Iteration 6850 | Loss: 34.477549\n",
      "Iteration 6875 | Loss: 34.441679\n",
      "Iteration 6900 | Loss: 34.405848\n",
      "Iteration 6925 | Loss: 34.370054\n",
      "Iteration 6950 | Loss: 34.334297\n",
      "Iteration 6975 | Loss: 34.298578\n",
      "Iteration 7000 | Loss: 34.262897\n",
      "Iteration 7025 | Loss: 34.227253\n",
      "Iteration 7050 | Loss: 34.191646\n",
      "Iteration 7075 | Loss: 34.156077\n",
      "Iteration 7100 | Loss: 34.120545\n",
      "Iteration 7125 | Loss: 34.085051\n",
      "Iteration 7150 | Loss: 34.049593\n",
      "Iteration 7175 | Loss: 34.014173\n",
      "Iteration 7200 | Loss: 33.978790\n",
      "Iteration 7225 | Loss: 33.943444\n",
      "Iteration 7250 | Loss: 33.908135\n",
      "Iteration 7275 | Loss: 33.872863\n",
      "Iteration 7300 | Loss: 33.837629\n",
      "Iteration 7325 | Loss: 33.802431\n",
      "Iteration 7350 | Loss: 33.767270\n",
      "Iteration 7375 | Loss: 33.732146\n",
      "Iteration 7400 | Loss: 33.697059\n",
      "Iteration 7425 | Loss: 33.662009\n",
      "Iteration 7450 | Loss: 33.626995\n",
      "Iteration 7475 | Loss: 33.592018\n",
      "Iteration 7500 | Loss: 33.557078\n",
      "Iteration 7525 | Loss: 33.522175\n",
      "Iteration 7550 | Loss: 33.487308\n",
      "Iteration 7575 | Loss: 33.452478\n",
      "Iteration 7600 | Loss: 33.417684\n",
      "Iteration 7625 | Loss: 33.382927\n",
      "Iteration 7650 | Loss: 33.348206\n",
      "Iteration 7675 | Loss: 33.313522\n",
      "Iteration 7700 | Loss: 33.278874\n",
      "Iteration 7725 | Loss: 33.244262\n",
      "Iteration 7750 | Loss: 33.209687\n",
      "Iteration 7775 | Loss: 33.175148\n",
      "Iteration 7800 | Loss: 33.140645\n",
      "Iteration 7825 | Loss: 33.106179\n",
      "Iteration 7850 | Loss: 33.071749\n",
      "Iteration 7875 | Loss: 33.037354\n",
      "Iteration 7900 | Loss: 33.002996\n",
      "Iteration 7925 | Loss: 32.968674\n",
      "Iteration 7950 | Loss: 32.934388\n",
      "Iteration 7975 | Loss: 32.900138\n",
      "Iteration 8000 | Loss: 32.865924\n",
      "Iteration 8025 | Loss: 32.831746\n",
      "Iteration 8050 | Loss: 32.797603\n",
      "Iteration 8075 | Loss: 32.763497\n",
      "Iteration 8100 | Loss: 32.729426\n",
      "Iteration 8125 | Loss: 32.695391\n",
      "Iteration 8150 | Loss: 32.661391\n",
      "Iteration 8175 | Loss: 32.627428\n",
      "Iteration 8200 | Loss: 32.593500\n",
      "Iteration 8225 | Loss: 32.559607\n",
      "Iteration 8250 | Loss: 32.525750\n",
      "Iteration 8275 | Loss: 32.491929\n",
      "Iteration 8300 | Loss: 32.458143\n",
      "Iteration 8325 | Loss: 32.424393\n",
      "Iteration 8350 | Loss: 32.390678\n",
      "Iteration 8375 | Loss: 32.356998\n",
      "Iteration 8400 | Loss: 32.323354\n",
      "Iteration 8425 | Loss: 32.289745\n",
      "Iteration 8450 | Loss: 32.256171\n",
      "Iteration 8475 | Loss: 32.222633\n",
      "Iteration 8500 | Loss: 32.189129\n",
      "Iteration 8525 | Loss: 32.155661\n",
      "Iteration 8550 | Loss: 32.122228\n",
      "Iteration 8575 | Loss: 32.088830\n",
      "Iteration 8600 | Loss: 32.055467\n",
      "Iteration 8625 | Loss: 32.022139\n",
      "Iteration 8650 | Loss: 31.988846\n",
      "Iteration 8675 | Loss: 31.955588\n",
      "Iteration 8700 | Loss: 31.922365\n",
      "Iteration 8725 | Loss: 31.889177\n",
      "Iteration 8750 | Loss: 31.856023\n",
      "Iteration 8775 | Loss: 31.822905\n",
      "Iteration 8800 | Loss: 31.789821\n",
      "Iteration 8825 | Loss: 31.756772\n",
      "Iteration 8850 | Loss: 31.723757\n",
      "Iteration 8875 | Loss: 31.690777\n",
      "Iteration 8900 | Loss: 31.657832\n",
      "Iteration 8925 | Loss: 31.624921\n",
      "Iteration 8950 | Loss: 31.592045\n",
      "Iteration 8975 | Loss: 31.559203\n",
      "Iteration 9000 | Loss: 31.526396\n",
      "Iteration 9025 | Loss: 31.493623\n",
      "Iteration 9050 | Loss: 31.460885\n",
      "Iteration 9075 | Loss: 31.428181\n",
      "Iteration 9100 | Loss: 31.395511\n",
      "Iteration 9125 | Loss: 31.362875\n",
      "Iteration 9150 | Loss: 31.330274\n",
      "Iteration 9175 | Loss: 31.297707\n",
      "Iteration 9200 | Loss: 31.265174\n",
      "Iteration 9225 | Loss: 31.232675\n",
      "Iteration 9250 | Loss: 31.200211\n",
      "Iteration 9275 | Loss: 31.167780\n",
      "Iteration 9300 | Loss: 31.135384\n",
      "Iteration 9325 | Loss: 31.103021\n",
      "Iteration 9350 | Loss: 31.070692\n",
      "Iteration 9375 | Loss: 31.038398\n",
      "Iteration 9400 | Loss: 31.006137\n",
      "Iteration 9425 | Loss: 30.973910\n",
      "Iteration 9450 | Loss: 30.941717\n",
      "Iteration 9475 | Loss: 30.909557\n",
      "Iteration 9500 | Loss: 30.877432\n",
      "Iteration 9525 | Loss: 30.845340\n",
      "Iteration 9550 | Loss: 30.813281\n",
      "Iteration 9575 | Loss: 30.781257\n",
      "Iteration 9600 | Loss: 30.749266\n",
      "Iteration 9625 | Loss: 30.717308\n",
      "Iteration 9650 | Loss: 30.685384\n",
      "Iteration 9675 | Loss: 30.653494\n",
      "Iteration 9700 | Loss: 30.621637\n",
      "Iteration 9725 | Loss: 30.589813\n",
      "Iteration 9750 | Loss: 30.558023\n",
      "Iteration 9775 | Loss: 30.526266\n",
      "Iteration 9800 | Loss: 30.494542\n",
      "Iteration 9825 | Loss: 30.462852\n",
      "Iteration 9850 | Loss: 30.431195\n",
      "Iteration 9875 | Loss: 30.399571\n",
      "Iteration 9900 | Loss: 30.367981\n",
      "Iteration 9925 | Loss: 30.336423\n",
      "Iteration 9950 | Loss: 30.304899\n",
      "Iteration 9975 | Loss: 30.273408\n",
      "Iteration 10000 | Loss: 30.241950\n",
      "Iteration 10025 | Loss: 30.210524\n",
      "Iteration 10050 | Loss: 30.179132\n",
      "Iteration 10075 | Loss: 30.147773\n",
      "Iteration 10100 | Loss: 30.116446\n",
      "Iteration 10125 | Loss: 30.085153\n",
      "Iteration 10150 | Loss: 30.053892\n",
      "Iteration 10175 | Loss: 30.022664\n",
      "Iteration 10200 | Loss: 29.991469\n",
      "Iteration 10225 | Loss: 29.960306\n",
      "Iteration 10250 | Loss: 29.929177\n",
      "Iteration 10275 | Loss: 29.898080\n",
      "Iteration 10300 | Loss: 29.867015\n",
      "Iteration 10325 | Loss: 29.835983\n",
      "Iteration 10350 | Loss: 29.804984\n",
      "Iteration 10375 | Loss: 29.774017\n",
      "Iteration 10400 | Loss: 29.743083\n",
      "Iteration 10425 | Loss: 29.712181\n",
      "Iteration 10450 | Loss: 29.681312\n",
      "Iteration 10475 | Loss: 29.650474\n",
      "Iteration 10500 | Loss: 29.619670\n",
      "Iteration 10525 | Loss: 29.588897\n",
      "Iteration 10550 | Loss: 29.558157\n",
      "Iteration 10575 | Loss: 29.527449\n",
      "Iteration 10600 | Loss: 29.496774\n",
      "Iteration 10625 | Loss: 29.466130\n",
      "Iteration 10650 | Loss: 29.435519\n",
      "Iteration 10675 | Loss: 29.404940\n",
      "Iteration 10700 | Loss: 29.374393\n",
      "Iteration 10725 | Loss: 29.343878\n",
      "Iteration 10750 | Loss: 29.313395\n",
      "Iteration 10775 | Loss: 29.282944\n",
      "Iteration 10800 | Loss: 29.252525\n",
      "Iteration 10825 | Loss: 29.222137\n",
      "Iteration 10850 | Loss: 29.191782\n",
      "Iteration 10875 | Loss: 29.161459\n",
      "Iteration 10900 | Loss: 29.131167\n",
      "Iteration 10925 | Loss: 29.100907\n",
      "Iteration 10950 | Loss: 29.070679\n",
      "Iteration 10975 | Loss: 29.040482\n",
      "Iteration 11000 | Loss: 29.010318\n",
      "Iteration 11025 | Loss: 28.980185\n",
      "Iteration 11050 | Loss: 28.950083\n",
      "Iteration 11075 | Loss: 28.920013\n",
      "Iteration 11100 | Loss: 28.889975\n",
      "Iteration 11125 | Loss: 28.859968\n",
      "Iteration 11150 | Loss: 28.829993\n",
      "Iteration 11175 | Loss: 28.800049\n",
      "Iteration 11200 | Loss: 28.770136\n",
      "Iteration 11225 | Loss: 28.740255\n",
      "Iteration 11250 | Loss: 28.710405\n",
      "Iteration 11275 | Loss: 28.680587\n",
      "Iteration 11300 | Loss: 28.650800\n",
      "Iteration 11325 | Loss: 28.621044\n",
      "Iteration 11350 | Loss: 28.591319\n",
      "Iteration 11375 | Loss: 28.561626\n",
      "Iteration 11400 | Loss: 28.531963\n",
      "Iteration 11425 | Loss: 28.502332\n",
      "Iteration 11450 | Loss: 28.472732\n",
      "Iteration 11475 | Loss: 28.443163\n",
      "Iteration 11500 | Loss: 28.413625\n",
      "Iteration 11525 | Loss: 28.384118\n",
      "Iteration 11550 | Loss: 28.354641\n",
      "Iteration 11575 | Loss: 28.325196\n",
      "Iteration 11600 | Loss: 28.295782\n",
      "Iteration 11625 | Loss: 28.266398\n",
      "Iteration 11650 | Loss: 28.237046\n",
      "Iteration 11675 | Loss: 28.207724\n",
      "Iteration 11700 | Loss: 28.178433\n",
      "Iteration 11725 | Loss: 28.149172\n",
      "Iteration 11750 | Loss: 28.119943\n",
      "Iteration 11775 | Loss: 28.090744\n",
      "Iteration 11800 | Loss: 28.061575\n",
      "Iteration 11825 | Loss: 28.032438\n",
      "Iteration 11850 | Loss: 28.003330\n",
      "Iteration 11875 | Loss: 27.974254\n",
      "Iteration 11900 | Loss: 27.945208\n",
      "Iteration 11925 | Loss: 27.916192\n",
      "Iteration 11950 | Loss: 27.887207\n",
      "Iteration 11975 | Loss: 27.858252\n",
      "Iteration 12000 | Loss: 27.829327\n",
      "Iteration 12025 | Loss: 27.800433\n",
      "Iteration 12050 | Loss: 27.771569\n",
      "Iteration 12075 | Loss: 27.742736\n",
      "Iteration 12100 | Loss: 27.713933\n",
      "Iteration 12125 | Loss: 27.685160\n",
      "Iteration 12150 | Loss: 27.656417\n",
      "Iteration 12175 | Loss: 27.627704\n",
      "Iteration 12200 | Loss: 27.599021\n",
      "Iteration 12225 | Loss: 27.570369\n",
      "Iteration 12250 | Loss: 27.541746\n",
      "Iteration 12275 | Loss: 27.513154\n",
      "Iteration 12300 | Loss: 27.484592\n",
      "Iteration 12325 | Loss: 27.456059\n",
      "Iteration 12350 | Loss: 27.427557\n",
      "Iteration 12375 | Loss: 27.399084\n",
      "Iteration 12400 | Loss: 27.370641\n",
      "Iteration 12425 | Loss: 27.342228\n",
      "Iteration 12450 | Loss: 27.313845\n",
      "Iteration 12475 | Loss: 27.285492\n",
      "Iteration 12500 | Loss: 27.257168\n",
      "Iteration 12525 | Loss: 27.228874\n",
      "Iteration 12550 | Loss: 27.200610\n",
      "Iteration 12575 | Loss: 27.172376\n",
      "Iteration 12600 | Loss: 27.144171\n",
      "Iteration 12625 | Loss: 27.115995\n",
      "Iteration 12650 | Loss: 27.087850\n",
      "Iteration 12675 | Loss: 27.059733\n",
      "Iteration 12700 | Loss: 27.031647\n",
      "Iteration 12725 | Loss: 27.003589\n",
      "Iteration 12750 | Loss: 26.975561\n",
      "Iteration 12775 | Loss: 26.947563\n",
      "Iteration 12800 | Loss: 26.919594\n",
      "Iteration 12825 | Loss: 26.891654\n",
      "Iteration 12850 | Loss: 26.863744\n",
      "Iteration 12875 | Loss: 26.835862\n",
      "Iteration 12900 | Loss: 26.808011\n",
      "Iteration 12925 | Loss: 26.780188\n",
      "Iteration 12950 | Loss: 26.752394\n",
      "Iteration 12975 | Loss: 26.724630\n",
      "Iteration 13000 | Loss: 26.696895\n",
      "Iteration 13025 | Loss: 26.669189\n",
      "Iteration 13050 | Loss: 26.641512\n",
      "Iteration 13075 | Loss: 26.613864\n",
      "Iteration 13100 | Loss: 26.586245\n",
      "Iteration 13125 | Loss: 26.558654\n",
      "Iteration 13150 | Loss: 26.531093\n",
      "Iteration 13175 | Loss: 26.503561\n",
      "Iteration 13200 | Loss: 26.476058\n",
      "Iteration 13225 | Loss: 26.448583\n",
      "Iteration 13250 | Loss: 26.421138\n",
      "Iteration 13275 | Loss: 26.393721\n",
      "Iteration 13300 | Loss: 26.366333\n",
      "Iteration 13325 | Loss: 26.338973\n",
      "Iteration 13350 | Loss: 26.311643\n",
      "Iteration 13375 | Loss: 26.284341\n",
      "Iteration 13400 | Loss: 26.257068\n",
      "Iteration 13425 | Loss: 26.229823\n",
      "Iteration 13450 | Loss: 26.202607\n",
      "Iteration 13475 | Loss: 26.175419\n",
      "Iteration 13500 | Loss: 26.148260\n",
      "Iteration 13525 | Loss: 26.121129\n",
      "Iteration 13550 | Loss: 26.094027\n",
      "Iteration 13575 | Loss: 26.066954\n",
      "Iteration 13600 | Loss: 26.039908\n",
      "Iteration 13625 | Loss: 26.012891\n",
      "Iteration 13650 | Loss: 25.985903\n",
      "Iteration 13675 | Loss: 25.958942\n",
      "Iteration 13700 | Loss: 25.932010\n",
      "Iteration 13725 | Loss: 25.905107\n",
      "Iteration 13750 | Loss: 25.878231\n",
      "Iteration 13775 | Loss: 25.851384\n",
      "Iteration 13800 | Loss: 25.824565\n",
      "Iteration 13825 | Loss: 25.797774\n",
      "Iteration 13850 | Loss: 25.771011\n",
      "Iteration 13875 | Loss: 25.744276\n",
      "Iteration 13900 | Loss: 25.717569\n",
      "Iteration 13925 | Loss: 25.690890\n",
      "Iteration 13950 | Loss: 25.664240\n",
      "Iteration 13975 | Loss: 25.637617\n",
      "Iteration 14000 | Loss: 25.611022\n",
      "Iteration 14025 | Loss: 25.584455\n",
      "Iteration 14050 | Loss: 25.557916\n",
      "Iteration 14075 | Loss: 25.531404\n",
      "Iteration 14100 | Loss: 25.504921\n",
      "Iteration 14125 | Loss: 25.478465\n",
      "Iteration 14150 | Loss: 25.452037\n",
      "Iteration 14175 | Loss: 25.425637\n",
      "Iteration 14200 | Loss: 25.399264\n",
      "Iteration 14225 | Loss: 25.372920\n",
      "Iteration 14250 | Loss: 25.346602\n",
      "Iteration 14275 | Loss: 25.320313\n",
      "Iteration 14300 | Loss: 25.294051\n",
      "Iteration 14325 | Loss: 25.267816\n",
      "Iteration 14350 | Loss: 25.241609\n",
      "Iteration 14375 | Loss: 25.215430\n",
      "Iteration 14400 | Loss: 25.189278\n",
      "Iteration 14425 | Loss: 25.163153\n",
      "Iteration 14450 | Loss: 25.137056\n",
      "Iteration 14475 | Loss: 25.110986\n",
      "Iteration 14500 | Loss: 25.084943\n",
      "Iteration 14525 | Loss: 25.058928\n",
      "Iteration 14550 | Loss: 25.032940\n",
      "Iteration 14575 | Loss: 25.006980\n",
      "Iteration 14600 | Loss: 24.981046\n",
      "Iteration 14625 | Loss: 24.955140\n",
      "Iteration 14650 | Loss: 24.929261\n",
      "Iteration 14675 | Loss: 24.903409\n",
      "Iteration 14700 | Loss: 24.877585\n",
      "Iteration 14725 | Loss: 24.851787\n",
      "Iteration 14750 | Loss: 24.826017\n",
      "Iteration 14775 | Loss: 24.800273\n",
      "Iteration 14800 | Loss: 24.774556\n",
      "Iteration 14825 | Loss: 24.748867\n",
      "Iteration 14850 | Loss: 24.723204\n",
      "Iteration 14875 | Loss: 24.697569\n",
      "Iteration 14900 | Loss: 24.671960\n",
      "Iteration 14925 | Loss: 24.646378\n",
      "Iteration 14950 | Loss: 24.620823\n",
      "Iteration 14975 | Loss: 24.595294\n",
      "Iteration 15000 | Loss: 24.569793\n",
      "Iteration 15025 | Loss: 24.544318\n",
      "Iteration 15050 | Loss: 24.518870\n",
      "Iteration 15075 | Loss: 24.493449\n",
      "Iteration 15100 | Loss: 24.468054\n",
      "Iteration 15125 | Loss: 24.442686\n",
      "Iteration 15150 | Loss: 24.417345\n",
      "Iteration 15175 | Loss: 24.392030\n",
      "Iteration 15200 | Loss: 24.366741\n",
      "Iteration 15225 | Loss: 24.341480\n",
      "Iteration 15250 | Loss: 24.316244\n",
      "Iteration 15275 | Loss: 24.291036\n",
      "Iteration 15300 | Loss: 24.265853\n",
      "Iteration 15325 | Loss: 24.240697\n",
      "Iteration 15350 | Loss: 24.215568\n",
      "Iteration 15375 | Loss: 24.190464\n",
      "Iteration 15400 | Loss: 24.165388\n",
      "Iteration 15425 | Loss: 24.140337\n",
      "Iteration 15450 | Loss: 24.115313\n",
      "Iteration 15475 | Loss: 24.090315\n",
      "Iteration 15500 | Loss: 24.065343\n",
      "Iteration 15525 | Loss: 24.040397\n",
      "Iteration 15550 | Loss: 24.015478\n",
      "Iteration 15575 | Loss: 23.990585\n",
      "Iteration 15600 | Loss: 23.965717\n",
      "Iteration 15625 | Loss: 23.940876\n",
      "Iteration 15650 | Loss: 23.916061\n",
      "Iteration 15675 | Loss: 23.891272\n",
      "Iteration 15700 | Loss: 23.866509\n",
      "Iteration 15725 | Loss: 23.841772\n",
      "Iteration 15750 | Loss: 23.817061\n",
      "Iteration 15775 | Loss: 23.792376\n",
      "Iteration 15800 | Loss: 23.767717\n",
      "Iteration 15825 | Loss: 23.743083\n",
      "Iteration 15850 | Loss: 23.718476\n",
      "Iteration 15875 | Loss: 23.693894\n",
      "Iteration 15900 | Loss: 23.669338\n",
      "Iteration 15925 | Loss: 23.644808\n",
      "Iteration 15950 | Loss: 23.620303\n",
      "Iteration 15975 | Loss: 23.595824\n",
      "Iteration 16000 | Loss: 23.571371\n",
      "Iteration 16025 | Loss: 23.546944\n",
      "Iteration 16050 | Loss: 23.522542\n",
      "Iteration 16075 | Loss: 23.498166\n",
      "Iteration 16100 | Loss: 23.473815\n",
      "Iteration 16125 | Loss: 23.449490\n",
      "Iteration 16150 | Loss: 23.425190\n",
      "Iteration 16175 | Loss: 23.400916\n",
      "Iteration 16200 | Loss: 23.376667\n",
      "Iteration 16225 | Loss: 23.352444\n",
      "Iteration 16250 | Loss: 23.328246\n",
      "Iteration 16275 | Loss: 23.304074\n",
      "Iteration 16300 | Loss: 23.279927\n",
      "Iteration 16325 | Loss: 23.255805\n",
      "Iteration 16350 | Loss: 23.231709\n",
      "Iteration 16375 | Loss: 23.207637\n",
      "Iteration 16400 | Loss: 23.183591\n",
      "Iteration 16425 | Loss: 23.159571\n",
      "Iteration 16450 | Loss: 23.135575\n",
      "Iteration 16475 | Loss: 23.111605\n",
      "Iteration 16500 | Loss: 23.087660\n",
      "Iteration 16525 | Loss: 23.063740\n",
      "Iteration 16550 | Loss: 23.039845\n",
      "Iteration 16575 | Loss: 23.015975\n",
      "Iteration 16600 | Loss: 22.992130\n",
      "Iteration 16625 | Loss: 22.968310\n",
      "Iteration 16650 | Loss: 22.944515\n",
      "Iteration 16675 | Loss: 22.920745\n",
      "Iteration 16700 | Loss: 22.897000\n",
      "Iteration 16725 | Loss: 22.873280\n",
      "Iteration 16750 | Loss: 22.849585\n",
      "Iteration 16775 | Loss: 22.825914\n",
      "Iteration 16800 | Loss: 22.802269\n",
      "Iteration 16825 | Loss: 22.778648\n",
      "Iteration 16850 | Loss: 22.755052\n",
      "Iteration 16875 | Loss: 22.731481\n",
      "Iteration 16900 | Loss: 22.707935\n",
      "Iteration 16925 | Loss: 22.684413\n",
      "Iteration 16950 | Loss: 22.660916\n",
      "Iteration 16975 | Loss: 22.637443\n",
      "Iteration 17000 | Loss: 22.613995\n",
      "Iteration 17025 | Loss: 22.590572\n",
      "Iteration 17050 | Loss: 22.567173\n",
      "Iteration 17075 | Loss: 22.543799\n",
      "Iteration 17100 | Loss: 22.520450\n",
      "Iteration 17125 | Loss: 22.497124\n",
      "Iteration 17150 | Loss: 22.473824\n",
      "Iteration 17175 | Loss: 22.450548\n",
      "Iteration 17200 | Loss: 22.427296\n",
      "Iteration 17225 | Loss: 22.404068\n",
      "Iteration 17250 | Loss: 22.380865\n",
      "Iteration 17275 | Loss: 22.357687\n",
      "Iteration 17300 | Loss: 22.334532\n",
      "Iteration 17325 | Loss: 22.311402\n",
      "Iteration 17350 | Loss: 22.288296\n",
      "Iteration 17375 | Loss: 22.265215\n",
      "Iteration 17400 | Loss: 22.242157\n",
      "Iteration 17425 | Loss: 22.219124\n",
      "Iteration 17450 | Loss: 22.196115\n",
      "Iteration 17475 | Loss: 22.173130\n",
      "Iteration 17500 | Loss: 22.150169\n",
      "Iteration 17525 | Loss: 22.127232\n",
      "Iteration 17550 | Loss: 22.104320\n",
      "Iteration 17575 | Loss: 22.081431\n",
      "Iteration 17600 | Loss: 22.058566\n",
      "Iteration 17625 | Loss: 22.035726\n",
      "Iteration 17650 | Loss: 22.012909\n",
      "Iteration 17675 | Loss: 21.990116\n",
      "Iteration 17700 | Loss: 21.967347\n",
      "Iteration 17725 | Loss: 21.944602\n",
      "Iteration 17750 | Loss: 21.921881\n",
      "Iteration 17775 | Loss: 21.899184\n",
      "Iteration 17800 | Loss: 21.876511\n",
      "Iteration 17825 | Loss: 21.853861\n",
      "Iteration 17850 | Loss: 21.831235\n",
      "Iteration 17875 | Loss: 21.808633\n",
      "Iteration 17900 | Loss: 21.786054\n",
      "Iteration 17925 | Loss: 21.763499\n",
      "Iteration 17950 | Loss: 21.740968\n",
      "Iteration 17975 | Loss: 21.718461\n",
      "Iteration 18000 | Loss: 21.695977\n",
      "Iteration 18025 | Loss: 21.673516\n",
      "Iteration 18050 | Loss: 21.651080\n",
      "Iteration 18075 | Loss: 21.628666\n",
      "Iteration 18100 | Loss: 21.606277\n",
      "Iteration 18125 | Loss: 21.583910\n",
      "Iteration 18150 | Loss: 21.561568\n",
      "Iteration 18175 | Loss: 21.539248\n",
      "Iteration 18200 | Loss: 21.516952\n",
      "Iteration 18225 | Loss: 21.494680\n",
      "Iteration 18250 | Loss: 21.472430\n",
      "Iteration 18275 | Loss: 21.450205\n",
      "Iteration 18300 | Loss: 21.428002\n",
      "Iteration 18325 | Loss: 21.405823\n",
      "Iteration 18350 | Loss: 21.383667\n",
      "Iteration 18375 | Loss: 21.361534\n",
      "Iteration 18400 | Loss: 21.339424\n",
      "Iteration 18425 | Loss: 21.317338\n",
      "Iteration 18450 | Loss: 21.295275\n",
      "Iteration 18475 | Loss: 21.273235\n",
      "Iteration 18500 | Loss: 21.251218\n",
      "Iteration 18525 | Loss: 21.229224\n",
      "Iteration 18550 | Loss: 21.207253\n",
      "Iteration 18575 | Loss: 21.185305\n",
      "Iteration 18600 | Loss: 21.163381\n",
      "Iteration 18625 | Loss: 21.141479\n",
      "Iteration 18650 | Loss: 21.119600\n",
      "Iteration 18675 | Loss: 21.097744\n",
      "Iteration 18700 | Loss: 21.075911\n",
      "Iteration 18725 | Loss: 21.054101\n",
      "Iteration 18750 | Loss: 21.032314\n",
      "Iteration 18775 | Loss: 21.010550\n",
      "Iteration 18800 | Loss: 20.988809\n",
      "Iteration 18825 | Loss: 20.967090\n",
      "Iteration 18850 | Loss: 20.945394\n",
      "Iteration 18875 | Loss: 20.923721\n",
      "Iteration 18900 | Loss: 20.902071\n",
      "Iteration 18925 | Loss: 20.880443\n",
      "Iteration 18950 | Loss: 20.858838\n",
      "Iteration 18975 | Loss: 20.837256\n",
      "Iteration 19000 | Loss: 20.815696\n",
      "Iteration 19025 | Loss: 20.794159\n",
      "Iteration 19050 | Loss: 20.772645\n",
      "Iteration 19075 | Loss: 20.751153\n",
      "Iteration 19100 | Loss: 20.729683\n",
      "Iteration 19125 | Loss: 20.708237\n",
      "Iteration 19150 | Loss: 20.686812\n",
      "Iteration 19175 | Loss: 20.665410\n",
      "Iteration 19200 | Loss: 20.644031\n",
      "Iteration 19225 | Loss: 20.622674\n",
      "Iteration 19250 | Loss: 20.601339\n",
      "Iteration 19275 | Loss: 20.580027\n",
      "Iteration 19300 | Loss: 20.558737\n",
      "Iteration 19325 | Loss: 20.537470\n",
      "Iteration 19350 | Loss: 20.516224\n",
      "Iteration 19375 | Loss: 20.495001\n",
      "Iteration 19400 | Loss: 20.473801\n",
      "Iteration 19425 | Loss: 20.452622\n",
      "Iteration 19450 | Loss: 20.431466\n",
      "Iteration 19475 | Loss: 20.410332\n",
      "Iteration 19500 | Loss: 20.389220\n",
      "Iteration 19525 | Loss: 20.368130\n",
      "Iteration 19550 | Loss: 20.347063\n",
      "Iteration 19575 | Loss: 20.326017\n",
      "Iteration 19600 | Loss: 20.304994\n",
      "Iteration 19625 | Loss: 20.283992\n",
      "Iteration 19650 | Loss: 20.263013\n",
      "Iteration 19675 | Loss: 20.242055\n",
      "Iteration 19700 | Loss: 20.221120\n",
      "Iteration 19725 | Loss: 20.200207\n",
      "Iteration 19750 | Loss: 20.179315\n",
      "Iteration 19775 | Loss: 20.158445\n",
      "Iteration 19800 | Loss: 20.137598\n",
      "Iteration 19825 | Loss: 20.116772\n",
      "Iteration 19850 | Loss: 20.095968\n",
      "Iteration 19875 | Loss: 20.075186\n",
      "Iteration 19900 | Loss: 20.054425\n",
      "Iteration 19925 | Loss: 20.033687\n",
      "Iteration 19950 | Loss: 20.012970\n",
      "Iteration 19975 | Loss: 19.992275\n",
      "Iteration 20000 | Loss: 19.971601\n",
      "Iteration 20025 | Loss: 19.950949\n",
      "Iteration 20050 | Loss: 19.930319\n",
      "Iteration 20075 | Loss: 19.909711\n",
      "Iteration 20100 | Loss: 19.889124\n",
      "Iteration 20125 | Loss: 19.868559\n",
      "Iteration 20150 | Loss: 19.848015\n",
      "Iteration 20175 | Loss: 19.827493\n",
      "Iteration 20200 | Loss: 19.806992\n",
      "Iteration 20225 | Loss: 19.786513\n",
      "Iteration 20250 | Loss: 19.766056\n",
      "Iteration 20275 | Loss: 19.745619\n",
      "Iteration 20300 | Loss: 19.725205\n",
      "Iteration 20325 | Loss: 19.704811\n",
      "Iteration 20350 | Loss: 19.684439\n",
      "Iteration 20375 | Loss: 19.664089\n",
      "Iteration 20400 | Loss: 19.643760\n",
      "Iteration 20425 | Loss: 19.623452\n",
      "Iteration 20450 | Loss: 19.603165\n",
      "Iteration 20475 | Loss: 19.582900\n",
      "Iteration 20500 | Loss: 19.562656\n",
      "Iteration 20525 | Loss: 19.542433\n",
      "Iteration 20550 | Loss: 19.522231\n",
      "Iteration 20575 | Loss: 19.502051\n",
      "Iteration 20600 | Loss: 19.481891\n",
      "Iteration 20625 | Loss: 19.461753\n",
      "Iteration 20650 | Loss: 19.441636\n",
      "Iteration 20675 | Loss: 19.421540\n",
      "Iteration 20700 | Loss: 19.401465\n",
      "Iteration 20725 | Loss: 19.381412\n",
      "Iteration 20750 | Loss: 19.361379\n",
      "Iteration 20775 | Loss: 19.341367\n",
      "Iteration 20800 | Loss: 19.321376\n",
      "Iteration 20825 | Loss: 19.301407\n",
      "Iteration 20850 | Loss: 19.281458\n",
      "Iteration 20875 | Loss: 19.261530\n",
      "Iteration 20900 | Loss: 19.241623\n",
      "Iteration 20925 | Loss: 19.221737\n",
      "Iteration 20950 | Loss: 19.201871\n",
      "Iteration 20975 | Loss: 19.182027\n",
      "Iteration 21000 | Loss: 19.162203\n",
      "Iteration 21025 | Loss: 19.142400\n",
      "Iteration 21050 | Loss: 19.122618\n",
      "Iteration 21075 | Loss: 19.102857\n",
      "Iteration 21100 | Loss: 19.083116\n",
      "Iteration 21125 | Loss: 19.063396\n",
      "Iteration 21150 | Loss: 19.043697\n",
      "Iteration 21175 | Loss: 19.024018\n",
      "Iteration 21200 | Loss: 19.004360\n",
      "Iteration 21225 | Loss: 18.984723\n",
      "Iteration 21250 | Loss: 18.965106\n",
      "Iteration 21275 | Loss: 18.945510\n",
      "Iteration 21300 | Loss: 18.925935\n",
      "Iteration 21325 | Loss: 18.906380\n",
      "Iteration 21350 | Loss: 18.886845\n",
      "Iteration 21375 | Loss: 18.867331\n",
      "Iteration 21400 | Loss: 18.847837\n",
      "Iteration 21425 | Loss: 18.828364\n",
      "Iteration 21450 | Loss: 18.808911\n",
      "Iteration 21475 | Loss: 18.789479\n",
      "Iteration 21500 | Loss: 18.770067\n",
      "Iteration 21525 | Loss: 18.750675\n",
      "Iteration 21550 | Loss: 18.731304\n",
      "Iteration 21575 | Loss: 18.711953\n",
      "Iteration 21600 | Loss: 18.692622\n",
      "Iteration 21625 | Loss: 18.673312\n",
      "Iteration 21650 | Loss: 18.654022\n",
      "Iteration 21675 | Loss: 18.634752\n",
      "Iteration 21700 | Loss: 18.615502\n",
      "Iteration 21725 | Loss: 18.596273\n",
      "Iteration 21750 | Loss: 18.577063\n",
      "Iteration 21775 | Loss: 18.557874\n",
      "Iteration 21800 | Loss: 18.538705\n",
      "Iteration 21825 | Loss: 18.519556\n",
      "Iteration 21850 | Loss: 18.500427\n",
      "Iteration 21875 | Loss: 18.481319\n",
      "Iteration 21900 | Loss: 18.462230\n",
      "Iteration 21925 | Loss: 18.443161\n",
      "Iteration 21950 | Loss: 18.424112\n",
      "Iteration 21975 | Loss: 18.405083\n",
      "Iteration 22000 | Loss: 18.386075\n",
      "Iteration 22025 | Loss: 18.367086\n",
      "Iteration 22050 | Loss: 18.348117\n",
      "Iteration 22075 | Loss: 18.329168\n",
      "Iteration 22100 | Loss: 18.310238\n",
      "Iteration 22125 | Loss: 18.291329\n",
      "Iteration 22150 | Loss: 18.272439\n",
      "Iteration 22175 | Loss: 18.253570\n",
      "Iteration 22200 | Loss: 18.234720\n",
      "Iteration 22225 | Loss: 18.215890\n",
      "Iteration 22250 | Loss: 18.197079\n",
      "Iteration 22275 | Loss: 18.178288\n",
      "Iteration 22300 | Loss: 18.159517\n",
      "Iteration 22325 | Loss: 18.140766\n",
      "Iteration 22350 | Loss: 18.122035\n",
      "Iteration 22375 | Loss: 18.103323\n",
      "Iteration 22400 | Loss: 18.084630\n",
      "Iteration 22425 | Loss: 18.065957\n",
      "Iteration 22450 | Loss: 18.047304\n",
      "Iteration 22475 | Loss: 18.028671\n",
      "Iteration 22500 | Loss: 18.010057\n",
      "Iteration 22525 | Loss: 17.991462\n",
      "Iteration 22550 | Loss: 17.972887\n",
      "Iteration 22575 | Loss: 17.954331\n",
      "Iteration 22600 | Loss: 17.935795\n",
      "Iteration 22625 | Loss: 17.917278\n",
      "Iteration 22650 | Loss: 17.898781\n",
      "Iteration 22675 | Loss: 17.880303\n",
      "Iteration 22700 | Loss: 17.861845\n",
      "Iteration 22725 | Loss: 17.843406\n",
      "Iteration 22750 | Loss: 17.824986\n",
      "Iteration 22775 | Loss: 17.806585\n",
      "Iteration 22800 | Loss: 17.788204\n",
      "Iteration 22825 | Loss: 17.769842\n",
      "Iteration 22850 | Loss: 17.751500\n",
      "Iteration 22875 | Loss: 17.733176\n",
      "Iteration 22900 | Loss: 17.714872\n",
      "Iteration 22925 | Loss: 17.696587\n",
      "Iteration 22950 | Loss: 17.678321\n",
      "Iteration 22975 | Loss: 17.660075\n",
      "Iteration 23000 | Loss: 17.641847\n",
      "Iteration 23025 | Loss: 17.623639\n",
      "Iteration 23050 | Loss: 17.605449\n",
      "Iteration 23075 | Loss: 17.587279\n",
      "Iteration 23100 | Loss: 17.569128\n",
      "Iteration 23125 | Loss: 17.550996\n",
      "Iteration 23150 | Loss: 17.532883\n",
      "Iteration 23175 | Loss: 17.514788\n",
      "Iteration 23200 | Loss: 17.496713\n",
      "Iteration 23225 | Loss: 17.478657\n",
      "Iteration 23250 | Loss: 17.460620\n",
      "Iteration 23275 | Loss: 17.442601\n",
      "Iteration 23300 | Loss: 17.424602\n",
      "Iteration 23325 | Loss: 17.406621\n",
      "Iteration 23350 | Loss: 17.388660\n",
      "Iteration 23375 | Loss: 17.370717\n",
      "Iteration 23400 | Loss: 17.352793\n",
      "Iteration 23425 | Loss: 17.334887\n",
      "Iteration 23450 | Loss: 17.317001\n",
      "Iteration 23475 | Loss: 17.299133\n",
      "Iteration 23500 | Loss: 17.281284\n",
      "Iteration 23525 | Loss: 17.263454\n",
      "Iteration 23550 | Loss: 17.245642\n",
      "Iteration 23575 | Loss: 17.227849\n",
      "Iteration 23600 | Loss: 17.210075\n",
      "Iteration 23625 | Loss: 17.192319\n",
      "Iteration 23650 | Loss: 17.174582\n",
      "Iteration 23675 | Loss: 17.156864\n",
      "Iteration 23700 | Loss: 17.139164\n",
      "Iteration 23725 | Loss: 17.121483\n",
      "Iteration 23750 | Loss: 17.103820\n",
      "Iteration 23775 | Loss: 17.086176\n",
      "Iteration 23800 | Loss: 17.068550\n",
      "Iteration 23825 | Loss: 17.050943\n",
      "Iteration 23850 | Loss: 17.033354\n",
      "Iteration 23875 | Loss: 17.015784\n",
      "Iteration 23900 | Loss: 16.998232\n",
      "Iteration 23925 | Loss: 16.980699\n",
      "Iteration 23950 | Loss: 16.963183\n",
      "Iteration 23975 | Loss: 16.945687\n",
      "Iteration 24000 | Loss: 16.928208\n",
      "Iteration 24025 | Loss: 16.910748\n",
      "Iteration 24050 | Loss: 16.893307\n",
      "Iteration 24075 | Loss: 16.875883\n",
      "Iteration 24100 | Loss: 16.858478\n",
      "Iteration 24125 | Loss: 16.841091\n",
      "Iteration 24150 | Loss: 16.823722\n",
      "Iteration 24175 | Loss: 16.806372\n",
      "Iteration 24200 | Loss: 16.789040\n",
      "Iteration 24225 | Loss: 16.771726\n",
      "Iteration 24250 | Loss: 16.754430\n",
      "Iteration 24275 | Loss: 16.737152\n",
      "Iteration 24300 | Loss: 16.719892\n",
      "Iteration 24325 | Loss: 16.702650\n",
      "Iteration 24350 | Loss: 16.685427\n",
      "Iteration 24375 | Loss: 16.668222\n",
      "Iteration 24400 | Loss: 16.651034\n",
      "Iteration 24425 | Loss: 16.633865\n",
      "Iteration 24450 | Loss: 16.616713\n",
      "Iteration 24475 | Loss: 16.599580\n",
      "Iteration 24500 | Loss: 16.582465\n",
      "Iteration 24525 | Loss: 16.565367\n",
      "Iteration 24550 | Loss: 16.548288\n",
      "Iteration 24575 | Loss: 16.531226\n",
      "Iteration 24600 | Loss: 16.514182\n",
      "Iteration 24625 | Loss: 16.497156\n",
      "Iteration 24650 | Loss: 16.480148\n",
      "Iteration 24675 | Loss: 16.463158\n",
      "Iteration 24700 | Loss: 16.446186\n",
      "Iteration 24725 | Loss: 16.429231\n",
      "Iteration 24750 | Loss: 16.412294\n",
      "Iteration 24775 | Loss: 16.395375\n",
      "Iteration 24800 | Loss: 16.378474\n",
      "Iteration 24825 | Loss: 16.361590\n",
      "Iteration 24850 | Loss: 16.344725\n",
      "Iteration 24875 | Loss: 16.327876\n",
      "Iteration 24900 | Loss: 16.311046\n",
      "Iteration 24925 | Loss: 16.294233\n",
      "Iteration 24950 | Loss: 16.277438\n",
      "Iteration 24975 | Loss: 16.260660\n",
      "Iteration 25000 | Loss: 16.243900\n",
      "Iteration 25025 | Loss: 16.227158\n",
      "Iteration 25050 | Loss: 16.210433\n",
      "Iteration 25075 | Loss: 16.193725\n",
      "Iteration 25100 | Loss: 16.177036\n",
      "Iteration 25125 | Loss: 16.160363\n",
      "Iteration 25150 | Loss: 16.143708\n",
      "Iteration 25175 | Loss: 16.127071\n",
      "Iteration 25200 | Loss: 16.110451\n",
      "Iteration 25225 | Loss: 16.093849\n",
      "Iteration 25250 | Loss: 16.077264\n",
      "Iteration 25275 | Loss: 16.060696\n",
      "Iteration 25300 | Loss: 16.044146\n",
      "Iteration 25325 | Loss: 16.027613\n",
      "Iteration 25350 | Loss: 16.011097\n",
      "Iteration 25375 | Loss: 15.994599\n",
      "Iteration 25400 | Loss: 15.978118\n",
      "Iteration 25425 | Loss: 15.961654\n",
      "Iteration 25450 | Loss: 15.945207\n",
      "Iteration 25475 | Loss: 15.928778\n",
      "Iteration 25500 | Loss: 15.912366\n",
      "Iteration 25525 | Loss: 15.895971\n",
      "Iteration 25550 | Loss: 15.879594\n",
      "Iteration 25575 | Loss: 15.863233\n",
      "Iteration 25600 | Loss: 15.846890\n",
      "Iteration 25625 | Loss: 15.830564\n",
      "Iteration 25650 | Loss: 15.814255\n",
      "Iteration 25675 | Loss: 15.797963\n",
      "Iteration 25700 | Loss: 15.781688\n",
      "Iteration 25725 | Loss: 15.765430\n",
      "Iteration 25750 | Loss: 15.749190\n",
      "Iteration 25775 | Loss: 15.732966\n",
      "Iteration 25800 | Loss: 15.716759\n",
      "Iteration 25825 | Loss: 15.700570\n",
      "Iteration 25850 | Loss: 15.684397\n",
      "Iteration 25875 | Loss: 15.668241\n",
      "Iteration 25900 | Loss: 15.652102\n",
      "Iteration 25925 | Loss: 15.635980\n",
      "Iteration 25950 | Loss: 15.619875\n",
      "Iteration 25975 | Loss: 15.603787\n",
      "Iteration 26000 | Loss: 15.587716\n",
      "Iteration 26025 | Loss: 15.571662\n",
      "Iteration 26050 | Loss: 15.555624\n",
      "Iteration 26075 | Loss: 15.539604\n",
      "Iteration 26100 | Loss: 15.523600\n",
      "Iteration 26125 | Loss: 15.507612\n",
      "Iteration 26150 | Loss: 15.491642\n",
      "Iteration 26175 | Loss: 15.475688\n",
      "Iteration 26200 | Loss: 15.459752\n",
      "Iteration 26225 | Loss: 15.443831\n",
      "Iteration 26250 | Loss: 15.427928\n",
      "Iteration 26275 | Loss: 15.412041\n",
      "Iteration 26300 | Loss: 15.396171\n",
      "Iteration 26325 | Loss: 15.380317\n",
      "Iteration 26350 | Loss: 15.364481\n",
      "Iteration 26375 | Loss: 15.348660\n",
      "Iteration 26400 | Loss: 15.332857\n",
      "Iteration 26425 | Loss: 15.317069\n",
      "Iteration 26450 | Loss: 15.301299\n",
      "Iteration 26475 | Loss: 15.285545\n",
      "Iteration 26500 | Loss: 15.269807\n",
      "Iteration 26525 | Loss: 15.254086\n",
      "Iteration 26550 | Loss: 15.238382\n",
      "Iteration 26575 | Loss: 15.222694\n",
      "Iteration 26600 | Loss: 15.207022\n",
      "Iteration 26625 | Loss: 15.191367\n",
      "Iteration 26650 | Loss: 15.175728\n",
      "Iteration 26675 | Loss: 15.160106\n",
      "Iteration 26700 | Loss: 15.144500\n",
      "Iteration 26725 | Loss: 15.128910\n",
      "Iteration 26750 | Loss: 15.113337\n",
      "Iteration 26775 | Loss: 15.097780\n",
      "Iteration 26800 | Loss: 15.082239\n",
      "Iteration 26825 | Loss: 15.066715\n",
      "Iteration 26850 | Loss: 15.051207\n",
      "Iteration 26875 | Loss: 15.035715\n",
      "Iteration 26900 | Loss: 15.020240\n",
      "Iteration 26925 | Loss: 15.004780\n",
      "Iteration 26950 | Loss: 14.989337\n",
      "Iteration 26975 | Loss: 14.973910\n",
      "Iteration 27000 | Loss: 14.958499\n",
      "Iteration 27025 | Loss: 14.943105\n",
      "Iteration 27050 | Loss: 14.927726\n",
      "Iteration 27075 | Loss: 14.912364\n",
      "Iteration 27100 | Loss: 14.897018\n",
      "Iteration 27125 | Loss: 14.881688\n",
      "Iteration 27150 | Loss: 14.866374\n",
      "Iteration 27175 | Loss: 14.851076\n",
      "Iteration 27200 | Loss: 14.835794\n",
      "Iteration 27225 | Loss: 14.820528\n",
      "Iteration 27250 | Loss: 14.805278\n",
      "Iteration 27275 | Loss: 14.790044\n",
      "Iteration 27300 | Loss: 14.774826\n",
      "Iteration 27325 | Loss: 14.759624\n",
      "Iteration 27350 | Loss: 14.744438\n",
      "Iteration 27375 | Loss: 14.729268\n",
      "Iteration 27400 | Loss: 14.714114\n",
      "Iteration 27425 | Loss: 14.698975\n",
      "Iteration 27450 | Loss: 14.683853\n",
      "Iteration 27475 | Loss: 14.668746\n",
      "Iteration 27500 | Loss: 14.653655\n",
      "Iteration 27525 | Loss: 14.638580\n",
      "Iteration 27550 | Loss: 14.623521\n",
      "Iteration 27575 | Loss: 14.608478\n",
      "Iteration 27600 | Loss: 14.593450\n",
      "Iteration 27625 | Loss: 14.578439\n",
      "Iteration 27650 | Loss: 14.563442\n",
      "Iteration 27675 | Loss: 14.548462\n",
      "Iteration 27700 | Loss: 14.533497\n",
      "Iteration 27725 | Loss: 14.518549\n",
      "Iteration 27750 | Loss: 14.503615\n",
      "Iteration 27775 | Loss: 14.488698\n",
      "Iteration 27800 | Loss: 14.473796\n",
      "Iteration 27825 | Loss: 14.458909\n",
      "Iteration 27850 | Loss: 14.444039\n",
      "Iteration 27875 | Loss: 14.429183\n",
      "Iteration 27900 | Loss: 14.414344\n",
      "Iteration 27925 | Loss: 14.399520\n",
      "Iteration 27950 | Loss: 14.384711\n",
      "Iteration 27975 | Loss: 14.369918\n",
      "Iteration 28000 | Loss: 14.355141\n",
      "Iteration 28025 | Loss: 14.340379\n",
      "Iteration 28050 | Loss: 14.325632\n",
      "Iteration 28075 | Loss: 14.310901\n",
      "Iteration 28100 | Loss: 14.296186\n",
      "Iteration 28125 | Loss: 14.281486\n",
      "Iteration 28150 | Loss: 14.266801\n",
      "Iteration 28175 | Loss: 14.252132\n",
      "Iteration 28200 | Loss: 14.237478\n",
      "Iteration 28225 | Loss: 14.222839\n",
      "Iteration 28250 | Loss: 14.208216\n",
      "Iteration 28275 | Loss: 14.193608\n",
      "Iteration 28300 | Loss: 14.179016\n",
      "Iteration 28325 | Loss: 14.164438\n",
      "Iteration 28350 | Loss: 14.149876\n",
      "Iteration 28375 | Loss: 14.135330\n",
      "Iteration 28400 | Loss: 14.120798\n",
      "Iteration 28425 | Loss: 14.106282\n",
      "Iteration 28450 | Loss: 14.091781\n",
      "Iteration 28475 | Loss: 14.077295\n",
      "Iteration 28500 | Loss: 14.062824\n",
      "Iteration 28525 | Loss: 14.048369\n",
      "Iteration 28550 | Loss: 14.033929\n",
      "Iteration 28575 | Loss: 14.019503\n",
      "Iteration 28600 | Loss: 14.005093\n",
      "Iteration 28625 | Loss: 13.990698\n",
      "Iteration 28650 | Loss: 13.976319\n",
      "Iteration 28675 | Loss: 13.961954\n",
      "Iteration 28700 | Loss: 13.947604\n",
      "Iteration 28725 | Loss: 13.933270\n",
      "Iteration 28750 | Loss: 13.918950\n",
      "Iteration 28775 | Loss: 13.904645\n",
      "Iteration 28800 | Loss: 13.890356\n",
      "Iteration 28825 | Loss: 13.876081\n",
      "Iteration 28850 | Loss: 13.861821\n",
      "Iteration 28875 | Loss: 13.847577\n",
      "Iteration 28900 | Loss: 13.833347\n",
      "Iteration 28925 | Loss: 13.819132\n",
      "Iteration 28950 | Loss: 13.804932\n",
      "Iteration 28975 | Loss: 13.790747\n",
      "Iteration 29000 | Loss: 13.776577\n",
      "Iteration 29025 | Loss: 13.762422\n",
      "Iteration 29050 | Loss: 13.748281\n",
      "Iteration 29075 | Loss: 13.734156\n",
      "Iteration 29100 | Loss: 13.720045\n",
      "Iteration 29125 | Loss: 13.705949\n",
      "Iteration 29150 | Loss: 13.691867\n",
      "Iteration 29175 | Loss: 13.677801\n",
      "Iteration 29200 | Loss: 13.663749\n",
      "Iteration 29225 | Loss: 13.649712\n",
      "Iteration 29250 | Loss: 13.635690\n",
      "Iteration 29275 | Loss: 13.621682\n",
      "Iteration 29300 | Loss: 13.607689\n",
      "Iteration 29325 | Loss: 13.593711\n",
      "Iteration 29350 | Loss: 13.579747\n",
      "Iteration 29375 | Loss: 13.565798\n",
      "Iteration 29400 | Loss: 13.551864\n",
      "Iteration 29425 | Loss: 13.537944\n",
      "Iteration 29450 | Loss: 13.524039\n",
      "Iteration 29475 | Loss: 13.510149\n",
      "Iteration 29500 | Loss: 13.496273\n",
      "Iteration 29525 | Loss: 13.482411\n",
      "Iteration 29550 | Loss: 13.468564\n",
      "Iteration 29575 | Loss: 13.454732\n",
      "Iteration 29600 | Loss: 13.440914\n",
      "Iteration 29625 | Loss: 13.427111\n",
      "Iteration 29650 | Loss: 13.413322\n",
      "Iteration 29675 | Loss: 13.399547\n",
      "Iteration 29700 | Loss: 13.385787\n",
      "Iteration 29725 | Loss: 13.372042\n",
      "Iteration 29750 | Loss: 13.358311\n",
      "Iteration 29775 | Loss: 13.344594\n",
      "Iteration 29800 | Loss: 13.330891\n",
      "Iteration 29825 | Loss: 13.317203\n",
      "Iteration 29850 | Loss: 13.303530\n",
      "Iteration 29875 | Loss: 13.289870\n",
      "Iteration 29900 | Loss: 13.276225\n",
      "Iteration 29925 | Loss: 13.262595\n",
      "Iteration 29950 | Loss: 13.248978\n",
      "Iteration 29975 | Loss: 13.235376\n",
      "Iteration 30000 | Loss: 13.221788\n",
      "Iteration 30025 | Loss: 13.208214\n",
      "Iteration 30050 | Loss: 13.194655\n",
      "Iteration 30075 | Loss: 13.181110\n",
      "Iteration 30100 | Loss: 13.167579\n",
      "Iteration 30125 | Loss: 13.154062\n",
      "Iteration 30150 | Loss: 13.140559\n",
      "Iteration 30175 | Loss: 13.127071\n",
      "Iteration 30200 | Loss: 13.113597\n",
      "Iteration 30225 | Loss: 13.100136\n",
      "Iteration 30250 | Loss: 13.086690\n",
      "Iteration 30275 | Loss: 13.073258\n",
      "Iteration 30300 | Loss: 13.059840\n",
      "Iteration 30325 | Loss: 13.046436\n",
      "Iteration 30350 | Loss: 13.033047\n",
      "Iteration 30375 | Loss: 13.019671\n",
      "Iteration 30400 | Loss: 13.006309\n",
      "Iteration 30425 | Loss: 12.992961\n",
      "Iteration 30450 | Loss: 12.979628\n",
      "Iteration 30475 | Loss: 12.966308\n",
      "Iteration 30500 | Loss: 12.953002\n",
      "Iteration 30525 | Loss: 12.939710\n",
      "Iteration 30550 | Loss: 12.926432\n",
      "Iteration 30575 | Loss: 12.913168\n",
      "Iteration 30600 | Loss: 12.899918\n",
      "Iteration 30625 | Loss: 12.886682\n",
      "Iteration 30650 | Loss: 12.873460\n",
      "Iteration 30675 | Loss: 12.860251\n",
      "Iteration 30700 | Loss: 12.847057\n",
      "Iteration 30725 | Loss: 12.833876\n",
      "Iteration 30750 | Loss: 12.820709\n",
      "Iteration 30775 | Loss: 12.807556\n",
      "Iteration 30800 | Loss: 12.794416\n",
      "Iteration 30825 | Loss: 12.781291\n",
      "Iteration 30850 | Loss: 12.768179\n",
      "Iteration 30875 | Loss: 12.755081\n",
      "Iteration 30900 | Loss: 12.741997\n",
      "Iteration 30925 | Loss: 12.728926\n",
      "Iteration 30950 | Loss: 12.715869\n",
      "Iteration 30975 | Loss: 12.702826\n",
      "Iteration 31000 | Loss: 12.689796\n",
      "Iteration 31025 | Loss: 12.676780\n",
      "Iteration 31050 | Loss: 12.663778\n",
      "Iteration 31075 | Loss: 12.650790\n",
      "Iteration 31100 | Loss: 12.637815\n",
      "Iteration 31125 | Loss: 12.624853\n",
      "Iteration 31150 | Loss: 12.611905\n",
      "Iteration 31175 | Loss: 12.598971\n",
      "Iteration 31200 | Loss: 12.586050\n",
      "Iteration 31225 | Loss: 12.573143\n",
      "Iteration 31250 | Loss: 12.560250\n",
      "Iteration 31275 | Loss: 12.547370\n",
      "Iteration 31300 | Loss: 12.534503\n",
      "Iteration 31325 | Loss: 12.521650\n",
      "Iteration 31350 | Loss: 12.508810\n",
      "Iteration 31375 | Loss: 12.495984\n",
      "Iteration 31400 | Loss: 12.483171\n",
      "Iteration 31425 | Loss: 12.470372\n",
      "Iteration 31450 | Loss: 12.457586\n",
      "Iteration 31475 | Loss: 12.444814\n",
      "Iteration 31500 | Loss: 12.432055\n",
      "Iteration 31525 | Loss: 12.419309\n",
      "Iteration 31550 | Loss: 12.406577\n",
      "Iteration 31575 | Loss: 12.393858\n",
      "Iteration 31600 | Loss: 12.381152\n",
      "Iteration 31625 | Loss: 12.368460\n",
      "Iteration 31650 | Loss: 12.355781\n",
      "Iteration 31675 | Loss: 12.343115\n",
      "Iteration 31700 | Loss: 12.330463\n",
      "Iteration 31725 | Loss: 12.317824\n",
      "Iteration 31750 | Loss: 12.305198\n",
      "Iteration 31775 | Loss: 12.292585\n",
      "Iteration 31800 | Loss: 12.279985\n",
      "Iteration 31825 | Loss: 12.267399\n",
      "Iteration 31850 | Loss: 12.254826\n",
      "Iteration 31875 | Loss: 12.242266\n",
      "Iteration 31900 | Loss: 12.229720\n",
      "Iteration 31925 | Loss: 12.217186\n",
      "Iteration 31950 | Loss: 12.204666\n",
      "Iteration 31975 | Loss: 12.192158\n",
      "Iteration 32000 | Loss: 12.179664\n",
      "Iteration 32025 | Loss: 12.167183\n",
      "Iteration 32050 | Loss: 12.154715\n",
      "Iteration 32075 | Loss: 12.142260\n",
      "Iteration 32100 | Loss: 12.129818\n",
      "Iteration 32125 | Loss: 12.117389\n",
      "Iteration 32150 | Loss: 12.104974\n",
      "Iteration 32175 | Loss: 12.092571\n",
      "Iteration 32200 | Loss: 12.080181\n",
      "Iteration 32225 | Loss: 12.067804\n",
      "Iteration 32250 | Loss: 12.055440\n",
      "Iteration 32275 | Loss: 12.043090\n",
      "Iteration 32300 | Loss: 12.030752\n",
      "Iteration 32325 | Loss: 12.018427\n",
      "Iteration 32350 | Loss: 12.006115\n",
      "Iteration 32375 | Loss: 11.993816\n",
      "Iteration 32400 | Loss: 11.981529\n",
      "Iteration 32425 | Loss: 11.969256\n",
      "Iteration 32450 | Loss: 11.956995\n",
      "Iteration 32475 | Loss: 11.944748\n",
      "Iteration 32500 | Loss: 11.932513\n",
      "Iteration 32525 | Loss: 11.920291\n",
      "Iteration 32550 | Loss: 11.908082\n",
      "Iteration 32575 | Loss: 11.895885\n",
      "Iteration 32600 | Loss: 11.883702\n",
      "Iteration 32625 | Loss: 11.871531\n",
      "Iteration 32650 | Loss: 11.859373\n",
      "Iteration 32675 | Loss: 11.847228\n",
      "Iteration 32700 | Loss: 11.835095\n",
      "Iteration 32725 | Loss: 11.822975\n",
      "Iteration 32750 | Loss: 11.810868\n",
      "Iteration 32775 | Loss: 11.798774\n",
      "Iteration 32800 | Loss: 11.786692\n",
      "Iteration 32825 | Loss: 11.774623\n",
      "Iteration 32850 | Loss: 11.762566\n",
      "Iteration 32875 | Loss: 11.750523\n",
      "Iteration 32900 | Loss: 11.738491\n",
      "Iteration 32925 | Loss: 11.726473\n",
      "Iteration 32950 | Loss: 11.714467\n",
      "Iteration 32975 | Loss: 11.702473\n",
      "Iteration 33000 | Loss: 11.690493\n",
      "Iteration 33025 | Loss: 11.678524\n",
      "Iteration 33050 | Loss: 11.666569\n",
      "Iteration 33075 | Loss: 11.654625\n",
      "Iteration 33100 | Loss: 11.642695\n",
      "Iteration 33125 | Loss: 11.630777\n",
      "Iteration 33150 | Loss: 11.618871\n",
      "Iteration 33175 | Loss: 11.606978\n",
      "Iteration 33200 | Loss: 11.595097\n",
      "Iteration 33225 | Loss: 11.583229\n",
      "Iteration 33250 | Loss: 11.571373\n",
      "Iteration 33275 | Loss: 11.559530\n",
      "Iteration 33300 | Loss: 11.547699\n",
      "Iteration 33325 | Loss: 11.535880\n",
      "Iteration 33350 | Loss: 11.524074\n",
      "Iteration 33375 | Loss: 11.512280\n",
      "Iteration 33400 | Loss: 11.500499\n",
      "Iteration 33425 | Loss: 11.488730\n",
      "Iteration 33450 | Loss: 11.476973\n",
      "Iteration 33475 | Loss: 11.465228\n",
      "Iteration 33500 | Loss: 11.453496\n",
      "Iteration 33525 | Loss: 11.441777\n",
      "Iteration 33550 | Loss: 11.430069\n",
      "Iteration 33575 | Loss: 11.418374\n",
      "Iteration 33600 | Loss: 11.406691\n",
      "Iteration 33625 | Loss: 11.395020\n",
      "Iteration 33650 | Loss: 11.383362\n",
      "Iteration 33675 | Loss: 11.371715\n",
      "Iteration 33700 | Loss: 11.360081\n",
      "Iteration 33725 | Loss: 11.348459\n",
      "Iteration 33750 | Loss: 11.336850\n",
      "Iteration 33775 | Loss: 11.325252\n",
      "Iteration 33800 | Loss: 11.313667\n",
      "Iteration 33825 | Loss: 11.302093\n",
      "Iteration 33850 | Loss: 11.290532\n",
      "Iteration 33875 | Loss: 11.278983\n",
      "Iteration 33900 | Loss: 11.267447\n",
      "Iteration 33925 | Loss: 11.255922\n",
      "Iteration 33950 | Loss: 11.244409\n",
      "Iteration 33975 | Loss: 11.232908\n",
      "Iteration 34000 | Loss: 11.221420\n",
      "Iteration 34025 | Loss: 11.209943\n",
      "Iteration 34050 | Loss: 11.198479\n",
      "Iteration 34075 | Loss: 11.187026\n",
      "Iteration 34100 | Loss: 11.175586\n",
      "Iteration 34125 | Loss: 11.164157\n",
      "Iteration 34150 | Loss: 11.152741\n",
      "Iteration 34175 | Loss: 11.141336\n",
      "Iteration 34200 | Loss: 11.129944\n",
      "Iteration 34225 | Loss: 11.118563\n",
      "Iteration 34250 | Loss: 11.107194\n",
      "Iteration 34275 | Loss: 11.095838\n",
      "Iteration 34300 | Loss: 11.084493\n",
      "Iteration 34325 | Loss: 11.073160\n",
      "Iteration 34350 | Loss: 11.061839\n",
      "Iteration 34375 | Loss: 11.050530\n",
      "Iteration 34400 | Loss: 11.039232\n",
      "Iteration 34425 | Loss: 11.027947\n",
      "Iteration 34450 | Loss: 11.016673\n",
      "Iteration 34475 | Loss: 11.005411\n",
      "Iteration 34500 | Loss: 10.994161\n",
      "Iteration 34525 | Loss: 10.982923\n",
      "Iteration 34550 | Loss: 10.971696\n",
      "Iteration 34575 | Loss: 10.960481\n",
      "Iteration 34600 | Loss: 10.949278\n",
      "Iteration 34625 | Loss: 10.938087\n",
      "Iteration 34650 | Loss: 10.926908\n",
      "Iteration 34675 | Loss: 10.915740\n",
      "Iteration 34700 | Loss: 10.904584\n",
      "Iteration 34725 | Loss: 10.893439\n",
      "Iteration 34750 | Loss: 10.882307\n",
      "Iteration 34775 | Loss: 10.871186\n",
      "Iteration 34800 | Loss: 10.860076\n",
      "Iteration 34825 | Loss: 10.848978\n",
      "Iteration 34850 | Loss: 10.837892\n",
      "Iteration 34875 | Loss: 10.826818\n",
      "Iteration 34900 | Loss: 10.815755\n",
      "Iteration 34925 | Loss: 10.804704\n",
      "Iteration 34950 | Loss: 10.793664\n",
      "Iteration 34975 | Loss: 10.782636\n",
      "Iteration 35000 | Loss: 10.771619\n",
      "Iteration 35025 | Loss: 10.760614\n",
      "Iteration 35050 | Loss: 10.749621\n",
      "Iteration 35075 | Loss: 10.738639\n",
      "Iteration 35100 | Loss: 10.727668\n",
      "Iteration 35125 | Loss: 10.716710\n",
      "Iteration 35150 | Loss: 10.705762\n",
      "Iteration 35175 | Loss: 10.694826\n",
      "Iteration 35200 | Loss: 10.683902\n",
      "Iteration 35225 | Loss: 10.672989\n",
      "Iteration 35250 | Loss: 10.662087\n",
      "Iteration 35275 | Loss: 10.651197\n",
      "Iteration 35300 | Loss: 10.640318\n",
      "Iteration 35325 | Loss: 10.629451\n",
      "Iteration 35350 | Loss: 10.618595\n",
      "Iteration 35375 | Loss: 10.607750\n",
      "Iteration 35400 | Loss: 10.596917\n",
      "Iteration 35425 | Loss: 10.586095\n",
      "Iteration 35450 | Loss: 10.575284\n",
      "Iteration 35475 | Loss: 10.564485\n",
      "Iteration 35500 | Loss: 10.553697\n",
      "Iteration 35525 | Loss: 10.542921\n",
      "Iteration 35550 | Loss: 10.532155\n",
      "Iteration 35575 | Loss: 10.521401\n",
      "Iteration 35600 | Loss: 10.510659\n",
      "Iteration 35625 | Loss: 10.499927\n",
      "Iteration 35650 | Loss: 10.489207\n",
      "Iteration 35675 | Loss: 10.478498\n",
      "Iteration 35700 | Loss: 10.467800\n",
      "Iteration 35725 | Loss: 10.457114\n",
      "Iteration 35750 | Loss: 10.446438\n",
      "Iteration 35775 | Loss: 10.435774\n",
      "Iteration 35800 | Loss: 10.425121\n",
      "Iteration 35825 | Loss: 10.414480\n",
      "Iteration 35850 | Loss: 10.403849\n",
      "Iteration 35875 | Loss: 10.393229\n",
      "Iteration 35900 | Loss: 10.382621\n",
      "Iteration 35925 | Loss: 10.372024\n",
      "Iteration 35950 | Loss: 10.361438\n",
      "Iteration 35975 | Loss: 10.350863\n",
      "Iteration 36000 | Loss: 10.340299\n",
      "Iteration 36025 | Loss: 10.329746\n",
      "Iteration 36050 | Loss: 10.319204\n",
      "Iteration 36075 | Loss: 10.308673\n",
      "Iteration 36100 | Loss: 10.298153\n",
      "Iteration 36125 | Loss: 10.287645\n",
      "Iteration 36150 | Loss: 10.277147\n",
      "Iteration 36175 | Loss: 10.266660\n",
      "Iteration 36200 | Loss: 10.256185\n",
      "Iteration 36225 | Loss: 10.245720\n",
      "Iteration 36250 | Loss: 10.235266\n",
      "Iteration 36275 | Loss: 10.224823\n",
      "Iteration 36300 | Loss: 10.214392\n",
      "Iteration 36325 | Loss: 10.203971\n",
      "Iteration 36350 | Loss: 10.193561\n",
      "Iteration 36375 | Loss: 10.183162\n",
      "Iteration 36400 | Loss: 10.172773\n",
      "Iteration 36425 | Loss: 10.162396\n",
      "Iteration 36450 | Loss: 10.152030\n",
      "Iteration 36475 | Loss: 10.141674\n",
      "Iteration 36500 | Loss: 10.131329\n",
      "Iteration 36525 | Loss: 10.120996\n",
      "Iteration 36550 | Loss: 10.110673\n",
      "Iteration 36575 | Loss: 10.100360\n",
      "Iteration 36600 | Loss: 10.090059\n",
      "Iteration 36625 | Loss: 10.079768\n",
      "Iteration 36650 | Loss: 10.069489\n",
      "Iteration 36675 | Loss: 10.059220\n",
      "Iteration 36700 | Loss: 10.048961\n",
      "Iteration 36725 | Loss: 10.038714\n",
      "Iteration 36750 | Loss: 10.028477\n",
      "Iteration 36775 | Loss: 10.018251\n",
      "Iteration 36800 | Loss: 10.008036\n",
      "Iteration 36825 | Loss: 9.997831\n",
      "Iteration 36850 | Loss: 9.987637\n",
      "Iteration 36875 | Loss: 9.977454\n",
      "Iteration 36900 | Loss: 9.967281\n",
      "Iteration 36925 | Loss: 9.957120\n",
      "Iteration 36950 | Loss: 9.946968\n",
      "Iteration 36975 | Loss: 9.936828\n",
      "Iteration 37000 | Loss: 9.926698\n",
      "Iteration 37025 | Loss: 9.916578\n",
      "Iteration 37050 | Loss: 9.906470\n",
      "Iteration 37075 | Loss: 9.896372\n",
      "Iteration 37100 | Loss: 9.886284\n",
      "Iteration 37125 | Loss: 9.876207\n",
      "Iteration 37150 | Loss: 9.866141\n",
      "Iteration 37175 | Loss: 9.856085\n",
      "Iteration 37200 | Loss: 9.846039\n",
      "Iteration 37225 | Loss: 9.836005\n",
      "Iteration 37250 | Loss: 9.825980\n",
      "Iteration 37275 | Loss: 9.815966\n",
      "Iteration 37300 | Loss: 9.805963\n",
      "Iteration 37325 | Loss: 9.795970\n",
      "Iteration 37350 | Loss: 9.785988\n",
      "Iteration 37375 | Loss: 9.776016\n",
      "Iteration 37400 | Loss: 9.766055\n",
      "Iteration 37425 | Loss: 9.756104\n",
      "Iteration 37450 | Loss: 9.746163\n",
      "Iteration 37475 | Loss: 9.736233\n",
      "Iteration 37500 | Loss: 9.726313\n",
      "Iteration 37525 | Loss: 9.716404\n",
      "Iteration 37550 | Loss: 9.706505\n",
      "Iteration 37575 | Loss: 9.696617\n",
      "Iteration 37600 | Loss: 9.686738\n",
      "Iteration 37625 | Loss: 9.676871\n",
      "Iteration 37650 | Loss: 9.667013\n",
      "Iteration 37675 | Loss: 9.657166\n",
      "Iteration 37700 | Loss: 9.647329\n",
      "Iteration 37725 | Loss: 9.637503\n",
      "Iteration 37750 | Loss: 9.627686\n",
      "Iteration 37775 | Loss: 9.617880\n",
      "Iteration 37800 | Loss: 9.608085\n",
      "Iteration 37825 | Loss: 9.598299\n",
      "Iteration 37850 | Loss: 9.588524\n",
      "Iteration 37875 | Loss: 9.578759\n",
      "Iteration 37900 | Loss: 9.569005\n",
      "Iteration 37925 | Loss: 9.559260\n",
      "Iteration 37950 | Loss: 9.549526\n",
      "Iteration 37975 | Loss: 9.539802\n",
      "Iteration 38000 | Loss: 9.530088\n",
      "Iteration 38025 | Loss: 9.520385\n",
      "Iteration 38050 | Loss: 9.510691\n",
      "Iteration 38075 | Loss: 9.501008\n",
      "Iteration 38100 | Loss: 9.491335\n",
      "Iteration 38125 | Loss: 9.481672\n",
      "Iteration 38150 | Loss: 9.472019\n",
      "Iteration 38175 | Loss: 9.462376\n",
      "Iteration 38200 | Loss: 9.452743\n",
      "Iteration 38225 | Loss: 9.443121\n",
      "Iteration 38250 | Loss: 9.433508\n",
      "Iteration 38275 | Loss: 9.423906\n",
      "Iteration 38300 | Loss: 9.414314\n",
      "Iteration 38325 | Loss: 9.404731\n",
      "Iteration 38350 | Loss: 9.395159\n",
      "Iteration 38375 | Loss: 9.385597\n",
      "Iteration 38400 | Loss: 9.376045\n",
      "Iteration 38425 | Loss: 9.366502\n",
      "Iteration 38450 | Loss: 9.356970\n",
      "Iteration 38475 | Loss: 9.347448\n",
      "Iteration 38500 | Loss: 9.337936\n",
      "Iteration 38525 | Loss: 9.328434\n",
      "Iteration 38550 | Loss: 9.318941\n",
      "Iteration 38575 | Loss: 9.309459\n",
      "Iteration 38600 | Loss: 9.299987\n",
      "Iteration 38625 | Loss: 9.290524\n",
      "Iteration 38650 | Loss: 9.281072\n",
      "Iteration 38675 | Loss: 9.271629\n",
      "Iteration 38700 | Loss: 9.262196\n",
      "Iteration 38725 | Loss: 9.252773\n",
      "Iteration 38750 | Loss: 9.243360\n",
      "Iteration 38775 | Loss: 9.233957\n",
      "Iteration 38800 | Loss: 9.224564\n",
      "Iteration 38825 | Loss: 9.215181\n",
      "Iteration 38850 | Loss: 9.205807\n",
      "Iteration 38875 | Loss: 9.196443\n",
      "Iteration 38900 | Loss: 9.187089\n",
      "Iteration 38925 | Loss: 9.177745\n",
      "Iteration 38950 | Loss: 9.168411\n",
      "Iteration 38975 | Loss: 9.159086\n",
      "Iteration 39000 | Loss: 9.149771\n",
      "Iteration 39025 | Loss: 9.140466\n",
      "Iteration 39050 | Loss: 9.131171\n",
      "Iteration 39075 | Loss: 9.121886\n",
      "Iteration 39100 | Loss: 9.112610\n",
      "Iteration 39125 | Loss: 9.103344\n",
      "Iteration 39150 | Loss: 9.094087\n",
      "Iteration 39175 | Loss: 9.084841\n",
      "Iteration 39200 | Loss: 9.075604\n",
      "Iteration 39225 | Loss: 9.066377\n",
      "Iteration 39250 | Loss: 9.057159\n",
      "Iteration 39275 | Loss: 9.047951\n",
      "Iteration 39300 | Loss: 9.038753\n",
      "Iteration 39325 | Loss: 9.029564\n",
      "Iteration 39350 | Loss: 9.020385\n",
      "Iteration 39375 | Loss: 9.011216\n",
      "Iteration 39400 | Loss: 9.002056\n",
      "Iteration 39425 | Loss: 8.992906\n",
      "Iteration 39450 | Loss: 8.983765\n",
      "Iteration 39475 | Loss: 8.974634\n",
      "Iteration 39500 | Loss: 8.965512\n",
      "Iteration 39525 | Loss: 8.956401\n",
      "Iteration 39550 | Loss: 8.947298\n",
      "Iteration 39575 | Loss: 8.938205\n",
      "Iteration 39600 | Loss: 8.929122\n",
      "Iteration 39625 | Loss: 8.920048\n",
      "Iteration 39650 | Loss: 8.910984\n",
      "Iteration 39675 | Loss: 8.901929\n",
      "Iteration 39700 | Loss: 8.892884\n",
      "Iteration 39725 | Loss: 8.883848\n",
      "Iteration 39750 | Loss: 8.874822\n",
      "Iteration 39775 | Loss: 8.865805\n",
      "Iteration 39800 | Loss: 8.856798\n",
      "Iteration 39825 | Loss: 8.847800\n",
      "Iteration 39850 | Loss: 8.838811\n",
      "Iteration 39875 | Loss: 8.829832\n",
      "Iteration 39900 | Loss: 8.820862\n",
      "Iteration 39925 | Loss: 8.811902\n",
      "Iteration 39950 | Loss: 8.802951\n",
      "Iteration 39975 | Loss: 8.794010\n",
      "Iteration 40000 | Loss: 8.785078\n",
      "Iteration 40025 | Loss: 8.776155\n",
      "Iteration 40050 | Loss: 8.767241\n",
      "Iteration 40075 | Loss: 8.758337\n",
      "Iteration 40100 | Loss: 8.749442\n",
      "Iteration 40125 | Loss: 8.740557\n",
      "Iteration 40150 | Loss: 8.731681\n",
      "Iteration 40175 | Loss: 8.722814\n",
      "Iteration 40200 | Loss: 8.713956\n",
      "Iteration 40225 | Loss: 8.705108\n",
      "Iteration 40250 | Loss: 8.696269\n",
      "Iteration 40275 | Loss: 8.687439\n",
      "Iteration 40300 | Loss: 8.678619\n",
      "Iteration 40325 | Loss: 8.669808\n",
      "Iteration 40350 | Loss: 8.661006\n",
      "Iteration 40375 | Loss: 8.652213\n",
      "Iteration 40400 | Loss: 8.643430\n",
      "Iteration 40425 | Loss: 8.634655\n",
      "Iteration 40450 | Loss: 8.625890\n",
      "Iteration 40475 | Loss: 8.617134\n",
      "Iteration 40500 | Loss: 8.608387\n",
      "Iteration 40525 | Loss: 8.599650\n",
      "Iteration 40550 | Loss: 8.590921\n",
      "Iteration 40575 | Loss: 8.582202\n",
      "Iteration 40600 | Loss: 8.573492\n",
      "Iteration 40625 | Loss: 8.564791\n",
      "Iteration 40650 | Loss: 8.556099\n",
      "Iteration 40675 | Loss: 8.547416\n",
      "Iteration 40700 | Loss: 8.538742\n",
      "Iteration 40725 | Loss: 8.530078\n",
      "Iteration 40750 | Loss: 8.521422\n",
      "Iteration 40775 | Loss: 8.512776\n",
      "Iteration 40800 | Loss: 8.504138\n",
      "Iteration 40825 | Loss: 8.495510\n",
      "Iteration 40850 | Loss: 8.486891\n",
      "Iteration 40875 | Loss: 8.478280\n",
      "Iteration 40900 | Loss: 8.469679\n",
      "Iteration 40925 | Loss: 8.461087\n",
      "Iteration 40950 | Loss: 8.452504\n",
      "Iteration 40975 | Loss: 8.443929\n",
      "Iteration 41000 | Loss: 8.435364\n",
      "Iteration 41025 | Loss: 8.426808\n",
      "Iteration 41050 | Loss: 8.418260\n",
      "Iteration 41075 | Loss: 8.409722\n",
      "Iteration 41100 | Loss: 8.401193\n",
      "Iteration 41125 | Loss: 8.392672\n",
      "Iteration 41150 | Loss: 8.384161\n",
      "Iteration 41175 | Loss: 8.375658\n",
      "Iteration 41200 | Loss: 8.367164\n",
      "Iteration 41225 | Loss: 8.358680\n",
      "Iteration 41250 | Loss: 8.350204\n",
      "Iteration 41275 | Loss: 8.341737\n",
      "Iteration 41300 | Loss: 8.333278\n",
      "Iteration 41325 | Loss: 8.324829\n",
      "Iteration 41350 | Loss: 8.316389\n",
      "Iteration 41375 | Loss: 8.307957\n",
      "Iteration 41400 | Loss: 8.299534\n",
      "Iteration 41425 | Loss: 8.291120\n",
      "Iteration 41450 | Loss: 8.282715\n",
      "Iteration 41475 | Loss: 8.274319\n",
      "Iteration 41500 | Loss: 8.265932\n",
      "Iteration 41525 | Loss: 8.257553\n",
      "Iteration 41550 | Loss: 8.249183\n",
      "Iteration 41575 | Loss: 8.240822\n",
      "Iteration 41600 | Loss: 8.232469\n",
      "Iteration 41625 | Loss: 8.224126\n",
      "Iteration 41650 | Loss: 8.215791\n",
      "Iteration 41675 | Loss: 8.207465\n",
      "Iteration 41700 | Loss: 8.199147\n",
      "Iteration 41725 | Loss: 8.190839\n",
      "Iteration 41750 | Loss: 8.182538\n",
      "Iteration 41775 | Loss: 8.174247\n",
      "Iteration 41800 | Loss: 8.165965\n",
      "Iteration 41825 | Loss: 8.157691\n",
      "Iteration 41850 | Loss: 8.149425\n",
      "Iteration 41875 | Loss: 8.141169\n",
      "Iteration 41900 | Loss: 8.132921\n",
      "Iteration 41925 | Loss: 8.124681\n",
      "Iteration 41950 | Loss: 8.116451\n",
      "Iteration 41975 | Loss: 8.108229\n",
      "Iteration 42000 | Loss: 8.100015\n",
      "Iteration 42025 | Loss: 8.091810\n",
      "Iteration 42050 | Loss: 8.083614\n",
      "Iteration 42075 | Loss: 8.075427\n",
      "Iteration 42100 | Loss: 8.067247\n",
      "Iteration 42125 | Loss: 8.059077\n",
      "Iteration 42150 | Loss: 8.050915\n",
      "Iteration 42175 | Loss: 8.042762\n",
      "Iteration 42200 | Loss: 8.034617\n",
      "Iteration 42225 | Loss: 8.026481\n",
      "Iteration 42250 | Loss: 8.018353\n",
      "Iteration 42275 | Loss: 8.010234\n",
      "Iteration 42300 | Loss: 8.002123\n",
      "Iteration 42325 | Loss: 7.994021\n",
      "Iteration 42350 | Loss: 7.985927\n",
      "Iteration 42375 | Loss: 7.977842\n",
      "Iteration 42400 | Loss: 7.969765\n",
      "Iteration 42425 | Loss: 7.961696\n",
      "Iteration 42450 | Loss: 7.953637\n",
      "Iteration 42475 | Loss: 7.945585\n",
      "Iteration 42500 | Loss: 7.937542\n",
      "Iteration 42525 | Loss: 7.929508\n",
      "Iteration 42550 | Loss: 7.921481\n",
      "Iteration 42575 | Loss: 7.913464\n",
      "Iteration 42600 | Loss: 7.905454\n",
      "Iteration 42625 | Loss: 7.897453\n",
      "Iteration 42650 | Loss: 7.889461\n",
      "Iteration 42675 | Loss: 7.881477\n",
      "Iteration 42700 | Loss: 7.873501\n",
      "Iteration 42725 | Loss: 7.865533\n",
      "Iteration 42750 | Loss: 7.857574\n",
      "Iteration 42775 | Loss: 7.849624\n",
      "Iteration 42800 | Loss: 7.841681\n",
      "Iteration 42825 | Loss: 7.833747\n",
      "Iteration 42850 | Loss: 7.825821\n",
      "Iteration 42875 | Loss: 7.817904\n",
      "Iteration 42900 | Loss: 7.809995\n",
      "Iteration 42925 | Loss: 7.802094\n",
      "Iteration 42950 | Loss: 7.794201\n",
      "Iteration 42975 | Loss: 7.786317\n",
      "Iteration 43000 | Loss: 7.778441\n",
      "Iteration 43025 | Loss: 7.770573\n",
      "Iteration 43050 | Loss: 7.762713\n",
      "Iteration 43075 | Loss: 7.754862\n",
      "Iteration 43100 | Loss: 7.747019\n",
      "Iteration 43125 | Loss: 7.739184\n",
      "Iteration 43150 | Loss: 7.731357\n",
      "Iteration 43175 | Loss: 7.723539\n",
      "Iteration 43200 | Loss: 7.715728\n",
      "Iteration 43225 | Loss: 7.707926\n",
      "Iteration 43250 | Loss: 7.700132\n",
      "Iteration 43275 | Loss: 7.692346\n",
      "Iteration 43300 | Loss: 7.684569\n",
      "Iteration 43325 | Loss: 7.676799\n",
      "Iteration 43350 | Loss: 7.669038\n",
      "Iteration 43375 | Loss: 7.661285\n",
      "Iteration 43400 | Loss: 7.653540\n",
      "Iteration 43425 | Loss: 7.645803\n",
      "Iteration 43450 | Loss: 7.638074\n",
      "Iteration 43475 | Loss: 7.630353\n",
      "Iteration 43500 | Loss: 7.622641\n",
      "Iteration 43525 | Loss: 7.614936\n",
      "Iteration 43550 | Loss: 7.607240\n",
      "Iteration 43575 | Loss: 7.599551\n",
      "Iteration 43600 | Loss: 7.591871\n",
      "Iteration 43625 | Loss: 7.584198\n",
      "Iteration 43650 | Loss: 7.576534\n",
      "Iteration 43675 | Loss: 7.568878\n",
      "Iteration 43700 | Loss: 7.561230\n",
      "Iteration 43725 | Loss: 7.553589\n",
      "Iteration 43750 | Loss: 7.545957\n",
      "Iteration 43775 | Loss: 7.538333\n",
      "Iteration 43800 | Loss: 7.530717\n",
      "Iteration 43825 | Loss: 7.523109\n",
      "Iteration 43850 | Loss: 7.515508\n",
      "Iteration 43875 | Loss: 7.507916\n",
      "Iteration 43900 | Loss: 7.500332\n",
      "Iteration 43925 | Loss: 7.492755\n",
      "Iteration 43950 | Loss: 7.485187\n",
      "Iteration 43975 | Loss: 7.477626\n",
      "Iteration 44000 | Loss: 7.470074\n",
      "Iteration 44025 | Loss: 7.462529\n",
      "Iteration 44050 | Loss: 7.454992\n",
      "Iteration 44075 | Loss: 7.447463\n",
      "Iteration 44100 | Loss: 7.439942\n",
      "Iteration 44125 | Loss: 7.432429\n",
      "Iteration 44150 | Loss: 7.424924\n",
      "Iteration 44175 | Loss: 7.417427\n",
      "Iteration 44200 | Loss: 7.409937\n",
      "Iteration 44225 | Loss: 7.402455\n",
      "Iteration 44250 | Loss: 7.394982\n",
      "Iteration 44275 | Loss: 7.387516\n",
      "Iteration 44300 | Loss: 7.380057\n",
      "Iteration 44325 | Loss: 7.372607\n",
      "Iteration 44350 | Loss: 7.365165\n",
      "Iteration 44375 | Loss: 7.357730\n",
      "Iteration 44400 | Loss: 7.350303\n",
      "Iteration 44425 | Loss: 7.342884\n",
      "Iteration 44450 | Loss: 7.335472\n",
      "Iteration 44475 | Loss: 7.328069\n",
      "Iteration 44500 | Loss: 7.320673\n",
      "Iteration 44525 | Loss: 7.313285\n",
      "Iteration 44550 | Loss: 7.305904\n",
      "Iteration 44575 | Loss: 7.298532\n",
      "Iteration 44600 | Loss: 7.291167\n",
      "Iteration 44625 | Loss: 7.283809\n",
      "Iteration 44650 | Loss: 7.276460\n",
      "Iteration 44675 | Loss: 7.269118\n",
      "Iteration 44700 | Loss: 7.261784\n",
      "Iteration 44725 | Loss: 7.254458\n",
      "Iteration 44750 | Loss: 7.247139\n",
      "Iteration 44775 | Loss: 7.239828\n",
      "Iteration 44800 | Loss: 7.232524\n",
      "Iteration 44825 | Loss: 7.225229\n",
      "Iteration 44850 | Loss: 7.217940\n",
      "Iteration 44875 | Loss: 7.210660\n",
      "Iteration 44900 | Loss: 7.203387\n",
      "Iteration 44925 | Loss: 7.196122\n",
      "Iteration 44950 | Loss: 7.188864\n",
      "Iteration 44975 | Loss: 7.181614\n",
      "Iteration 45000 | Loss: 7.174372\n",
      "Iteration 45025 | Loss: 7.167137\n",
      "Iteration 45050 | Loss: 7.159910\n",
      "Iteration 45075 | Loss: 7.152690\n",
      "Iteration 45100 | Loss: 7.145478\n",
      "Iteration 45125 | Loss: 7.138273\n",
      "Iteration 45150 | Loss: 7.131076\n",
      "Iteration 45175 | Loss: 7.123887\n",
      "Iteration 45200 | Loss: 7.116705\n",
      "Iteration 45225 | Loss: 7.109531\n",
      "Iteration 45250 | Loss: 7.102364\n",
      "Iteration 45275 | Loss: 7.095204\n",
      "Iteration 45300 | Loss: 7.088053\n",
      "Iteration 45325 | Loss: 7.080908\n",
      "Iteration 45350 | Loss: 7.073771\n",
      "Iteration 45375 | Loss: 7.066642\n",
      "Iteration 45400 | Loss: 7.059520\n",
      "Iteration 45425 | Loss: 7.052405\n",
      "Iteration 45450 | Loss: 7.045298\n",
      "Iteration 45475 | Loss: 7.038199\n",
      "Iteration 45500 | Loss: 7.031107\n",
      "Iteration 45525 | Loss: 7.024022\n",
      "Iteration 45550 | Loss: 7.016945\n",
      "Iteration 45575 | Loss: 7.009875\n",
      "Iteration 45600 | Loss: 7.002812\n",
      "Iteration 45625 | Loss: 6.995757\n",
      "Iteration 45650 | Loss: 6.988710\n",
      "Iteration 45675 | Loss: 6.981669\n",
      "Iteration 45700 | Loss: 6.974636\n",
      "Iteration 45725 | Loss: 6.967611\n",
      "Iteration 45750 | Loss: 6.960593\n",
      "Iteration 45775 | Loss: 6.953582\n",
      "Iteration 45800 | Loss: 6.946578\n",
      "Iteration 45825 | Loss: 6.939582\n",
      "Iteration 45850 | Loss: 6.932594\n",
      "Iteration 45875 | Loss: 6.925612\n",
      "Iteration 45900 | Loss: 6.918638\n",
      "Iteration 45925 | Loss: 6.911671\n",
      "Iteration 45950 | Loss: 6.904711\n",
      "Iteration 45975 | Loss: 6.897759\n",
      "Iteration 46000 | Loss: 6.890814\n",
      "Iteration 46025 | Loss: 6.883877\n",
      "Iteration 46050 | Loss: 6.876946\n",
      "Iteration 46075 | Loss: 6.870023\n",
      "Iteration 46100 | Loss: 6.863107\n",
      "Iteration 46125 | Loss: 6.856198\n",
      "Iteration 46150 | Loss: 6.849297\n",
      "Iteration 46175 | Loss: 6.842403\n",
      "Iteration 46200 | Loss: 6.835516\n",
      "Iteration 46225 | Loss: 6.828636\n",
      "Iteration 46250 | Loss: 6.821763\n",
      "Iteration 46275 | Loss: 6.814898\n",
      "Iteration 46300 | Loss: 6.808040\n",
      "Iteration 46325 | Loss: 6.801189\n",
      "Iteration 46350 | Loss: 6.794345\n",
      "Iteration 46375 | Loss: 6.787509\n",
      "Iteration 46400 | Loss: 6.780679\n",
      "Iteration 46425 | Loss: 6.773857\n",
      "Iteration 46450 | Loss: 6.767042\n",
      "Iteration 46475 | Loss: 6.760234\n",
      "Iteration 46500 | Loss: 6.753433\n",
      "Iteration 46525 | Loss: 6.746639\n",
      "Iteration 46550 | Loss: 6.739852\n",
      "Iteration 46575 | Loss: 6.733073\n",
      "Iteration 46600 | Loss: 6.726300\n",
      "Iteration 46625 | Loss: 6.719535\n",
      "Iteration 46650 | Loss: 6.712777\n",
      "Iteration 46675 | Loss: 6.706026\n",
      "Iteration 46700 | Loss: 6.699282\n",
      "Iteration 46725 | Loss: 6.692545\n",
      "Iteration 46750 | Loss: 6.685815\n",
      "Iteration 46775 | Loss: 6.679092\n",
      "Iteration 46800 | Loss: 6.672376\n",
      "Iteration 46825 | Loss: 6.665667\n",
      "Iteration 46850 | Loss: 6.658965\n",
      "Iteration 46875 | Loss: 6.652270\n",
      "Iteration 46900 | Loss: 6.645583\n",
      "Iteration 46925 | Loss: 6.638902\n",
      "Iteration 46950 | Loss: 6.632228\n",
      "Iteration 46975 | Loss: 6.625561\n",
      "Iteration 47000 | Loss: 6.618902\n",
      "Iteration 47025 | Loss: 6.612249\n",
      "Iteration 47050 | Loss: 6.605603\n",
      "Iteration 47075 | Loss: 6.598964\n",
      "Iteration 47100 | Loss: 6.592332\n",
      "Iteration 47125 | Loss: 6.585707\n",
      "Iteration 47150 | Loss: 6.579089\n",
      "Iteration 47175 | Loss: 6.572478\n",
      "Iteration 47200 | Loss: 6.565874\n",
      "Iteration 47225 | Loss: 6.559277\n",
      "Iteration 47250 | Loss: 6.552687\n",
      "Iteration 47275 | Loss: 6.546103\n",
      "Iteration 47300 | Loss: 6.539527\n",
      "Iteration 47325 | Loss: 6.532957\n",
      "Iteration 47350 | Loss: 6.526394\n",
      "Iteration 47375 | Loss: 6.519838\n",
      "Iteration 47400 | Loss: 6.513289\n",
      "Iteration 47425 | Loss: 6.506747\n",
      "Iteration 47450 | Loss: 6.500212\n",
      "Iteration 47475 | Loss: 6.493684\n",
      "Iteration 47500 | Loss: 6.487162\n",
      "Iteration 47525 | Loss: 6.480647\n",
      "Iteration 47550 | Loss: 6.474139\n",
      "Iteration 47575 | Loss: 6.467638\n",
      "Iteration 47600 | Loss: 6.461144\n",
      "Iteration 47625 | Loss: 6.454656\n",
      "Iteration 47650 | Loss: 6.448176\n",
      "Iteration 47675 | Loss: 6.441702\n",
      "Iteration 47700 | Loss: 6.435235\n",
      "Iteration 47725 | Loss: 6.428774\n",
      "Iteration 47750 | Loss: 6.422321\n",
      "Iteration 47775 | Loss: 6.415874\n",
      "Iteration 47800 | Loss: 6.409434\n",
      "Iteration 47825 | Loss: 6.403001\n",
      "Iteration 47850 | Loss: 6.396574\n",
      "Iteration 47875 | Loss: 6.390154\n",
      "Iteration 47900 | Loss: 6.383741\n",
      "Iteration 47925 | Loss: 6.377335\n",
      "Iteration 47950 | Loss: 6.370935\n",
      "Iteration 47975 | Loss: 6.364542\n",
      "Iteration 48000 | Loss: 6.358156\n",
      "Iteration 48025 | Loss: 6.351776\n",
      "Iteration 48050 | Loss: 6.345403\n",
      "Iteration 48075 | Loss: 6.339037\n",
      "Iteration 48100 | Loss: 6.332677\n",
      "Iteration 48125 | Loss: 6.326324\n",
      "Iteration 48150 | Loss: 6.319978\n",
      "Iteration 48175 | Loss: 6.313639\n",
      "Iteration 48200 | Loss: 6.307306\n",
      "Iteration 48225 | Loss: 6.300979\n",
      "Iteration 48250 | Loss: 6.294660\n",
      "Iteration 48275 | Loss: 6.288347\n",
      "Iteration 48300 | Loss: 6.282040\n",
      "Iteration 48325 | Loss: 6.275740\n",
      "Iteration 48350 | Loss: 6.269447\n",
      "Iteration 48375 | Loss: 6.263160\n",
      "Iteration 48400 | Loss: 6.256880\n",
      "Iteration 48425 | Loss: 6.250607\n",
      "Iteration 48450 | Loss: 6.244340\n",
      "Iteration 48475 | Loss: 6.238080\n",
      "Iteration 48500 | Loss: 6.231826\n",
      "Iteration 48525 | Loss: 6.225579\n",
      "Iteration 48550 | Loss: 6.219338\n",
      "Iteration 48575 | Loss: 6.213104\n",
      "Iteration 48600 | Loss: 6.206876\n",
      "Iteration 48625 | Loss: 6.200655\n",
      "Iteration 48650 | Loss: 6.194441\n",
      "Iteration 48675 | Loss: 6.188232\n",
      "Iteration 48700 | Loss: 6.182031\n",
      "Iteration 48725 | Loss: 6.175836\n",
      "Iteration 48750 | Loss: 6.169647\n",
      "Iteration 48775 | Loss: 6.163465\n",
      "Iteration 48800 | Loss: 6.157290\n",
      "Iteration 48825 | Loss: 6.151120\n",
      "Iteration 48850 | Loss: 6.144958\n",
      "Iteration 48875 | Loss: 6.138802\n",
      "Iteration 48900 | Loss: 6.132652\n",
      "Iteration 48925 | Loss: 6.126508\n",
      "Iteration 48950 | Loss: 6.120372\n",
      "Iteration 48975 | Loss: 6.114241\n",
      "Iteration 49000 | Loss: 6.108117\n",
      "Iteration 49025 | Loss: 6.101999\n",
      "Iteration 49050 | Loss: 6.095888\n",
      "Iteration 49075 | Loss: 6.089783\n",
      "Iteration 49100 | Loss: 6.083685\n",
      "Iteration 49125 | Loss: 6.077593\n",
      "Iteration 49150 | Loss: 6.071507\n",
      "Iteration 49175 | Loss: 6.065428\n",
      "Iteration 49200 | Loss: 6.059355\n",
      "Iteration 49225 | Loss: 6.053289\n",
      "Iteration 49250 | Loss: 6.047228\n",
      "Iteration 49275 | Loss: 6.041175\n",
      "Iteration 49300 | Loss: 6.035127\n",
      "Iteration 49325 | Loss: 6.029086\n",
      "Iteration 49350 | Loss: 6.023051\n",
      "Iteration 49375 | Loss: 6.017023\n",
      "Iteration 49400 | Loss: 6.011001\n",
      "Iteration 49425 | Loss: 6.004985\n",
      "Iteration 49450 | Loss: 5.998975\n",
      "Iteration 49475 | Loss: 5.992972\n",
      "Iteration 49500 | Loss: 5.986975\n",
      "Iteration 49525 | Loss: 5.980984\n",
      "Iteration 49550 | Loss: 5.975000\n",
      "Iteration 49575 | Loss: 5.969022\n",
      "Iteration 49600 | Loss: 5.963050\n",
      "Iteration 49625 | Loss: 5.957084\n",
      "Iteration 49650 | Loss: 5.951125\n",
      "Iteration 49675 | Loss: 5.945171\n",
      "Iteration 49700 | Loss: 5.939225\n",
      "Iteration 49725 | Loss: 5.933284\n",
      "Iteration 49750 | Loss: 5.927349\n",
      "Iteration 49775 | Loss: 5.921421\n",
      "Iteration 49800 | Loss: 5.915499\n",
      "Iteration 49825 | Loss: 5.909583\n",
      "Iteration 49850 | Loss: 5.903674\n",
      "Iteration 49875 | Loss: 5.897770\n",
      "Iteration 49900 | Loss: 5.891873\n",
      "Iteration 49925 | Loss: 5.885982\n",
      "Iteration 49950 | Loss: 5.880097\n",
      "Iteration 49975 | Loss: 5.874218\n",
      "Iteration 50000 | Loss: 5.868346\n",
      "Iteration 50025 | Loss: 5.862479\n",
      "Iteration 50050 | Loss: 5.856619\n",
      "Iteration 50075 | Loss: 5.850765\n",
      "Iteration 50100 | Loss: 5.844917\n",
      "Iteration 50125 | Loss: 5.839075\n",
      "Iteration 50150 | Loss: 5.833239\n",
      "Iteration 50175 | Loss: 5.827410\n",
      "Iteration 50200 | Loss: 5.821586\n",
      "Iteration 50225 | Loss: 5.815769\n",
      "Iteration 50250 | Loss: 5.809957\n",
      "Iteration 50275 | Loss: 5.804152\n",
      "Iteration 50300 | Loss: 5.798353\n",
      "Iteration 50325 | Loss: 5.792560\n",
      "Iteration 50350 | Loss: 5.786773\n",
      "Iteration 50375 | Loss: 5.780992\n",
      "Iteration 50400 | Loss: 5.775217\n",
      "Iteration 50425 | Loss: 5.769448\n",
      "Iteration 50450 | Loss: 5.763685\n",
      "Iteration 50475 | Loss: 5.757929\n",
      "Iteration 50500 | Loss: 5.752178\n",
      "Iteration 50525 | Loss: 5.746433\n",
      "Iteration 50550 | Loss: 5.740695\n",
      "Iteration 50575 | Loss: 5.734962\n",
      "Iteration 50600 | Loss: 5.729235\n",
      "Iteration 50625 | Loss: 5.723515\n",
      "Iteration 50650 | Loss: 5.717800\n",
      "Iteration 50675 | Loss: 5.712091\n",
      "Iteration 50700 | Loss: 5.706388\n",
      "Iteration 50725 | Loss: 5.700692\n",
      "Iteration 50750 | Loss: 5.695001\n",
      "Iteration 50775 | Loss: 5.689316\n",
      "Iteration 50800 | Loss: 5.683637\n",
      "Iteration 50825 | Loss: 5.677964\n",
      "Iteration 50850 | Loss: 5.672297\n",
      "Iteration 50875 | Loss: 5.666636\n",
      "Iteration 50900 | Loss: 5.660981\n",
      "Iteration 50925 | Loss: 5.655332\n",
      "Iteration 50950 | Loss: 5.649689\n",
      "Iteration 50975 | Loss: 5.644051\n",
      "Iteration 51000 | Loss: 5.638420\n",
      "Iteration 51025 | Loss: 5.632794\n",
      "Iteration 51050 | Loss: 5.627175\n",
      "Iteration 51075 | Loss: 5.621561\n",
      "Iteration 51100 | Loss: 5.615953\n",
      "Iteration 51125 | Loss: 5.610351\n",
      "Iteration 51150 | Loss: 5.604755\n",
      "Iteration 51175 | Loss: 5.599165\n",
      "Iteration 51200 | Loss: 5.593580\n",
      "Iteration 51225 | Loss: 5.588002\n",
      "Iteration 51250 | Loss: 5.582429\n",
      "Iteration 51275 | Loss: 5.576862\n",
      "Iteration 51300 | Loss: 5.571301\n",
      "Iteration 51325 | Loss: 5.565746\n",
      "Iteration 51350 | Loss: 5.560197\n",
      "Iteration 51375 | Loss: 5.554653\n",
      "Iteration 51400 | Loss: 5.549115\n",
      "Iteration 51425 | Loss: 5.543583\n",
      "Iteration 51450 | Loss: 5.538057\n",
      "Iteration 51475 | Loss: 5.532537\n",
      "Iteration 51500 | Loss: 5.527022\n",
      "Iteration 51525 | Loss: 5.521513\n",
      "Iteration 51550 | Loss: 5.516010\n",
      "Iteration 51575 | Loss: 5.510513\n",
      "Iteration 51600 | Loss: 5.505021\n",
      "Iteration 51625 | Loss: 5.499536\n",
      "Iteration 51650 | Loss: 5.494055\n",
      "Iteration 51675 | Loss: 5.488581\n",
      "Iteration 51700 | Loss: 5.483113\n",
      "Iteration 51725 | Loss: 5.477650\n",
      "Iteration 51750 | Loss: 5.472193\n",
      "Iteration 51775 | Loss: 5.466741\n",
      "Iteration 51800 | Loss: 5.461296\n",
      "Iteration 51825 | Loss: 5.455856\n",
      "Iteration 51850 | Loss: 5.450421\n",
      "Iteration 51875 | Loss: 5.444993\n",
      "Iteration 51900 | Loss: 5.439570\n",
      "Iteration 51925 | Loss: 5.434153\n",
      "Iteration 51950 | Loss: 5.428741\n",
      "Iteration 51975 | Loss: 5.423335\n",
      "Iteration 52000 | Loss: 5.417935\n",
      "Iteration 52025 | Loss: 5.412540\n",
      "Iteration 52050 | Loss: 5.407152\n",
      "Iteration 52075 | Loss: 5.401768\n",
      "Iteration 52100 | Loss: 5.396391\n",
      "Iteration 52125 | Loss: 5.391019\n",
      "Iteration 52150 | Loss: 5.385652\n",
      "Iteration 52175 | Loss: 5.380292\n",
      "Iteration 52200 | Loss: 5.374936\n",
      "Iteration 52225 | Loss: 5.369587\n",
      "Iteration 52250 | Loss: 5.364243\n",
      "Iteration 52275 | Loss: 5.358905\n",
      "Iteration 52300 | Loss: 5.353572\n",
      "Iteration 52325 | Loss: 5.348245\n",
      "Iteration 52350 | Loss: 5.342923\n",
      "Iteration 52375 | Loss: 5.337607\n",
      "Iteration 52400 | Loss: 5.332297\n",
      "Iteration 52425 | Loss: 5.326992\n",
      "Iteration 52450 | Loss: 5.321693\n",
      "Iteration 52475 | Loss: 5.316399\n",
      "Iteration 52500 | Loss: 5.311111\n",
      "Iteration 52525 | Loss: 5.305828\n",
      "Iteration 52550 | Loss: 5.300551\n",
      "Iteration 52575 | Loss: 5.295280\n",
      "Iteration 52600 | Loss: 5.290013\n",
      "Iteration 52625 | Loss: 5.284753\n",
      "Iteration 52650 | Loss: 5.279498\n",
      "Iteration 52675 | Loss: 5.274248\n",
      "Iteration 52700 | Loss: 5.269004\n",
      "Iteration 52725 | Loss: 5.263766\n",
      "Iteration 52750 | Loss: 5.258533\n",
      "Iteration 52775 | Loss: 5.253305\n",
      "Iteration 52800 | Loss: 5.248083\n",
      "Iteration 52825 | Loss: 5.242866\n",
      "Iteration 52850 | Loss: 5.237655\n",
      "Iteration 52875 | Loss: 5.232450\n",
      "Iteration 52900 | Loss: 5.227249\n",
      "Iteration 52925 | Loss: 5.222054\n",
      "Iteration 52950 | Loss: 5.216865\n",
      "Iteration 52975 | Loss: 5.211681\n",
      "Iteration 53000 | Loss: 5.206503\n",
      "Iteration 53025 | Loss: 5.201330\n",
      "Iteration 53050 | Loss: 5.196162\n",
      "Iteration 53075 | Loss: 5.191000\n",
      "Iteration 53100 | Loss: 5.185843\n",
      "Iteration 53125 | Loss: 5.180692\n",
      "Iteration 53150 | Loss: 5.175545\n",
      "Iteration 53175 | Loss: 5.170405\n",
      "Iteration 53200 | Loss: 5.165270\n",
      "Iteration 53225 | Loss: 5.160140\n",
      "Iteration 53250 | Loss: 5.155015\n",
      "Iteration 53275 | Loss: 5.149896\n",
      "Iteration 53300 | Loss: 5.144782\n",
      "Iteration 53325 | Loss: 5.139674\n",
      "Iteration 53350 | Loss: 5.134571\n",
      "Iteration 53375 | Loss: 5.129473\n",
      "Iteration 53400 | Loss: 5.124381\n",
      "Iteration 53425 | Loss: 5.119294\n",
      "Iteration 53450 | Loss: 5.114212\n",
      "Iteration 53475 | Loss: 5.109136\n",
      "Iteration 53500 | Loss: 5.104065\n",
      "Iteration 53525 | Loss: 5.098999\n",
      "Iteration 53550 | Loss: 5.093938\n",
      "Iteration 53575 | Loss: 5.088883\n",
      "Iteration 53600 | Loss: 5.083833\n",
      "Iteration 53625 | Loss: 5.078789\n",
      "Iteration 53650 | Loss: 5.073749\n",
      "Iteration 53675 | Loss: 5.068715\n",
      "Iteration 53700 | Loss: 5.063687\n",
      "Iteration 53725 | Loss: 5.058663\n",
      "Iteration 53750 | Loss: 5.053645\n",
      "Iteration 53775 | Loss: 5.048632\n",
      "Iteration 53800 | Loss: 5.043624\n",
      "Iteration 53825 | Loss: 5.038622\n",
      "Iteration 53850 | Loss: 5.033625\n",
      "Iteration 53875 | Loss: 5.028633\n",
      "Iteration 53900 | Loss: 5.023646\n",
      "Iteration 53925 | Loss: 5.018664\n",
      "Iteration 53950 | Loss: 5.013688\n",
      "Iteration 53975 | Loss: 5.008717\n",
      "Iteration 54000 | Loss: 5.003751\n",
      "Iteration 54025 | Loss: 4.998791\n",
      "Iteration 54050 | Loss: 4.993835\n",
      "Iteration 54075 | Loss: 4.988885\n",
      "Iteration 54100 | Loss: 4.983940\n",
      "Iteration 54125 | Loss: 4.979000\n",
      "Iteration 54150 | Loss: 4.974065\n",
      "Iteration 54175 | Loss: 4.969135\n",
      "Iteration 54200 | Loss: 4.964211\n",
      "Iteration 54225 | Loss: 4.959292\n",
      "Iteration 54250 | Loss: 4.954378\n",
      "Iteration 54275 | Loss: 4.949469\n",
      "Iteration 54300 | Loss: 4.944565\n",
      "Iteration 54325 | Loss: 4.939666\n",
      "Iteration 54350 | Loss: 4.934773\n",
      "Iteration 54375 | Loss: 4.929884\n",
      "Iteration 54400 | Loss: 4.925001\n",
      "Iteration 54425 | Loss: 4.920123\n",
      "Iteration 54450 | Loss: 4.915250\n",
      "Iteration 54475 | Loss: 4.910382\n",
      "Iteration 54500 | Loss: 4.905519\n",
      "Iteration 54525 | Loss: 4.900661\n",
      "Iteration 54550 | Loss: 4.895808\n",
      "Iteration 54575 | Loss: 4.890961\n",
      "Iteration 54600 | Loss: 4.886118\n",
      "Iteration 54625 | Loss: 4.881281\n",
      "Iteration 54650 | Loss: 4.876448\n",
      "Iteration 54675 | Loss: 4.871621\n",
      "Iteration 54700 | Loss: 4.866799\n",
      "Iteration 54725 | Loss: 4.861981\n",
      "Iteration 54750 | Loss: 4.857169\n",
      "Iteration 54775 | Loss: 4.852362\n",
      "Iteration 54800 | Loss: 4.847560\n",
      "Iteration 54825 | Loss: 4.842763\n",
      "Iteration 54850 | Loss: 4.837971\n",
      "Iteration 54875 | Loss: 4.833184\n",
      "Iteration 54900 | Loss: 4.828402\n",
      "Iteration 54925 | Loss: 4.823625\n",
      "Iteration 54950 | Loss: 4.818853\n",
      "Iteration 54975 | Loss: 4.814086\n",
      "Iteration 55000 | Loss: 4.809324\n",
      "Iteration 55025 | Loss: 4.804567\n",
      "Iteration 55050 | Loss: 4.799815\n",
      "Iteration 55075 | Loss: 4.795068\n",
      "Iteration 55100 | Loss: 4.790325\n",
      "Iteration 55125 | Loss: 4.785588\n",
      "Iteration 55150 | Loss: 4.780856\n",
      "Iteration 55175 | Loss: 4.776129\n",
      "Iteration 55200 | Loss: 4.771407\n",
      "Iteration 55225 | Loss: 4.766689\n",
      "Iteration 55250 | Loss: 4.761977\n",
      "Iteration 55275 | Loss: 4.757270\n",
      "Iteration 55300 | Loss: 4.752567\n",
      "Iteration 55325 | Loss: 4.747870\n",
      "Iteration 55350 | Loss: 4.743177\n",
      "Iteration 55375 | Loss: 4.738489\n",
      "Iteration 55400 | Loss: 4.733806\n",
      "Iteration 55425 | Loss: 4.729128\n",
      "Iteration 55450 | Loss: 4.724455\n",
      "Iteration 55475 | Loss: 4.719787\n",
      "Iteration 55500 | Loss: 4.715124\n",
      "Iteration 55525 | Loss: 4.710466\n",
      "Iteration 55550 | Loss: 4.705812\n",
      "Iteration 55575 | Loss: 4.701164\n",
      "Iteration 55600 | Loss: 4.696520\n",
      "Iteration 55625 | Loss: 4.691881\n",
      "Iteration 55650 | Loss: 4.687247\n",
      "Iteration 55675 | Loss: 4.682618\n",
      "Iteration 55700 | Loss: 4.677993\n",
      "Iteration 55725 | Loss: 4.673374\n",
      "Iteration 55750 | Loss: 4.668759\n",
      "Iteration 55775 | Loss: 4.664150\n",
      "Iteration 55800 | Loss: 4.659545\n",
      "Iteration 55825 | Loss: 4.654944\n",
      "Iteration 55850 | Loss: 4.650349\n",
      "Iteration 55875 | Loss: 4.645759\n",
      "Iteration 55900 | Loss: 4.641173\n",
      "Iteration 55925 | Loss: 4.636592\n",
      "Iteration 55950 | Loss: 4.632016\n",
      "Iteration 55975 | Loss: 4.627445\n",
      "Iteration 56000 | Loss: 4.622878\n",
      "Iteration 56025 | Loss: 4.618316\n",
      "Iteration 56050 | Loss: 4.613759\n",
      "Iteration 56075 | Loss: 4.609207\n",
      "Iteration 56100 | Loss: 4.604660\n",
      "Iteration 56125 | Loss: 4.600117\n",
      "Iteration 56150 | Loss: 4.595579\n",
      "Iteration 56175 | Loss: 4.591046\n",
      "Iteration 56200 | Loss: 4.586518\n",
      "Iteration 56225 | Loss: 4.581994\n",
      "Iteration 56250 | Loss: 4.577475\n",
      "Iteration 56275 | Loss: 4.572961\n",
      "Iteration 56300 | Loss: 4.568452\n",
      "Iteration 56325 | Loss: 4.563947\n",
      "Iteration 56350 | Loss: 4.559447\n",
      "Iteration 56375 | Loss: 4.554952\n",
      "Iteration 56400 | Loss: 4.550461\n",
      "Iteration 56425 | Loss: 4.545975\n",
      "Iteration 56450 | Loss: 4.541494\n",
      "Iteration 56475 | Loss: 4.537017\n",
      "Iteration 56500 | Loss: 4.532546\n",
      "Iteration 56525 | Loss: 4.528078\n",
      "Iteration 56550 | Loss: 4.523616\n",
      "Iteration 56575 | Loss: 4.519158\n",
      "Iteration 56600 | Loss: 4.514705\n",
      "Iteration 56625 | Loss: 4.510257\n",
      "Iteration 56650 | Loss: 4.505813\n",
      "Iteration 56675 | Loss: 4.501374\n",
      "Iteration 56700 | Loss: 4.496939\n",
      "Iteration 56725 | Loss: 4.492509\n",
      "Iteration 56750 | Loss: 4.488084\n",
      "Iteration 56775 | Loss: 4.483664\n",
      "Iteration 56800 | Loss: 4.479248\n",
      "Iteration 56825 | Loss: 4.474837\n",
      "Iteration 56850 | Loss: 4.470430\n",
      "Iteration 56875 | Loss: 4.466028\n",
      "Iteration 56900 | Loss: 4.461630\n",
      "Iteration 56925 | Loss: 4.457237\n",
      "Iteration 56950 | Loss: 4.452849\n",
      "Iteration 56975 | Loss: 4.448466\n",
      "Iteration 57000 | Loss: 4.444087\n",
      "Iteration 57025 | Loss: 4.439712\n",
      "Iteration 57050 | Loss: 4.435342\n",
      "Iteration 57075 | Loss: 4.430977\n",
      "Iteration 57100 | Loss: 4.426616\n",
      "Iteration 57125 | Loss: 4.422260\n",
      "Iteration 57150 | Loss: 4.417908\n",
      "Iteration 57175 | Loss: 4.413561\n",
      "Iteration 57200 | Loss: 4.409219\n",
      "Iteration 57225 | Loss: 4.404881\n",
      "Iteration 57250 | Loss: 4.400547\n",
      "Iteration 57275 | Loss: 4.396219\n",
      "Iteration 57300 | Loss: 4.391894\n",
      "Iteration 57325 | Loss: 4.387574\n",
      "Iteration 57350 | Loss: 4.383259\n",
      "Iteration 57375 | Loss: 4.378948\n",
      "Iteration 57400 | Loss: 4.374642\n",
      "Iteration 57425 | Loss: 4.370340\n",
      "Iteration 57450 | Loss: 4.366043\n",
      "Iteration 57475 | Loss: 4.361750\n",
      "Iteration 57500 | Loss: 4.357462\n",
      "Iteration 57525 | Loss: 4.353178\n",
      "Iteration 57550 | Loss: 4.348899\n",
      "Iteration 57575 | Loss: 4.344624\n",
      "Iteration 57600 | Loss: 4.340354\n",
      "Iteration 57625 | Loss: 4.336088\n",
      "Iteration 57650 | Loss: 4.331827\n",
      "Iteration 57675 | Loss: 4.327570\n",
      "Iteration 57700 | Loss: 4.323318\n",
      "Iteration 57725 | Loss: 4.319070\n",
      "Iteration 57750 | Loss: 4.314826\n",
      "Iteration 57775 | Loss: 4.310587\n",
      "Iteration 57800 | Loss: 4.306352\n",
      "Iteration 57825 | Loss: 4.302122\n",
      "Iteration 57850 | Loss: 4.297896\n",
      "Iteration 57875 | Loss: 4.293675\n",
      "Iteration 57900 | Loss: 4.289458\n",
      "Iteration 57925 | Loss: 4.285246\n",
      "Iteration 57950 | Loss: 4.281037\n",
      "Iteration 57975 | Loss: 4.276834\n",
      "Iteration 58000 | Loss: 4.272634\n",
      "Iteration 58025 | Loss: 4.268440\n",
      "Iteration 58050 | Loss: 4.264249\n",
      "Iteration 58075 | Loss: 4.260063\n",
      "Iteration 58100 | Loss: 4.255881\n",
      "Iteration 58125 | Loss: 4.251704\n",
      "Iteration 58150 | Loss: 4.247531\n",
      "Iteration 58175 | Loss: 4.243362\n",
      "Iteration 58200 | Loss: 4.239198\n",
      "Iteration 58225 | Loss: 4.235038\n",
      "Iteration 58250 | Loss: 4.230883\n",
      "Iteration 58275 | Loss: 4.226731\n",
      "Iteration 58300 | Loss: 4.222585\n",
      "Iteration 58325 | Loss: 4.218442\n",
      "Iteration 58350 | Loss: 4.214304\n",
      "Iteration 58375 | Loss: 4.210170\n",
      "Iteration 58400 | Loss: 4.206041\n",
      "Iteration 58425 | Loss: 4.201916\n",
      "Iteration 58450 | Loss: 4.197795\n",
      "Iteration 58475 | Loss: 4.193678\n",
      "Iteration 58500 | Loss: 4.189566\n",
      "Iteration 58525 | Loss: 4.185458\n",
      "Iteration 58550 | Loss: 4.181354\n",
      "Iteration 58575 | Loss: 4.177255\n",
      "Iteration 58600 | Loss: 4.173160\n",
      "Iteration 58625 | Loss: 4.169069\n",
      "Iteration 58650 | Loss: 4.164983\n",
      "Iteration 58675 | Loss: 4.160901\n",
      "Iteration 58700 | Loss: 4.156823\n",
      "Iteration 58725 | Loss: 4.152749\n",
      "Iteration 58750 | Loss: 4.148680\n",
      "Iteration 58775 | Loss: 4.144615\n",
      "Iteration 58800 | Loss: 4.140554\n",
      "Iteration 58825 | Loss: 4.136498\n",
      "Iteration 58850 | Loss: 4.132445\n",
      "Iteration 58875 | Loss: 4.128397\n",
      "Iteration 58900 | Loss: 4.124353\n",
      "Iteration 58925 | Loss: 4.120314\n",
      "Iteration 58950 | Loss: 4.116278\n",
      "Iteration 58975 | Loss: 4.112247\n",
      "Iteration 59000 | Loss: 4.108220\n",
      "Iteration 59025 | Loss: 4.104198\n",
      "Iteration 59050 | Loss: 4.100179\n",
      "Iteration 59075 | Loss: 4.096165\n",
      "Iteration 59100 | Loss: 4.092155\n",
      "Iteration 59125 | Loss: 4.088149\n",
      "Iteration 59150 | Loss: 4.084147\n",
      "Iteration 59175 | Loss: 4.080150\n",
      "Iteration 59200 | Loss: 4.076156\n",
      "Iteration 59225 | Loss: 4.072167\n",
      "Iteration 59250 | Loss: 4.068182\n",
      "Iteration 59275 | Loss: 4.064201\n",
      "Iteration 59300 | Loss: 4.060225\n",
      "Iteration 59325 | Loss: 4.056252\n",
      "Iteration 59350 | Loss: 4.052284\n",
      "Iteration 59375 | Loss: 4.048320\n",
      "Iteration 59400 | Loss: 4.044360\n",
      "Iteration 59425 | Loss: 4.040404\n",
      "Iteration 59450 | Loss: 4.036452\n",
      "Iteration 59475 | Loss: 4.032505\n",
      "Iteration 59500 | Loss: 4.028561\n",
      "Iteration 59525 | Loss: 4.024622\n",
      "Iteration 59550 | Loss: 4.020687\n",
      "Iteration 59575 | Loss: 4.016756\n",
      "Iteration 59600 | Loss: 4.012829\n",
      "Iteration 59625 | Loss: 4.008906\n",
      "Iteration 59650 | Loss: 4.004988\n",
      "Iteration 59675 | Loss: 4.001073\n",
      "Iteration 59700 | Loss: 3.997162\n",
      "Iteration 59725 | Loss: 3.993256\n",
      "Iteration 59750 | Loss: 3.989354\n",
      "Iteration 59775 | Loss: 3.985456\n",
      "Iteration 59800 | Loss: 3.981561\n",
      "Iteration 59825 | Loss: 3.977671\n",
      "Iteration 59850 | Loss: 3.973785\n",
      "Iteration 59875 | Loss: 3.969903\n",
      "Iteration 59900 | Loss: 3.966026\n",
      "Iteration 59925 | Loss: 3.962152\n",
      "Iteration 59950 | Loss: 3.958282\n",
      "Iteration 59975 | Loss: 3.954416\n",
      "Iteration 60000 | Loss: 3.950555\n",
      "Iteration 60025 | Loss: 3.946697\n",
      "Iteration 60050 | Loss: 3.942844\n",
      "Iteration 60075 | Loss: 3.938994\n",
      "Iteration 60100 | Loss: 3.935149\n",
      "Iteration 60125 | Loss: 3.931307\n",
      "Iteration 60150 | Loss: 3.927470\n",
      "Iteration 60175 | Loss: 3.923636\n",
      "Iteration 60200 | Loss: 3.919807\n",
      "Iteration 60225 | Loss: 3.915981\n",
      "Iteration 60250 | Loss: 3.912160\n",
      "Iteration 60275 | Loss: 3.908343\n",
      "Iteration 60300 | Loss: 3.904529\n",
      "Iteration 60325 | Loss: 3.900720\n",
      "Iteration 60350 | Loss: 3.896915\n",
      "Iteration 60375 | Loss: 3.893113\n",
      "Iteration 60400 | Loss: 3.889316\n",
      "Iteration 60425 | Loss: 3.885522\n",
      "Iteration 60450 | Loss: 3.881733\n",
      "Iteration 60475 | Loss: 3.877947\n",
      "Iteration 60500 | Loss: 3.874166\n",
      "Iteration 60525 | Loss: 3.870388\n",
      "Iteration 60550 | Loss: 3.866614\n",
      "Iteration 60575 | Loss: 3.862845\n",
      "Iteration 60600 | Loss: 3.859079\n",
      "Iteration 60625 | Loss: 3.855317\n",
      "Iteration 60650 | Loss: 3.851559\n",
      "Iteration 60675 | Loss: 3.847805\n",
      "Iteration 60700 | Loss: 3.844055\n",
      "Iteration 60725 | Loss: 3.840309\n",
      "Iteration 60750 | Loss: 3.836567\n",
      "Iteration 60775 | Loss: 3.832829\n",
      "Iteration 60800 | Loss: 3.829095\n",
      "Iteration 60825 | Loss: 3.825364\n",
      "Iteration 60850 | Loss: 3.821638\n",
      "Iteration 60875 | Loss: 3.817915\n",
      "Iteration 60900 | Loss: 3.814197\n",
      "Iteration 60925 | Loss: 3.810482\n",
      "Iteration 60950 | Loss: 3.806771\n",
      "Iteration 60975 | Loss: 3.803064\n",
      "Iteration 61000 | Loss: 3.799361\n",
      "Iteration 61025 | Loss: 3.795661\n",
      "Iteration 61050 | Loss: 3.791966\n",
      "Iteration 61075 | Loss: 3.788275\n",
      "Iteration 61100 | Loss: 3.784587\n",
      "Iteration 61125 | Loss: 3.780903\n",
      "Iteration 61150 | Loss: 3.777223\n",
      "Iteration 61175 | Loss: 3.773547\n",
      "Iteration 61200 | Loss: 3.769875\n",
      "Iteration 61225 | Loss: 3.766207\n",
      "Iteration 61250 | Loss: 3.762542\n",
      "Iteration 61275 | Loss: 3.758881\n",
      "Iteration 61300 | Loss: 3.755224\n",
      "Iteration 61325 | Loss: 3.751571\n",
      "Iteration 61350 | Loss: 3.747922\n",
      "Iteration 61375 | Loss: 3.744277\n",
      "Iteration 61400 | Loss: 3.740635\n",
      "Iteration 61425 | Loss: 3.736997\n",
      "Iteration 61450 | Loss: 3.733363\n",
      "Iteration 61475 | Loss: 3.729733\n",
      "Iteration 61500 | Loss: 3.726107\n",
      "Iteration 61525 | Loss: 3.722484\n",
      "Iteration 61550 | Loss: 3.718866\n",
      "Iteration 61575 | Loss: 3.715251\n",
      "Iteration 61600 | Loss: 3.711640\n",
      "Iteration 61625 | Loss: 3.708032\n",
      "Iteration 61650 | Loss: 3.704429\n",
      "Iteration 61675 | Loss: 3.700829\n",
      "Iteration 61700 | Loss: 3.697233\n",
      "Iteration 61725 | Loss: 3.693640\n",
      "Iteration 61750 | Loss: 3.690052\n",
      "Iteration 61775 | Loss: 3.686467\n",
      "Iteration 61800 | Loss: 3.682886\n",
      "Iteration 61825 | Loss: 3.679309\n",
      "Iteration 61850 | Loss: 3.675735\n",
      "Iteration 61875 | Loss: 3.672165\n",
      "Iteration 61900 | Loss: 3.668599\n",
      "Iteration 61925 | Loss: 3.665037\n",
      "Iteration 61950 | Loss: 3.661478\n",
      "Iteration 61975 | Loss: 3.657923\n",
      "Iteration 62000 | Loss: 3.654372\n",
      "Iteration 62025 | Loss: 3.650825\n",
      "Iteration 62050 | Loss: 3.647281\n",
      "Iteration 62075 | Loss: 3.643741\n",
      "Iteration 62100 | Loss: 3.640205\n",
      "Iteration 62125 | Loss: 3.636672\n",
      "Iteration 62150 | Loss: 3.633143\n",
      "Iteration 62175 | Loss: 3.629618\n",
      "Iteration 62200 | Loss: 3.626097\n",
      "Iteration 62225 | Loss: 3.622579\n",
      "Iteration 62250 | Loss: 3.619065\n",
      "Iteration 62275 | Loss: 3.615554\n",
      "Iteration 62300 | Loss: 3.612047\n",
      "Iteration 62325 | Loss: 3.608544\n",
      "Iteration 62350 | Loss: 3.605045\n",
      "Iteration 62375 | Loss: 3.601549\n",
      "Iteration 62400 | Loss: 3.598057\n",
      "Iteration 62425 | Loss: 3.594569\n",
      "Iteration 62450 | Loss: 3.591084\n",
      "Iteration 62475 | Loss: 3.587603\n",
      "Iteration 62500 | Loss: 3.584125\n",
      "Iteration 62525 | Loss: 3.580651\n",
      "Iteration 62550 | Loss: 3.577181\n",
      "Iteration 62575 | Loss: 3.573714\n",
      "Iteration 62600 | Loss: 3.570251\n",
      "Iteration 62625 | Loss: 3.566792\n",
      "Iteration 62650 | Loss: 3.563336\n",
      "Iteration 62675 | Loss: 3.559884\n",
      "Iteration 62700 | Loss: 3.556436\n",
      "Iteration 62725 | Loss: 3.552991\n",
      "Iteration 62750 | Loss: 3.549550\n",
      "Iteration 62775 | Loss: 3.546112\n",
      "Iteration 62800 | Loss: 3.542678\n",
      "Iteration 62825 | Loss: 3.539247\n",
      "Iteration 62850 | Loss: 3.535820\n",
      "Iteration 62875 | Loss: 3.532397\n",
      "Iteration 62900 | Loss: 3.528977\n",
      "Iteration 62925 | Loss: 3.525561\n",
      "Iteration 62950 | Loss: 3.522149\n",
      "Iteration 62975 | Loss: 3.518740\n",
      "Iteration 63000 | Loss: 3.515334\n",
      "Iteration 63025 | Loss: 3.511933\n",
      "Iteration 63050 | Loss: 3.508534\n",
      "Iteration 63075 | Loss: 3.505140\n",
      "Iteration 63100 | Loss: 3.501748\n",
      "Iteration 63125 | Loss: 3.498361\n",
      "Iteration 63150 | Loss: 3.494977\n",
      "Iteration 63175 | Loss: 3.491596\n",
      "Iteration 63200 | Loss: 3.488219\n",
      "Iteration 63225 | Loss: 3.484846\n",
      "Iteration 63250 | Loss: 3.481476\n",
      "Iteration 63275 | Loss: 3.478110\n",
      "Iteration 63300 | Loss: 3.474747\n",
      "Iteration 63325 | Loss: 3.471387\n",
      "Iteration 63350 | Loss: 3.468032\n",
      "Iteration 63375 | Loss: 3.464679\n",
      "Iteration 63400 | Loss: 3.461330\n",
      "Iteration 63425 | Loss: 3.457985\n",
      "Iteration 63450 | Loss: 3.454643\n",
      "Iteration 63475 | Loss: 3.451305\n",
      "Iteration 63500 | Loss: 3.447970\n",
      "Iteration 63525 | Loss: 3.444639\n",
      "Iteration 63550 | Loss: 3.441311\n",
      "Iteration 63575 | Loss: 3.437987\n",
      "Iteration 63600 | Loss: 3.434666\n",
      "Iteration 63625 | Loss: 3.431349\n",
      "Iteration 63650 | Loss: 3.428035\n",
      "Iteration 63675 | Loss: 3.424724\n",
      "Iteration 63700 | Loss: 3.421417\n",
      "Iteration 63725 | Loss: 3.418114\n",
      "Iteration 63750 | Loss: 3.414814\n",
      "Iteration 63775 | Loss: 3.411517\n",
      "Iteration 63800 | Loss: 3.408224\n",
      "Iteration 63825 | Loss: 3.404934\n",
      "Iteration 63850 | Loss: 3.401648\n",
      "Iteration 63875 | Loss: 3.398365\n",
      "Iteration 63900 | Loss: 3.395086\n",
      "Iteration 63925 | Loss: 3.391810\n",
      "Iteration 63950 | Loss: 3.388538\n",
      "Iteration 63975 | Loss: 3.385268\n",
      "Iteration 64000 | Loss: 3.382003\n",
      "Iteration 64025 | Loss: 3.378741\n",
      "Iteration 64050 | Loss: 3.375482\n",
      "Iteration 64075 | Loss: 3.372226\n",
      "Iteration 64100 | Loss: 3.368974\n",
      "Iteration 64125 | Loss: 3.365726\n",
      "Iteration 64150 | Loss: 3.362481\n",
      "Iteration 64175 | Loss: 3.359239\n",
      "Iteration 64200 | Loss: 3.356000\n",
      "Iteration 64225 | Loss: 3.352765\n",
      "Iteration 64250 | Loss: 3.349534\n",
      "Iteration 64275 | Loss: 3.346306\n",
      "Iteration 64300 | Loss: 3.343081\n",
      "Iteration 64325 | Loss: 3.339859\n",
      "Iteration 64350 | Loss: 3.336641\n",
      "Iteration 64375 | Loss: 3.333426\n",
      "Iteration 64400 | Loss: 3.330215\n",
      "Iteration 64425 | Loss: 3.327007\n",
      "Iteration 64450 | Loss: 3.323802\n",
      "Iteration 64475 | Loss: 3.320601\n",
      "Iteration 64500 | Loss: 3.317403\n",
      "Iteration 64525 | Loss: 3.314209\n",
      "Iteration 64550 | Loss: 3.311017\n",
      "Iteration 64575 | Loss: 3.307829\n",
      "Iteration 64600 | Loss: 3.304645\n",
      "Iteration 64625 | Loss: 3.301464\n",
      "Iteration 64650 | Loss: 3.298286\n",
      "Iteration 64675 | Loss: 3.295111\n",
      "Iteration 64700 | Loss: 3.291940\n",
      "Iteration 64725 | Loss: 3.288772\n",
      "Iteration 64750 | Loss: 3.285607\n",
      "Iteration 64775 | Loss: 3.282446\n",
      "Iteration 64800 | Loss: 3.279288\n",
      "Iteration 64825 | Loss: 3.276133\n",
      "Iteration 64850 | Loss: 3.272982\n",
      "Iteration 64875 | Loss: 3.269834\n",
      "Iteration 64900 | Loss: 3.266689\n",
      "Iteration 64925 | Loss: 3.263548\n",
      "Iteration 64950 | Loss: 3.260409\n",
      "Iteration 64975 | Loss: 3.257275\n",
      "Iteration 65000 | Loss: 3.254143\n",
      "Iteration 65025 | Loss: 3.251015\n",
      "Iteration 65050 | Loss: 3.247890\n",
      "Iteration 65075 | Loss: 3.244768\n",
      "Iteration 65100 | Loss: 3.241649\n",
      "Iteration 65125 | Loss: 3.238534\n",
      "Iteration 65150 | Loss: 3.235422\n",
      "Iteration 65175 | Loss: 3.232313\n",
      "Iteration 65200 | Loss: 3.229208\n",
      "Iteration 65225 | Loss: 3.226105\n",
      "Iteration 65250 | Loss: 3.223006\n",
      "Iteration 65275 | Loss: 3.219911\n",
      "Iteration 65300 | Loss: 3.216818\n",
      "Iteration 65325 | Loss: 3.213729\n",
      "Iteration 65350 | Loss: 3.210643\n",
      "Iteration 65375 | Loss: 3.207560\n",
      "Iteration 65400 | Loss: 3.204480\n",
      "Iteration 65425 | Loss: 3.201404\n",
      "Iteration 65450 | Loss: 3.198331\n",
      "Iteration 65475 | Loss: 3.195261\n",
      "Iteration 65500 | Loss: 3.192194\n",
      "Iteration 65525 | Loss: 3.189131\n",
      "Iteration 65550 | Loss: 3.186071\n",
      "Iteration 65575 | Loss: 3.183013\n",
      "Iteration 65600 | Loss: 3.179960\n",
      "Iteration 65625 | Loss: 3.176909\n",
      "Iteration 65650 | Loss: 3.173861\n",
      "Iteration 65675 | Loss: 3.170817\n",
      "Iteration 65700 | Loss: 3.167776\n",
      "Iteration 65725 | Loss: 3.164738\n",
      "Iteration 65750 | Loss: 3.161703\n",
      "Iteration 65775 | Loss: 3.158672\n",
      "Iteration 65800 | Loss: 3.155643\n",
      "Iteration 65825 | Loss: 3.152618\n",
      "Iteration 65850 | Loss: 3.149596\n",
      "Iteration 65875 | Loss: 3.146577\n",
      "Iteration 65900 | Loss: 3.143561\n",
      "Iteration 65925 | Loss: 3.140549\n",
      "Iteration 65950 | Loss: 3.137539\n",
      "Iteration 65975 | Loss: 3.134533\n",
      "Iteration 66000 | Loss: 3.131530\n",
      "Iteration 66025 | Loss: 3.128530\n",
      "Iteration 66050 | Loss: 3.125533\n",
      "Iteration 66075 | Loss: 3.122540\n",
      "Iteration 66100 | Loss: 3.119549\n",
      "Iteration 66125 | Loss: 3.116562\n",
      "Iteration 66150 | Loss: 3.113577\n",
      "Iteration 66175 | Loss: 3.110596\n",
      "Iteration 66200 | Loss: 3.107618\n",
      "Iteration 66225 | Loss: 3.104643\n",
      "Iteration 66250 | Loss: 3.101671\n",
      "Iteration 66275 | Loss: 3.098703\n",
      "Iteration 66300 | Loss: 3.095737\n",
      "Iteration 66325 | Loss: 3.092774\n",
      "Iteration 66350 | Loss: 3.089815\n",
      "Iteration 66375 | Loss: 3.086859\n",
      "Iteration 66400 | Loss: 3.083906\n",
      "Iteration 66425 | Loss: 3.080955\n",
      "Iteration 66450 | Loss: 3.078008\n",
      "Iteration 66475 | Loss: 3.075064\n",
      "Iteration 66500 | Loss: 3.072124\n",
      "Iteration 66525 | Loss: 3.069186\n",
      "Iteration 66550 | Loss: 3.066251\n",
      "Iteration 66575 | Loss: 3.063319\n",
      "Iteration 66600 | Loss: 3.060391\n",
      "Iteration 66625 | Loss: 3.057465\n",
      "Iteration 66650 | Loss: 3.054543\n",
      "Iteration 66675 | Loss: 3.051624\n",
      "Iteration 66700 | Loss: 3.048707\n",
      "Iteration 66725 | Loss: 3.045794\n",
      "Iteration 66750 | Loss: 3.042884\n",
      "Iteration 66775 | Loss: 3.039977\n",
      "Iteration 66800 | Loss: 3.037072\n",
      "Iteration 66825 | Loss: 3.034171\n",
      "Iteration 66850 | Loss: 3.031273\n",
      "Iteration 66875 | Loss: 3.028378\n",
      "Iteration 66900 | Loss: 3.025486\n",
      "Iteration 66925 | Loss: 3.022597\n",
      "Iteration 66950 | Loss: 3.019711\n",
      "Iteration 66975 | Loss: 3.016829\n",
      "Iteration 67000 | Loss: 3.013949\n",
      "Iteration 67025 | Loss: 3.011072\n",
      "Iteration 67050 | Loss: 3.008198\n",
      "Iteration 67075 | Loss: 3.005327\n",
      "Iteration 67100 | Loss: 3.002459\n",
      "Iteration 67125 | Loss: 2.999594\n",
      "Iteration 67150 | Loss: 2.996733\n",
      "Iteration 67175 | Loss: 2.993874\n",
      "Iteration 67200 | Loss: 2.991018\n",
      "Iteration 67225 | Loss: 2.988165\n",
      "Iteration 67250 | Loss: 2.985315\n",
      "Iteration 67275 | Loss: 2.982468\n",
      "Iteration 67300 | Loss: 2.979624\n",
      "Iteration 67325 | Loss: 2.976783\n",
      "Iteration 67350 | Loss: 2.973945\n",
      "Iteration 67375 | Loss: 2.971110\n",
      "Iteration 67400 | Loss: 2.968278\n",
      "Iteration 67425 | Loss: 2.965449\n",
      "Iteration 67450 | Loss: 2.962623\n",
      "Iteration 67475 | Loss: 2.959800\n",
      "Iteration 67500 | Loss: 2.956980\n",
      "Iteration 67525 | Loss: 2.954163\n",
      "Iteration 67550 | Loss: 2.951348\n",
      "Iteration 67575 | Loss: 2.948537\n",
      "Iteration 67600 | Loss: 2.945729\n",
      "Iteration 67625 | Loss: 2.942923\n",
      "Iteration 67650 | Loss: 2.940121\n",
      "Iteration 67675 | Loss: 2.937321\n",
      "Iteration 67700 | Loss: 2.934524\n",
      "Iteration 67725 | Loss: 2.931731\n",
      "Iteration 67750 | Loss: 2.928940\n",
      "Iteration 67775 | Loss: 2.926152\n",
      "Iteration 67800 | Loss: 2.923367\n",
      "Iteration 67825 | Loss: 2.920585\n",
      "Iteration 67850 | Loss: 2.917806\n",
      "Iteration 67875 | Loss: 2.915030\n",
      "Iteration 67900 | Loss: 2.912256\n",
      "Iteration 67925 | Loss: 2.909486\n",
      "Iteration 67950 | Loss: 2.906718\n",
      "Iteration 67975 | Loss: 2.903954\n",
      "Iteration 68000 | Loss: 2.901192\n",
      "Iteration 68025 | Loss: 2.898433\n",
      "Iteration 68050 | Loss: 2.895677\n",
      "Iteration 68075 | Loss: 2.892924\n",
      "Iteration 68100 | Loss: 2.890174\n",
      "Iteration 68125 | Loss: 2.887427\n",
      "Iteration 68150 | Loss: 2.884682\n",
      "Iteration 68175 | Loss: 2.881941\n",
      "Iteration 68200 | Loss: 2.879202\n",
      "Iteration 68225 | Loss: 2.876466\n",
      "Iteration 68250 | Loss: 2.873733\n",
      "Iteration 68275 | Loss: 2.871003\n",
      "Iteration 68300 | Loss: 2.868276\n",
      "Iteration 68325 | Loss: 2.865552\n",
      "Iteration 68350 | Loss: 2.862830\n",
      "Iteration 68375 | Loss: 2.860112\n",
      "Iteration 68400 | Loss: 2.857396\n",
      "Iteration 68425 | Loss: 2.854683\n",
      "Iteration 68450 | Loss: 2.851973\n",
      "Iteration 68475 | Loss: 2.849265\n",
      "Iteration 68500 | Loss: 2.846561\n",
      "Iteration 68525 | Loss: 2.843859\n",
      "Iteration 68550 | Loss: 2.841160\n",
      "Iteration 68575 | Loss: 2.838464\n",
      "Iteration 68600 | Loss: 2.835771\n",
      "Iteration 68625 | Loss: 2.833081\n",
      "Iteration 68650 | Loss: 2.830393\n",
      "Iteration 68675 | Loss: 2.827709\n",
      "Iteration 68700 | Loss: 2.825027\n",
      "Iteration 68725 | Loss: 2.822348\n",
      "Iteration 68750 | Loss: 2.819671\n",
      "Iteration 68775 | Loss: 2.816998\n",
      "Iteration 68800 | Loss: 2.814327\n",
      "Iteration 68825 | Loss: 2.811659\n",
      "Iteration 68850 | Loss: 2.808994\n",
      "Iteration 68875 | Loss: 2.806332\n",
      "Iteration 68900 | Loss: 2.803672\n",
      "Iteration 68925 | Loss: 2.801016\n",
      "Iteration 68950 | Loss: 2.798362\n",
      "Iteration 68975 | Loss: 2.795710\n",
      "Iteration 69000 | Loss: 2.793062\n",
      "Iteration 69025 | Loss: 2.790416\n",
      "Iteration 69050 | Loss: 2.787774\n",
      "Iteration 69075 | Loss: 2.785133\n",
      "Iteration 69100 | Loss: 2.782496\n",
      "Iteration 69125 | Loss: 2.779862\n",
      "Iteration 69150 | Loss: 2.777230\n",
      "Iteration 69175 | Loss: 2.774601\n",
      "Iteration 69200 | Loss: 2.771974\n",
      "Iteration 69225 | Loss: 2.769351\n",
      "Iteration 69250 | Loss: 2.766730\n",
      "Iteration 69275 | Loss: 2.764112\n",
      "Iteration 69300 | Loss: 2.761497\n",
      "Iteration 69325 | Loss: 2.758884\n",
      "Iteration 69350 | Loss: 2.756274\n",
      "Iteration 69375 | Loss: 2.753667\n",
      "Iteration 69400 | Loss: 2.751063\n",
      "Iteration 69425 | Loss: 2.748461\n",
      "Iteration 69450 | Loss: 2.745862\n",
      "Iteration 69475 | Loss: 2.743266\n",
      "Iteration 69500 | Loss: 2.740672\n",
      "Iteration 69525 | Loss: 2.738082\n",
      "Iteration 69550 | Loss: 2.735494\n",
      "Iteration 69575 | Loss: 2.732908\n",
      "Iteration 69600 | Loss: 2.730325\n",
      "Iteration 69625 | Loss: 2.727746\n",
      "Iteration 69650 | Loss: 2.725168\n",
      "Iteration 69675 | Loss: 2.722594\n",
      "Iteration 69700 | Loss: 2.720022\n",
      "Iteration 69725 | Loss: 2.717453\n",
      "Iteration 69750 | Loss: 2.714886\n",
      "Iteration 69775 | Loss: 2.712322\n",
      "Iteration 69800 | Loss: 2.709761\n",
      "Iteration 69825 | Loss: 2.707203\n",
      "Iteration 69850 | Loss: 2.704647\n",
      "Iteration 69875 | Loss: 2.702094\n",
      "Iteration 69900 | Loss: 2.699544\n",
      "Iteration 69925 | Loss: 2.696996\n",
      "Iteration 69950 | Loss: 2.694451\n",
      "Iteration 69975 | Loss: 2.691908\n",
      "Iteration 70000 | Loss: 2.689369\n",
      "Iteration 70025 | Loss: 2.686832\n",
      "Iteration 70050 | Loss: 2.684297\n",
      "Iteration 70075 | Loss: 2.681765\n",
      "Iteration 70100 | Loss: 2.679236\n",
      "Iteration 70125 | Loss: 2.676710\n",
      "Iteration 70150 | Loss: 2.674186\n",
      "Iteration 70175 | Loss: 2.671665\n",
      "Iteration 70200 | Loss: 2.669146\n",
      "Iteration 70225 | Loss: 2.666630\n",
      "Iteration 70250 | Loss: 2.664117\n",
      "Iteration 70275 | Loss: 2.661606\n",
      "Iteration 70300 | Loss: 2.659098\n",
      "Iteration 70325 | Loss: 2.656593\n",
      "Iteration 70350 | Loss: 2.654090\n",
      "Iteration 70375 | Loss: 2.651590\n",
      "Iteration 70400 | Loss: 2.649093\n",
      "Iteration 70425 | Loss: 2.646598\n",
      "Iteration 70450 | Loss: 2.644105\n",
      "Iteration 70475 | Loss: 2.641616\n",
      "Iteration 70500 | Loss: 2.639128\n",
      "Iteration 70525 | Loss: 2.636644\n",
      "Iteration 70550 | Loss: 2.634162\n",
      "Iteration 70575 | Loss: 2.631683\n",
      "Iteration 70600 | Loss: 2.629206\n",
      "Iteration 70625 | Loss: 2.626732\n",
      "Iteration 70650 | Loss: 2.624260\n",
      "Iteration 70675 | Loss: 2.621792\n",
      "Iteration 70700 | Loss: 2.619325\n",
      "Iteration 70725 | Loss: 2.616861\n",
      "Iteration 70750 | Loss: 2.614400\n",
      "Iteration 70775 | Loss: 2.611942\n",
      "Iteration 70800 | Loss: 2.609486\n",
      "Iteration 70825 | Loss: 2.607032\n",
      "Iteration 70850 | Loss: 2.604581\n",
      "Iteration 70875 | Loss: 2.602133\n",
      "Iteration 70900 | Loss: 2.599687\n",
      "Iteration 70925 | Loss: 2.597244\n",
      "Iteration 70950 | Loss: 2.594803\n",
      "Iteration 70975 | Loss: 2.592365\n",
      "Iteration 71000 | Loss: 2.589930\n",
      "Iteration 71025 | Loss: 2.587497\n",
      "Iteration 71050 | Loss: 2.585066\n",
      "Iteration 71075 | Loss: 2.582638\n",
      "Iteration 71100 | Loss: 2.580213\n",
      "Iteration 71125 | Loss: 2.577790\n",
      "Iteration 71150 | Loss: 2.575370\n",
      "Iteration 71175 | Loss: 2.572952\n",
      "Iteration 71200 | Loss: 2.570537\n",
      "Iteration 71225 | Loss: 2.568124\n",
      "Iteration 71250 | Loss: 2.565714\n",
      "Iteration 71275 | Loss: 2.563306\n",
      "Iteration 71300 | Loss: 2.560901\n",
      "Iteration 71325 | Loss: 2.558499\n",
      "Iteration 71350 | Loss: 2.556099\n",
      "Iteration 71375 | Loss: 2.553701\n",
      "Iteration 71400 | Loss: 2.551306\n",
      "Iteration 71425 | Loss: 2.548913\n",
      "Iteration 71450 | Loss: 2.546523\n",
      "Iteration 71475 | Loss: 2.544136\n",
      "Iteration 71500 | Loss: 2.541751\n",
      "Iteration 71525 | Loss: 2.539368\n",
      "Iteration 71550 | Loss: 2.536988\n",
      "Iteration 71575 | Loss: 2.534610\n",
      "Iteration 71600 | Loss: 2.532235\n",
      "Iteration 71625 | Loss: 2.529863\n",
      "Iteration 71650 | Loss: 2.527493\n",
      "Iteration 71675 | Loss: 2.525125\n",
      "Iteration 71700 | Loss: 2.522760\n",
      "Iteration 71725 | Loss: 2.520397\n",
      "Iteration 71750 | Loss: 2.518037\n",
      "Iteration 71775 | Loss: 2.515679\n",
      "Iteration 71800 | Loss: 2.513324\n",
      "Iteration 71825 | Loss: 2.510971\n",
      "Iteration 71850 | Loss: 2.508621\n",
      "Iteration 71875 | Loss: 2.506273\n",
      "Iteration 71900 | Loss: 2.503928\n",
      "Iteration 71925 | Loss: 2.501585\n",
      "Iteration 71950 | Loss: 2.499244\n",
      "Iteration 71975 | Loss: 2.496906\n",
      "Iteration 72000 | Loss: 2.494570\n",
      "Iteration 72025 | Loss: 2.492237\n",
      "Iteration 72050 | Loss: 2.489906\n",
      "Iteration 72075 | Loss: 2.487578\n",
      "Iteration 72100 | Loss: 2.485252\n",
      "Iteration 72125 | Loss: 2.482929\n",
      "Iteration 72150 | Loss: 2.480608\n",
      "Iteration 72175 | Loss: 2.478289\n",
      "Iteration 72200 | Loss: 2.475973\n",
      "Iteration 72225 | Loss: 2.473659\n",
      "Iteration 72250 | Loss: 2.471348\n",
      "Iteration 72275 | Loss: 2.469039\n",
      "Iteration 72300 | Loss: 2.466733\n",
      "Iteration 72325 | Loss: 2.464429\n",
      "Iteration 72350 | Loss: 2.462127\n",
      "Iteration 72375 | Loss: 2.459828\n",
      "Iteration 72400 | Loss: 2.457531\n",
      "Iteration 72425 | Loss: 2.455237\n",
      "Iteration 72450 | Loss: 2.452945\n",
      "Iteration 72475 | Loss: 2.450655\n",
      "Iteration 72500 | Loss: 2.448368\n",
      "Iteration 72525 | Loss: 2.446083\n",
      "Iteration 72550 | Loss: 2.443801\n",
      "Iteration 72575 | Loss: 2.441521\n",
      "Iteration 72600 | Loss: 2.439243\n",
      "Iteration 72625 | Loss: 2.436968\n",
      "Iteration 72650 | Loss: 2.434695\n",
      "Iteration 72675 | Loss: 2.432424\n",
      "Iteration 72700 | Loss: 2.430156\n",
      "Iteration 72725 | Loss: 2.427890\n",
      "Iteration 72750 | Loss: 2.425627\n",
      "Iteration 72775 | Loss: 2.423366\n",
      "Iteration 72800 | Loss: 2.421107\n",
      "Iteration 72825 | Loss: 2.418851\n",
      "Iteration 72850 | Loss: 2.416597\n",
      "Iteration 72875 | Loss: 2.414346\n",
      "Iteration 72900 | Loss: 2.412096\n",
      "Iteration 72925 | Loss: 2.409850\n",
      "Iteration 72950 | Loss: 2.407605\n",
      "Iteration 72975 | Loss: 2.405363\n",
      "Iteration 73000 | Loss: 2.403123\n",
      "Iteration 73025 | Loss: 2.400886\n",
      "Iteration 73050 | Loss: 2.398651\n",
      "Iteration 73075 | Loss: 2.396418\n",
      "Iteration 73100 | Loss: 2.394187\n",
      "Iteration 73125 | Loss: 2.391959\n",
      "Iteration 73150 | Loss: 2.389733\n",
      "Iteration 73175 | Loss: 2.387510\n",
      "Iteration 73200 | Loss: 2.385289\n",
      "Iteration 73225 | Loss: 2.383070\n",
      "Iteration 73250 | Loss: 2.380854\n",
      "Iteration 73275 | Loss: 2.378639\n",
      "Iteration 73300 | Loss: 2.376428\n",
      "Iteration 73325 | Loss: 2.374218\n",
      "Iteration 73350 | Loss: 2.372011\n",
      "Iteration 73375 | Loss: 2.369806\n",
      "Iteration 73400 | Loss: 2.367603\n",
      "Iteration 73425 | Loss: 2.365403\n",
      "Iteration 73450 | Loss: 2.363205\n",
      "Iteration 73475 | Loss: 2.361009\n",
      "Iteration 73500 | Loss: 2.358816\n",
      "Iteration 73525 | Loss: 2.356625\n",
      "Iteration 73550 | Loss: 2.354436\n",
      "Iteration 73575 | Loss: 2.352250\n",
      "Iteration 73600 | Loss: 2.350065\n",
      "Iteration 73625 | Loss: 2.347884\n",
      "Iteration 73650 | Loss: 2.345704\n",
      "Iteration 73675 | Loss: 2.343527\n",
      "Iteration 73700 | Loss: 2.341351\n",
      "Iteration 73725 | Loss: 2.339179\n",
      "Iteration 73750 | Loss: 2.337008\n",
      "Iteration 73775 | Loss: 2.334840\n",
      "Iteration 73800 | Loss: 2.332674\n",
      "Iteration 73825 | Loss: 2.330510\n",
      "Iteration 73850 | Loss: 2.328349\n",
      "Iteration 73875 | Loss: 2.326189\n",
      "Iteration 73900 | Loss: 2.324033\n",
      "Iteration 73925 | Loss: 2.321878\n",
      "Iteration 73950 | Loss: 2.319725\n",
      "Iteration 73975 | Loss: 2.317575\n",
      "Iteration 74000 | Loss: 2.315427\n",
      "Iteration 74025 | Loss: 2.313282\n",
      "Iteration 74050 | Loss: 2.311138\n",
      "Iteration 74075 | Loss: 2.308997\n",
      "Iteration 74100 | Loss: 2.306858\n",
      "Iteration 74125 | Loss: 2.304721\n",
      "Iteration 74150 | Loss: 2.302587\n",
      "Iteration 74175 | Loss: 2.300455\n",
      "Iteration 74200 | Loss: 2.298325\n",
      "Iteration 74225 | Loss: 2.296197\n",
      "Iteration 74250 | Loss: 2.294071\n",
      "Iteration 74275 | Loss: 2.291948\n",
      "Iteration 74300 | Loss: 2.289827\n",
      "Iteration 74325 | Loss: 2.287708\n",
      "Iteration 74350 | Loss: 2.285591\n",
      "Iteration 74375 | Loss: 2.283477\n",
      "Iteration 74400 | Loss: 2.281365\n",
      "Iteration 74425 | Loss: 2.279255\n",
      "Iteration 74450 | Loss: 2.277147\n",
      "Iteration 74475 | Loss: 2.275041\n",
      "Iteration 74500 | Loss: 2.272938\n",
      "Iteration 74525 | Loss: 2.270837\n",
      "Iteration 74550 | Loss: 2.268738\n",
      "Iteration 74575 | Loss: 2.266641\n",
      "Iteration 74600 | Loss: 2.264546\n",
      "Iteration 74625 | Loss: 2.262454\n",
      "Iteration 74650 | Loss: 2.260364\n",
      "Iteration 74675 | Loss: 2.258276\n",
      "Iteration 74700 | Loss: 2.256190\n",
      "Iteration 74725 | Loss: 2.254106\n",
      "Iteration 74750 | Loss: 2.252025\n",
      "Iteration 74775 | Loss: 2.249945\n",
      "Iteration 74800 | Loss: 2.247868\n",
      "Iteration 74825 | Loss: 2.245793\n",
      "Iteration 74850 | Loss: 2.243720\n",
      "Iteration 74875 | Loss: 2.241650\n",
      "Iteration 74900 | Loss: 2.239581\n",
      "Iteration 74925 | Loss: 2.237515\n",
      "Iteration 74950 | Loss: 2.235451\n",
      "Iteration 74975 | Loss: 2.233389\n",
      "Iteration 75000 | Loss: 2.231329\n",
      "Iteration 75025 | Loss: 2.229271\n",
      "Iteration 75050 | Loss: 2.227216\n",
      "Iteration 75075 | Loss: 2.225162\n",
      "Iteration 75100 | Loss: 2.223111\n",
      "Iteration 75125 | Loss: 2.221062\n",
      "Iteration 75150 | Loss: 2.219015\n",
      "Iteration 75175 | Loss: 2.216971\n",
      "Iteration 75200 | Loss: 2.214928\n",
      "Iteration 75225 | Loss: 2.212887\n",
      "Iteration 75250 | Loss: 2.210849\n",
      "Iteration 75275 | Loss: 2.208813\n",
      "Iteration 75300 | Loss: 2.206779\n",
      "Iteration 75325 | Loss: 2.204747\n",
      "Iteration 75350 | Loss: 2.202717\n",
      "Iteration 75375 | Loss: 2.200689\n",
      "Iteration 75400 | Loss: 2.198664\n",
      "Iteration 75425 | Loss: 2.196640\n",
      "Iteration 75450 | Loss: 2.194619\n",
      "Iteration 75475 | Loss: 2.192600\n",
      "Iteration 75500 | Loss: 2.190582\n",
      "Iteration 75525 | Loss: 2.188567\n",
      "Iteration 75550 | Loss: 2.186555\n",
      "Iteration 75575 | Loss: 2.184544\n",
      "Iteration 75600 | Loss: 2.182535\n",
      "Iteration 75625 | Loss: 2.180528\n",
      "Iteration 75650 | Loss: 2.178524\n",
      "Iteration 75675 | Loss: 2.176522\n",
      "Iteration 75700 | Loss: 2.174521\n",
      "Iteration 75725 | Loss: 2.172523\n",
      "Iteration 75750 | Loss: 2.170527\n",
      "Iteration 75775 | Loss: 2.168533\n",
      "Iteration 75800 | Loss: 2.166541\n",
      "Iteration 75825 | Loss: 2.164551\n",
      "Iteration 75850 | Loss: 2.162563\n",
      "Iteration 75875 | Loss: 2.160578\n",
      "Iteration 75900 | Loss: 2.158594\n",
      "Iteration 75925 | Loss: 2.156613\n",
      "Iteration 75950 | Loss: 2.154633\n",
      "Iteration 75975 | Loss: 2.152656\n",
      "Iteration 76000 | Loss: 2.150680\n",
      "Iteration 76025 | Loss: 2.148707\n",
      "Iteration 76050 | Loss: 2.146736\n",
      "Iteration 76075 | Loss: 2.144767\n",
      "Iteration 76100 | Loss: 2.142800\n",
      "Iteration 76125 | Loss: 2.140835\n",
      "Iteration 76150 | Loss: 2.138872\n",
      "Iteration 76175 | Loss: 2.136911\n",
      "Iteration 76200 | Loss: 2.134952\n",
      "Iteration 76225 | Loss: 2.132995\n",
      "Iteration 76250 | Loss: 2.131040\n",
      "Iteration 76275 | Loss: 2.129088\n",
      "Iteration 76300 | Loss: 2.127137\n",
      "Iteration 76325 | Loss: 2.125188\n",
      "Iteration 76350 | Loss: 2.123242\n",
      "Iteration 76375 | Loss: 2.121297\n",
      "Iteration 76400 | Loss: 2.119355\n",
      "Iteration 76425 | Loss: 2.117414\n",
      "Iteration 76450 | Loss: 2.115476\n",
      "Iteration 76475 | Loss: 2.113540\n",
      "Iteration 76500 | Loss: 2.111605\n",
      "Iteration 76525 | Loss: 2.109673\n",
      "Iteration 76550 | Loss: 2.107742\n",
      "Iteration 76575 | Loss: 2.105814\n",
      "Iteration 76600 | Loss: 2.103888\n",
      "Iteration 76625 | Loss: 2.101964\n",
      "Iteration 76650 | Loss: 2.100041\n",
      "Iteration 76675 | Loss: 2.098121\n",
      "Iteration 76700 | Loss: 2.096203\n",
      "Iteration 76725 | Loss: 2.094286\n",
      "Iteration 76750 | Loss: 2.092372\n",
      "Iteration 76775 | Loss: 2.090460\n",
      "Iteration 76800 | Loss: 2.088550\n",
      "Iteration 76825 | Loss: 2.086642\n",
      "Iteration 76850 | Loss: 2.084735\n",
      "Iteration 76875 | Loss: 2.082831\n",
      "Iteration 76900 | Loss: 2.080929\n",
      "Iteration 76925 | Loss: 2.079029\n",
      "Iteration 76950 | Loss: 2.077130\n",
      "Iteration 76975 | Loss: 2.075234\n",
      "Iteration 77000 | Loss: 2.073340\n",
      "Iteration 77025 | Loss: 2.071447\n",
      "Iteration 77050 | Loss: 2.069557\n",
      "Iteration 77075 | Loss: 2.067669\n",
      "Iteration 77100 | Loss: 2.065782\n",
      "Iteration 77125 | Loss: 2.063898\n",
      "Iteration 77150 | Loss: 2.062015\n",
      "Iteration 77175 | Loss: 2.060135\n",
      "Iteration 77200 | Loss: 2.058256\n",
      "Iteration 77225 | Loss: 2.056380\n",
      "Iteration 77250 | Loss: 2.054505\n",
      "Iteration 77275 | Loss: 2.052633\n",
      "Iteration 77300 | Loss: 2.050762\n",
      "Iteration 77325 | Loss: 2.048893\n",
      "Iteration 77350 | Loss: 2.047027\n",
      "Iteration 77375 | Loss: 2.045162\n",
      "Iteration 77400 | Loss: 2.043299\n",
      "Iteration 77425 | Loss: 2.041438\n",
      "Iteration 77450 | Loss: 2.039579\n",
      "Iteration 77475 | Loss: 2.037722\n",
      "Iteration 77500 | Loss: 2.035867\n",
      "Iteration 77525 | Loss: 2.034014\n",
      "Iteration 77550 | Loss: 2.032163\n",
      "Iteration 77575 | Loss: 2.030314\n",
      "Iteration 77600 | Loss: 2.028466\n",
      "Iteration 77625 | Loss: 2.026621\n",
      "Iteration 77650 | Loss: 2.024778\n",
      "Iteration 77675 | Loss: 2.022936\n",
      "Iteration 77700 | Loss: 2.021097\n",
      "Iteration 77725 | Loss: 2.019259\n",
      "Iteration 77750 | Loss: 2.017423\n",
      "Iteration 77775 | Loss: 2.015589\n",
      "Iteration 77800 | Loss: 2.013758\n",
      "Iteration 77825 | Loss: 2.011928\n",
      "Iteration 77850 | Loss: 2.010099\n",
      "Iteration 77875 | Loss: 2.008273\n",
      "Iteration 77900 | Loss: 2.006449\n",
      "Iteration 77925 | Loss: 2.004627\n",
      "Iteration 77950 | Loss: 2.002806\n",
      "Iteration 77975 | Loss: 2.000988\n",
      "Iteration 78000 | Loss: 1.999171\n",
      "Iteration 78025 | Loss: 1.997357\n",
      "Iteration 78050 | Loss: 1.995544\n",
      "Iteration 78075 | Loss: 1.993733\n",
      "Iteration 78100 | Loss: 1.991924\n",
      "Iteration 78125 | Loss: 1.990117\n",
      "Iteration 78150 | Loss: 1.988311\n",
      "Iteration 78175 | Loss: 1.986508\n",
      "Iteration 78200 | Loss: 1.984707\n",
      "Iteration 78225 | Loss: 1.982907\n",
      "Iteration 78250 | Loss: 1.981109\n",
      "Iteration 78275 | Loss: 1.979314\n",
      "Iteration 78300 | Loss: 1.977520\n",
      "Iteration 78325 | Loss: 1.975728\n",
      "Iteration 78350 | Loss: 1.973937\n",
      "Iteration 78375 | Loss: 1.972149\n",
      "Iteration 78400 | Loss: 1.970363\n",
      "Iteration 78425 | Loss: 1.968578\n",
      "Iteration 78450 | Loss: 1.966796\n",
      "Iteration 78475 | Loss: 1.965015\n",
      "Iteration 78500 | Loss: 1.963236\n",
      "Iteration 78525 | Loss: 1.961459\n",
      "Iteration 78550 | Loss: 1.959683\n",
      "Iteration 78575 | Loss: 1.957910\n",
      "Iteration 78600 | Loss: 1.956138\n",
      "Iteration 78625 | Loss: 1.954369\n",
      "Iteration 78650 | Loss: 1.952601\n",
      "Iteration 78675 | Loss: 1.950835\n",
      "Iteration 78700 | Loss: 1.949071\n",
      "Iteration 78725 | Loss: 1.947309\n",
      "Iteration 78750 | Loss: 1.945548\n",
      "Iteration 78775 | Loss: 1.943790\n",
      "Iteration 78800 | Loss: 1.942033\n",
      "Iteration 78825 | Loss: 1.940278\n",
      "Iteration 78850 | Loss: 1.938525\n",
      "Iteration 78875 | Loss: 1.936774\n",
      "Iteration 78900 | Loss: 1.935024\n",
      "Iteration 78925 | Loss: 1.933277\n",
      "Iteration 78950 | Loss: 1.931531\n",
      "Iteration 78975 | Loss: 1.929787\n",
      "Iteration 79000 | Loss: 1.928045\n",
      "Iteration 79025 | Loss: 1.926305\n",
      "Iteration 79050 | Loss: 1.924566\n",
      "Iteration 79075 | Loss: 1.922829\n",
      "Iteration 79100 | Loss: 1.921095\n",
      "Iteration 79125 | Loss: 1.919362\n",
      "Iteration 79150 | Loss: 1.917630\n",
      "Iteration 79175 | Loss: 1.915901\n",
      "Iteration 79200 | Loss: 1.914173\n",
      "Iteration 79225 | Loss: 1.912448\n",
      "Iteration 79250 | Loss: 1.910724\n",
      "Iteration 79275 | Loss: 1.909002\n",
      "Iteration 79300 | Loss: 1.907281\n",
      "Iteration 79325 | Loss: 1.905563\n",
      "Iteration 79350 | Loss: 1.903846\n",
      "Iteration 79375 | Loss: 1.902131\n",
      "Iteration 79400 | Loss: 1.900418\n",
      "Iteration 79425 | Loss: 1.898706\n",
      "Iteration 79450 | Loss: 1.896997\n",
      "Iteration 79475 | Loss: 1.895289\n",
      "Iteration 79500 | Loss: 1.893583\n",
      "Iteration 79525 | Loss: 1.891879\n",
      "Iteration 79550 | Loss: 1.890176\n",
      "Iteration 79575 | Loss: 1.888476\n",
      "Iteration 79600 | Loss: 1.886777\n",
      "Iteration 79625 | Loss: 1.885080\n",
      "Iteration 79650 | Loss: 1.883384\n",
      "Iteration 79675 | Loss: 1.881691\n",
      "Iteration 79700 | Loss: 1.879999\n",
      "Iteration 79725 | Loss: 1.878309\n",
      "Iteration 79750 | Loss: 1.876621\n",
      "Iteration 79775 | Loss: 1.874934\n",
      "Iteration 79800 | Loss: 1.873250\n",
      "Iteration 79825 | Loss: 1.871567\n",
      "Iteration 79850 | Loss: 1.869886\n",
      "Iteration 79875 | Loss: 1.868206\n",
      "Iteration 79900 | Loss: 1.866529\n",
      "Iteration 79925 | Loss: 1.864853\n",
      "Iteration 79950 | Loss: 1.863178\n",
      "Iteration 79975 | Loss: 1.861506\n",
      "Iteration 80000 | Loss: 1.859835\n",
      "Iteration 80025 | Loss: 1.858167\n",
      "Iteration 80050 | Loss: 1.856499\n",
      "Iteration 80075 | Loss: 1.854834\n",
      "Iteration 80100 | Loss: 1.853170\n",
      "Iteration 80125 | Loss: 1.851508\n",
      "Iteration 80150 | Loss: 1.849848\n",
      "Iteration 80175 | Loss: 1.848190\n",
      "Iteration 80200 | Loss: 1.846533\n",
      "Iteration 80225 | Loss: 1.844878\n",
      "Iteration 80250 | Loss: 1.843225\n",
      "Iteration 80275 | Loss: 1.841573\n",
      "Iteration 80300 | Loss: 1.839923\n",
      "Iteration 80325 | Loss: 1.838275\n",
      "Iteration 80350 | Loss: 1.836629\n",
      "Iteration 80375 | Loss: 1.834984\n",
      "Iteration 80400 | Loss: 1.833341\n",
      "Iteration 80425 | Loss: 1.831700\n",
      "Iteration 80450 | Loss: 1.830061\n",
      "Iteration 80475 | Loss: 1.828423\n",
      "Iteration 80500 | Loss: 1.826787\n",
      "Iteration 80525 | Loss: 1.825153\n",
      "Iteration 80550 | Loss: 1.823520\n",
      "Iteration 80575 | Loss: 1.821889\n",
      "Iteration 80600 | Loss: 1.820260\n",
      "Iteration 80625 | Loss: 1.818632\n",
      "Iteration 80650 | Loss: 1.817007\n",
      "Iteration 80675 | Loss: 1.815383\n",
      "Iteration 80700 | Loss: 1.813760\n",
      "Iteration 80725 | Loss: 1.812139\n",
      "Iteration 80750 | Loss: 1.810520\n",
      "Iteration 80775 | Loss: 1.808903\n",
      "Iteration 80800 | Loss: 1.807288\n",
      "Iteration 80825 | Loss: 1.805674\n",
      "Iteration 80850 | Loss: 1.804061\n",
      "Iteration 80875 | Loss: 1.802451\n",
      "Iteration 80900 | Loss: 1.800842\n",
      "Iteration 80925 | Loss: 1.799235\n",
      "Iteration 80950 | Loss: 1.797629\n",
      "Iteration 80975 | Loss: 1.796025\n",
      "Iteration 81000 | Loss: 1.794423\n",
      "Iteration 81025 | Loss: 1.792823\n",
      "Iteration 81050 | Loss: 1.791224\n",
      "Iteration 81075 | Loss: 1.789627\n",
      "Iteration 81100 | Loss: 1.788032\n",
      "Iteration 81125 | Loss: 1.786438\n",
      "Iteration 81150 | Loss: 1.784846\n",
      "Iteration 81175 | Loss: 1.783255\n",
      "Iteration 81200 | Loss: 1.781666\n",
      "Iteration 81225 | Loss: 1.780079\n",
      "Iteration 81250 | Loss: 1.778494\n",
      "Iteration 81275 | Loss: 1.776910\n",
      "Iteration 81300 | Loss: 1.775328\n",
      "Iteration 81325 | Loss: 1.773747\n",
      "Iteration 81350 | Loss: 1.772169\n",
      "Iteration 81375 | Loss: 1.770591\n",
      "Iteration 81400 | Loss: 1.769016\n",
      "Iteration 81425 | Loss: 1.767442\n",
      "Iteration 81450 | Loss: 1.765870\n",
      "Iteration 81475 | Loss: 1.764299\n",
      "Iteration 81500 | Loss: 1.762730\n",
      "Iteration 81525 | Loss: 1.761163\n",
      "Iteration 81550 | Loss: 1.759597\n",
      "Iteration 81575 | Loss: 1.758033\n",
      "Iteration 81600 | Loss: 1.756471\n",
      "Iteration 81625 | Loss: 1.754910\n",
      "Iteration 81650 | Loss: 1.753351\n",
      "Iteration 81675 | Loss: 1.751793\n",
      "Iteration 81700 | Loss: 1.750238\n",
      "Iteration 81725 | Loss: 1.748683\n",
      "Iteration 81750 | Loss: 1.747131\n",
      "Iteration 81775 | Loss: 1.745580\n",
      "Iteration 81800 | Loss: 1.744030\n",
      "Iteration 81825 | Loss: 1.742483\n",
      "Iteration 81850 | Loss: 1.740937\n",
      "Iteration 81875 | Loss: 1.739392\n",
      "Iteration 81900 | Loss: 1.737849\n",
      "Iteration 81925 | Loss: 1.736308\n",
      "Iteration 81950 | Loss: 1.734768\n",
      "Iteration 81975 | Loss: 1.733230\n",
      "Iteration 82000 | Loss: 1.731694\n",
      "Iteration 82025 | Loss: 1.730159\n",
      "Iteration 82050 | Loss: 1.728626\n",
      "Iteration 82075 | Loss: 1.727094\n",
      "Iteration 82100 | Loss: 1.725564\n",
      "Iteration 82125 | Loss: 1.724036\n",
      "Iteration 82150 | Loss: 1.722509\n",
      "Iteration 82175 | Loss: 1.720984\n",
      "Iteration 82200 | Loss: 1.719460\n",
      "Iteration 82225 | Loss: 1.717938\n",
      "Iteration 82250 | Loss: 1.716417\n",
      "Iteration 82275 | Loss: 1.714899\n",
      "Iteration 82300 | Loss: 1.713381\n",
      "Iteration 82325 | Loss: 1.711866\n",
      "Iteration 82350 | Loss: 1.710352\n",
      "Iteration 82375 | Loss: 1.708839\n",
      "Iteration 82400 | Loss: 1.707328\n",
      "Iteration 82425 | Loss: 1.705819\n",
      "Iteration 82450 | Loss: 1.704311\n",
      "Iteration 82475 | Loss: 1.702805\n",
      "Iteration 82500 | Loss: 1.701300\n",
      "Iteration 82525 | Loss: 1.699797\n",
      "Iteration 82550 | Loss: 1.698296\n",
      "Iteration 82575 | Loss: 1.696796\n",
      "Iteration 82600 | Loss: 1.695298\n",
      "Iteration 82625 | Loss: 1.693801\n",
      "Iteration 82650 | Loss: 1.692306\n",
      "Iteration 82675 | Loss: 1.690812\n",
      "Iteration 82700 | Loss: 1.689320\n",
      "Iteration 82725 | Loss: 1.687829\n",
      "Iteration 82750 | Loss: 1.686340\n",
      "Iteration 82775 | Loss: 1.684853\n",
      "Iteration 82800 | Loss: 1.683367\n",
      "Iteration 82825 | Loss: 1.681883\n",
      "Iteration 82850 | Loss: 1.680400\n",
      "Iteration 82875 | Loss: 1.678919\n",
      "Iteration 82900 | Loss: 1.677439\n",
      "Iteration 82925 | Loss: 1.675961\n",
      "Iteration 82950 | Loss: 1.674485\n",
      "Iteration 82975 | Loss: 1.673010\n",
      "Iteration 83000 | Loss: 1.671536\n",
      "Iteration 83025 | Loss: 1.670065\n",
      "Iteration 83050 | Loss: 1.668594\n",
      "Iteration 83075 | Loss: 1.667125\n",
      "Iteration 83100 | Loss: 1.665658\n",
      "Iteration 83125 | Loss: 1.664192\n",
      "Iteration 83150 | Loss: 1.662728\n",
      "Iteration 83175 | Loss: 1.661265\n",
      "Iteration 83200 | Loss: 1.659804\n",
      "Iteration 83225 | Loss: 1.658345\n",
      "Iteration 83250 | Loss: 1.656887\n",
      "Iteration 83275 | Loss: 1.655430\n",
      "Iteration 83300 | Loss: 1.653975\n",
      "Iteration 83325 | Loss: 1.652521\n",
      "Iteration 83350 | Loss: 1.651069\n",
      "Iteration 83375 | Loss: 1.649619\n",
      "Iteration 83400 | Loss: 1.648170\n",
      "Iteration 83425 | Loss: 1.646723\n",
      "Iteration 83450 | Loss: 1.645277\n",
      "Iteration 83475 | Loss: 1.643832\n",
      "Iteration 83500 | Loss: 1.642389\n",
      "Iteration 83525 | Loss: 1.640948\n",
      "Iteration 83550 | Loss: 1.639508\n",
      "Iteration 83575 | Loss: 1.638070\n",
      "Iteration 83600 | Loss: 1.636633\n",
      "Iteration 83625 | Loss: 1.635197\n",
      "Iteration 83650 | Loss: 1.633763\n",
      "Iteration 83675 | Loss: 1.632331\n",
      "Iteration 83700 | Loss: 1.630900\n",
      "Iteration 83725 | Loss: 1.629471\n",
      "Iteration 83750 | Loss: 1.628043\n",
      "Iteration 83775 | Loss: 1.626617\n",
      "Iteration 83800 | Loss: 1.625192\n",
      "Iteration 83825 | Loss: 1.623768\n",
      "Iteration 83850 | Loss: 1.622346\n",
      "Iteration 83875 | Loss: 1.620926\n",
      "Iteration 83900 | Loss: 1.619507\n",
      "Iteration 83925 | Loss: 1.618089\n",
      "Iteration 83950 | Loss: 1.616673\n",
      "Iteration 83975 | Loss: 1.615259\n",
      "Iteration 84000 | Loss: 1.613846\n",
      "Iteration 84025 | Loss: 1.612434\n",
      "Iteration 84050 | Loss: 1.611024\n",
      "Iteration 84075 | Loss: 1.609616\n",
      "Iteration 84100 | Loss: 1.608209\n",
      "Iteration 84125 | Loss: 1.606803\n",
      "Iteration 84150 | Loss: 1.605399\n",
      "Iteration 84175 | Loss: 1.603996\n",
      "Iteration 84200 | Loss: 1.602595\n",
      "Iteration 84225 | Loss: 1.601195\n",
      "Iteration 84250 | Loss: 1.599797\n",
      "Iteration 84275 | Loss: 1.598400\n",
      "Iteration 84300 | Loss: 1.597005\n",
      "Iteration 84325 | Loss: 1.595611\n",
      "Iteration 84350 | Loss: 1.594218\n",
      "Iteration 84375 | Loss: 1.592827\n",
      "Iteration 84400 | Loss: 1.591438\n",
      "Iteration 84425 | Loss: 1.590050\n",
      "Iteration 84450 | Loss: 1.588663\n",
      "Iteration 84475 | Loss: 1.587278\n",
      "Iteration 84500 | Loss: 1.585894\n",
      "Iteration 84525 | Loss: 1.584512\n",
      "Iteration 84550 | Loss: 1.583131\n",
      "Iteration 84575 | Loss: 1.581751\n",
      "Iteration 84600 | Loss: 1.580373\n",
      "Iteration 84625 | Loss: 1.578997\n",
      "Iteration 84650 | Loss: 1.577622\n",
      "Iteration 84675 | Loss: 1.576248\n",
      "Iteration 84700 | Loss: 1.574876\n",
      "Iteration 84725 | Loss: 1.573505\n",
      "Iteration 84750 | Loss: 1.572136\n",
      "Iteration 84775 | Loss: 1.570768\n",
      "Iteration 84800 | Loss: 1.569401\n",
      "Iteration 84825 | Loss: 1.568036\n",
      "Iteration 84850 | Loss: 1.566673\n",
      "Iteration 84875 | Loss: 1.565311\n",
      "Iteration 84900 | Loss: 1.563950\n",
      "Iteration 84925 | Loss: 1.562590\n",
      "Iteration 84950 | Loss: 1.561233\n",
      "Iteration 84975 | Loss: 1.559876\n",
      "Iteration 85000 | Loss: 1.558521\n",
      "Iteration 85025 | Loss: 1.557167\n",
      "Iteration 85050 | Loss: 1.555815\n",
      "Iteration 85075 | Loss: 1.554464\n",
      "Iteration 85100 | Loss: 1.553115\n",
      "Iteration 85125 | Loss: 1.551767\n",
      "Iteration 85150 | Loss: 1.550420\n",
      "Iteration 85175 | Loss: 1.549075\n",
      "Iteration 85200 | Loss: 1.547731\n",
      "Iteration 85225 | Loss: 1.546389\n",
      "Iteration 85250 | Loss: 1.545048\n",
      "Iteration 85275 | Loss: 1.543708\n",
      "Iteration 85300 | Loss: 1.542370\n",
      "Iteration 85325 | Loss: 1.541033\n",
      "Iteration 85350 | Loss: 1.539698\n",
      "Iteration 85375 | Loss: 1.538364\n",
      "Iteration 85400 | Loss: 1.537031\n",
      "Iteration 85425 | Loss: 1.535700\n",
      "Iteration 85450 | Loss: 1.534370\n",
      "Iteration 85475 | Loss: 1.533042\n",
      "Iteration 85500 | Loss: 1.531715\n",
      "Iteration 85525 | Loss: 1.530389\n",
      "Iteration 85550 | Loss: 1.529065\n",
      "Iteration 85575 | Loss: 1.527742\n",
      "Iteration 85600 | Loss: 1.526421\n",
      "Iteration 85625 | Loss: 1.525101\n",
      "Iteration 85650 | Loss: 1.523782\n",
      "Iteration 85675 | Loss: 1.522465\n",
      "Iteration 85700 | Loss: 1.521149\n",
      "Iteration 85725 | Loss: 1.519834\n",
      "Iteration 85750 | Loss: 1.518521\n",
      "Iteration 85775 | Loss: 1.517209\n",
      "Iteration 85800 | Loss: 1.515899\n",
      "Iteration 85825 | Loss: 1.514590\n",
      "Iteration 85850 | Loss: 1.513282\n",
      "Iteration 85875 | Loss: 1.511976\n",
      "Iteration 85900 | Loss: 1.510671\n",
      "Iteration 85925 | Loss: 1.509367\n",
      "Iteration 85950 | Loss: 1.508065\n",
      "Iteration 85975 | Loss: 1.506764\n",
      "Iteration 86000 | Loss: 1.505464\n",
      "Iteration 86025 | Loss: 1.504166\n",
      "Iteration 86050 | Loss: 1.502869\n",
      "Iteration 86075 | Loss: 1.501574\n",
      "Iteration 86100 | Loss: 1.500280\n",
      "Iteration 86125 | Loss: 1.498987\n",
      "Iteration 86150 | Loss: 1.497696\n",
      "Iteration 86175 | Loss: 1.496406\n",
      "Iteration 86200 | Loss: 1.495117\n",
      "Iteration 86225 | Loss: 1.493830\n",
      "Iteration 86250 | Loss: 1.492544\n",
      "Iteration 86275 | Loss: 1.491259\n",
      "Iteration 86300 | Loss: 1.489976\n",
      "Iteration 86325 | Loss: 1.488694\n",
      "Iteration 86350 | Loss: 1.487413\n",
      "Iteration 86375 | Loss: 1.486134\n",
      "Iteration 86400 | Loss: 1.484856\n",
      "Iteration 86425 | Loss: 1.483579\n",
      "Iteration 86450 | Loss: 1.482304\n",
      "Iteration 86475 | Loss: 1.481030\n",
      "Iteration 86500 | Loss: 1.479757\n",
      "Iteration 86525 | Loss: 1.478486\n",
      "Iteration 86550 | Loss: 1.477216\n",
      "Iteration 86575 | Loss: 1.475948\n",
      "Iteration 86600 | Loss: 1.474680\n",
      "Iteration 86625 | Loss: 1.473414\n",
      "Iteration 86650 | Loss: 1.472150\n",
      "Iteration 86675 | Loss: 1.470886\n",
      "Iteration 86700 | Loss: 1.469624\n",
      "Iteration 86725 | Loss: 1.468364\n",
      "Iteration 86750 | Loss: 1.467104\n",
      "Iteration 86775 | Loss: 1.465846\n",
      "Iteration 86800 | Loss: 1.464590\n",
      "Iteration 86825 | Loss: 1.463334\n",
      "Iteration 86850 | Loss: 1.462080\n",
      "Iteration 86875 | Loss: 1.460827\n",
      "Iteration 86900 | Loss: 1.459576\n",
      "Iteration 86925 | Loss: 1.458326\n",
      "Iteration 86950 | Loss: 1.457077\n",
      "Iteration 86975 | Loss: 1.455829\n",
      "Iteration 87000 | Loss: 1.454583\n",
      "Iteration 87025 | Loss: 1.453338\n",
      "Iteration 87050 | Loss: 1.452094\n",
      "Iteration 87075 | Loss: 1.450852\n",
      "Iteration 87100 | Loss: 1.449611\n",
      "Iteration 87125 | Loss: 1.448371\n",
      "Iteration 87150 | Loss: 1.447133\n",
      "Iteration 87175 | Loss: 1.445896\n",
      "Iteration 87200 | Loss: 1.444660\n",
      "Iteration 87225 | Loss: 1.443425\n",
      "Iteration 87250 | Loss: 1.442192\n",
      "Iteration 87275 | Loss: 1.440960\n",
      "Iteration 87300 | Loss: 1.439729\n",
      "Iteration 87325 | Loss: 1.438500\n",
      "Iteration 87350 | Loss: 1.437272\n",
      "Iteration 87375 | Loss: 1.436045\n",
      "Iteration 87400 | Loss: 1.434819\n",
      "Iteration 87425 | Loss: 1.433595\n",
      "Iteration 87450 | Loss: 1.432372\n",
      "Iteration 87475 | Loss: 1.431151\n",
      "Iteration 87500 | Loss: 1.429930\n",
      "Iteration 87525 | Loss: 1.428711\n",
      "Iteration 87550 | Loss: 1.427493\n",
      "Iteration 87575 | Loss: 1.426276\n",
      "Iteration 87600 | Loss: 1.425061\n",
      "Iteration 87625 | Loss: 1.423847\n",
      "Iteration 87650 | Loss: 1.422634\n",
      "Iteration 87675 | Loss: 1.421423\n",
      "Iteration 87700 | Loss: 1.420212\n",
      "Iteration 87725 | Loss: 1.419003\n",
      "Iteration 87750 | Loss: 1.417796\n",
      "Iteration 87775 | Loss: 1.416589\n",
      "Iteration 87800 | Loss: 1.415384\n",
      "Iteration 87825 | Loss: 1.414180\n",
      "Iteration 87850 | Loss: 1.412977\n",
      "Iteration 87875 | Loss: 1.411776\n",
      "Iteration 87900 | Loss: 1.410576\n",
      "Iteration 87925 | Loss: 1.409377\n",
      "Iteration 87950 | Loss: 1.408179\n",
      "Iteration 87975 | Loss: 1.406983\n",
      "Iteration 88000 | Loss: 1.405788\n",
      "Iteration 88025 | Loss: 1.404594\n",
      "Iteration 88050 | Loss: 1.403401\n",
      "Iteration 88075 | Loss: 1.402210\n",
      "Iteration 88100 | Loss: 1.401020\n",
      "Iteration 88125 | Loss: 1.399831\n",
      "Iteration 88150 | Loss: 1.398643\n",
      "Iteration 88175 | Loss: 1.397457\n",
      "Iteration 88200 | Loss: 1.396271\n",
      "Iteration 88225 | Loss: 1.395087\n",
      "Iteration 88250 | Loss: 1.393905\n",
      "Iteration 88275 | Loss: 1.392723\n",
      "Iteration 88300 | Loss: 1.391543\n",
      "Iteration 88325 | Loss: 1.390364\n",
      "Iteration 88350 | Loss: 1.389186\n",
      "Iteration 88375 | Loss: 1.388010\n",
      "Iteration 88400 | Loss: 1.386834\n",
      "Iteration 88425 | Loss: 1.385660\n",
      "Iteration 88450 | Loss: 1.384487\n",
      "Iteration 88475 | Loss: 1.383316\n",
      "Iteration 88500 | Loss: 1.382145\n",
      "Iteration 88525 | Loss: 1.380976\n",
      "Iteration 88550 | Loss: 1.379808\n",
      "Iteration 88575 | Loss: 1.378642\n",
      "Iteration 88600 | Loss: 1.377476\n",
      "Iteration 88625 | Loss: 1.376312\n",
      "Iteration 88650 | Loss: 1.375149\n",
      "Iteration 88675 | Loss: 1.373987\n",
      "Iteration 88700 | Loss: 1.372826\n",
      "Iteration 88725 | Loss: 1.371667\n",
      "Iteration 88750 | Loss: 1.370509\n",
      "Iteration 88775 | Loss: 1.369352\n",
      "Iteration 88800 | Loss: 1.368196\n",
      "Iteration 88825 | Loss: 1.367041\n",
      "Iteration 88850 | Loss: 1.365888\n",
      "Iteration 88875 | Loss: 1.364736\n",
      "Iteration 88900 | Loss: 1.363585\n",
      "Iteration 88925 | Loss: 1.362435\n",
      "Iteration 88950 | Loss: 1.361286\n",
      "Iteration 88975 | Loss: 1.360139\n",
      "Iteration 89000 | Loss: 1.358993\n",
      "Iteration 89025 | Loss: 1.357848\n",
      "Iteration 89050 | Loss: 1.356704\n",
      "Iteration 89075 | Loss: 1.355562\n",
      "Iteration 89100 | Loss: 1.354420\n",
      "Iteration 89125 | Loss: 1.353280\n",
      "Iteration 89150 | Loss: 1.352141\n",
      "Iteration 89175 | Loss: 1.351003\n",
      "Iteration 89200 | Loss: 1.349867\n",
      "Iteration 89225 | Loss: 1.348731\n",
      "Iteration 89250 | Loss: 1.347597\n",
      "Iteration 89275 | Loss: 1.346464\n",
      "Iteration 89300 | Loss: 1.345332\n",
      "Iteration 89325 | Loss: 1.344201\n",
      "Iteration 89350 | Loss: 1.343072\n",
      "Iteration 89375 | Loss: 1.341944\n",
      "Iteration 89400 | Loss: 1.340816\n",
      "Iteration 89425 | Loss: 1.339691\n",
      "Iteration 89450 | Loss: 1.338566\n",
      "Iteration 89475 | Loss: 1.337442\n",
      "Iteration 89500 | Loss: 1.336320\n",
      "Iteration 89525 | Loss: 1.335199\n",
      "Iteration 89550 | Loss: 1.334078\n",
      "Iteration 89575 | Loss: 1.332960\n",
      "Iteration 89600 | Loss: 1.331842\n",
      "Iteration 89625 | Loss: 1.330725\n",
      "Iteration 89650 | Loss: 1.329610\n",
      "Iteration 89675 | Loss: 1.328496\n",
      "Iteration 89700 | Loss: 1.327383\n",
      "Iteration 89725 | Loss: 1.326271\n",
      "Iteration 89750 | Loss: 1.325160\n",
      "Iteration 89775 | Loss: 1.324050\n",
      "Iteration 89800 | Loss: 1.322942\n",
      "Iteration 89825 | Loss: 1.321835\n",
      "Iteration 89850 | Loss: 1.320729\n",
      "Iteration 89875 | Loss: 1.319624\n",
      "Iteration 89900 | Loss: 1.318520\n",
      "Iteration 89925 | Loss: 1.317417\n",
      "Iteration 89950 | Loss: 1.316316\n",
      "Iteration 89975 | Loss: 1.315215\n",
      "Iteration 90000 | Loss: 1.314116\n",
      "Iteration 90025 | Loss: 1.313018\n",
      "Iteration 90050 | Loss: 1.311921\n",
      "Iteration 90075 | Loss: 1.310826\n",
      "Iteration 90100 | Loss: 1.309731\n",
      "Iteration 90125 | Loss: 1.308638\n",
      "Iteration 90150 | Loss: 1.307545\n",
      "Iteration 90175 | Loss: 1.306454\n",
      "Iteration 90200 | Loss: 1.305364\n",
      "Iteration 90225 | Loss: 1.304275\n",
      "Iteration 90250 | Loss: 1.303188\n",
      "Iteration 90275 | Loss: 1.302101\n",
      "Iteration 90300 | Loss: 1.301015\n",
      "Iteration 90325 | Loss: 1.299931\n",
      "Iteration 90350 | Loss: 1.298848\n",
      "Iteration 90375 | Loss: 1.297766\n",
      "Iteration 90400 | Loss: 1.296685\n",
      "Iteration 90425 | Loss: 1.295605\n",
      "Iteration 90450 | Loss: 1.294527\n",
      "Iteration 90475 | Loss: 1.293449\n",
      "Iteration 90500 | Loss: 1.292373\n",
      "Iteration 90525 | Loss: 1.291297\n",
      "Iteration 90550 | Loss: 1.290223\n",
      "Iteration 90575 | Loss: 1.289150\n",
      "Iteration 90600 | Loss: 1.288078\n",
      "Iteration 90625 | Loss: 1.287007\n",
      "Iteration 90650 | Loss: 1.285938\n",
      "Iteration 90675 | Loss: 1.284869\n",
      "Iteration 90700 | Loss: 1.283802\n",
      "Iteration 90725 | Loss: 1.282735\n",
      "Iteration 90750 | Loss: 1.281670\n",
      "Iteration 90775 | Loss: 1.280606\n",
      "Iteration 90800 | Loss: 1.279543\n",
      "Iteration 90825 | Loss: 1.278481\n",
      "Iteration 90850 | Loss: 1.277420\n",
      "Iteration 90875 | Loss: 1.276361\n",
      "Iteration 90900 | Loss: 1.275302\n",
      "Iteration 90925 | Loss: 1.274245\n",
      "Iteration 90950 | Loss: 1.273189\n",
      "Iteration 90975 | Loss: 1.272133\n",
      "Iteration 91000 | Loss: 1.271079\n",
      "Iteration 91025 | Loss: 1.270026\n",
      "Iteration 91050 | Loss: 1.268974\n",
      "Iteration 91075 | Loss: 1.267923\n",
      "Iteration 91100 | Loss: 1.266874\n",
      "Iteration 91125 | Loss: 1.265825\n",
      "Iteration 91150 | Loss: 1.264778\n",
      "Iteration 91175 | Loss: 1.263731\n",
      "Iteration 91200 | Loss: 1.262686\n",
      "Iteration 91225 | Loss: 1.261642\n",
      "Iteration 91250 | Loss: 1.260598\n",
      "Iteration 91275 | Loss: 1.259556\n",
      "Iteration 91300 | Loss: 1.258515\n",
      "Iteration 91325 | Loss: 1.257476\n",
      "Iteration 91350 | Loss: 1.256437\n",
      "Iteration 91375 | Loss: 1.255399\n",
      "Iteration 91400 | Loss: 1.254362\n",
      "Iteration 91425 | Loss: 1.253327\n",
      "Iteration 91450 | Loss: 1.252292\n",
      "Iteration 91475 | Loss: 1.251259\n",
      "Iteration 91500 | Loss: 1.250227\n",
      "Iteration 91525 | Loss: 1.249196\n",
      "Iteration 91550 | Loss: 1.248165\n",
      "Iteration 91575 | Loss: 1.247136\n",
      "Iteration 91600 | Loss: 1.246108\n",
      "Iteration 91625 | Loss: 1.245082\n",
      "Iteration 91650 | Loss: 1.244056\n",
      "Iteration 91675 | Loss: 1.243031\n",
      "Iteration 91700 | Loss: 1.242007\n",
      "Iteration 91725 | Loss: 1.240985\n",
      "Iteration 91750 | Loss: 1.239963\n",
      "Iteration 91775 | Loss: 1.238943\n",
      "Iteration 91800 | Loss: 1.237923\n",
      "Iteration 91825 | Loss: 1.236905\n",
      "Iteration 91850 | Loss: 1.235888\n",
      "Iteration 91875 | Loss: 1.234871\n",
      "Iteration 91900 | Loss: 1.233856\n",
      "Iteration 91925 | Loss: 1.232842\n",
      "Iteration 91950 | Loss: 1.231829\n",
      "Iteration 91975 | Loss: 1.230817\n",
      "Iteration 92000 | Loss: 1.229806\n",
      "Iteration 92025 | Loss: 1.228796\n",
      "Iteration 92050 | Loss: 1.227788\n",
      "Iteration 92075 | Loss: 1.226780\n",
      "Iteration 92100 | Loss: 1.225773\n",
      "Iteration 92125 | Loss: 1.224768\n",
      "Iteration 92150 | Loss: 1.223763\n",
      "Iteration 92175 | Loss: 1.222759\n",
      "Iteration 92200 | Loss: 1.221757\n",
      "Iteration 92225 | Loss: 1.220756\n",
      "Iteration 92250 | Loss: 1.219755\n",
      "Iteration 92275 | Loss: 1.218756\n",
      "Iteration 92300 | Loss: 1.217757\n",
      "Iteration 92325 | Loss: 1.216760\n",
      "Iteration 92350 | Loss: 1.215764\n",
      "Iteration 92375 | Loss: 1.214769\n",
      "Iteration 92400 | Loss: 1.213775\n",
      "Iteration 92425 | Loss: 1.212782\n",
      "Iteration 92450 | Loss: 1.211790\n",
      "Iteration 92475 | Loss: 1.210799\n",
      "Iteration 92500 | Loss: 1.209809\n",
      "Iteration 92525 | Loss: 1.208820\n",
      "Iteration 92550 | Loss: 1.207832\n",
      "Iteration 92575 | Loss: 1.206845\n",
      "Iteration 92600 | Loss: 1.205859\n",
      "Iteration 92625 | Loss: 1.204874\n",
      "Iteration 92650 | Loss: 1.203890\n",
      "Iteration 92675 | Loss: 1.202908\n",
      "Iteration 92700 | Loss: 1.201926\n",
      "Iteration 92725 | Loss: 1.200945\n",
      "Iteration 92750 | Loss: 1.199966\n",
      "Iteration 92775 | Loss: 1.198987\n",
      "Iteration 92800 | Loss: 1.198009\n",
      "Iteration 92825 | Loss: 1.197033\n",
      "Iteration 92850 | Loss: 1.196057\n",
      "Iteration 92875 | Loss: 1.195083\n",
      "Iteration 92900 | Loss: 1.194109\n",
      "Iteration 92925 | Loss: 1.193136\n",
      "Iteration 92950 | Loss: 1.192165\n",
      "Iteration 92975 | Loss: 1.191194\n",
      "Iteration 93000 | Loss: 1.190225\n",
      "Iteration 93025 | Loss: 1.189257\n",
      "Iteration 93050 | Loss: 1.188289\n",
      "Iteration 93075 | Loss: 1.187323\n",
      "Iteration 93100 | Loss: 1.186357\n",
      "Iteration 93125 | Loss: 1.185393\n",
      "Iteration 93150 | Loss: 1.184429\n",
      "Iteration 93175 | Loss: 1.183467\n",
      "Iteration 93200 | Loss: 1.182506\n",
      "Iteration 93225 | Loss: 1.181545\n",
      "Iteration 93250 | Loss: 1.180586\n",
      "Iteration 93275 | Loss: 1.179627\n",
      "Iteration 93300 | Loss: 1.178670\n",
      "Iteration 93325 | Loss: 1.177714\n",
      "Iteration 93350 | Loss: 1.176758\n",
      "Iteration 93375 | Loss: 1.175804\n",
      "Iteration 93400 | Loss: 1.174851\n",
      "Iteration 93425 | Loss: 1.173898\n",
      "Iteration 93450 | Loss: 1.172947\n",
      "Iteration 93475 | Loss: 1.171996\n",
      "Iteration 93500 | Loss: 1.171047\n",
      "Iteration 93525 | Loss: 1.170099\n",
      "Iteration 93550 | Loss: 1.169151\n",
      "Iteration 93575 | Loss: 1.168205\n",
      "Iteration 93600 | Loss: 1.167259\n",
      "Iteration 93625 | Loss: 1.166315\n",
      "Iteration 93650 | Loss: 1.165371\n",
      "Iteration 93675 | Loss: 1.164429\n",
      "Iteration 93700 | Loss: 1.163487\n",
      "Iteration 93725 | Loss: 1.162547\n",
      "Iteration 93750 | Loss: 1.161607\n",
      "Iteration 93775 | Loss: 1.160669\n",
      "Iteration 93800 | Loss: 1.159731\n",
      "Iteration 93825 | Loss: 1.158795\n",
      "Iteration 93850 | Loss: 1.157859\n",
      "Iteration 93875 | Loss: 1.156925\n",
      "Iteration 93900 | Loss: 1.155991\n",
      "Iteration 93925 | Loss: 1.155058\n",
      "Iteration 93950 | Loss: 1.154127\n",
      "Iteration 93975 | Loss: 1.153196\n",
      "Iteration 94000 | Loss: 1.152266\n",
      "Iteration 94025 | Loss: 1.151337\n",
      "Iteration 94050 | Loss: 1.150409\n",
      "Iteration 94075 | Loss: 1.149483\n",
      "Iteration 94100 | Loss: 1.148557\n",
      "Iteration 94125 | Loss: 1.147632\n",
      "Iteration 94150 | Loss: 1.146708\n",
      "Iteration 94175 | Loss: 1.145785\n",
      "Iteration 94200 | Loss: 1.144863\n",
      "Iteration 94225 | Loss: 1.143942\n",
      "Iteration 94250 | Loss: 1.143022\n",
      "Iteration 94275 | Loss: 1.142103\n",
      "Iteration 94300 | Loss: 1.141185\n",
      "Iteration 94325 | Loss: 1.140268\n",
      "Iteration 94350 | Loss: 1.139351\n",
      "Iteration 94375 | Loss: 1.138436\n",
      "Iteration 94400 | Loss: 1.137522\n",
      "Iteration 94425 | Loss: 1.136608\n",
      "Iteration 94450 | Loss: 1.135696\n",
      "Iteration 94475 | Loss: 1.134785\n",
      "Iteration 94500 | Loss: 1.133874\n",
      "Iteration 94525 | Loss: 1.132964\n",
      "Iteration 94550 | Loss: 1.132056\n",
      "Iteration 94575 | Loss: 1.131148\n",
      "Iteration 94600 | Loss: 1.130242\n",
      "Iteration 94625 | Loss: 1.129336\n",
      "Iteration 94650 | Loss: 1.128431\n",
      "Iteration 94675 | Loss: 1.127527\n",
      "Iteration 94700 | Loss: 1.126624\n",
      "Iteration 94725 | Loss: 1.125722\n",
      "Iteration 94750 | Loss: 1.124821\n",
      "Iteration 94775 | Loss: 1.123921\n",
      "Iteration 94800 | Loss: 1.123022\n",
      "Iteration 94825 | Loss: 1.122124\n",
      "Iteration 94850 | Loss: 1.121227\n",
      "Iteration 94875 | Loss: 1.120330\n",
      "Iteration 94900 | Loss: 1.119435\n",
      "Iteration 94925 | Loss: 1.118541\n",
      "Iteration 94950 | Loss: 1.117647\n",
      "Iteration 94975 | Loss: 1.116754\n",
      "Iteration 95000 | Loss: 1.115863\n",
      "Iteration 95025 | Loss: 1.114972\n",
      "Iteration 95050 | Loss: 1.114082\n",
      "Iteration 95075 | Loss: 1.113193\n",
      "Iteration 95100 | Loss: 1.112306\n",
      "Iteration 95125 | Loss: 1.111419\n",
      "Iteration 95150 | Loss: 1.110533\n",
      "Iteration 95175 | Loss: 1.109647\n",
      "Iteration 95200 | Loss: 1.108763\n",
      "Iteration 95225 | Loss: 1.107880\n",
      "Iteration 95250 | Loss: 1.106998\n",
      "Iteration 95275 | Loss: 1.106116\n",
      "Iteration 95300 | Loss: 1.105236\n",
      "Iteration 95325 | Loss: 1.104356\n",
      "Iteration 95350 | Loss: 1.103477\n",
      "Iteration 95375 | Loss: 1.102600\n",
      "Iteration 95400 | Loss: 1.101723\n",
      "Iteration 95425 | Loss: 1.100847\n",
      "Iteration 95450 | Loss: 1.099972\n",
      "Iteration 95475 | Loss: 1.099098\n",
      "Iteration 95500 | Loss: 1.098225\n",
      "Iteration 95525 | Loss: 1.097352\n",
      "Iteration 95550 | Loss: 1.096481\n",
      "Iteration 95575 | Loss: 1.095610\n",
      "Iteration 95600 | Loss: 1.094741\n",
      "Iteration 95625 | Loss: 1.093872\n",
      "Iteration 95650 | Loss: 1.093005\n",
      "Iteration 95675 | Loss: 1.092138\n",
      "Iteration 95700 | Loss: 1.091272\n",
      "Iteration 95725 | Loss: 1.090407\n",
      "Iteration 95750 | Loss: 1.089543\n",
      "Iteration 95775 | Loss: 1.088680\n",
      "Iteration 95800 | Loss: 1.087817\n",
      "Iteration 95825 | Loss: 1.086956\n",
      "Iteration 95850 | Loss: 1.086095\n",
      "Iteration 95875 | Loss: 1.085236\n",
      "Iteration 95900 | Loss: 1.084377\n",
      "Iteration 95925 | Loss: 1.083519\n",
      "Iteration 95950 | Loss: 1.082663\n",
      "Iteration 95975 | Loss: 1.081807\n",
      "Iteration 96000 | Loss: 1.080951\n",
      "Iteration 96025 | Loss: 1.080097\n",
      "Iteration 96050 | Loss: 1.079244\n",
      "Iteration 96075 | Loss: 1.078391\n",
      "Iteration 96100 | Loss: 1.077540\n",
      "Iteration 96125 | Loss: 1.076689\n",
      "Iteration 96150 | Loss: 1.075840\n",
      "Iteration 96175 | Loss: 1.074991\n",
      "Iteration 96200 | Loss: 1.074143\n",
      "Iteration 96225 | Loss: 1.073296\n",
      "Iteration 96250 | Loss: 1.072449\n",
      "Iteration 96275 | Loss: 1.071604\n",
      "Iteration 96300 | Loss: 1.070760\n",
      "Iteration 96325 | Loss: 1.069916\n",
      "Iteration 96350 | Loss: 1.069073\n",
      "Iteration 96375 | Loss: 1.068232\n",
      "Iteration 96400 | Loss: 1.067391\n",
      "Iteration 96425 | Loss: 1.066551\n",
      "Iteration 96450 | Loss: 1.065712\n",
      "Iteration 96475 | Loss: 1.064873\n",
      "Iteration 96500 | Loss: 1.064036\n",
      "Iteration 96525 | Loss: 1.063199\n",
      "Iteration 96550 | Loss: 1.062364\n",
      "Iteration 96575 | Loss: 1.061529\n",
      "Iteration 96600 | Loss: 1.060695\n",
      "Iteration 96625 | Loss: 1.059862\n",
      "Iteration 96650 | Loss: 1.059030\n",
      "Iteration 96675 | Loss: 1.058199\n",
      "Iteration 96700 | Loss: 1.057368\n",
      "Iteration 96725 | Loss: 1.056539\n",
      "Iteration 96750 | Loss: 1.055710\n",
      "Iteration 96775 | Loss: 1.054882\n",
      "Iteration 96800 | Loss: 1.054055\n",
      "Iteration 96825 | Loss: 1.053229\n",
      "Iteration 96850 | Loss: 1.052404\n",
      "Iteration 96875 | Loss: 1.051580\n",
      "Iteration 96900 | Loss: 1.050756\n",
      "Iteration 96925 | Loss: 1.049933\n",
      "Iteration 96950 | Loss: 1.049112\n",
      "Iteration 96975 | Loss: 1.048291\n",
      "Iteration 97000 | Loss: 1.047471\n",
      "Iteration 97025 | Loss: 1.046652\n",
      "Iteration 97050 | Loss: 1.045833\n",
      "Iteration 97075 | Loss: 1.045016\n",
      "Iteration 97100 | Loss: 1.044199\n",
      "Iteration 97125 | Loss: 1.043383\n",
      "Iteration 97150 | Loss: 1.042568\n",
      "Iteration 97175 | Loss: 1.041754\n",
      "Iteration 97200 | Loss: 1.040941\n",
      "Iteration 97225 | Loss: 1.040129\n",
      "Iteration 97250 | Loss: 1.039317\n",
      "Iteration 97275 | Loss: 1.038506\n",
      "Iteration 97300 | Loss: 1.037697\n",
      "Iteration 97325 | Loss: 1.036888\n",
      "Iteration 97350 | Loss: 1.036080\n",
      "Iteration 97375 | Loss: 1.035272\n",
      "Iteration 97400 | Loss: 1.034466\n",
      "Iteration 97425 | Loss: 1.033660\n",
      "Iteration 97450 | Loss: 1.032855\n",
      "Iteration 97475 | Loss: 1.032052\n",
      "Iteration 97500 | Loss: 1.031248\n",
      "Iteration 97525 | Loss: 1.030446\n",
      "Iteration 97550 | Loss: 1.029645\n",
      "Iteration 97575 | Loss: 1.028844\n",
      "Iteration 97600 | Loss: 1.028045\n",
      "Iteration 97625 | Loss: 1.027246\n",
      "Iteration 97650 | Loss: 1.026448\n",
      "Iteration 97675 | Loss: 1.025650\n",
      "Iteration 97700 | Loss: 1.024854\n",
      "Iteration 97725 | Loss: 1.024058\n",
      "Iteration 97750 | Loss: 1.023264\n",
      "Iteration 97775 | Loss: 1.022470\n",
      "Iteration 97800 | Loss: 1.021677\n",
      "Iteration 97825 | Loss: 1.020884\n",
      "Iteration 97850 | Loss: 1.020093\n",
      "Iteration 97875 | Loss: 1.019303\n",
      "Iteration 97900 | Loss: 1.018513\n",
      "Iteration 97925 | Loss: 1.017724\n",
      "Iteration 97950 | Loss: 1.016936\n",
      "Iteration 97975 | Loss: 1.016148\n",
      "Iteration 98000 | Loss: 1.015362\n",
      "Iteration 98025 | Loss: 1.014576\n",
      "Iteration 98050 | Loss: 1.013792\n",
      "Iteration 98075 | Loss: 1.013008\n",
      "Iteration 98100 | Loss: 1.012224\n",
      "Iteration 98125 | Loss: 1.011442\n",
      "Iteration 98150 | Loss: 1.010661\n",
      "Iteration 98175 | Loss: 1.009880\n",
      "Iteration 98200 | Loss: 1.009100\n",
      "Iteration 98225 | Loss: 1.008321\n",
      "Iteration 98250 | Loss: 1.007543\n",
      "Iteration 98275 | Loss: 1.006765\n",
      "Iteration 98300 | Loss: 1.005988\n",
      "Iteration 98325 | Loss: 1.005213\n",
      "Iteration 98350 | Loss: 1.004438\n",
      "Iteration 98375 | Loss: 1.003663\n",
      "Iteration 98400 | Loss: 1.002890\n",
      "Iteration 98425 | Loss: 1.002117\n",
      "Iteration 98450 | Loss: 1.001346\n",
      "Iteration 98475 | Loss: 1.000575\n",
      "Iteration 98500 | Loss: 0.999804\n",
      "Iteration 98525 | Loss: 0.999035\n",
      "Iteration 98550 | Loss: 0.998267\n",
      "Iteration 98575 | Loss: 0.997499\n",
      "Iteration 98600 | Loss: 0.996732\n",
      "Iteration 98625 | Loss: 0.995966\n",
      "Iteration 98650 | Loss: 0.995200\n",
      "Iteration 98675 | Loss: 0.994436\n",
      "Iteration 98700 | Loss: 0.993672\n",
      "Iteration 98725 | Loss: 0.992909\n",
      "Iteration 98750 | Loss: 0.992147\n",
      "Iteration 98775 | Loss: 0.991386\n",
      "Iteration 98800 | Loss: 0.990625\n",
      "Iteration 98825 | Loss: 0.989865\n",
      "Iteration 98850 | Loss: 0.989106\n",
      "Iteration 98875 | Loss: 0.988348\n",
      "Iteration 98900 | Loss: 0.987591\n",
      "Iteration 98925 | Loss: 0.986834\n",
      "Iteration 98950 | Loss: 0.986078\n",
      "Iteration 98975 | Loss: 0.985323\n",
      "Iteration 99000 | Loss: 0.984569\n",
      "Iteration 99025 | Loss: 0.983816\n",
      "Iteration 99050 | Loss: 0.983063\n",
      "Iteration 99075 | Loss: 0.982311\n",
      "Iteration 99100 | Loss: 0.981560\n",
      "Iteration 99125 | Loss: 0.980810\n",
      "Iteration 99150 | Loss: 0.980060\n",
      "Iteration 99175 | Loss: 0.979311\n",
      "Iteration 99200 | Loss: 0.978563\n",
      "Iteration 99225 | Loss: 0.977816\n",
      "Iteration 99250 | Loss: 0.977070\n",
      "Iteration 99275 | Loss: 0.976324\n",
      "Iteration 99300 | Loss: 0.975579\n",
      "Iteration 99325 | Loss: 0.974835\n",
      "Iteration 99350 | Loss: 0.974092\n",
      "Iteration 99375 | Loss: 0.973350\n",
      "Iteration 99400 | Loss: 0.972608\n",
      "Iteration 99425 | Loss: 0.971867\n",
      "Iteration 99450 | Loss: 0.971127\n",
      "Iteration 99475 | Loss: 0.970388\n",
      "Iteration 99500 | Loss: 0.969649\n",
      "Iteration 99525 | Loss: 0.968911\n",
      "Iteration 99550 | Loss: 0.968174\n",
      "Iteration 99575 | Loss: 0.967438\n",
      "Iteration 99600 | Loss: 0.966702\n",
      "Iteration 99625 | Loss: 0.965967\n",
      "Iteration 99650 | Loss: 0.965233\n",
      "Iteration 99675 | Loss: 0.964500\n",
      "Iteration 99700 | Loss: 0.963768\n",
      "Iteration 99725 | Loss: 0.963036\n",
      "Iteration 99750 | Loss: 0.962305\n",
      "Iteration 99775 | Loss: 0.961575\n",
      "Iteration 99800 | Loss: 0.960845\n",
      "Iteration 99825 | Loss: 0.960117\n",
      "Iteration 99850 | Loss: 0.959389\n",
      "Iteration 99875 | Loss: 0.958662\n",
      "Iteration 99900 | Loss: 0.957936\n",
      "Iteration 99925 | Loss: 0.957210\n",
      "Iteration 99950 | Loss: 0.956485\n",
      "Iteration 99975 | Loss: 0.955761\n",
      "Iteration 100000 | Loss: 0.955038\n",
      "Iteration 100025 | Loss: 0.954315\n",
      "Iteration 100050 | Loss: 0.953593\n",
      "Iteration 100075 | Loss: 0.952872\n",
      "Iteration 100100 | Loss: 0.952152\n",
      "Iteration 100125 | Loss: 0.951432\n",
      "Iteration 100150 | Loss: 0.950714\n",
      "Iteration 100175 | Loss: 0.949996\n",
      "Iteration 100200 | Loss: 0.949278\n",
      "Iteration 100225 | Loss: 0.948562\n",
      "Iteration 100250 | Loss: 0.947846\n",
      "Iteration 100275 | Loss: 0.947131\n",
      "Iteration 100300 | Loss: 0.946417\n",
      "Iteration 100325 | Loss: 0.945703\n",
      "Iteration 100350 | Loss: 0.944990\n",
      "Iteration 100375 | Loss: 0.944278\n",
      "Iteration 100400 | Loss: 0.943567\n",
      "Iteration 100425 | Loss: 0.942856\n",
      "Iteration 100450 | Loss: 0.942146\n",
      "Iteration 100475 | Loss: 0.941437\n",
      "Iteration 100500 | Loss: 0.940729\n",
      "Iteration 100525 | Loss: 0.940021\n",
      "Iteration 100550 | Loss: 0.939314\n",
      "Iteration 100575 | Loss: 0.938608\n",
      "Iteration 100600 | Loss: 0.937903\n",
      "Iteration 100625 | Loss: 0.937198\n",
      "Iteration 100650 | Loss: 0.936494\n",
      "Iteration 100675 | Loss: 0.935791\n",
      "Iteration 100700 | Loss: 0.935089\n",
      "Iteration 100725 | Loss: 0.934387\n",
      "Iteration 100750 | Loss: 0.933686\n",
      "Iteration 100775 | Loss: 0.932986\n",
      "Iteration 100800 | Loss: 0.932286\n",
      "Iteration 100825 | Loss: 0.931587\n",
      "Iteration 100850 | Loss: 0.930889\n",
      "Iteration 100875 | Loss: 0.930192\n",
      "Iteration 100900 | Loss: 0.929495\n",
      "Iteration 100925 | Loss: 0.928800\n",
      "Iteration 100950 | Loss: 0.928104\n",
      "Iteration 100975 | Loss: 0.927410\n",
      "Iteration 101000 | Loss: 0.926716\n",
      "Iteration 101025 | Loss: 0.926023\n",
      "Iteration 101050 | Loss: 0.925331\n",
      "Iteration 101075 | Loss: 0.924640\n",
      "Iteration 101100 | Loss: 0.923949\n",
      "Iteration 101125 | Loss: 0.923259\n",
      "Iteration 101150 | Loss: 0.922569\n",
      "Iteration 101175 | Loss: 0.921881\n",
      "Iteration 101200 | Loss: 0.921193\n",
      "Iteration 101225 | Loss: 0.920506\n",
      "Iteration 101250 | Loss: 0.919819\n",
      "Iteration 101275 | Loss: 0.919133\n",
      "Iteration 101300 | Loss: 0.918448\n",
      "Iteration 101325 | Loss: 0.917764\n",
      "Iteration 101350 | Loss: 0.917080\n",
      "Iteration 101375 | Loss: 0.916398\n",
      "Iteration 101400 | Loss: 0.915715\n",
      "Iteration 101425 | Loss: 0.915034\n",
      "Iteration 101450 | Loss: 0.914353\n",
      "Iteration 101475 | Loss: 0.913673\n",
      "Iteration 101500 | Loss: 0.912994\n",
      "Iteration 101525 | Loss: 0.912315\n",
      "Iteration 101550 | Loss: 0.911637\n",
      "Iteration 101575 | Loss: 0.910960\n",
      "Iteration 101600 | Loss: 0.910284\n",
      "Iteration 101625 | Loss: 0.909608\n",
      "Iteration 101650 | Loss: 0.908933\n",
      "Iteration 101675 | Loss: 0.908258\n",
      "Iteration 101700 | Loss: 0.907585\n",
      "Iteration 101725 | Loss: 0.906912\n",
      "Iteration 101750 | Loss: 0.906239\n",
      "Iteration 101775 | Loss: 0.905568\n",
      "Iteration 101800 | Loss: 0.904897\n",
      "Iteration 101825 | Loss: 0.904227\n",
      "Iteration 101850 | Loss: 0.903557\n",
      "Iteration 101875 | Loss: 0.902889\n",
      "Iteration 101900 | Loss: 0.902221\n",
      "Iteration 101925 | Loss: 0.901553\n",
      "Iteration 101950 | Loss: 0.900887\n",
      "Iteration 101975 | Loss: 0.900221\n",
      "Iteration 102000 | Loss: 0.899555\n",
      "Iteration 102025 | Loss: 0.898891\n",
      "Iteration 102050 | Loss: 0.898227\n",
      "Iteration 102075 | Loss: 0.897564\n",
      "Iteration 102100 | Loss: 0.896901\n",
      "Iteration 102125 | Loss: 0.896239\n",
      "Iteration 102150 | Loss: 0.895578\n",
      "Iteration 102175 | Loss: 0.894918\n",
      "Iteration 102200 | Loss: 0.894258\n",
      "Iteration 102225 | Loss: 0.893599\n",
      "Iteration 102250 | Loss: 0.892941\n",
      "Iteration 102275 | Loss: 0.892283\n",
      "Iteration 102300 | Loss: 0.891626\n",
      "Iteration 102325 | Loss: 0.890970\n",
      "Iteration 102350 | Loss: 0.890314\n",
      "Iteration 102375 | Loss: 0.889659\n",
      "Iteration 102400 | Loss: 0.889005\n",
      "Iteration 102425 | Loss: 0.888351\n",
      "Iteration 102450 | Loss: 0.887699\n",
      "Iteration 102475 | Loss: 0.887046\n",
      "Iteration 102500 | Loss: 0.886395\n",
      "Iteration 102525 | Loss: 0.885744\n",
      "Iteration 102550 | Loss: 0.885094\n",
      "Iteration 102575 | Loss: 0.884445\n",
      "Iteration 102600 | Loss: 0.883796\n",
      "Iteration 102625 | Loss: 0.883148\n",
      "Iteration 102650 | Loss: 0.882500\n",
      "Iteration 102675 | Loss: 0.881853\n",
      "Iteration 102700 | Loss: 0.881207\n",
      "Iteration 102725 | Loss: 0.880562\n",
      "Iteration 102750 | Loss: 0.879917\n",
      "Iteration 102775 | Loss: 0.879273\n",
      "Iteration 102800 | Loss: 0.878630\n",
      "Iteration 102825 | Loss: 0.877987\n",
      "Iteration 102850 | Loss: 0.877345\n",
      "Iteration 102875 | Loss: 0.876704\n",
      "Iteration 102900 | Loss: 0.876063\n",
      "Iteration 102925 | Loss: 0.875423\n",
      "Iteration 102950 | Loss: 0.874784\n",
      "Iteration 102975 | Loss: 0.874145\n",
      "Iteration 103000 | Loss: 0.873507\n",
      "Iteration 103025 | Loss: 0.872870\n",
      "Iteration 103050 | Loss: 0.872233\n",
      "Iteration 103075 | Loss: 0.871597\n",
      "Iteration 103100 | Loss: 0.870962\n",
      "Iteration 103125 | Loss: 0.870327\n",
      "Iteration 103150 | Loss: 0.869693\n",
      "Iteration 103175 | Loss: 0.869060\n",
      "Iteration 103200 | Loss: 0.868427\n",
      "Iteration 103225 | Loss: 0.867795\n",
      "Iteration 103250 | Loss: 0.867163\n",
      "Iteration 103275 | Loss: 0.866533\n",
      "Iteration 103300 | Loss: 0.865903\n",
      "Iteration 103325 | Loss: 0.865273\n",
      "Iteration 103350 | Loss: 0.864644\n",
      "Iteration 103375 | Loss: 0.864016\n",
      "Iteration 103400 | Loss: 0.863389\n",
      "Iteration 103425 | Loss: 0.862762\n",
      "Iteration 103450 | Loss: 0.862136\n",
      "Iteration 103475 | Loss: 0.861511\n",
      "Iteration 103500 | Loss: 0.860886\n",
      "Iteration 103525 | Loss: 0.860262\n",
      "Iteration 103550 | Loss: 0.859638\n",
      "Iteration 103575 | Loss: 0.859015\n",
      "Iteration 103600 | Loss: 0.858393\n",
      "Iteration 103625 | Loss: 0.857772\n",
      "Iteration 103650 | Loss: 0.857151\n",
      "Iteration 103675 | Loss: 0.856530\n",
      "Iteration 103700 | Loss: 0.855911\n",
      "Iteration 103725 | Loss: 0.855292\n",
      "Iteration 103750 | Loss: 0.854673\n",
      "Iteration 103775 | Loss: 0.854056\n",
      "Iteration 103800 | Loss: 0.853439\n",
      "Iteration 103825 | Loss: 0.852822\n",
      "Iteration 103850 | Loss: 0.852207\n",
      "Iteration 103875 | Loss: 0.851592\n",
      "Iteration 103900 | Loss: 0.850977\n",
      "Iteration 103925 | Loss: 0.850363\n",
      "Iteration 103950 | Loss: 0.849750\n",
      "Iteration 103975 | Loss: 0.849138\n",
      "Iteration 104000 | Loss: 0.848526\n",
      "Iteration 104025 | Loss: 0.847915\n",
      "Iteration 104050 | Loss: 0.847304\n",
      "Iteration 104075 | Loss: 0.846694\n",
      "Iteration 104100 | Loss: 0.846085\n",
      "Iteration 104125 | Loss: 0.845476\n",
      "Iteration 104150 | Loss: 0.844868\n",
      "Iteration 104175 | Loss: 0.844261\n",
      "Iteration 104200 | Loss: 0.843654\n",
      "Iteration 104225 | Loss: 0.843048\n",
      "Iteration 104250 | Loss: 0.842442\n",
      "Iteration 104275 | Loss: 0.841837\n",
      "Iteration 104300 | Loss: 0.841233\n",
      "Iteration 104325 | Loss: 0.840629\n",
      "Iteration 104350 | Loss: 0.840026\n",
      "Iteration 104375 | Loss: 0.839424\n",
      "Iteration 104400 | Loss: 0.838822\n",
      "Iteration 104425 | Loss: 0.838221\n",
      "Iteration 104450 | Loss: 0.837621\n",
      "Iteration 104475 | Loss: 0.837021\n",
      "Iteration 104500 | Loss: 0.836422\n",
      "Iteration 104525 | Loss: 0.835823\n",
      "Iteration 104550 | Loss: 0.835225\n",
      "Iteration 104575 | Loss: 0.834628\n",
      "Iteration 104600 | Loss: 0.834031\n",
      "Iteration 104625 | Loss: 0.833435\n",
      "Iteration 104650 | Loss: 0.832839\n",
      "Iteration 104675 | Loss: 0.832245\n",
      "Iteration 104700 | Loss: 0.831650\n",
      "Iteration 104725 | Loss: 0.831057\n",
      "Iteration 104750 | Loss: 0.830464\n",
      "Iteration 104775 | Loss: 0.829871\n",
      "Iteration 104800 | Loss: 0.829280\n",
      "Iteration 104825 | Loss: 0.828689\n",
      "Iteration 104850 | Loss: 0.828098\n",
      "Iteration 104875 | Loss: 0.827508\n",
      "Iteration 104900 | Loss: 0.826919\n",
      "Iteration 104925 | Loss: 0.826330\n",
      "Iteration 104950 | Loss: 0.825742\n",
      "Iteration 104975 | Loss: 0.825155\n",
      "Iteration 105000 | Loss: 0.824568\n",
      "Iteration 105025 | Loss: 0.823982\n",
      "Iteration 105050 | Loss: 0.823396\n",
      "Iteration 105075 | Loss: 0.822811\n",
      "Iteration 105100 | Loss: 0.822227\n",
      "Iteration 105125 | Loss: 0.821643\n",
      "Iteration 105150 | Loss: 0.821060\n",
      "Iteration 105175 | Loss: 0.820477\n",
      "Iteration 105200 | Loss: 0.819895\n",
      "Iteration 105225 | Loss: 0.819314\n",
      "Iteration 105250 | Loss: 0.818733\n",
      "Iteration 105275 | Loss: 0.818153\n",
      "Iteration 105300 | Loss: 0.817574\n",
      "Iteration 105325 | Loss: 0.816995\n",
      "Iteration 105350 | Loss: 0.816417\n",
      "Iteration 105375 | Loss: 0.815839\n",
      "Iteration 105400 | Loss: 0.815262\n",
      "Iteration 105425 | Loss: 0.814685\n",
      "Iteration 105450 | Loss: 0.814110\n",
      "Iteration 105475 | Loss: 0.813534\n",
      "Iteration 105500 | Loss: 0.812960\n",
      "Iteration 105525 | Loss: 0.812385\n",
      "Iteration 105550 | Loss: 0.811812\n",
      "Iteration 105575 | Loss: 0.811239\n",
      "Iteration 105600 | Loss: 0.810667\n",
      "Iteration 105625 | Loss: 0.810095\n",
      "Iteration 105650 | Loss: 0.809524\n",
      "Iteration 105675 | Loss: 0.808954\n",
      "Iteration 105700 | Loss: 0.808384\n",
      "Iteration 105725 | Loss: 0.807814\n",
      "Iteration 105750 | Loss: 0.807246\n",
      "Iteration 105775 | Loss: 0.806678\n",
      "Iteration 105800 | Loss: 0.806110\n",
      "Iteration 105825 | Loss: 0.805543\n",
      "Iteration 105850 | Loss: 0.804977\n",
      "Iteration 105875 | Loss: 0.804411\n",
      "Iteration 105900 | Loss: 0.803846\n",
      "Iteration 105925 | Loss: 0.803282\n",
      "Iteration 105950 | Loss: 0.802718\n",
      "Iteration 105975 | Loss: 0.802154\n",
      "Iteration 106000 | Loss: 0.801591\n",
      "Iteration 106025 | Loss: 0.801029\n",
      "Iteration 106050 | Loss: 0.800468\n",
      "Iteration 106075 | Loss: 0.799907\n",
      "Iteration 106100 | Loss: 0.799346\n",
      "Iteration 106125 | Loss: 0.798786\n",
      "Iteration 106150 | Loss: 0.798227\n",
      "Iteration 106175 | Loss: 0.797668\n",
      "Iteration 106200 | Loss: 0.797110\n",
      "Iteration 106225 | Loss: 0.796553\n",
      "Iteration 106250 | Loss: 0.795996\n",
      "Iteration 106275 | Loss: 0.795440\n",
      "Iteration 106300 | Loss: 0.794884\n",
      "Iteration 106325 | Loss: 0.794329\n",
      "Iteration 106350 | Loss: 0.793774\n",
      "Iteration 106375 | Loss: 0.793220\n",
      "Iteration 106400 | Loss: 0.792667\n",
      "Iteration 106425 | Loss: 0.792114\n",
      "Iteration 106450 | Loss: 0.791561\n",
      "Iteration 106475 | Loss: 0.791010\n",
      "Iteration 106500 | Loss: 0.790459\n",
      "Iteration 106525 | Loss: 0.789908\n",
      "Iteration 106550 | Loss: 0.789358\n",
      "Iteration 106575 | Loss: 0.788809\n",
      "Iteration 106600 | Loss: 0.788260\n",
      "Iteration 106625 | Loss: 0.787712\n",
      "Iteration 106650 | Loss: 0.787164\n",
      "Iteration 106675 | Loss: 0.786617\n",
      "Iteration 106700 | Loss: 0.786070\n",
      "Iteration 106725 | Loss: 0.785524\n",
      "Iteration 106750 | Loss: 0.784979\n",
      "Iteration 106775 | Loss: 0.784434\n",
      "Iteration 106800 | Loss: 0.783890\n",
      "Iteration 106825 | Loss: 0.783346\n",
      "Iteration 106850 | Loss: 0.782803\n",
      "Iteration 106875 | Loss: 0.782260\n",
      "Iteration 106900 | Loss: 0.781718\n",
      "Iteration 106925 | Loss: 0.781177\n",
      "Iteration 106950 | Loss: 0.780636\n",
      "Iteration 106975 | Loss: 0.780096\n",
      "Iteration 107000 | Loss: 0.779556\n",
      "Iteration 107025 | Loss: 0.779017\n",
      "Iteration 107050 | Loss: 0.778478\n",
      "Iteration 107075 | Loss: 0.777940\n",
      "Iteration 107100 | Loss: 0.777403\n",
      "Iteration 107125 | Loss: 0.776866\n",
      "Iteration 107150 | Loss: 0.776329\n",
      "Iteration 107175 | Loss: 0.775794\n",
      "Iteration 107200 | Loss: 0.775258\n",
      "Iteration 107225 | Loss: 0.774724\n",
      "Iteration 107250 | Loss: 0.774190\n",
      "Iteration 107275 | Loss: 0.773656\n",
      "Iteration 107300 | Loss: 0.773123\n",
      "Iteration 107325 | Loss: 0.772591\n",
      "Iteration 107350 | Loss: 0.772059\n",
      "Iteration 107375 | Loss: 0.771527\n",
      "Iteration 107400 | Loss: 0.770997\n",
      "Iteration 107425 | Loss: 0.770466\n",
      "Iteration 107450 | Loss: 0.769937\n",
      "Iteration 107475 | Loss: 0.769408\n",
      "Iteration 107500 | Loss: 0.768879\n",
      "Iteration 107525 | Loss: 0.768351\n",
      "Iteration 107550 | Loss: 0.767824\n",
      "Iteration 107575 | Loss: 0.767297\n",
      "Iteration 107600 | Loss: 0.766770\n",
      "Iteration 107625 | Loss: 0.766245\n",
      "Iteration 107650 | Loss: 0.765719\n",
      "Iteration 107675 | Loss: 0.765195\n",
      "Iteration 107700 | Loss: 0.764670\n",
      "Iteration 107725 | Loss: 0.764147\n",
      "Iteration 107750 | Loss: 0.763624\n",
      "Iteration 107775 | Loss: 0.763101\n",
      "Iteration 107800 | Loss: 0.762579\n",
      "Iteration 107825 | Loss: 0.762058\n",
      "Iteration 107850 | Loss: 0.761537\n",
      "Iteration 107875 | Loss: 0.761017\n",
      "Iteration 107900 | Loss: 0.760497\n",
      "Iteration 107925 | Loss: 0.759978\n",
      "Iteration 107950 | Loss: 0.759459\n",
      "Iteration 107975 | Loss: 0.758941\n",
      "Iteration 108000 | Loss: 0.758423\n",
      "Iteration 108025 | Loss: 0.757906\n",
      "Iteration 108050 | Loss: 0.757390\n",
      "Iteration 108075 | Loss: 0.756874\n",
      "Iteration 108100 | Loss: 0.756358\n",
      "Iteration 108125 | Loss: 0.755843\n",
      "Iteration 108150 | Loss: 0.755329\n",
      "Iteration 108175 | Loss: 0.754815\n",
      "Iteration 108200 | Loss: 0.754302\n",
      "Iteration 108225 | Loss: 0.753789\n",
      "Iteration 108250 | Loss: 0.753277\n",
      "Iteration 108275 | Loss: 0.752765\n",
      "Iteration 108300 | Loss: 0.752254\n",
      "Iteration 108325 | Loss: 0.751743\n",
      "Iteration 108350 | Loss: 0.751233\n",
      "Iteration 108375 | Loss: 0.750723\n",
      "Iteration 108400 | Loss: 0.750214\n",
      "Iteration 108425 | Loss: 0.749706\n",
      "Iteration 108450 | Loss: 0.749198\n",
      "Iteration 108475 | Loss: 0.748690\n",
      "Iteration 108500 | Loss: 0.748184\n",
      "Iteration 108525 | Loss: 0.747677\n",
      "Iteration 108550 | Loss: 0.747171\n",
      "Iteration 108575 | Loss: 0.746666\n",
      "Iteration 108600 | Loss: 0.746161\n",
      "Iteration 108625 | Loss: 0.745657\n",
      "Iteration 108650 | Loss: 0.745153\n",
      "Iteration 108675 | Loss: 0.744650\n",
      "Iteration 108700 | Loss: 0.744147\n",
      "Iteration 108725 | Loss: 0.743645\n",
      "Iteration 108750 | Loss: 0.743143\n",
      "Iteration 108775 | Loss: 0.742642\n",
      "Iteration 108800 | Loss: 0.742142\n",
      "Iteration 108825 | Loss: 0.741642\n",
      "Iteration 108850 | Loss: 0.741142\n",
      "Iteration 108875 | Loss: 0.740643\n",
      "Iteration 108900 | Loss: 0.740145\n",
      "Iteration 108925 | Loss: 0.739647\n",
      "Iteration 108950 | Loss: 0.739149\n",
      "Iteration 108975 | Loss: 0.738652\n",
      "Iteration 109000 | Loss: 0.738156\n",
      "Iteration 109025 | Loss: 0.737660\n",
      "Iteration 109050 | Loss: 0.737165\n",
      "Iteration 109075 | Loss: 0.736670\n",
      "Iteration 109100 | Loss: 0.736175\n",
      "Iteration 109125 | Loss: 0.735681\n",
      "Iteration 109150 | Loss: 0.735188\n",
      "Iteration 109175 | Loss: 0.734695\n",
      "Iteration 109200 | Loss: 0.734203\n",
      "Iteration 109225 | Loss: 0.733711\n",
      "Iteration 109250 | Loss: 0.733220\n",
      "Iteration 109275 | Loss: 0.732729\n",
      "Iteration 109300 | Loss: 0.732239\n",
      "Iteration 109325 | Loss: 0.731749\n",
      "Iteration 109350 | Loss: 0.731260\n",
      "Iteration 109375 | Loss: 0.730771\n",
      "Iteration 109400 | Loss: 0.730283\n",
      "Iteration 109425 | Loss: 0.729796\n",
      "Iteration 109450 | Loss: 0.729308\n",
      "Iteration 109475 | Loss: 0.728822\n",
      "Iteration 109500 | Loss: 0.728336\n",
      "Iteration 109525 | Loss: 0.727850\n",
      "Iteration 109550 | Loss: 0.727365\n",
      "Iteration 109575 | Loss: 0.726880\n",
      "Iteration 109600 | Loss: 0.726396\n",
      "Iteration 109625 | Loss: 0.725912\n",
      "Iteration 109650 | Loss: 0.725429\n",
      "Iteration 109675 | Loss: 0.724947\n",
      "Iteration 109700 | Loss: 0.724465\n",
      "Iteration 109725 | Loss: 0.723983\n",
      "Iteration 109750 | Loss: 0.723502\n",
      "Iteration 109775 | Loss: 0.723021\n",
      "Iteration 109800 | Loss: 0.722541\n",
      "Iteration 109825 | Loss: 0.722062\n",
      "Iteration 109850 | Loss: 0.721582\n",
      "Iteration 109875 | Loss: 0.721104\n",
      "Iteration 109900 | Loss: 0.720626\n",
      "Iteration 109925 | Loss: 0.720148\n",
      "Iteration 109950 | Loss: 0.719671\n",
      "Iteration 109975 | Loss: 0.719195\n",
      "Iteration 110000 | Loss: 0.718718\n",
      "Iteration 110025 | Loss: 0.718243\n",
      "Iteration 110050 | Loss: 0.717768\n",
      "Iteration 110075 | Loss: 0.717293\n",
      "Iteration 110100 | Loss: 0.716819\n",
      "Iteration 110125 | Loss: 0.716345\n",
      "Iteration 110150 | Loss: 0.715872\n",
      "Iteration 110175 | Loss: 0.715400\n",
      "Iteration 110200 | Loss: 0.714928\n",
      "Iteration 110225 | Loss: 0.714456\n",
      "Iteration 110250 | Loss: 0.713985\n",
      "Iteration 110275 | Loss: 0.713514\n",
      "Iteration 110300 | Loss: 0.713044\n",
      "Iteration 110325 | Loss: 0.712574\n",
      "Iteration 110350 | Loss: 0.712105\n",
      "Iteration 110375 | Loss: 0.711636\n",
      "Iteration 110400 | Loss: 0.711168\n",
      "Iteration 110425 | Loss: 0.710701\n",
      "Iteration 110450 | Loss: 0.710233\n",
      "Iteration 110475 | Loss: 0.709767\n",
      "Iteration 110500 | Loss: 0.709300\n",
      "Iteration 110525 | Loss: 0.708835\n",
      "Iteration 110550 | Loss: 0.708369\n",
      "Iteration 110575 | Loss: 0.707905\n",
      "Iteration 110600 | Loss: 0.707440\n",
      "Iteration 110625 | Loss: 0.706976\n",
      "Iteration 110650 | Loss: 0.706513\n",
      "Iteration 110675 | Loss: 0.706050\n",
      "Iteration 110700 | Loss: 0.705588\n",
      "Iteration 110725 | Loss: 0.705126\n",
      "Iteration 110750 | Loss: 0.704665\n",
      "Iteration 110775 | Loss: 0.704204\n",
      "Iteration 110800 | Loss: 0.703743\n",
      "Iteration 110825 | Loss: 0.703283\n",
      "Iteration 110850 | Loss: 0.702824\n",
      "Iteration 110875 | Loss: 0.702365\n",
      "Iteration 110900 | Loss: 0.701906\n",
      "Iteration 110925 | Loss: 0.701448\n",
      "Iteration 110950 | Loss: 0.700991\n",
      "Iteration 110975 | Loss: 0.700534\n",
      "Iteration 111000 | Loss: 0.700077\n",
      "Iteration 111025 | Loss: 0.699621\n",
      "Iteration 111050 | Loss: 0.699165\n",
      "Iteration 111075 | Loss: 0.698710\n",
      "Iteration 111100 | Loss: 0.698255\n",
      "Iteration 111125 | Loss: 0.697801\n",
      "Iteration 111150 | Loss: 0.697348\n",
      "Iteration 111175 | Loss: 0.696894\n",
      "Iteration 111200 | Loss: 0.696441\n",
      "Iteration 111225 | Loss: 0.695989\n",
      "Iteration 111250 | Loss: 0.695537\n",
      "Iteration 111275 | Loss: 0.695086\n",
      "Iteration 111300 | Loss: 0.694635\n",
      "Iteration 111325 | Loss: 0.694185\n",
      "Iteration 111350 | Loss: 0.693735\n",
      "Iteration 111375 | Loss: 0.693285\n",
      "Iteration 111400 | Loss: 0.692836\n",
      "Iteration 111425 | Loss: 0.692388\n",
      "Iteration 111450 | Loss: 0.691939\n",
      "Iteration 111475 | Loss: 0.691492\n",
      "Iteration 111500 | Loss: 0.691045\n",
      "Iteration 111525 | Loss: 0.690598\n",
      "Iteration 111550 | Loss: 0.690152\n",
      "Iteration 111575 | Loss: 0.689706\n",
      "Iteration 111600 | Loss: 0.689261\n",
      "Iteration 111625 | Loss: 0.688816\n",
      "Iteration 111650 | Loss: 0.688372\n",
      "Iteration 111675 | Loss: 0.687928\n",
      "Iteration 111700 | Loss: 0.687484\n",
      "Iteration 111725 | Loss: 0.687041\n",
      "Iteration 111750 | Loss: 0.686599\n",
      "Iteration 111775 | Loss: 0.686157\n",
      "Iteration 111800 | Loss: 0.685715\n",
      "Iteration 111825 | Loss: 0.685274\n",
      "Iteration 111850 | Loss: 0.684833\n",
      "Iteration 111875 | Loss: 0.684393\n",
      "Iteration 111900 | Loss: 0.683953\n",
      "Iteration 111925 | Loss: 0.683514\n",
      "Iteration 111950 | Loss: 0.683075\n",
      "Iteration 111975 | Loss: 0.682637\n",
      "Iteration 112000 | Loss: 0.682199\n",
      "Iteration 112025 | Loss: 0.681762\n",
      "Iteration 112050 | Loss: 0.681325\n",
      "Iteration 112075 | Loss: 0.680888\n",
      "Iteration 112100 | Loss: 0.680452\n",
      "Iteration 112125 | Loss: 0.680016\n",
      "Iteration 112150 | Loss: 0.679581\n",
      "Iteration 112175 | Loss: 0.679147\n",
      "Iteration 112200 | Loss: 0.678712\n",
      "Iteration 112225 | Loss: 0.678279\n",
      "Iteration 112250 | Loss: 0.677845\n",
      "Iteration 112275 | Loss: 0.677412\n",
      "Iteration 112300 | Loss: 0.676980\n",
      "Iteration 112325 | Loss: 0.676548\n",
      "Iteration 112350 | Loss: 0.676116\n",
      "Iteration 112375 | Loss: 0.675685\n",
      "Iteration 112400 | Loss: 0.675255\n",
      "Iteration 112425 | Loss: 0.674824\n",
      "Iteration 112450 | Loss: 0.674395\n",
      "Iteration 112475 | Loss: 0.673965\n",
      "Iteration 112500 | Loss: 0.673537\n",
      "Iteration 112525 | Loss: 0.673108\n",
      "Iteration 112550 | Loss: 0.672680\n",
      "Iteration 112575 | Loss: 0.672253\n",
      "Iteration 112600 | Loss: 0.671826\n",
      "Iteration 112625 | Loss: 0.671399\n",
      "Iteration 112650 | Loss: 0.670973\n",
      "Iteration 112675 | Loss: 0.670547\n",
      "Iteration 112700 | Loss: 0.670122\n",
      "Iteration 112725 | Loss: 0.669697\n",
      "Iteration 112750 | Loss: 0.669273\n",
      "Iteration 112775 | Loss: 0.668849\n",
      "Iteration 112800 | Loss: 0.668425\n",
      "Iteration 112825 | Loss: 0.668002\n",
      "Iteration 112850 | Loss: 0.667580\n",
      "Iteration 112875 | Loss: 0.667157\n",
      "Iteration 112900 | Loss: 0.666736\n",
      "Iteration 112925 | Loss: 0.666314\n",
      "Iteration 112950 | Loss: 0.665894\n",
      "Iteration 112975 | Loss: 0.665473\n",
      "Iteration 113000 | Loss: 0.665053\n",
      "Iteration 113025 | Loss: 0.664634\n",
      "Iteration 113050 | Loss: 0.664215\n",
      "Iteration 113075 | Loss: 0.663796\n",
      "Iteration 113100 | Loss: 0.663378\n",
      "Iteration 113125 | Loss: 0.662960\n",
      "Iteration 113150 | Loss: 0.662543\n",
      "Iteration 113175 | Loss: 0.662126\n",
      "Iteration 113200 | Loss: 0.661709\n",
      "Iteration 113225 | Loss: 0.661293\n",
      "Iteration 113250 | Loss: 0.660877\n",
      "Iteration 113275 | Loss: 0.660462\n",
      "Iteration 113300 | Loss: 0.660048\n",
      "Iteration 113325 | Loss: 0.659633\n",
      "Iteration 113350 | Loss: 0.659219\n",
      "Iteration 113375 | Loss: 0.658806\n",
      "Iteration 113400 | Loss: 0.658393\n",
      "Iteration 113425 | Loss: 0.657980\n",
      "Iteration 113450 | Loss: 0.657568\n",
      "Iteration 113475 | Loss: 0.657157\n",
      "Iteration 113500 | Loss: 0.656745\n",
      "Iteration 113525 | Loss: 0.656334\n",
      "Iteration 113550 | Loss: 0.655924\n",
      "Iteration 113575 | Loss: 0.655514\n",
      "Iteration 113600 | Loss: 0.655104\n",
      "Iteration 113625 | Loss: 0.654695\n",
      "Iteration 113650 | Loss: 0.654287\n",
      "Iteration 113675 | Loss: 0.653878\n",
      "Iteration 113700 | Loss: 0.653470\n",
      "Iteration 113725 | Loss: 0.653063\n",
      "Iteration 113750 | Loss: 0.652656\n",
      "Iteration 113775 | Loss: 0.652249\n",
      "Iteration 113800 | Loss: 0.651843\n",
      "Iteration 113825 | Loss: 0.651438\n",
      "Iteration 113850 | Loss: 0.651032\n",
      "Iteration 113875 | Loss: 0.650627\n",
      "Iteration 113900 | Loss: 0.650223\n",
      "Iteration 113925 | Loss: 0.649819\n",
      "Iteration 113950 | Loss: 0.649415\n",
      "Iteration 113975 | Loss: 0.649012\n",
      "Iteration 114000 | Loss: 0.648609\n",
      "Iteration 114025 | Loss: 0.648207\n",
      "Iteration 114050 | Loss: 0.647805\n",
      "Iteration 114075 | Loss: 0.647403\n",
      "Iteration 114100 | Loss: 0.647002\n",
      "Iteration 114125 | Loss: 0.646602\n",
      "Iteration 114150 | Loss: 0.646201\n",
      "Iteration 114175 | Loss: 0.645802\n",
      "Iteration 114200 | Loss: 0.645402\n",
      "Iteration 114225 | Loss: 0.645003\n",
      "Iteration 114250 | Loss: 0.644605\n",
      "Iteration 114275 | Loss: 0.644206\n",
      "Iteration 114300 | Loss: 0.643809\n",
      "Iteration 114325 | Loss: 0.643411\n",
      "Iteration 114350 | Loss: 0.643014\n",
      "Iteration 114375 | Loss: 0.642618\n",
      "Iteration 114400 | Loss: 0.642222\n",
      "Iteration 114425 | Loss: 0.641826\n",
      "Iteration 114450 | Loss: 0.641431\n",
      "Iteration 114475 | Loss: 0.641036\n",
      "Iteration 114500 | Loss: 0.640641\n",
      "Iteration 114525 | Loss: 0.640247\n",
      "Iteration 114550 | Loss: 0.639854\n",
      "Iteration 114575 | Loss: 0.639461\n",
      "Iteration 114600 | Loss: 0.639068\n",
      "Iteration 114625 | Loss: 0.638675\n",
      "Iteration 114650 | Loss: 0.638283\n",
      "Iteration 114675 | Loss: 0.637892\n",
      "Iteration 114700 | Loss: 0.637501\n",
      "Iteration 114725 | Loss: 0.637110\n",
      "Iteration 114750 | Loss: 0.636720\n",
      "Iteration 114775 | Loss: 0.636330\n",
      "Iteration 114800 | Loss: 0.635940\n",
      "Iteration 114825 | Loss: 0.635551\n",
      "Iteration 114850 | Loss: 0.635162\n",
      "Iteration 114875 | Loss: 0.634774\n",
      "Iteration 114900 | Loss: 0.634386\n",
      "Iteration 114925 | Loss: 0.633999\n",
      "Iteration 114950 | Loss: 0.633612\n",
      "Iteration 114975 | Loss: 0.633225\n",
      "Iteration 115000 | Loss: 0.632839\n",
      "Iteration 115025 | Loss: 0.632453\n",
      "Iteration 115050 | Loss: 0.632067\n",
      "Iteration 115075 | Loss: 0.631682\n",
      "Iteration 115100 | Loss: 0.631297\n",
      "Iteration 115125 | Loss: 0.630913\n",
      "Iteration 115150 | Loss: 0.630529\n",
      "Iteration 115175 | Loss: 0.630146\n",
      "Iteration 115200 | Loss: 0.629763\n",
      "Iteration 115225 | Loss: 0.629380\n",
      "Iteration 115250 | Loss: 0.628998\n",
      "Iteration 115275 | Loss: 0.628616\n",
      "Iteration 115300 | Loss: 0.628234\n",
      "Iteration 115325 | Loss: 0.627853\n",
      "Iteration 115350 | Loss: 0.627473\n",
      "Iteration 115375 | Loss: 0.627092\n",
      "Iteration 115400 | Loss: 0.626713\n",
      "Iteration 115425 | Loss: 0.626333\n",
      "Iteration 115450 | Loss: 0.625954\n",
      "Iteration 115475 | Loss: 0.625575\n",
      "Iteration 115500 | Loss: 0.625197\n",
      "Iteration 115525 | Loss: 0.624819\n",
      "Iteration 115550 | Loss: 0.624442\n",
      "Iteration 115575 | Loss: 0.624065\n",
      "Iteration 115600 | Loss: 0.623688\n",
      "Iteration 115625 | Loss: 0.623311\n",
      "Iteration 115650 | Loss: 0.622936\n",
      "Iteration 115675 | Loss: 0.622560\n",
      "Iteration 115700 | Loss: 0.622185\n",
      "Iteration 115725 | Loss: 0.621810\n",
      "Iteration 115750 | Loss: 0.621436\n",
      "Iteration 115775 | Loss: 0.621062\n",
      "Iteration 115800 | Loss: 0.620688\n",
      "Iteration 115825 | Loss: 0.620315\n",
      "Iteration 115850 | Loss: 0.619942\n",
      "Iteration 115875 | Loss: 0.619570\n",
      "Iteration 115900 | Loss: 0.619198\n",
      "Iteration 115925 | Loss: 0.618826\n",
      "Iteration 115950 | Loss: 0.618455\n",
      "Iteration 115975 | Loss: 0.618084\n",
      "Iteration 116000 | Loss: 0.617714\n",
      "Iteration 116025 | Loss: 0.617343\n",
      "Iteration 116050 | Loss: 0.616974\n",
      "Iteration 116075 | Loss: 0.616604\n",
      "Iteration 116100 | Loss: 0.616236\n",
      "Iteration 116125 | Loss: 0.615867\n",
      "Iteration 116150 | Loss: 0.615499\n",
      "Iteration 116175 | Loss: 0.615131\n",
      "Iteration 116200 | Loss: 0.614764\n",
      "Iteration 116225 | Loss: 0.614397\n",
      "Iteration 116250 | Loss: 0.614030\n",
      "Iteration 116275 | Loss: 0.613664\n",
      "Iteration 116300 | Loss: 0.613298\n",
      "Iteration 116325 | Loss: 0.612933\n",
      "Iteration 116350 | Loss: 0.612567\n",
      "Iteration 116375 | Loss: 0.612203\n",
      "Iteration 116400 | Loss: 0.611838\n",
      "Iteration 116425 | Loss: 0.611474\n",
      "Iteration 116450 | Loss: 0.611111\n",
      "Iteration 116475 | Loss: 0.610748\n",
      "Iteration 116500 | Loss: 0.610385\n",
      "Iteration 116525 | Loss: 0.610022\n",
      "Iteration 116550 | Loss: 0.609660\n",
      "Iteration 116575 | Loss: 0.609299\n",
      "Iteration 116600 | Loss: 0.608937\n",
      "Iteration 116625 | Loss: 0.608577\n",
      "Iteration 116650 | Loss: 0.608216\n",
      "Iteration 116675 | Loss: 0.607856\n",
      "Iteration 116700 | Loss: 0.607496\n",
      "Iteration 116725 | Loss: 0.607137\n",
      "Iteration 116750 | Loss: 0.606778\n",
      "Iteration 116775 | Loss: 0.606419\n",
      "Iteration 116800 | Loss: 0.606061\n",
      "Iteration 116825 | Loss: 0.605703\n",
      "Iteration 116850 | Loss: 0.605345\n",
      "Iteration 116875 | Loss: 0.604988\n",
      "Iteration 116900 | Loss: 0.604631\n",
      "Iteration 116925 | Loss: 0.604275\n",
      "Iteration 116950 | Loss: 0.603919\n",
      "Iteration 116975 | Loss: 0.603563\n",
      "Iteration 117000 | Loss: 0.603208\n",
      "Iteration 117025 | Loss: 0.602853\n",
      "Iteration 117050 | Loss: 0.602498\n",
      "Iteration 117075 | Loss: 0.602144\n",
      "Iteration 117100 | Loss: 0.601790\n",
      "Iteration 117125 | Loss: 0.601437\n",
      "Iteration 117150 | Loss: 0.601084\n",
      "Iteration 117175 | Loss: 0.600731\n",
      "Iteration 117200 | Loss: 0.600379\n",
      "Iteration 117225 | Loss: 0.600027\n",
      "Iteration 117250 | Loss: 0.599675\n",
      "Iteration 117275 | Loss: 0.599324\n",
      "Iteration 117300 | Loss: 0.598973\n",
      "Iteration 117325 | Loss: 0.598622\n",
      "Iteration 117350 | Loss: 0.598272\n",
      "Iteration 117375 | Loss: 0.597923\n",
      "Iteration 117400 | Loss: 0.597573\n",
      "Iteration 117425 | Loss: 0.597224\n",
      "Iteration 117450 | Loss: 0.596875\n",
      "Iteration 117475 | Loss: 0.596527\n",
      "Iteration 117500 | Loss: 0.596179\n",
      "Iteration 117525 | Loss: 0.595832\n",
      "Iteration 117550 | Loss: 0.595484\n",
      "Iteration 117575 | Loss: 0.595137\n",
      "Iteration 117600 | Loss: 0.594791\n",
      "Iteration 117625 | Loss: 0.594445\n",
      "Iteration 117650 | Loss: 0.594099\n",
      "Iteration 117675 | Loss: 0.593754\n",
      "Iteration 117700 | Loss: 0.593408\n",
      "Iteration 117725 | Loss: 0.593064\n",
      "Iteration 117750 | Loss: 0.592719\n",
      "Iteration 117775 | Loss: 0.592375\n",
      "Iteration 117800 | Loss: 0.592032\n",
      "Iteration 117825 | Loss: 0.591689\n",
      "Iteration 117850 | Loss: 0.591346\n",
      "Iteration 117875 | Loss: 0.591003\n",
      "Iteration 117900 | Loss: 0.590661\n",
      "Iteration 117925 | Loss: 0.590319\n",
      "Iteration 117950 | Loss: 0.589978\n",
      "Iteration 117975 | Loss: 0.589637\n",
      "Iteration 118000 | Loss: 0.589296\n",
      "Iteration 118025 | Loss: 0.588955\n",
      "Iteration 118050 | Loss: 0.588615\n",
      "Iteration 118075 | Loss: 0.588276\n",
      "Iteration 118100 | Loss: 0.587936\n",
      "Iteration 118125 | Loss: 0.587597\n",
      "Iteration 118150 | Loss: 0.587259\n",
      "Iteration 118175 | Loss: 0.586920\n",
      "Iteration 118200 | Loss: 0.586582\n",
      "Iteration 118225 | Loss: 0.586245\n",
      "Iteration 118250 | Loss: 0.585908\n",
      "Iteration 118275 | Loss: 0.585571\n",
      "Iteration 118300 | Loss: 0.585234\n",
      "Iteration 118325 | Loss: 0.584898\n",
      "Iteration 118350 | Loss: 0.584562\n",
      "Iteration 118375 | Loss: 0.584227\n",
      "Iteration 118400 | Loss: 0.583892\n",
      "Iteration 118425 | Loss: 0.583557\n",
      "Iteration 118450 | Loss: 0.583223\n",
      "Iteration 118475 | Loss: 0.582889\n",
      "Iteration 118500 | Loss: 0.582555\n",
      "Iteration 118525 | Loss: 0.582221\n",
      "Iteration 118550 | Loss: 0.581888\n",
      "Iteration 118575 | Loss: 0.581556\n",
      "Iteration 118600 | Loss: 0.581223\n",
      "Iteration 118625 | Loss: 0.580891\n",
      "Iteration 118650 | Loss: 0.580560\n",
      "Iteration 118675 | Loss: 0.580229\n",
      "Iteration 118700 | Loss: 0.579898\n",
      "Iteration 118725 | Loss: 0.579567\n",
      "Iteration 118750 | Loss: 0.579237\n",
      "Iteration 118775 | Loss: 0.578907\n",
      "Iteration 118800 | Loss: 0.578577\n",
      "Iteration 118825 | Loss: 0.578248\n",
      "Iteration 118850 | Loss: 0.577919\n",
      "Iteration 118875 | Loss: 0.577591\n",
      "Iteration 118900 | Loss: 0.577263\n",
      "Iteration 118925 | Loss: 0.576935\n",
      "Iteration 118950 | Loss: 0.576607\n",
      "Iteration 118975 | Loss: 0.576280\n",
      "Iteration 119000 | Loss: 0.575953\n",
      "Iteration 119025 | Loss: 0.575627\n",
      "Iteration 119050 | Loss: 0.575301\n",
      "Iteration 119075 | Loss: 0.574975\n",
      "Iteration 119100 | Loss: 0.574649\n",
      "Iteration 119125 | Loss: 0.574324\n",
      "Iteration 119150 | Loss: 0.574000\n",
      "Iteration 119175 | Loss: 0.573675\n",
      "Iteration 119200 | Loss: 0.573351\n",
      "Iteration 119225 | Loss: 0.573027\n",
      "Iteration 119250 | Loss: 0.572704\n",
      "Iteration 119275 | Loss: 0.572381\n",
      "Iteration 119300 | Loss: 0.572058\n",
      "Iteration 119325 | Loss: 0.571736\n",
      "Iteration 119350 | Loss: 0.571414\n",
      "Iteration 119375 | Loss: 0.571092\n",
      "Iteration 119400 | Loss: 0.570770\n",
      "Iteration 119425 | Loss: 0.570449\n",
      "Iteration 119450 | Loss: 0.570129\n",
      "Iteration 119475 | Loss: 0.569808\n",
      "Iteration 119500 | Loss: 0.569488\n",
      "Iteration 119525 | Loss: 0.569169\n",
      "Iteration 119550 | Loss: 0.568849\n",
      "Iteration 119575 | Loss: 0.568530\n",
      "Iteration 119600 | Loss: 0.568211\n",
      "Iteration 119625 | Loss: 0.567893\n",
      "Iteration 119650 | Loss: 0.567575\n",
      "Iteration 119675 | Loss: 0.567257\n",
      "Iteration 119700 | Loss: 0.566940\n",
      "Iteration 119725 | Loss: 0.566623\n",
      "Iteration 119750 | Loss: 0.566306\n",
      "Iteration 119775 | Loss: 0.565990\n",
      "Iteration 119800 | Loss: 0.565674\n",
      "Iteration 119825 | Loss: 0.565358\n",
      "Iteration 119850 | Loss: 0.565042\n",
      "Iteration 119875 | Loss: 0.564727\n",
      "Iteration 119900 | Loss: 0.564413\n",
      "Iteration 119925 | Loss: 0.564098\n",
      "Iteration 119950 | Loss: 0.563784\n",
      "Iteration 119975 | Loss: 0.563470\n",
      "Iteration 120000 | Loss: 0.563157\n",
      "Iteration 120025 | Loss: 0.562844\n",
      "Iteration 120050 | Loss: 0.562531\n",
      "Iteration 120075 | Loss: 0.562219\n",
      "Iteration 120100 | Loss: 0.561906\n",
      "Iteration 120125 | Loss: 0.561595\n",
      "Iteration 120150 | Loss: 0.561283\n",
      "Iteration 120175 | Loss: 0.560972\n",
      "Iteration 120200 | Loss: 0.560661\n",
      "Iteration 120225 | Loss: 0.560351\n",
      "Iteration 120250 | Loss: 0.560040\n",
      "Iteration 120275 | Loss: 0.559731\n",
      "Iteration 120300 | Loss: 0.559421\n",
      "Iteration 120325 | Loss: 0.559112\n",
      "Iteration 120350 | Loss: 0.558803\n",
      "Iteration 120375 | Loss: 0.558494\n",
      "Iteration 120400 | Loss: 0.558186\n",
      "Iteration 120425 | Loss: 0.557878\n",
      "Iteration 120450 | Loss: 0.557571\n",
      "Iteration 120475 | Loss: 0.557263\n",
      "Iteration 120500 | Loss: 0.556956\n",
      "Iteration 120525 | Loss: 0.556650\n",
      "Iteration 120550 | Loss: 0.556344\n",
      "Iteration 120575 | Loss: 0.556038\n",
      "Iteration 120600 | Loss: 0.555732\n",
      "Iteration 120625 | Loss: 0.555427\n",
      "Iteration 120650 | Loss: 0.555121\n",
      "Iteration 120675 | Loss: 0.554817\n",
      "Iteration 120700 | Loss: 0.554512\n",
      "Iteration 120725 | Loss: 0.554208\n",
      "Iteration 120750 | Loss: 0.553905\n",
      "Iteration 120775 | Loss: 0.553601\n",
      "Iteration 120800 | Loss: 0.553298\n",
      "Iteration 120825 | Loss: 0.552995\n",
      "Iteration 120850 | Loss: 0.552693\n",
      "Iteration 120875 | Loss: 0.552390\n",
      "Iteration 120900 | Loss: 0.552089\n",
      "Iteration 120925 | Loss: 0.551787\n",
      "Iteration 120950 | Loss: 0.551486\n",
      "Iteration 120975 | Loss: 0.551185\n",
      "Iteration 121000 | Loss: 0.550884\n",
      "Iteration 121025 | Loss: 0.550584\n",
      "Iteration 121050 | Loss: 0.550284\n",
      "Iteration 121075 | Loss: 0.549984\n",
      "Iteration 121100 | Loss: 0.549685\n",
      "Iteration 121125 | Loss: 0.549386\n",
      "Iteration 121150 | Loss: 0.549087\n",
      "Iteration 121175 | Loss: 0.548789\n",
      "Iteration 121200 | Loss: 0.548491\n",
      "Iteration 121225 | Loss: 0.548193\n",
      "Iteration 121250 | Loss: 0.547895\n",
      "Iteration 121275 | Loss: 0.547598\n",
      "Iteration 121300 | Loss: 0.547301\n",
      "Iteration 121325 | Loss: 0.547005\n",
      "Iteration 121350 | Loss: 0.546709\n",
      "Iteration 121375 | Loss: 0.546413\n",
      "Iteration 121400 | Loss: 0.546117\n",
      "Iteration 121425 | Loss: 0.545822\n",
      "Iteration 121450 | Loss: 0.545527\n",
      "Iteration 121475 | Loss: 0.545232\n",
      "Iteration 121500 | Loss: 0.544938\n",
      "Iteration 121525 | Loss: 0.544644\n",
      "Iteration 121550 | Loss: 0.544350\n",
      "Iteration 121575 | Loss: 0.544056\n",
      "Iteration 121600 | Loss: 0.543763\n",
      "Iteration 121625 | Loss: 0.543470\n",
      "Iteration 121650 | Loss: 0.543178\n",
      "Iteration 121675 | Loss: 0.542885\n",
      "Iteration 121700 | Loss: 0.542594\n",
      "Iteration 121725 | Loss: 0.542302\n",
      "Iteration 121750 | Loss: 0.542011\n",
      "Iteration 121775 | Loss: 0.541720\n",
      "Iteration 121800 | Loss: 0.541429\n",
      "Iteration 121825 | Loss: 0.541138\n",
      "Iteration 121850 | Loss: 0.540848\n",
      "Iteration 121875 | Loss: 0.540558\n",
      "Iteration 121900 | Loss: 0.540269\n",
      "Iteration 121925 | Loss: 0.539980\n",
      "Iteration 121950 | Loss: 0.539691\n",
      "Iteration 121975 | Loss: 0.539402\n",
      "Iteration 122000 | Loss: 0.539114\n",
      "Iteration 122025 | Loss: 0.538826\n",
      "Iteration 122050 | Loss: 0.538538\n",
      "Iteration 122075 | Loss: 0.538251\n",
      "Iteration 122100 | Loss: 0.537964\n",
      "Iteration 122125 | Loss: 0.537677\n",
      "Iteration 122150 | Loss: 0.537390\n",
      "Iteration 122175 | Loss: 0.537104\n",
      "Iteration 122200 | Loss: 0.536818\n",
      "Iteration 122225 | Loss: 0.536533\n",
      "Iteration 122250 | Loss: 0.536247\n",
      "Iteration 122275 | Loss: 0.535962\n",
      "Iteration 122300 | Loss: 0.535678\n",
      "Iteration 122325 | Loss: 0.535393\n",
      "Iteration 122350 | Loss: 0.535109\n",
      "Iteration 122375 | Loss: 0.534825\n",
      "Iteration 122400 | Loss: 0.534542\n",
      "Iteration 122425 | Loss: 0.534259\n",
      "Iteration 122450 | Loss: 0.533976\n",
      "Iteration 122475 | Loss: 0.533693\n",
      "Iteration 122500 | Loss: 0.533411\n",
      "Iteration 122525 | Loss: 0.533129\n",
      "Iteration 122550 | Loss: 0.532847\n",
      "Iteration 122575 | Loss: 0.532565\n",
      "Iteration 122600 | Loss: 0.532284\n",
      "Iteration 122625 | Loss: 0.532003\n",
      "Iteration 122650 | Loss: 0.531723\n",
      "Iteration 122675 | Loss: 0.531442\n",
      "Iteration 122700 | Loss: 0.531162\n",
      "Iteration 122725 | Loss: 0.530883\n",
      "Iteration 122750 | Loss: 0.530603\n",
      "Iteration 122775 | Loss: 0.530324\n",
      "Iteration 122800 | Loss: 0.530045\n",
      "Iteration 122825 | Loss: 0.529767\n",
      "Iteration 122850 | Loss: 0.529489\n",
      "Iteration 122875 | Loss: 0.529211\n",
      "Iteration 122900 | Loss: 0.528933\n",
      "Iteration 122925 | Loss: 0.528656\n",
      "Iteration 122950 | Loss: 0.528379\n",
      "Iteration 122975 | Loss: 0.528102\n",
      "Iteration 123000 | Loss: 0.527825\n",
      "Iteration 123025 | Loss: 0.527549\n",
      "Iteration 123050 | Loss: 0.527273\n",
      "Iteration 123075 | Loss: 0.526997\n",
      "Iteration 123100 | Loss: 0.526722\n",
      "Iteration 123125 | Loss: 0.526447\n",
      "Iteration 123150 | Loss: 0.526172\n",
      "Iteration 123175 | Loss: 0.525898\n",
      "Iteration 123200 | Loss: 0.525624\n",
      "Iteration 123225 | Loss: 0.525350\n",
      "Iteration 123250 | Loss: 0.525076\n",
      "Iteration 123275 | Loss: 0.524803\n",
      "Iteration 123300 | Loss: 0.524530\n",
      "Iteration 123325 | Loss: 0.524257\n",
      "Iteration 123350 | Loss: 0.523984\n",
      "Iteration 123375 | Loss: 0.523712\n",
      "Iteration 123400 | Loss: 0.523440\n",
      "Iteration 123425 | Loss: 0.523169\n",
      "Iteration 123450 | Loss: 0.522897\n",
      "Iteration 123475 | Loss: 0.522626\n",
      "Iteration 123500 | Loss: 0.522355\n",
      "Iteration 123525 | Loss: 0.522085\n",
      "Iteration 123550 | Loss: 0.521815\n",
      "Iteration 123575 | Loss: 0.521545\n",
      "Iteration 123600 | Loss: 0.521275\n",
      "Iteration 123625 | Loss: 0.521006\n",
      "Iteration 123650 | Loss: 0.520737\n",
      "Iteration 123675 | Loss: 0.520468\n",
      "Iteration 123700 | Loss: 0.520199\n",
      "Iteration 123725 | Loss: 0.519931\n",
      "Iteration 123750 | Loss: 0.519663\n",
      "Iteration 123775 | Loss: 0.519395\n",
      "Iteration 123800 | Loss: 0.519128\n",
      "Iteration 123825 | Loss: 0.518861\n",
      "Iteration 123850 | Loss: 0.518594\n",
      "Iteration 123875 | Loss: 0.518327\n",
      "Iteration 123900 | Loss: 0.518061\n",
      "Iteration 123925 | Loss: 0.517795\n",
      "Iteration 123950 | Loss: 0.517529\n",
      "Iteration 123975 | Loss: 0.517264\n",
      "Iteration 124000 | Loss: 0.516999\n",
      "Iteration 124025 | Loss: 0.516734\n",
      "Iteration 124050 | Loss: 0.516469\n",
      "Iteration 124075 | Loss: 0.516205\n",
      "Iteration 124100 | Loss: 0.515941\n",
      "Iteration 124125 | Loss: 0.515677\n",
      "Iteration 124150 | Loss: 0.515413\n",
      "Iteration 124175 | Loss: 0.515150\n",
      "Iteration 124200 | Loss: 0.514887\n",
      "Iteration 124225 | Loss: 0.514624\n",
      "Iteration 124250 | Loss: 0.514362\n",
      "Iteration 124275 | Loss: 0.514100\n",
      "Iteration 124300 | Loss: 0.513838\n",
      "Iteration 124325 | Loss: 0.513576\n",
      "Iteration 124350 | Loss: 0.513315\n",
      "Iteration 124375 | Loss: 0.513054\n",
      "Iteration 124400 | Loss: 0.512793\n",
      "Iteration 124425 | Loss: 0.512532\n",
      "Iteration 124450 | Loss: 0.512272\n",
      "Iteration 124475 | Loss: 0.512012\n",
      "Iteration 124500 | Loss: 0.511753\n",
      "Iteration 124525 | Loss: 0.511493\n",
      "Iteration 124550 | Loss: 0.511234\n",
      "Iteration 124575 | Loss: 0.510975\n",
      "Iteration 124600 | Loss: 0.510716\n",
      "Iteration 124625 | Loss: 0.510458\n",
      "Iteration 124650 | Loss: 0.510200\n",
      "Iteration 124675 | Loss: 0.509942\n",
      "Iteration 124700 | Loss: 0.509685\n",
      "Iteration 124725 | Loss: 0.509427\n",
      "Iteration 124750 | Loss: 0.509170\n",
      "Iteration 124775 | Loss: 0.508914\n",
      "Iteration 124800 | Loss: 0.508657\n",
      "Iteration 124825 | Loss: 0.508401\n",
      "Iteration 124850 | Loss: 0.508145\n",
      "Iteration 124875 | Loss: 0.507889\n",
      "Iteration 124900 | Loss: 0.507634\n",
      "Iteration 124925 | Loss: 0.507379\n",
      "Iteration 124950 | Loss: 0.507124\n",
      "Iteration 124975 | Loss: 0.506869\n",
      "Iteration 125000 | Loss: 0.506615\n",
      "Iteration 125025 | Loss: 0.506361\n",
      "Iteration 125050 | Loss: 0.506107\n",
      "Iteration 125075 | Loss: 0.505854\n",
      "Iteration 125100 | Loss: 0.505600\n",
      "Iteration 125125 | Loss: 0.505347\n",
      "Iteration 125150 | Loss: 0.505094\n",
      "Iteration 125175 | Loss: 0.504842\n",
      "Iteration 125200 | Loss: 0.504590\n",
      "Iteration 125225 | Loss: 0.504338\n",
      "Iteration 125250 | Loss: 0.504086\n",
      "Iteration 125275 | Loss: 0.503835\n",
      "Iteration 125300 | Loss: 0.503584\n",
      "Iteration 125325 | Loss: 0.503333\n",
      "Iteration 125350 | Loss: 0.503082\n",
      "Iteration 125375 | Loss: 0.502832\n",
      "Iteration 125400 | Loss: 0.502581\n",
      "Iteration 125425 | Loss: 0.502332\n",
      "Iteration 125450 | Loss: 0.502082\n",
      "Iteration 125475 | Loss: 0.501833\n",
      "Iteration 125500 | Loss: 0.501584\n",
      "Iteration 125525 | Loss: 0.501335\n",
      "Iteration 125550 | Loss: 0.501086\n",
      "Iteration 125575 | Loss: 0.500838\n",
      "Iteration 125600 | Loss: 0.500590\n",
      "Iteration 125625 | Loss: 0.500342\n",
      "Iteration 125650 | Loss: 0.500095\n",
      "Iteration 125675 | Loss: 0.499847\n",
      "Iteration 125700 | Loss: 0.499600\n",
      "Iteration 125725 | Loss: 0.499354\n",
      "Iteration 125750 | Loss: 0.499107\n",
      "Iteration 125775 | Loss: 0.498861\n",
      "Iteration 125800 | Loss: 0.498615\n",
      "Iteration 125825 | Loss: 0.498369\n",
      "Iteration 125850 | Loss: 0.498124\n",
      "Iteration 125875 | Loss: 0.497878\n",
      "Iteration 125900 | Loss: 0.497633\n",
      "Iteration 125925 | Loss: 0.497389\n",
      "Iteration 125950 | Loss: 0.497144\n",
      "Iteration 125975 | Loss: 0.496900\n",
      "Iteration 126000 | Loss: 0.496656\n",
      "Iteration 126025 | Loss: 0.496413\n",
      "Iteration 126050 | Loss: 0.496169\n",
      "Iteration 126075 | Loss: 0.495926\n",
      "Iteration 126100 | Loss: 0.495683\n",
      "Iteration 126125 | Loss: 0.495440\n",
      "Iteration 126150 | Loss: 0.495198\n",
      "Iteration 126175 | Loss: 0.494956\n",
      "Iteration 126200 | Loss: 0.494714\n",
      "Iteration 126225 | Loss: 0.494472\n",
      "Iteration 126250 | Loss: 0.494231\n",
      "Iteration 126275 | Loss: 0.493990\n",
      "Iteration 126300 | Loss: 0.493749\n",
      "Iteration 126325 | Loss: 0.493508\n",
      "Iteration 126350 | Loss: 0.493268\n",
      "Iteration 126375 | Loss: 0.493028\n",
      "Iteration 126400 | Loss: 0.492788\n",
      "Iteration 126425 | Loss: 0.492548\n",
      "Iteration 126450 | Loss: 0.492309\n",
      "Iteration 126475 | Loss: 0.492070\n",
      "Iteration 126500 | Loss: 0.491831\n",
      "Iteration 126525 | Loss: 0.491592\n",
      "Iteration 126550 | Loss: 0.491354\n",
      "Iteration 126575 | Loss: 0.491116\n",
      "Iteration 126600 | Loss: 0.490878\n",
      "Iteration 126625 | Loss: 0.490640\n",
      "Iteration 126650 | Loss: 0.490403\n",
      "Iteration 126675 | Loss: 0.490165\n",
      "Iteration 126700 | Loss: 0.489929\n",
      "Iteration 126725 | Loss: 0.489692\n",
      "Iteration 126750 | Loss: 0.489455\n",
      "Iteration 126775 | Loss: 0.489219\n",
      "Iteration 126800 | Loss: 0.488983\n",
      "Iteration 126825 | Loss: 0.488748\n",
      "Iteration 126850 | Loss: 0.488512\n",
      "Iteration 126875 | Loss: 0.488277\n",
      "Iteration 126900 | Loss: 0.488042\n",
      "Iteration 126925 | Loss: 0.487808\n",
      "Iteration 126950 | Loss: 0.487573\n",
      "Iteration 126975 | Loss: 0.487339\n",
      "Iteration 127000 | Loss: 0.487105\n",
      "Iteration 127025 | Loss: 0.486871\n",
      "Iteration 127050 | Loss: 0.486638\n",
      "Iteration 127075 | Loss: 0.486405\n",
      "Iteration 127100 | Loss: 0.486172\n",
      "Iteration 127125 | Loss: 0.485939\n",
      "Iteration 127150 | Loss: 0.485706\n",
      "Iteration 127175 | Loss: 0.485474\n",
      "Iteration 127200 | Loss: 0.485242\n",
      "Iteration 127225 | Loss: 0.485010\n",
      "Iteration 127250 | Loss: 0.484779\n",
      "Iteration 127275 | Loss: 0.484548\n",
      "Iteration 127300 | Loss: 0.484316\n",
      "Iteration 127325 | Loss: 0.484086\n",
      "Iteration 127350 | Loss: 0.483855\n",
      "Iteration 127375 | Loss: 0.483625\n",
      "Iteration 127400 | Loss: 0.483395\n",
      "Iteration 127425 | Loss: 0.483165\n",
      "Iteration 127450 | Loss: 0.482935\n",
      "Iteration 127475 | Loss: 0.482706\n",
      "Iteration 127500 | Loss: 0.482477\n",
      "Iteration 127525 | Loss: 0.482248\n",
      "Iteration 127550 | Loss: 0.482019\n",
      "Iteration 127575 | Loss: 0.481791\n",
      "Iteration 127600 | Loss: 0.481563\n",
      "Iteration 127625 | Loss: 0.481335\n",
      "Iteration 127650 | Loss: 0.481107\n",
      "Iteration 127675 | Loss: 0.480880\n",
      "Iteration 127700 | Loss: 0.480652\n",
      "Iteration 127725 | Loss: 0.480426\n",
      "Iteration 127750 | Loss: 0.480199\n",
      "Iteration 127775 | Loss: 0.479972\n",
      "Iteration 127800 | Loss: 0.479746\n",
      "Iteration 127825 | Loss: 0.479520\n",
      "Iteration 127850 | Loss: 0.479294\n",
      "Iteration 127875 | Loss: 0.479069\n",
      "Iteration 127900 | Loss: 0.478843\n",
      "Iteration 127925 | Loss: 0.478618\n",
      "Iteration 127950 | Loss: 0.478393\n",
      "Iteration 127975 | Loss: 0.478169\n",
      "Iteration 128000 | Loss: 0.477944\n",
      "Iteration 128025 | Loss: 0.477720\n",
      "Iteration 128050 | Loss: 0.477496\n",
      "Iteration 128075 | Loss: 0.477273\n",
      "Iteration 128100 | Loss: 0.477049\n",
      "Iteration 128125 | Loss: 0.476826\n",
      "Iteration 128150 | Loss: 0.476603\n",
      "Iteration 128175 | Loss: 0.476380\n",
      "Iteration 128200 | Loss: 0.476158\n",
      "Iteration 128225 | Loss: 0.475935\n",
      "Iteration 128250 | Loss: 0.475713\n",
      "Iteration 128275 | Loss: 0.475492\n",
      "Iteration 128300 | Loss: 0.475270\n",
      "Iteration 128325 | Loss: 0.475049\n",
      "Iteration 128350 | Loss: 0.474828\n",
      "Iteration 128375 | Loss: 0.474607\n",
      "Iteration 128400 | Loss: 0.474386\n",
      "Iteration 128425 | Loss: 0.474166\n",
      "Iteration 128450 | Loss: 0.473945\n",
      "Iteration 128475 | Loss: 0.473725\n",
      "Iteration 128500 | Loss: 0.473506\n",
      "Iteration 128525 | Loss: 0.473286\n",
      "Iteration 128550 | Loss: 0.473067\n",
      "Iteration 128575 | Loss: 0.472848\n",
      "Iteration 128600 | Loss: 0.472629\n",
      "Iteration 128625 | Loss: 0.472410\n",
      "Iteration 128650 | Loss: 0.472192\n",
      "Iteration 128675 | Loss: 0.471974\n",
      "Iteration 128700 | Loss: 0.471756\n",
      "Iteration 128725 | Loss: 0.471538\n",
      "Iteration 128750 | Loss: 0.471321\n",
      "Iteration 128775 | Loss: 0.471104\n",
      "Iteration 128800 | Loss: 0.470887\n",
      "Iteration 128825 | Loss: 0.470670\n",
      "Iteration 128850 | Loss: 0.470453\n",
      "Iteration 128875 | Loss: 0.470237\n",
      "Iteration 128900 | Loss: 0.470021\n",
      "Iteration 128925 | Loss: 0.469805\n",
      "Iteration 128950 | Loss: 0.469589\n",
      "Iteration 128975 | Loss: 0.469374\n",
      "Iteration 129000 | Loss: 0.469159\n",
      "Iteration 129025 | Loss: 0.468944\n",
      "Iteration 129050 | Loss: 0.468729\n",
      "Iteration 129075 | Loss: 0.468514\n",
      "Iteration 129100 | Loss: 0.468300\n",
      "Iteration 129125 | Loss: 0.468086\n",
      "Iteration 129150 | Loss: 0.467872\n",
      "Iteration 129175 | Loss: 0.467659\n",
      "Iteration 129200 | Loss: 0.467445\n",
      "Iteration 129225 | Loss: 0.467232\n",
      "Iteration 129250 | Loss: 0.467019\n",
      "Iteration 129275 | Loss: 0.466806\n",
      "Iteration 129300 | Loss: 0.466594\n",
      "Iteration 129325 | Loss: 0.466381\n",
      "Iteration 129350 | Loss: 0.466169\n",
      "Iteration 129375 | Loss: 0.465957\n",
      "Iteration 129400 | Loss: 0.465746\n",
      "Iteration 129425 | Loss: 0.465534\n",
      "Iteration 129450 | Loss: 0.465323\n",
      "Iteration 129475 | Loss: 0.465112\n",
      "Iteration 129500 | Loss: 0.464902\n",
      "Iteration 129525 | Loss: 0.464691\n",
      "Iteration 129550 | Loss: 0.464481\n",
      "Iteration 129575 | Loss: 0.464271\n",
      "Iteration 129600 | Loss: 0.464061\n",
      "Iteration 129625 | Loss: 0.463851\n",
      "Iteration 129650 | Loss: 0.463642\n",
      "Iteration 129675 | Loss: 0.463432\n",
      "Iteration 129700 | Loss: 0.463223\n",
      "Iteration 129725 | Loss: 0.463015\n",
      "Iteration 129750 | Loss: 0.462806\n",
      "Iteration 129775 | Loss: 0.462598\n",
      "Iteration 129800 | Loss: 0.462390\n",
      "Iteration 129825 | Loss: 0.462182\n",
      "Iteration 129850 | Loss: 0.461974\n",
      "Iteration 129875 | Loss: 0.461766\n",
      "Iteration 129900 | Loss: 0.461559\n",
      "Iteration 129925 | Loss: 0.461352\n",
      "Iteration 129950 | Loss: 0.461145\n",
      "Iteration 129975 | Loss: 0.460939\n",
      "Iteration 130000 | Loss: 0.460732\n",
      "Iteration 130025 | Loss: 0.460526\n",
      "Iteration 130050 | Loss: 0.460320\n",
      "Iteration 130075 | Loss: 0.460114\n",
      "Iteration 130100 | Loss: 0.459909\n",
      "Iteration 130125 | Loss: 0.459704\n",
      "Iteration 130150 | Loss: 0.459498\n",
      "Iteration 130175 | Loss: 0.459294\n",
      "Iteration 130200 | Loss: 0.459089\n",
      "Iteration 130225 | Loss: 0.458884\n",
      "Iteration 130250 | Loss: 0.458680\n",
      "Iteration 130275 | Loss: 0.458476\n",
      "Iteration 130300 | Loss: 0.458272\n",
      "Iteration 130325 | Loss: 0.458069\n",
      "Iteration 130350 | Loss: 0.457865\n",
      "Iteration 130375 | Loss: 0.457662\n",
      "Iteration 130400 | Loss: 0.457459\n",
      "Iteration 130425 | Loss: 0.457256\n",
      "Iteration 130450 | Loss: 0.457054\n",
      "Iteration 130475 | Loss: 0.456851\n",
      "Iteration 130500 | Loss: 0.456649\n",
      "Iteration 130525 | Loss: 0.456447\n",
      "Iteration 130550 | Loss: 0.456246\n",
      "Iteration 130575 | Loss: 0.456044\n",
      "Iteration 130600 | Loss: 0.455843\n",
      "Iteration 130625 | Loss: 0.455642\n",
      "Iteration 130650 | Loss: 0.455441\n",
      "Iteration 130675 | Loss: 0.455240\n",
      "Iteration 130700 | Loss: 0.455040\n",
      "Iteration 130725 | Loss: 0.454840\n",
      "Iteration 130750 | Loss: 0.454640\n",
      "Iteration 130775 | Loss: 0.454440\n",
      "Iteration 130800 | Loss: 0.454240\n",
      "Iteration 130825 | Loss: 0.454041\n",
      "Iteration 130850 | Loss: 0.453842\n",
      "Iteration 130875 | Loss: 0.453643\n",
      "Iteration 130900 | Loss: 0.453444\n",
      "Iteration 130925 | Loss: 0.453245\n",
      "Iteration 130950 | Loss: 0.453047\n",
      "Iteration 130975 | Loss: 0.452849\n",
      "Iteration 131000 | Loss: 0.452651\n",
      "Iteration 131025 | Loss: 0.452453\n",
      "Iteration 131050 | Loss: 0.452255\n",
      "Iteration 131075 | Loss: 0.452058\n",
      "Iteration 131100 | Loss: 0.451861\n",
      "Iteration 131125 | Loss: 0.451664\n",
      "Iteration 131150 | Loss: 0.451467\n",
      "Iteration 131175 | Loss: 0.451271\n",
      "Iteration 131200 | Loss: 0.451074\n",
      "Iteration 131225 | Loss: 0.450878\n",
      "Iteration 131250 | Loss: 0.450682\n",
      "Iteration 131275 | Loss: 0.450487\n",
      "Iteration 131300 | Loss: 0.450291\n",
      "Iteration 131325 | Loss: 0.450096\n",
      "Iteration 131350 | Loss: 0.449901\n",
      "Iteration 131375 | Loss: 0.449706\n",
      "Iteration 131400 | Loss: 0.449511\n",
      "Iteration 131425 | Loss: 0.449317\n",
      "Iteration 131450 | Loss: 0.449123\n",
      "Iteration 131475 | Loss: 0.448929\n",
      "Iteration 131500 | Loss: 0.448735\n",
      "Iteration 131525 | Loss: 0.448541\n",
      "Iteration 131550 | Loss: 0.448348\n",
      "Iteration 131575 | Loss: 0.448154\n",
      "Iteration 131600 | Loss: 0.447961\n",
      "Iteration 131625 | Loss: 0.447768\n",
      "Iteration 131650 | Loss: 0.447576\n",
      "Iteration 131675 | Loss: 0.447383\n",
      "Iteration 131700 | Loss: 0.447191\n",
      "Iteration 131725 | Loss: 0.446999\n",
      "Iteration 131750 | Loss: 0.446807\n",
      "Iteration 131775 | Loss: 0.446616\n",
      "Iteration 131800 | Loss: 0.446424\n",
      "Iteration 131825 | Loss: 0.446233\n",
      "Iteration 131850 | Loss: 0.446042\n",
      "Iteration 131875 | Loss: 0.445851\n",
      "Iteration 131900 | Loss: 0.445660\n",
      "Iteration 131925 | Loss: 0.445470\n",
      "Iteration 131950 | Loss: 0.445280\n",
      "Iteration 131975 | Loss: 0.445090\n",
      "Iteration 132000 | Loss: 0.444900\n",
      "Iteration 132025 | Loss: 0.444710\n",
      "Iteration 132050 | Loss: 0.444521\n",
      "Iteration 132075 | Loss: 0.444331\n",
      "Iteration 132100 | Loss: 0.444142\n",
      "Iteration 132125 | Loss: 0.443953\n",
      "Iteration 132150 | Loss: 0.443765\n",
      "Iteration 132175 | Loss: 0.443576\n",
      "Iteration 132200 | Loss: 0.443388\n",
      "Iteration 132225 | Loss: 0.443200\n",
      "Iteration 132250 | Loss: 0.443012\n",
      "Iteration 132275 | Loss: 0.442824\n",
      "Iteration 132300 | Loss: 0.442637\n",
      "Iteration 132325 | Loss: 0.442449\n",
      "Iteration 132350 | Loss: 0.442262\n",
      "Iteration 132375 | Loss: 0.442075\n",
      "Iteration 132400 | Loss: 0.441889\n",
      "Iteration 132425 | Loss: 0.441702\n",
      "Iteration 132450 | Loss: 0.441516\n",
      "Iteration 132475 | Loss: 0.441330\n",
      "Iteration 132500 | Loss: 0.441144\n",
      "Iteration 132525 | Loss: 0.440958\n",
      "Iteration 132550 | Loss: 0.440773\n",
      "Iteration 132575 | Loss: 0.440587\n",
      "Iteration 132600 | Loss: 0.440402\n",
      "Iteration 132625 | Loss: 0.440217\n",
      "Iteration 132650 | Loss: 0.440032\n",
      "Iteration 132675 | Loss: 0.439848\n",
      "Iteration 132700 | Loss: 0.439663\n",
      "Iteration 132725 | Loss: 0.439479\n",
      "Iteration 132750 | Loss: 0.439295\n",
      "Iteration 132775 | Loss: 0.439111\n",
      "Iteration 132800 | Loss: 0.438928\n",
      "Iteration 132825 | Loss: 0.438744\n",
      "Iteration 132850 | Loss: 0.438561\n",
      "Iteration 132875 | Loss: 0.438378\n",
      "Iteration 132900 | Loss: 0.438195\n",
      "Iteration 132925 | Loss: 0.438012\n",
      "Iteration 132950 | Loss: 0.437830\n",
      "Iteration 132975 | Loss: 0.437648\n",
      "Iteration 133000 | Loss: 0.437466\n",
      "Iteration 133025 | Loss: 0.437284\n",
      "Iteration 133050 | Loss: 0.437102\n",
      "Iteration 133075 | Loss: 0.436920\n",
      "Iteration 133100 | Loss: 0.436739\n",
      "Iteration 133125 | Loss: 0.436558\n",
      "Iteration 133150 | Loss: 0.436377\n",
      "Iteration 133175 | Loss: 0.436196\n",
      "Iteration 133200 | Loss: 0.436016\n",
      "Iteration 133225 | Loss: 0.435835\n",
      "Iteration 133250 | Loss: 0.435655\n",
      "Iteration 133275 | Loss: 0.435475\n",
      "Iteration 133300 | Loss: 0.435295\n",
      "Iteration 133325 | Loss: 0.435116\n",
      "Iteration 133350 | Loss: 0.434936\n",
      "Iteration 133375 | Loss: 0.434757\n",
      "Iteration 133400 | Loss: 0.434578\n",
      "Iteration 133425 | Loss: 0.434399\n",
      "Iteration 133450 | Loss: 0.434220\n",
      "Iteration 133475 | Loss: 0.434042\n",
      "Iteration 133500 | Loss: 0.433863\n",
      "Iteration 133525 | Loss: 0.433685\n",
      "Iteration 133550 | Loss: 0.433507\n",
      "Iteration 133575 | Loss: 0.433330\n",
      "Iteration 133600 | Loss: 0.433152\n",
      "Iteration 133625 | Loss: 0.432975\n",
      "Iteration 133650 | Loss: 0.432797\n",
      "Iteration 133675 | Loss: 0.432620\n",
      "Iteration 133700 | Loss: 0.432443\n",
      "Iteration 133725 | Loss: 0.432267\n",
      "Iteration 133750 | Loss: 0.432090\n",
      "Iteration 133775 | Loss: 0.431914\n",
      "Iteration 133800 | Loss: 0.431738\n",
      "Iteration 133825 | Loss: 0.431562\n",
      "Iteration 133850 | Loss: 0.431386\n",
      "Iteration 133875 | Loss: 0.431211\n",
      "Iteration 133900 | Loss: 0.431035\n",
      "Iteration 133925 | Loss: 0.430860\n",
      "Iteration 133950 | Loss: 0.430685\n",
      "Iteration 133975 | Loss: 0.430510\n",
      "Iteration 134000 | Loss: 0.430336\n",
      "Iteration 134025 | Loss: 0.430161\n",
      "Iteration 134050 | Loss: 0.429987\n",
      "Iteration 134075 | Loss: 0.429813\n",
      "Iteration 134100 | Loss: 0.429639\n",
      "Iteration 134125 | Loss: 0.429465\n",
      "Iteration 134150 | Loss: 0.429292\n",
      "Iteration 134175 | Loss: 0.429118\n",
      "Iteration 134200 | Loss: 0.428945\n",
      "Iteration 134225 | Loss: 0.428772\n",
      "Iteration 134250 | Loss: 0.428599\n",
      "Iteration 134275 | Loss: 0.428427\n",
      "Iteration 134300 | Loss: 0.428254\n",
      "Iteration 134325 | Loss: 0.428082\n",
      "Iteration 134350 | Loss: 0.427910\n",
      "Iteration 134375 | Loss: 0.427738\n",
      "Iteration 134400 | Loss: 0.427566\n",
      "Iteration 134425 | Loss: 0.427394\n",
      "Iteration 134450 | Loss: 0.427223\n",
      "Iteration 134475 | Loss: 0.427052\n",
      "Iteration 134500 | Loss: 0.426881\n",
      "Iteration 134525 | Loss: 0.426710\n",
      "Iteration 134550 | Loss: 0.426539\n",
      "Iteration 134575 | Loss: 0.426369\n",
      "Iteration 134600 | Loss: 0.426198\n",
      "Iteration 134625 | Loss: 0.426028\n",
      "Iteration 134650 | Loss: 0.425858\n",
      "Iteration 134675 | Loss: 0.425689\n",
      "Iteration 134700 | Loss: 0.425519\n",
      "Iteration 134725 | Loss: 0.425349\n",
      "Iteration 134750 | Loss: 0.425180\n",
      "Iteration 134775 | Loss: 0.425011\n",
      "Iteration 134800 | Loss: 0.424842\n",
      "Iteration 134825 | Loss: 0.424673\n",
      "Iteration 134850 | Loss: 0.424505\n",
      "Iteration 134875 | Loss: 0.424337\n",
      "Iteration 134900 | Loss: 0.424168\n",
      "Iteration 134925 | Loss: 0.424000\n",
      "Iteration 134950 | Loss: 0.423832\n",
      "Iteration 134975 | Loss: 0.423665\n",
      "Iteration 135000 | Loss: 0.423497\n",
      "Iteration 135025 | Loss: 0.423330\n",
      "Iteration 135050 | Loss: 0.423163\n",
      "Iteration 135075 | Loss: 0.422996\n",
      "Iteration 135100 | Loss: 0.422829\n",
      "Iteration 135125 | Loss: 0.422662\n",
      "Iteration 135150 | Loss: 0.422496\n",
      "Iteration 135175 | Loss: 0.422330\n",
      "Iteration 135200 | Loss: 0.422164\n",
      "Iteration 135225 | Loss: 0.421998\n",
      "Iteration 135250 | Loss: 0.421832\n",
      "Iteration 135275 | Loss: 0.421666\n",
      "Iteration 135300 | Loss: 0.421501\n",
      "Iteration 135325 | Loss: 0.421336\n",
      "Iteration 135350 | Loss: 0.421171\n",
      "Iteration 135375 | Loss: 0.421006\n",
      "Iteration 135400 | Loss: 0.420841\n",
      "Iteration 135425 | Loss: 0.420676\n",
      "Iteration 135450 | Loss: 0.420512\n",
      "Iteration 135475 | Loss: 0.420348\n",
      "Iteration 135500 | Loss: 0.420184\n",
      "Iteration 135525 | Loss: 0.420020\n",
      "Iteration 135550 | Loss: 0.419856\n",
      "Iteration 135575 | Loss: 0.419693\n",
      "Iteration 135600 | Loss: 0.419529\n",
      "Iteration 135625 | Loss: 0.419366\n",
      "Iteration 135650 | Loss: 0.419203\n",
      "Iteration 135675 | Loss: 0.419040\n",
      "Iteration 135700 | Loss: 0.418877\n",
      "Iteration 135725 | Loss: 0.418715\n",
      "Iteration 135750 | Loss: 0.418553\n",
      "Iteration 135775 | Loss: 0.418390\n",
      "Iteration 135800 | Loss: 0.418228\n",
      "Iteration 135825 | Loss: 0.418067\n",
      "Iteration 135850 | Loss: 0.417905\n",
      "Iteration 135875 | Loss: 0.417743\n",
      "Iteration 135900 | Loss: 0.417582\n",
      "Iteration 135925 | Loss: 0.417421\n",
      "Iteration 135950 | Loss: 0.417260\n",
      "Iteration 135975 | Loss: 0.417099\n",
      "Iteration 136000 | Loss: 0.416939\n",
      "Iteration 136025 | Loss: 0.416778\n",
      "Iteration 136050 | Loss: 0.416618\n",
      "Iteration 136075 | Loss: 0.416458\n",
      "Iteration 136100 | Loss: 0.416298\n",
      "Iteration 136125 | Loss: 0.416138\n",
      "Iteration 136150 | Loss: 0.415978\n",
      "Iteration 136175 | Loss: 0.415819\n",
      "Iteration 136200 | Loss: 0.415659\n",
      "Iteration 136225 | Loss: 0.415500\n",
      "Iteration 136250 | Loss: 0.415341\n",
      "Iteration 136275 | Loss: 0.415182\n",
      "Iteration 136300 | Loss: 0.415024\n",
      "Iteration 136325 | Loss: 0.414865\n",
      "Iteration 136350 | Loss: 0.414707\n",
      "Iteration 136375 | Loss: 0.414549\n",
      "Iteration 136400 | Loss: 0.414391\n",
      "Iteration 136425 | Loss: 0.414233\n",
      "Iteration 136450 | Loss: 0.414075\n",
      "Iteration 136475 | Loss: 0.413918\n",
      "Iteration 136500 | Loss: 0.413760\n",
      "Iteration 136525 | Loss: 0.413603\n",
      "Iteration 136550 | Loss: 0.413446\n",
      "Iteration 136575 | Loss: 0.413289\n",
      "Iteration 136600 | Loss: 0.413133\n",
      "Iteration 136625 | Loss: 0.412976\n",
      "Iteration 136650 | Loss: 0.412820\n",
      "Iteration 136675 | Loss: 0.412664\n",
      "Iteration 136700 | Loss: 0.412508\n",
      "Iteration 136725 | Loss: 0.412352\n",
      "Iteration 136750 | Loss: 0.412196\n",
      "Iteration 136775 | Loss: 0.412041\n",
      "Iteration 136800 | Loss: 0.411885\n",
      "Iteration 136825 | Loss: 0.411730\n",
      "Iteration 136850 | Loss: 0.411575\n",
      "Iteration 136875 | Loss: 0.411420\n",
      "Iteration 136900 | Loss: 0.411265\n",
      "Iteration 136925 | Loss: 0.411111\n",
      "Iteration 136950 | Loss: 0.410956\n",
      "Iteration 136975 | Loss: 0.410802\n",
      "Iteration 137000 | Loss: 0.410648\n",
      "Iteration 137025 | Loss: 0.410494\n",
      "Iteration 137050 | Loss: 0.410340\n",
      "Iteration 137075 | Loss: 0.410187\n",
      "Iteration 137100 | Loss: 0.410033\n",
      "Iteration 137125 | Loss: 0.409880\n",
      "Iteration 137150 | Loss: 0.409727\n",
      "Iteration 137175 | Loss: 0.409574\n",
      "Iteration 137200 | Loss: 0.409421\n",
      "Iteration 137225 | Loss: 0.409268\n",
      "Iteration 137250 | Loss: 0.409116\n",
      "Iteration 137275 | Loss: 0.408964\n",
      "Iteration 137300 | Loss: 0.408812\n",
      "Iteration 137325 | Loss: 0.408660\n",
      "Iteration 137350 | Loss: 0.408508\n",
      "Iteration 137375 | Loss: 0.408356\n",
      "Iteration 137400 | Loss: 0.408204\n",
      "Iteration 137425 | Loss: 0.408053\n",
      "Iteration 137450 | Loss: 0.407902\n",
      "Iteration 137475 | Loss: 0.407751\n",
      "Iteration 137500 | Loss: 0.407600\n",
      "Iteration 137525 | Loss: 0.407449\n",
      "Iteration 137550 | Loss: 0.407299\n",
      "Iteration 137575 | Loss: 0.407148\n",
      "Iteration 137600 | Loss: 0.406998\n",
      "Iteration 137625 | Loss: 0.406848\n",
      "Iteration 137650 | Loss: 0.406698\n",
      "Iteration 137675 | Loss: 0.406548\n",
      "Iteration 137700 | Loss: 0.406398\n",
      "Iteration 137725 | Loss: 0.406249\n",
      "Iteration 137750 | Loss: 0.406100\n",
      "Iteration 137775 | Loss: 0.405950\n",
      "Iteration 137800 | Loss: 0.405801\n",
      "Iteration 137825 | Loss: 0.405652\n",
      "Iteration 137850 | Loss: 0.405504\n",
      "Iteration 137875 | Loss: 0.405355\n",
      "Iteration 137900 | Loss: 0.405207\n",
      "Iteration 137925 | Loss: 0.405059\n",
      "Iteration 137950 | Loss: 0.404910\n",
      "Iteration 137975 | Loss: 0.404763\n",
      "Iteration 138000 | Loss: 0.404615\n",
      "Iteration 138025 | Loss: 0.404467\n",
      "Iteration 138050 | Loss: 0.404320\n",
      "Iteration 138075 | Loss: 0.404172\n",
      "Iteration 138100 | Loss: 0.404025\n",
      "Iteration 138125 | Loss: 0.403878\n",
      "Iteration 138150 | Loss: 0.403731\n",
      "Iteration 138175 | Loss: 0.403585\n",
      "Iteration 138200 | Loss: 0.403438\n",
      "Iteration 138225 | Loss: 0.403292\n",
      "Iteration 138250 | Loss: 0.403145\n",
      "Iteration 138275 | Loss: 0.402999\n",
      "Iteration 138300 | Loss: 0.402853\n",
      "Iteration 138325 | Loss: 0.402708\n",
      "Iteration 138350 | Loss: 0.402562\n",
      "Iteration 138375 | Loss: 0.402416\n",
      "Iteration 138400 | Loss: 0.402271\n",
      "Iteration 138425 | Loss: 0.402126\n",
      "Iteration 138450 | Loss: 0.401981\n",
      "Iteration 138475 | Loss: 0.401836\n",
      "Iteration 138500 | Loss: 0.401691\n",
      "Iteration 138525 | Loss: 0.401547\n",
      "Iteration 138550 | Loss: 0.401402\n",
      "Iteration 138575 | Loss: 0.401258\n",
      "Iteration 138600 | Loss: 0.401114\n",
      "Iteration 138625 | Loss: 0.400970\n",
      "Iteration 138650 | Loss: 0.400826\n",
      "Iteration 138675 | Loss: 0.400682\n",
      "Iteration 138700 | Loss: 0.400539\n",
      "Iteration 138725 | Loss: 0.400395\n",
      "Iteration 138750 | Loss: 0.400252\n",
      "Iteration 138775 | Loss: 0.400109\n",
      "Iteration 138800 | Loss: 0.399966\n",
      "Iteration 138825 | Loss: 0.399823\n",
      "Iteration 138850 | Loss: 0.399681\n",
      "Iteration 138875 | Loss: 0.399538\n",
      "Iteration 138900 | Loss: 0.399396\n",
      "Iteration 138925 | Loss: 0.399254\n",
      "Iteration 138950 | Loss: 0.399112\n",
      "Iteration 138975 | Loss: 0.398970\n",
      "Iteration 139000 | Loss: 0.398828\n",
      "Iteration 139025 | Loss: 0.398687\n",
      "Iteration 139050 | Loss: 0.398545\n",
      "Iteration 139075 | Loss: 0.398404\n",
      "Iteration 139100 | Loss: 0.398263\n",
      "Iteration 139125 | Loss: 0.398122\n",
      "Iteration 139150 | Loss: 0.397981\n",
      "Iteration 139175 | Loss: 0.397840\n",
      "Iteration 139200 | Loss: 0.397699\n",
      "Iteration 139225 | Loss: 0.397559\n",
      "Iteration 139250 | Loss: 0.397419\n",
      "Iteration 139275 | Loss: 0.397279\n",
      "Iteration 139300 | Loss: 0.397139\n",
      "Iteration 139325 | Loss: 0.396999\n",
      "Iteration 139350 | Loss: 0.396859\n",
      "Iteration 139375 | Loss: 0.396720\n",
      "Iteration 139400 | Loss: 0.396580\n",
      "Iteration 139425 | Loss: 0.396441\n",
      "Iteration 139450 | Loss: 0.396302\n",
      "Iteration 139475 | Loss: 0.396163\n",
      "Iteration 139500 | Loss: 0.396024\n",
      "Iteration 139525 | Loss: 0.395885\n",
      "Iteration 139550 | Loss: 0.395747\n",
      "Iteration 139575 | Loss: 0.395609\n",
      "Iteration 139600 | Loss: 0.395470\n",
      "Iteration 139625 | Loss: 0.395332\n",
      "Iteration 139650 | Loss: 0.395194\n",
      "Iteration 139675 | Loss: 0.395056\n",
      "Iteration 139700 | Loss: 0.394919\n",
      "Iteration 139725 | Loss: 0.394781\n",
      "Iteration 139750 | Loss: 0.394644\n",
      "Iteration 139775 | Loss: 0.394507\n",
      "Iteration 139800 | Loss: 0.394370\n",
      "Iteration 139825 | Loss: 0.394233\n",
      "Iteration 139850 | Loss: 0.394096\n",
      "Iteration 139875 | Loss: 0.393959\n",
      "Iteration 139900 | Loss: 0.393823\n",
      "Iteration 139925 | Loss: 0.393686\n",
      "Iteration 139950 | Loss: 0.393550\n",
      "Iteration 139975 | Loss: 0.393414\n",
      "Iteration 140000 | Loss: 0.393278\n",
      "Iteration 140025 | Loss: 0.393142\n",
      "Iteration 140050 | Loss: 0.393007\n",
      "Iteration 140075 | Loss: 0.392871\n",
      "Iteration 140100 | Loss: 0.392736\n",
      "Iteration 140125 | Loss: 0.392600\n",
      "Iteration 140150 | Loss: 0.392465\n",
      "Iteration 140175 | Loss: 0.392330\n",
      "Iteration 140200 | Loss: 0.392196\n",
      "Iteration 140225 | Loss: 0.392061\n",
      "Iteration 140250 | Loss: 0.391926\n",
      "Iteration 140275 | Loss: 0.391792\n",
      "Iteration 140300 | Loss: 0.391658\n",
      "Iteration 140325 | Loss: 0.391524\n",
      "Iteration 140350 | Loss: 0.391390\n",
      "Iteration 140375 | Loss: 0.391256\n",
      "Iteration 140400 | Loss: 0.391122\n",
      "Iteration 140425 | Loss: 0.390989\n",
      "Iteration 140450 | Loss: 0.390855\n",
      "Iteration 140475 | Loss: 0.390722\n",
      "Iteration 140500 | Loss: 0.390589\n",
      "Iteration 140525 | Loss: 0.390456\n",
      "Iteration 140550 | Loss: 0.390323\n",
      "Iteration 140575 | Loss: 0.390190\n",
      "Iteration 140600 | Loss: 0.390058\n",
      "Iteration 140625 | Loss: 0.389925\n",
      "Iteration 140650 | Loss: 0.389793\n",
      "Iteration 140675 | Loss: 0.389661\n",
      "Iteration 140700 | Loss: 0.389529\n",
      "Iteration 140725 | Loss: 0.389397\n",
      "Iteration 140750 | Loss: 0.389265\n",
      "Iteration 140775 | Loss: 0.389133\n",
      "Iteration 140800 | Loss: 0.389002\n",
      "Iteration 140825 | Loss: 0.388871\n",
      "Iteration 140850 | Loss: 0.388739\n",
      "Iteration 140875 | Loss: 0.388608\n",
      "Iteration 140900 | Loss: 0.388477\n",
      "Iteration 140925 | Loss: 0.388347\n",
      "Iteration 140950 | Loss: 0.388216\n",
      "Iteration 140975 | Loss: 0.388085\n",
      "Iteration 141000 | Loss: 0.387955\n",
      "Iteration 141025 | Loss: 0.387825\n",
      "Iteration 141050 | Loss: 0.387695\n",
      "Iteration 141075 | Loss: 0.387565\n",
      "Iteration 141100 | Loss: 0.387435\n",
      "Iteration 141125 | Loss: 0.387305\n",
      "Iteration 141150 | Loss: 0.387175\n",
      "Iteration 141175 | Loss: 0.387046\n",
      "Iteration 141200 | Loss: 0.386917\n",
      "Iteration 141225 | Loss: 0.386788\n",
      "Iteration 141250 | Loss: 0.386659\n",
      "Iteration 141275 | Loss: 0.386530\n",
      "Iteration 141300 | Loss: 0.386401\n",
      "Iteration 141325 | Loss: 0.386272\n",
      "Iteration 141350 | Loss: 0.386144\n",
      "Iteration 141375 | Loss: 0.386015\n",
      "Iteration 141400 | Loss: 0.385887\n",
      "Iteration 141425 | Loss: 0.385759\n",
      "Iteration 141450 | Loss: 0.385631\n",
      "Iteration 141475 | Loss: 0.385503\n",
      "Iteration 141500 | Loss: 0.385376\n",
      "Iteration 141525 | Loss: 0.385248\n",
      "Iteration 141550 | Loss: 0.385121\n",
      "Iteration 141575 | Loss: 0.384993\n",
      "Iteration 141600 | Loss: 0.384866\n",
      "Iteration 141625 | Loss: 0.384739\n",
      "Iteration 141650 | Loss: 0.384612\n",
      "Iteration 141675 | Loss: 0.384485\n",
      "Iteration 141700 | Loss: 0.384359\n",
      "Iteration 141725 | Loss: 0.384232\n",
      "Iteration 141750 | Loss: 0.384106\n",
      "Iteration 141775 | Loss: 0.383980\n",
      "Iteration 141800 | Loss: 0.383854\n",
      "Iteration 141825 | Loss: 0.383728\n",
      "Iteration 141850 | Loss: 0.383602\n",
      "Iteration 141875 | Loss: 0.383476\n",
      "Iteration 141900 | Loss: 0.383350\n",
      "Iteration 141925 | Loss: 0.383225\n",
      "Iteration 141950 | Loss: 0.383100\n",
      "Iteration 141975 | Loss: 0.382974\n",
      "Iteration 142000 | Loss: 0.382849\n",
      "Iteration 142025 | Loss: 0.382725\n",
      "Iteration 142050 | Loss: 0.382600\n",
      "Iteration 142075 | Loss: 0.382475\n",
      "Iteration 142100 | Loss: 0.382351\n",
      "Iteration 142125 | Loss: 0.382226\n",
      "Iteration 142150 | Loss: 0.382102\n",
      "Iteration 142175 | Loss: 0.381978\n",
      "Iteration 142200 | Loss: 0.381854\n",
      "Iteration 142225 | Loss: 0.381730\n",
      "Iteration 142250 | Loss: 0.381606\n",
      "Iteration 142275 | Loss: 0.381482\n",
      "Iteration 142300 | Loss: 0.381359\n",
      "Iteration 142325 | Loss: 0.381235\n",
      "Iteration 142350 | Loss: 0.381112\n",
      "Iteration 142375 | Loss: 0.380989\n",
      "Iteration 142400 | Loss: 0.380866\n",
      "Iteration 142425 | Loss: 0.380743\n",
      "Iteration 142450 | Loss: 0.380621\n",
      "Iteration 142475 | Loss: 0.380498\n",
      "Iteration 142500 | Loss: 0.380375\n",
      "Iteration 142525 | Loss: 0.380253\n",
      "Iteration 142550 | Loss: 0.380131\n",
      "Iteration 142575 | Loss: 0.380009\n",
      "Iteration 142600 | Loss: 0.379887\n",
      "Iteration 142625 | Loss: 0.379765\n",
      "Iteration 142650 | Loss: 0.379643\n",
      "Iteration 142675 | Loss: 0.379522\n",
      "Iteration 142700 | Loss: 0.379400\n",
      "Iteration 142725 | Loss: 0.379279\n",
      "Iteration 142750 | Loss: 0.379158\n",
      "Iteration 142775 | Loss: 0.379037\n",
      "Iteration 142800 | Loss: 0.378916\n",
      "Iteration 142825 | Loss: 0.378795\n",
      "Iteration 142850 | Loss: 0.378674\n",
      "Iteration 142875 | Loss: 0.378554\n",
      "Iteration 142900 | Loss: 0.378433\n",
      "Iteration 142925 | Loss: 0.378313\n",
      "Iteration 142950 | Loss: 0.378193\n",
      "Iteration 142975 | Loss: 0.378073\n",
      "Iteration 143000 | Loss: 0.377953\n",
      "Iteration 143025 | Loss: 0.377833\n",
      "Iteration 143050 | Loss: 0.377713\n",
      "Iteration 143075 | Loss: 0.377594\n",
      "Iteration 143100 | Loss: 0.377474\n",
      "Iteration 143125 | Loss: 0.377355\n",
      "Iteration 143150 | Loss: 0.377236\n",
      "Iteration 143175 | Loss: 0.377116\n",
      "Iteration 143200 | Loss: 0.376998\n",
      "Iteration 143225 | Loss: 0.376879\n",
      "Iteration 143250 | Loss: 0.376760\n",
      "Iteration 143275 | Loss: 0.376641\n",
      "Iteration 143300 | Loss: 0.376523\n",
      "Iteration 143325 | Loss: 0.376405\n",
      "Iteration 143350 | Loss: 0.376286\n",
      "Iteration 143375 | Loss: 0.376168\n",
      "Iteration 143400 | Loss: 0.376050\n",
      "Iteration 143425 | Loss: 0.375933\n",
      "Iteration 143450 | Loss: 0.375815\n",
      "Iteration 143475 | Loss: 0.375697\n",
      "Iteration 143500 | Loss: 0.375580\n",
      "Iteration 143525 | Loss: 0.375462\n",
      "Iteration 143550 | Loss: 0.375345\n",
      "Iteration 143575 | Loss: 0.375228\n",
      "Iteration 143600 | Loss: 0.375111\n",
      "Iteration 143625 | Loss: 0.374994\n",
      "Iteration 143650 | Loss: 0.374878\n",
      "Iteration 143675 | Loss: 0.374761\n",
      "Iteration 143700 | Loss: 0.374644\n",
      "Iteration 143725 | Loss: 0.374528\n",
      "Iteration 143750 | Loss: 0.374412\n",
      "Iteration 143775 | Loss: 0.374296\n",
      "Iteration 143800 | Loss: 0.374180\n",
      "Iteration 143825 | Loss: 0.374064\n",
      "Iteration 143850 | Loss: 0.373948\n",
      "Iteration 143875 | Loss: 0.373832\n",
      "Iteration 143900 | Loss: 0.373717\n",
      "Iteration 143925 | Loss: 0.373601\n",
      "Iteration 143950 | Loss: 0.373486\n",
      "Iteration 143975 | Loss: 0.373371\n",
      "Iteration 144000 | Loss: 0.373256\n",
      "Iteration 144025 | Loss: 0.373141\n",
      "Iteration 144050 | Loss: 0.373026\n",
      "Iteration 144075 | Loss: 0.372912\n",
      "Iteration 144100 | Loss: 0.372797\n",
      "Iteration 144125 | Loss: 0.372683\n",
      "Iteration 144150 | Loss: 0.372568\n",
      "Iteration 144175 | Loss: 0.372454\n",
      "Iteration 144200 | Loss: 0.372340\n",
      "Iteration 144225 | Loss: 0.372226\n",
      "Iteration 144250 | Loss: 0.372112\n",
      "Iteration 144275 | Loss: 0.371998\n",
      "Iteration 144300 | Loss: 0.371885\n",
      "Iteration 144325 | Loss: 0.371771\n",
      "Iteration 144350 | Loss: 0.371658\n",
      "Iteration 144375 | Loss: 0.371545\n",
      "Iteration 144400 | Loss: 0.371431\n",
      "Iteration 144425 | Loss: 0.371318\n",
      "Iteration 144450 | Loss: 0.371206\n",
      "Iteration 144475 | Loss: 0.371093\n",
      "Iteration 144500 | Loss: 0.370980\n",
      "Iteration 144525 | Loss: 0.370868\n",
      "Iteration 144550 | Loss: 0.370755\n",
      "Iteration 144575 | Loss: 0.370643\n",
      "Iteration 144600 | Loss: 0.370531\n",
      "Iteration 144625 | Loss: 0.370419\n",
      "Iteration 144650 | Loss: 0.370307\n",
      "Iteration 144675 | Loss: 0.370195\n",
      "Iteration 144700 | Loss: 0.370083\n",
      "Iteration 144725 | Loss: 0.369971\n",
      "Iteration 144750 | Loss: 0.369860\n",
      "Iteration 144775 | Loss: 0.369748\n",
      "Iteration 144800 | Loss: 0.369637\n",
      "Iteration 144825 | Loss: 0.369526\n",
      "Iteration 144850 | Loss: 0.369415\n",
      "Iteration 144875 | Loss: 0.369304\n",
      "Iteration 144900 | Loss: 0.369193\n",
      "Iteration 144925 | Loss: 0.369083\n",
      "Iteration 144950 | Loss: 0.368972\n",
      "Iteration 144975 | Loss: 0.368862\n",
      "Iteration 145000 | Loss: 0.368751\n",
      "Iteration 145025 | Loss: 0.368641\n",
      "Iteration 145050 | Loss: 0.368531\n",
      "Iteration 145075 | Loss: 0.368421\n",
      "Iteration 145100 | Loss: 0.368311\n",
      "Iteration 145125 | Loss: 0.368201\n",
      "Iteration 145150 | Loss: 0.368092\n",
      "Iteration 145175 | Loss: 0.367982\n",
      "Iteration 145200 | Loss: 0.367873\n",
      "Iteration 145225 | Loss: 0.367763\n",
      "Iteration 145250 | Loss: 0.367654\n",
      "Iteration 145275 | Loss: 0.367545\n",
      "Iteration 145300 | Loss: 0.367436\n",
      "Iteration 145325 | Loss: 0.367327\n",
      "Iteration 145350 | Loss: 0.367219\n",
      "Iteration 145375 | Loss: 0.367110\n",
      "Iteration 145400 | Loss: 0.367001\n",
      "Iteration 145425 | Loss: 0.366893\n",
      "Iteration 145450 | Loss: 0.366785\n",
      "Iteration 145475 | Loss: 0.366677\n",
      "Iteration 145500 | Loss: 0.366568\n",
      "Iteration 145525 | Loss: 0.366460\n",
      "Iteration 145550 | Loss: 0.366353\n",
      "Iteration 145575 | Loss: 0.366245\n",
      "Iteration 145600 | Loss: 0.366137\n",
      "Iteration 145625 | Loss: 0.366030\n",
      "Iteration 145650 | Loss: 0.365922\n",
      "Iteration 145675 | Loss: 0.365815\n",
      "Iteration 145700 | Loss: 0.365708\n",
      "Iteration 145725 | Loss: 0.365601\n",
      "Iteration 145750 | Loss: 0.365494\n",
      "Iteration 145775 | Loss: 0.365387\n",
      "Iteration 145800 | Loss: 0.365280\n",
      "Iteration 145825 | Loss: 0.365174\n",
      "Iteration 145850 | Loss: 0.365067\n",
      "Iteration 145875 | Loss: 0.364961\n",
      "Iteration 145900 | Loss: 0.364855\n",
      "Iteration 145925 | Loss: 0.364749\n",
      "Iteration 145950 | Loss: 0.364642\n",
      "Iteration 145975 | Loss: 0.364537\n",
      "Iteration 146000 | Loss: 0.364431\n",
      "Iteration 146025 | Loss: 0.364325\n",
      "Iteration 146050 | Loss: 0.364219\n",
      "Iteration 146075 | Loss: 0.364114\n",
      "Iteration 146100 | Loss: 0.364008\n",
      "Iteration 146125 | Loss: 0.363903\n",
      "Iteration 146150 | Loss: 0.363798\n",
      "Iteration 146175 | Loss: 0.363693\n",
      "Iteration 146200 | Loss: 0.363588\n",
      "Iteration 146225 | Loss: 0.363483\n",
      "Iteration 146250 | Loss: 0.363378\n",
      "Iteration 146275 | Loss: 0.363274\n",
      "Iteration 146300 | Loss: 0.363169\n",
      "Iteration 146325 | Loss: 0.363065\n",
      "Iteration 146350 | Loss: 0.362961\n",
      "Iteration 146375 | Loss: 0.362856\n",
      "Iteration 146400 | Loss: 0.362752\n",
      "Iteration 146425 | Loss: 0.362648\n",
      "Iteration 146450 | Loss: 0.362544\n",
      "Iteration 146475 | Loss: 0.362441\n",
      "Iteration 146500 | Loss: 0.362337\n",
      "Iteration 146525 | Loss: 0.362234\n",
      "Iteration 146550 | Loss: 0.362130\n",
      "Iteration 146575 | Loss: 0.362027\n",
      "Iteration 146600 | Loss: 0.361924\n",
      "Iteration 146625 | Loss: 0.361820\n",
      "Iteration 146650 | Loss: 0.361717\n",
      "Iteration 146675 | Loss: 0.361615\n",
      "Iteration 146700 | Loss: 0.361512\n",
      "Iteration 146725 | Loss: 0.361409\n",
      "Iteration 146750 | Loss: 0.361307\n",
      "Iteration 146775 | Loss: 0.361204\n",
      "Iteration 146800 | Loss: 0.361102\n",
      "Iteration 146825 | Loss: 0.360999\n",
      "Iteration 146850 | Loss: 0.360897\n",
      "Iteration 146875 | Loss: 0.360795\n",
      "Iteration 146900 | Loss: 0.360693\n",
      "Iteration 146925 | Loss: 0.360592\n",
      "Iteration 146950 | Loss: 0.360490\n",
      "Iteration 146975 | Loss: 0.360388\n",
      "Iteration 147000 | Loss: 0.360287\n",
      "Iteration 147025 | Loss: 0.360185\n",
      "Iteration 147050 | Loss: 0.360084\n",
      "Iteration 147075 | Loss: 0.359983\n",
      "Iteration 147100 | Loss: 0.359882\n",
      "Iteration 147125 | Loss: 0.359781\n",
      "Iteration 147150 | Loss: 0.359680\n",
      "Iteration 147175 | Loss: 0.359579\n",
      "Iteration 147200 | Loss: 0.359478\n",
      "Iteration 147225 | Loss: 0.359378\n",
      "Iteration 147250 | Loss: 0.359277\n",
      "Iteration 147275 | Loss: 0.359177\n",
      "Iteration 147300 | Loss: 0.359077\n",
      "Iteration 147325 | Loss: 0.358977\n",
      "Iteration 147350 | Loss: 0.358877\n",
      "Iteration 147375 | Loss: 0.358777\n",
      "Iteration 147400 | Loss: 0.358677\n",
      "Iteration 147425 | Loss: 0.358577\n",
      "Iteration 147450 | Loss: 0.358478\n",
      "Iteration 147475 | Loss: 0.358378\n",
      "Iteration 147500 | Loss: 0.358279\n",
      "Iteration 147525 | Loss: 0.358179\n",
      "Iteration 147550 | Loss: 0.358080\n",
      "Iteration 147575 | Loss: 0.357981\n",
      "Iteration 147600 | Loss: 0.357882\n",
      "Iteration 147625 | Loss: 0.357783\n",
      "Iteration 147650 | Loss: 0.357684\n",
      "Iteration 147675 | Loss: 0.357586\n",
      "Iteration 147700 | Loss: 0.357487\n",
      "Iteration 147725 | Loss: 0.357389\n",
      "Iteration 147750 | Loss: 0.357290\n",
      "Iteration 147775 | Loss: 0.357192\n",
      "Iteration 147800 | Loss: 0.357094\n",
      "Iteration 147825 | Loss: 0.356996\n",
      "Iteration 147850 | Loss: 0.356898\n",
      "Iteration 147875 | Loss: 0.356800\n",
      "Iteration 147900 | Loss: 0.356702\n",
      "Iteration 147925 | Loss: 0.356604\n",
      "Iteration 147950 | Loss: 0.356507\n",
      "Iteration 147975 | Loss: 0.356409\n",
      "Iteration 148000 | Loss: 0.356312\n",
      "Iteration 148025 | Loss: 0.356215\n",
      "Iteration 148050 | Loss: 0.356118\n",
      "Iteration 148075 | Loss: 0.356021\n",
      "Iteration 148100 | Loss: 0.355924\n",
      "Iteration 148125 | Loss: 0.355827\n",
      "Iteration 148150 | Loss: 0.355730\n",
      "Iteration 148175 | Loss: 0.355633\n",
      "Iteration 148200 | Loss: 0.355537\n",
      "Iteration 148225 | Loss: 0.355440\n",
      "Iteration 148250 | Loss: 0.355344\n",
      "Iteration 148275 | Loss: 0.355248\n",
      "Iteration 148300 | Loss: 0.355152\n",
      "Iteration 148325 | Loss: 0.355056\n",
      "Iteration 148350 | Loss: 0.354960\n",
      "Iteration 148375 | Loss: 0.354864\n",
      "Iteration 148400 | Loss: 0.354768\n",
      "Iteration 148425 | Loss: 0.354672\n",
      "Iteration 148450 | Loss: 0.354577\n",
      "Iteration 148475 | Loss: 0.354481\n",
      "Iteration 148500 | Loss: 0.354386\n",
      "Iteration 148525 | Loss: 0.354291\n",
      "Iteration 148550 | Loss: 0.354196\n",
      "Iteration 148575 | Loss: 0.354101\n",
      "Iteration 148600 | Loss: 0.354006\n",
      "Iteration 148625 | Loss: 0.353911\n",
      "Iteration 148650 | Loss: 0.353816\n",
      "Iteration 148675 | Loss: 0.353721\n",
      "Iteration 148700 | Loss: 0.353627\n",
      "Iteration 148725 | Loss: 0.353532\n",
      "Iteration 148750 | Loss: 0.353438\n",
      "Iteration 148775 | Loss: 0.353344\n",
      "Iteration 148800 | Loss: 0.353250\n",
      "Iteration 148825 | Loss: 0.353155\n",
      "Iteration 148850 | Loss: 0.353062\n",
      "Iteration 148875 | Loss: 0.352968\n",
      "Iteration 148900 | Loss: 0.352874\n",
      "Iteration 148925 | Loss: 0.352780\n",
      "Iteration 148950 | Loss: 0.352687\n",
      "Iteration 148975 | Loss: 0.352593\n",
      "Iteration 149000 | Loss: 0.352500\n",
      "Iteration 149025 | Loss: 0.352406\n",
      "Iteration 149050 | Loss: 0.352313\n",
      "Iteration 149075 | Loss: 0.352220\n",
      "Iteration 149100 | Loss: 0.352127\n",
      "Iteration 149125 | Loss: 0.352034\n",
      "Iteration 149150 | Loss: 0.351942\n",
      "Iteration 149175 | Loss: 0.351849\n",
      "Iteration 149200 | Loss: 0.351756\n",
      "Iteration 149225 | Loss: 0.351664\n",
      "Iteration 149250 | Loss: 0.351571\n",
      "Iteration 149275 | Loss: 0.351479\n",
      "Iteration 149300 | Loss: 0.351387\n",
      "Iteration 149325 | Loss: 0.351295\n",
      "Iteration 149350 | Loss: 0.351203\n",
      "Iteration 149375 | Loss: 0.351111\n",
      "Iteration 149400 | Loss: 0.351019\n",
      "Iteration 149425 | Loss: 0.350927\n",
      "Iteration 149450 | Loss: 0.350835\n",
      "Iteration 149475 | Loss: 0.350744\n",
      "Iteration 149500 | Loss: 0.350652\n",
      "Iteration 149525 | Loss: 0.350561\n",
      "Iteration 149550 | Loss: 0.350470\n",
      "Iteration 149575 | Loss: 0.350379\n",
      "Iteration 149600 | Loss: 0.350288\n",
      "Iteration 149625 | Loss: 0.350197\n",
      "Iteration 149650 | Loss: 0.350106\n",
      "Iteration 149675 | Loss: 0.350015\n",
      "Iteration 149700 | Loss: 0.349924\n",
      "Iteration 149725 | Loss: 0.349834\n",
      "Iteration 149750 | Loss: 0.349743\n",
      "Iteration 149775 | Loss: 0.349653\n",
      "Iteration 149800 | Loss: 0.349562\n",
      "Iteration 149825 | Loss: 0.349472\n",
      "Iteration 149850 | Loss: 0.349382\n",
      "Iteration 149875 | Loss: 0.349292\n",
      "Iteration 149900 | Loss: 0.349202\n",
      "Iteration 149925 | Loss: 0.349112\n",
      "Iteration 149950 | Loss: 0.349022\n",
      "Iteration 149975 | Loss: 0.348933\n",
      "Iteration 150000 | Loss: 0.348843\n",
      "Iteration 150025 | Loss: 0.348754\n",
      "Iteration 150050 | Loss: 0.348664\n",
      "Iteration 150075 | Loss: 0.348575\n",
      "Iteration 150100 | Loss: 0.348486\n",
      "Iteration 150125 | Loss: 0.348397\n",
      "Iteration 150150 | Loss: 0.348308\n",
      "Iteration 150175 | Loss: 0.348219\n",
      "Iteration 150200 | Loss: 0.348130\n",
      "Iteration 150225 | Loss: 0.348041\n",
      "Iteration 150250 | Loss: 0.347953\n",
      "Iteration 150275 | Loss: 0.347864\n",
      "Iteration 150300 | Loss: 0.347776\n",
      "Iteration 150325 | Loss: 0.347687\n",
      "Iteration 150350 | Loss: 0.347599\n",
      "Iteration 150375 | Loss: 0.347511\n",
      "Iteration 150400 | Loss: 0.347423\n",
      "Iteration 150425 | Loss: 0.347335\n",
      "Iteration 150450 | Loss: 0.347247\n",
      "Iteration 150475 | Loss: 0.347159\n",
      "Iteration 150500 | Loss: 0.347071\n",
      "Iteration 150525 | Loss: 0.346984\n",
      "Iteration 150550 | Loss: 0.346896\n",
      "Iteration 150575 | Loss: 0.346809\n",
      "Iteration 150600 | Loss: 0.346721\n",
      "Iteration 150625 | Loss: 0.346634\n",
      "Iteration 150650 | Loss: 0.346547\n",
      "Iteration 150675 | Loss: 0.346460\n",
      "Iteration 150700 | Loss: 0.346373\n",
      "Iteration 150725 | Loss: 0.346286\n",
      "Iteration 150750 | Loss: 0.346199\n",
      "Iteration 150775 | Loss: 0.346113\n",
      "Iteration 150800 | Loss: 0.346026\n",
      "Iteration 150825 | Loss: 0.345939\n",
      "Iteration 150850 | Loss: 0.345853\n",
      "Iteration 150875 | Loss: 0.345767\n",
      "Iteration 150900 | Loss: 0.345680\n",
      "Iteration 150925 | Loss: 0.345594\n",
      "Iteration 150950 | Loss: 0.345508\n",
      "Iteration 150975 | Loss: 0.345422\n",
      "Iteration 151000 | Loss: 0.345336\n",
      "Iteration 151025 | Loss: 0.345250\n",
      "Iteration 151050 | Loss: 0.345165\n",
      "Iteration 151075 | Loss: 0.345079\n",
      "Iteration 151100 | Loss: 0.344993\n",
      "Iteration 151125 | Loss: 0.344908\n",
      "Iteration 151150 | Loss: 0.344823\n",
      "Iteration 151175 | Loss: 0.344737\n",
      "Iteration 151200 | Loss: 0.344652\n",
      "Iteration 151225 | Loss: 0.344567\n",
      "Iteration 151250 | Loss: 0.344482\n",
      "Iteration 151275 | Loss: 0.344397\n",
      "Iteration 151300 | Loss: 0.344312\n",
      "Iteration 151325 | Loss: 0.344228\n",
      "Iteration 151350 | Loss: 0.344143\n",
      "Iteration 151375 | Loss: 0.344058\n",
      "Iteration 151400 | Loss: 0.343974\n",
      "Iteration 151425 | Loss: 0.343889\n",
      "Iteration 151450 | Loss: 0.343805\n",
      "Iteration 151475 | Loss: 0.343721\n",
      "Iteration 151500 | Loss: 0.343637\n",
      "Iteration 151525 | Loss: 0.343553\n",
      "Iteration 151550 | Loss: 0.343469\n",
      "Iteration 151575 | Loss: 0.343385\n",
      "Iteration 151600 | Loss: 0.343301\n",
      "Iteration 151625 | Loss: 0.343217\n",
      "Iteration 151650 | Loss: 0.343134\n",
      "Iteration 151675 | Loss: 0.343050\n",
      "Iteration 151700 | Loss: 0.342967\n",
      "Iteration 151725 | Loss: 0.342883\n",
      "Iteration 151750 | Loss: 0.342800\n",
      "Iteration 151775 | Loss: 0.342717\n",
      "Iteration 151800 | Loss: 0.342634\n",
      "Iteration 151825 | Loss: 0.342551\n",
      "Iteration 151850 | Loss: 0.342468\n",
      "Iteration 151875 | Loss: 0.342385\n",
      "Iteration 151900 | Loss: 0.342302\n",
      "Iteration 151925 | Loss: 0.342220\n",
      "Iteration 151950 | Loss: 0.342137\n",
      "Iteration 151975 | Loss: 0.342055\n",
      "Iteration 152000 | Loss: 0.341972\n",
      "Iteration 152025 | Loss: 0.341890\n",
      "Iteration 152050 | Loss: 0.341808\n",
      "Iteration 152075 | Loss: 0.341726\n",
      "Iteration 152100 | Loss: 0.341644\n",
      "Iteration 152125 | Loss: 0.341562\n",
      "Iteration 152150 | Loss: 0.341480\n",
      "Iteration 152175 | Loss: 0.341398\n",
      "Iteration 152200 | Loss: 0.341316\n",
      "Iteration 152225 | Loss: 0.341235\n",
      "Iteration 152250 | Loss: 0.341153\n",
      "Iteration 152275 | Loss: 0.341072\n",
      "Iteration 152300 | Loss: 0.340990\n",
      "Iteration 152325 | Loss: 0.340909\n",
      "Iteration 152350 | Loss: 0.340828\n",
      "Iteration 152375 | Loss: 0.340747\n",
      "Iteration 152400 | Loss: 0.340666\n",
      "Iteration 152425 | Loss: 0.340585\n",
      "Iteration 152450 | Loss: 0.340504\n",
      "Iteration 152475 | Loss: 0.340423\n",
      "Iteration 152500 | Loss: 0.340342\n",
      "Iteration 152525 | Loss: 0.340262\n",
      "Iteration 152550 | Loss: 0.340181\n",
      "Iteration 152575 | Loss: 0.340101\n",
      "Iteration 152600 | Loss: 0.340020\n",
      "Iteration 152625 | Loss: 0.339940\n",
      "Iteration 152650 | Loss: 0.339860\n",
      "Iteration 152675 | Loss: 0.339780\n",
      "Iteration 152700 | Loss: 0.339700\n",
      "Iteration 152725 | Loss: 0.339620\n",
      "Iteration 152750 | Loss: 0.339540\n",
      "Iteration 152775 | Loss: 0.339460\n",
      "Iteration 152800 | Loss: 0.339381\n",
      "Iteration 152825 | Loss: 0.339301\n",
      "Iteration 152850 | Loss: 0.339221\n",
      "Iteration 152875 | Loss: 0.339142\n",
      "Iteration 152900 | Loss: 0.339063\n",
      "Iteration 152925 | Loss: 0.338983\n",
      "Iteration 152950 | Loss: 0.338904\n",
      "Iteration 152975 | Loss: 0.338825\n",
      "Iteration 153000 | Loss: 0.338746\n",
      "Iteration 153025 | Loss: 0.338667\n",
      "Iteration 153050 | Loss: 0.338588\n",
      "Iteration 153075 | Loss: 0.338509\n",
      "Iteration 153100 | Loss: 0.338431\n",
      "Iteration 153125 | Loss: 0.338352\n",
      "Iteration 153150 | Loss: 0.338274\n",
      "Iteration 153175 | Loss: 0.338195\n",
      "Iteration 153200 | Loss: 0.338117\n",
      "Iteration 153225 | Loss: 0.338038\n",
      "Iteration 153250 | Loss: 0.337960\n",
      "Iteration 153275 | Loss: 0.337882\n",
      "Iteration 153300 | Loss: 0.337804\n",
      "Iteration 153325 | Loss: 0.337726\n",
      "Iteration 153350 | Loss: 0.337648\n",
      "Iteration 153375 | Loss: 0.337570\n",
      "Iteration 153400 | Loss: 0.337493\n",
      "Iteration 153425 | Loss: 0.337415\n",
      "Iteration 153450 | Loss: 0.337337\n",
      "Iteration 153475 | Loss: 0.337260\n",
      "Iteration 153500 | Loss: 0.337183\n",
      "Iteration 153525 | Loss: 0.337105\n",
      "Iteration 153550 | Loss: 0.337028\n",
      "Iteration 153575 | Loss: 0.336951\n",
      "Iteration 153600 | Loss: 0.336874\n",
      "Iteration 153625 | Loss: 0.336797\n",
      "Iteration 153650 | Loss: 0.336720\n",
      "Iteration 153675 | Loss: 0.336643\n",
      "Iteration 153700 | Loss: 0.336566\n",
      "Iteration 153725 | Loss: 0.336490\n",
      "Iteration 153750 | Loss: 0.336413\n",
      "Iteration 153775 | Loss: 0.336336\n",
      "Iteration 153800 | Loss: 0.336260\n",
      "Iteration 153825 | Loss: 0.336184\n",
      "Iteration 153850 | Loss: 0.336107\n",
      "Iteration 153875 | Loss: 0.336031\n",
      "Iteration 153900 | Loss: 0.335955\n",
      "Iteration 153925 | Loss: 0.335879\n",
      "Iteration 153950 | Loss: 0.335803\n",
      "Iteration 153975 | Loss: 0.335727\n",
      "Iteration 154000 | Loss: 0.335651\n",
      "Iteration 154025 | Loss: 0.335576\n",
      "Iteration 154050 | Loss: 0.335500\n",
      "Iteration 154075 | Loss: 0.335424\n",
      "Iteration 154100 | Loss: 0.335349\n",
      "Iteration 154125 | Loss: 0.335274\n",
      "Iteration 154150 | Loss: 0.335198\n",
      "Iteration 154175 | Loss: 0.335123\n",
      "Iteration 154200 | Loss: 0.335048\n",
      "Iteration 154225 | Loss: 0.334973\n",
      "Iteration 154250 | Loss: 0.334898\n",
      "Iteration 154275 | Loss: 0.334823\n",
      "Iteration 154300 | Loss: 0.334748\n",
      "Iteration 154325 | Loss: 0.334673\n",
      "Iteration 154350 | Loss: 0.334598\n",
      "Iteration 154375 | Loss: 0.334524\n",
      "Iteration 154400 | Loss: 0.334449\n",
      "Iteration 154425 | Loss: 0.334375\n",
      "Iteration 154450 | Loss: 0.334300\n",
      "Iteration 154475 | Loss: 0.334226\n",
      "Iteration 154500 | Loss: 0.334152\n",
      "Iteration 154525 | Loss: 0.334078\n",
      "Iteration 154550 | Loss: 0.334004\n",
      "Iteration 154575 | Loss: 0.333930\n",
      "Iteration 154600 | Loss: 0.333856\n",
      "Iteration 154625 | Loss: 0.333782\n",
      "Iteration 154650 | Loss: 0.333708\n",
      "Iteration 154675 | Loss: 0.333634\n",
      "Iteration 154700 | Loss: 0.333561\n",
      "Iteration 154725 | Loss: 0.333487\n",
      "Iteration 154750 | Loss: 0.333414\n",
      "Iteration 154775 | Loss: 0.333340\n",
      "Iteration 154800 | Loss: 0.333267\n",
      "Iteration 154825 | Loss: 0.333194\n",
      "Iteration 154850 | Loss: 0.333121\n",
      "Iteration 154875 | Loss: 0.333048\n",
      "Iteration 154900 | Loss: 0.332974\n",
      "Iteration 154925 | Loss: 0.332902\n",
      "Iteration 154950 | Loss: 0.332829\n",
      "Iteration 154975 | Loss: 0.332756\n",
      "Iteration 155000 | Loss: 0.332683\n",
      "Iteration 155025 | Loss: 0.332611\n",
      "Iteration 155050 | Loss: 0.332538\n",
      "Iteration 155075 | Loss: 0.332466\n",
      "Iteration 155100 | Loss: 0.332393\n",
      "Iteration 155125 | Loss: 0.332321\n",
      "Iteration 155150 | Loss: 0.332249\n",
      "Iteration 155175 | Loss: 0.332176\n",
      "Iteration 155200 | Loss: 0.332104\n",
      "Iteration 155225 | Loss: 0.332032\n",
      "Iteration 155250 | Loss: 0.331960\n",
      "Iteration 155275 | Loss: 0.331888\n",
      "Iteration 155300 | Loss: 0.331817\n",
      "Iteration 155325 | Loss: 0.331745\n",
      "Iteration 155350 | Loss: 0.331673\n",
      "Iteration 155375 | Loss: 0.331602\n",
      "Iteration 155400 | Loss: 0.331530\n",
      "Iteration 155425 | Loss: 0.331459\n",
      "Iteration 155450 | Loss: 0.331387\n",
      "Iteration 155475 | Loss: 0.331316\n",
      "Iteration 155500 | Loss: 0.331245\n",
      "Iteration 155525 | Loss: 0.331174\n",
      "Iteration 155550 | Loss: 0.331103\n",
      "Iteration 155575 | Loss: 0.331032\n",
      "Iteration 155600 | Loss: 0.330961\n",
      "Iteration 155625 | Loss: 0.330890\n",
      "Iteration 155650 | Loss: 0.330819\n",
      "Iteration 155675 | Loss: 0.330749\n",
      "Iteration 155700 | Loss: 0.330678\n",
      "Iteration 155725 | Loss: 0.330607\n",
      "Iteration 155750 | Loss: 0.330537\n",
      "Iteration 155775 | Loss: 0.330467\n",
      "Iteration 155800 | Loss: 0.330396\n",
      "Iteration 155825 | Loss: 0.330326\n",
      "Iteration 155850 | Loss: 0.330256\n",
      "Iteration 155875 | Loss: 0.330186\n",
      "Iteration 155900 | Loss: 0.330116\n",
      "Iteration 155925 | Loss: 0.330046\n",
      "Iteration 155950 | Loss: 0.329976\n",
      "Iteration 155975 | Loss: 0.329906\n",
      "Iteration 156000 | Loss: 0.329836\n",
      "Iteration 156025 | Loss: 0.329767\n",
      "Iteration 156050 | Loss: 0.329697\n",
      "Iteration 156075 | Loss: 0.329627\n",
      "Iteration 156100 | Loss: 0.329558\n",
      "Iteration 156125 | Loss: 0.329489\n",
      "Iteration 156150 | Loss: 0.329419\n",
      "Iteration 156175 | Loss: 0.329350\n",
      "Iteration 156200 | Loss: 0.329281\n",
      "Iteration 156225 | Loss: 0.329212\n",
      "Iteration 156250 | Loss: 0.329143\n",
      "Iteration 156275 | Loss: 0.329074\n",
      "Iteration 156300 | Loss: 0.329005\n",
      "Iteration 156325 | Loss: 0.328936\n",
      "Iteration 156350 | Loss: 0.328868\n",
      "Iteration 156375 | Loss: 0.328799\n",
      "Iteration 156400 | Loss: 0.328730\n",
      "Iteration 156425 | Loss: 0.328662\n",
      "Iteration 156450 | Loss: 0.328593\n",
      "Iteration 156475 | Loss: 0.328525\n",
      "Iteration 156500 | Loss: 0.328457\n",
      "Iteration 156525 | Loss: 0.328388\n",
      "Iteration 156550 | Loss: 0.328320\n",
      "Iteration 156575 | Loss: 0.328252\n",
      "Iteration 156600 | Loss: 0.328184\n",
      "Iteration 156625 | Loss: 0.328116\n",
      "Iteration 156650 | Loss: 0.328048\n",
      "Iteration 156675 | Loss: 0.327981\n",
      "Iteration 156700 | Loss: 0.327913\n",
      "Iteration 156725 | Loss: 0.327845\n",
      "Iteration 156750 | Loss: 0.327778\n",
      "Iteration 156775 | Loss: 0.327710\n",
      "Iteration 156800 | Loss: 0.327643\n",
      "Iteration 156825 | Loss: 0.327575\n",
      "Iteration 156850 | Loss: 0.327508\n",
      "Iteration 156875 | Loss: 0.327441\n",
      "Iteration 156900 | Loss: 0.327374\n",
      "Iteration 156925 | Loss: 0.327306\n",
      "Iteration 156950 | Loss: 0.327239\n",
      "Iteration 156975 | Loss: 0.327172\n",
      "Iteration 157000 | Loss: 0.327106\n",
      "Iteration 157025 | Loss: 0.327039\n",
      "Iteration 157050 | Loss: 0.326972\n",
      "Iteration 157075 | Loss: 0.326905\n",
      "Iteration 157100 | Loss: 0.326839\n",
      "Iteration 157125 | Loss: 0.326772\n",
      "Iteration 157150 | Loss: 0.326706\n",
      "Iteration 157175 | Loss: 0.326639\n",
      "Iteration 157200 | Loss: 0.326573\n",
      "Iteration 157225 | Loss: 0.326507\n",
      "Iteration 157250 | Loss: 0.326441\n",
      "Iteration 157275 | Loss: 0.326374\n",
      "Iteration 157300 | Loss: 0.326308\n",
      "Iteration 157325 | Loss: 0.326242\n",
      "Iteration 157350 | Loss: 0.326176\n",
      "Iteration 157375 | Loss: 0.326111\n",
      "Iteration 157400 | Loss: 0.326045\n",
      "Iteration 157425 | Loss: 0.325979\n",
      "Iteration 157450 | Loss: 0.325913\n",
      "Iteration 157475 | Loss: 0.325848\n",
      "Iteration 157500 | Loss: 0.325782\n",
      "Iteration 157525 | Loss: 0.325717\n",
      "Iteration 157550 | Loss: 0.325652\n",
      "Iteration 157575 | Loss: 0.325586\n",
      "Iteration 157600 | Loss: 0.325521\n",
      "Iteration 157625 | Loss: 0.325456\n",
      "Iteration 157650 | Loss: 0.325391\n",
      "Iteration 157675 | Loss: 0.325326\n",
      "Iteration 157700 | Loss: 0.325261\n",
      "Iteration 157725 | Loss: 0.325196\n",
      "Iteration 157750 | Loss: 0.325131\n",
      "Iteration 157775 | Loss: 0.325066\n",
      "Iteration 157800 | Loss: 0.325002\n",
      "Iteration 157825 | Loss: 0.324937\n",
      "Iteration 157850 | Loss: 0.324872\n",
      "Iteration 157875 | Loss: 0.324808\n",
      "Iteration 157900 | Loss: 0.324743\n",
      "Iteration 157925 | Loss: 0.324679\n",
      "Iteration 157950 | Loss: 0.324615\n",
      "Iteration 157975 | Loss: 0.324551\n",
      "Iteration 158000 | Loss: 0.324486\n",
      "Iteration 158025 | Loss: 0.324422\n",
      "Iteration 158050 | Loss: 0.324358\n",
      "Iteration 158075 | Loss: 0.324294\n",
      "Iteration 158100 | Loss: 0.324230\n",
      "Iteration 158125 | Loss: 0.324167\n",
      "Iteration 158150 | Loss: 0.324103\n",
      "Iteration 158175 | Loss: 0.324039\n",
      "Iteration 158200 | Loss: 0.323976\n",
      "Iteration 158225 | Loss: 0.323912\n",
      "Iteration 158250 | Loss: 0.323849\n",
      "Iteration 158275 | Loss: 0.323785\n",
      "Iteration 158300 | Loss: 0.323722\n",
      "Iteration 158325 | Loss: 0.323658\n",
      "Iteration 158350 | Loss: 0.323595\n",
      "Iteration 158375 | Loss: 0.323532\n",
      "Iteration 158400 | Loss: 0.323469\n",
      "Iteration 158425 | Loss: 0.323406\n",
      "Iteration 158450 | Loss: 0.323343\n",
      "Iteration 158475 | Loss: 0.323280\n",
      "Iteration 158500 | Loss: 0.323217\n",
      "Iteration 158525 | Loss: 0.323154\n",
      "Iteration 158550 | Loss: 0.323092\n",
      "Iteration 158575 | Loss: 0.323029\n",
      "Iteration 158600 | Loss: 0.322967\n",
      "Iteration 158625 | Loss: 0.322904\n",
      "Iteration 158650 | Loss: 0.322842\n",
      "Iteration 158675 | Loss: 0.322779\n",
      "Iteration 158700 | Loss: 0.322717\n",
      "Iteration 158725 | Loss: 0.322655\n",
      "Iteration 158750 | Loss: 0.322592\n",
      "Iteration 158775 | Loss: 0.322530\n",
      "Iteration 158800 | Loss: 0.322468\n",
      "Iteration 158825 | Loss: 0.322406\n",
      "Iteration 158850 | Loss: 0.322344\n",
      "Iteration 158875 | Loss: 0.322283\n",
      "Iteration 158900 | Loss: 0.322221\n",
      "Iteration 158925 | Loss: 0.322159\n",
      "Iteration 158950 | Loss: 0.322097\n",
      "Iteration 158975 | Loss: 0.322036\n",
      "Iteration 159000 | Loss: 0.321974\n",
      "Iteration 159025 | Loss: 0.321913\n",
      "Iteration 159050 | Loss: 0.321851\n",
      "Iteration 159075 | Loss: 0.321790\n",
      "Iteration 159100 | Loss: 0.321729\n",
      "Iteration 159125 | Loss: 0.321668\n",
      "Iteration 159150 | Loss: 0.321606\n",
      "Iteration 159175 | Loss: 0.321545\n",
      "Iteration 159200 | Loss: 0.321484\n",
      "Iteration 159225 | Loss: 0.321423\n",
      "Iteration 159250 | Loss: 0.321362\n",
      "Iteration 159275 | Loss: 0.321302\n",
      "Iteration 159300 | Loss: 0.321241\n",
      "Iteration 159325 | Loss: 0.321180\n",
      "Iteration 159350 | Loss: 0.321119\n",
      "Iteration 159375 | Loss: 0.321059\n",
      "Iteration 159400 | Loss: 0.320998\n",
      "Iteration 159425 | Loss: 0.320938\n",
      "Iteration 159450 | Loss: 0.320877\n",
      "Iteration 159475 | Loss: 0.320817\n",
      "Iteration 159500 | Loss: 0.320757\n",
      "Iteration 159525 | Loss: 0.320697\n",
      "Iteration 159550 | Loss: 0.320637\n",
      "Iteration 159575 | Loss: 0.320576\n",
      "Iteration 159600 | Loss: 0.320516\n",
      "Iteration 159625 | Loss: 0.320456\n",
      "Iteration 159650 | Loss: 0.320397\n",
      "Iteration 159675 | Loss: 0.320337\n",
      "Iteration 159700 | Loss: 0.320277\n",
      "Iteration 159725 | Loss: 0.320217\n",
      "Iteration 159750 | Loss: 0.320158\n",
      "Iteration 159775 | Loss: 0.320098\n",
      "Iteration 159800 | Loss: 0.320039\n",
      "Iteration 159825 | Loss: 0.319979\n",
      "Iteration 159850 | Loss: 0.319920\n",
      "Iteration 159875 | Loss: 0.319860\n",
      "Iteration 159900 | Loss: 0.319801\n",
      "Iteration 159925 | Loss: 0.319742\n",
      "Iteration 159950 | Loss: 0.319683\n",
      "Iteration 159975 | Loss: 0.319624\n",
      "Iteration 160000 | Loss: 0.319565\n",
      "Iteration 160025 | Loss: 0.319506\n",
      "Iteration 160050 | Loss: 0.319447\n",
      "Iteration 160075 | Loss: 0.319388\n",
      "Iteration 160100 | Loss: 0.319329\n",
      "Iteration 160125 | Loss: 0.319270\n",
      "Iteration 160150 | Loss: 0.319212\n",
      "Iteration 160175 | Loss: 0.319153\n",
      "Iteration 160200 | Loss: 0.319095\n",
      "Iteration 160225 | Loss: 0.319036\n",
      "Iteration 160250 | Loss: 0.318978\n",
      "Iteration 160275 | Loss: 0.318919\n",
      "Iteration 160300 | Loss: 0.318861\n",
      "Iteration 160325 | Loss: 0.318803\n",
      "Iteration 160350 | Loss: 0.318745\n",
      "Iteration 160375 | Loss: 0.318687\n",
      "Iteration 160400 | Loss: 0.318629\n",
      "Iteration 160425 | Loss: 0.318571\n",
      "Iteration 160450 | Loss: 0.318513\n",
      "Iteration 160475 | Loss: 0.318455\n",
      "Iteration 160500 | Loss: 0.318397\n",
      "Iteration 160525 | Loss: 0.318339\n",
      "Iteration 160550 | Loss: 0.318282\n",
      "Iteration 160575 | Loss: 0.318224\n",
      "Iteration 160600 | Loss: 0.318166\n",
      "Iteration 160625 | Loss: 0.318109\n",
      "Iteration 160650 | Loss: 0.318051\n",
      "Iteration 160675 | Loss: 0.317994\n",
      "Iteration 160700 | Loss: 0.317937\n",
      "Iteration 160725 | Loss: 0.317879\n",
      "Iteration 160750 | Loss: 0.317822\n",
      "Iteration 160775 | Loss: 0.317765\n",
      "Iteration 160800 | Loss: 0.317708\n",
      "Iteration 160825 | Loss: 0.317651\n",
      "Iteration 160850 | Loss: 0.317594\n",
      "Iteration 160875 | Loss: 0.317537\n",
      "Iteration 160900 | Loss: 0.317480\n",
      "Iteration 160925 | Loss: 0.317423\n",
      "Iteration 160950 | Loss: 0.317367\n",
      "Iteration 160975 | Loss: 0.317310\n",
      "Iteration 161000 | Loss: 0.317253\n",
      "Iteration 161025 | Loss: 0.317197\n",
      "Iteration 161050 | Loss: 0.317140\n",
      "Iteration 161075 | Loss: 0.317084\n",
      "Iteration 161100 | Loss: 0.317028\n",
      "Iteration 161125 | Loss: 0.316971\n",
      "Iteration 161150 | Loss: 0.316915\n",
      "Iteration 161175 | Loss: 0.316859\n",
      "Iteration 161200 | Loss: 0.316803\n",
      "Iteration 161225 | Loss: 0.316746\n",
      "Iteration 161250 | Loss: 0.316690\n",
      "Iteration 161275 | Loss: 0.316634\n",
      "Iteration 161300 | Loss: 0.316579\n",
      "Iteration 161325 | Loss: 0.316523\n",
      "Iteration 161350 | Loss: 0.316467\n",
      "Iteration 161375 | Loss: 0.316411\n",
      "Iteration 161400 | Loss: 0.316356\n",
      "Iteration 161425 | Loss: 0.316300\n",
      "Iteration 161450 | Loss: 0.316244\n",
      "Iteration 161475 | Loss: 0.316189\n",
      "Iteration 161500 | Loss: 0.316133\n",
      "Iteration 161525 | Loss: 0.316078\n",
      "Iteration 161550 | Loss: 0.316023\n",
      "Iteration 161575 | Loss: 0.315967\n",
      "Iteration 161600 | Loss: 0.315912\n",
      "Iteration 161625 | Loss: 0.315857\n",
      "Iteration 161650 | Loss: 0.315802\n",
      "Iteration 161675 | Loss: 0.315747\n",
      "Iteration 161700 | Loss: 0.315692\n",
      "Iteration 161725 | Loss: 0.315637\n",
      "Iteration 161750 | Loss: 0.315582\n",
      "Iteration 161775 | Loss: 0.315527\n",
      "Iteration 161800 | Loss: 0.315473\n",
      "Iteration 161825 | Loss: 0.315418\n",
      "Iteration 161850 | Loss: 0.315363\n",
      "Iteration 161875 | Loss: 0.315309\n",
      "Iteration 161900 | Loss: 0.315254\n",
      "Iteration 161925 | Loss: 0.315200\n",
      "Iteration 161950 | Loss: 0.315145\n",
      "Iteration 161975 | Loss: 0.315091\n",
      "Iteration 162000 | Loss: 0.315037\n",
      "Iteration 162025 | Loss: 0.314982\n",
      "Iteration 162050 | Loss: 0.314928\n",
      "Iteration 162075 | Loss: 0.314874\n",
      "Iteration 162100 | Loss: 0.314820\n",
      "Iteration 162125 | Loss: 0.314766\n",
      "Iteration 162150 | Loss: 0.314712\n",
      "Iteration 162175 | Loss: 0.314658\n",
      "Iteration 162200 | Loss: 0.314604\n",
      "Iteration 162225 | Loss: 0.314550\n",
      "Iteration 162250 | Loss: 0.314497\n",
      "Iteration 162275 | Loss: 0.314443\n",
      "Iteration 162300 | Loss: 0.314389\n",
      "Iteration 162325 | Loss: 0.314336\n",
      "Iteration 162350 | Loss: 0.314282\n",
      "Iteration 162375 | Loss: 0.314229\n",
      "Iteration 162400 | Loss: 0.314175\n",
      "Iteration 162425 | Loss: 0.314122\n",
      "Iteration 162450 | Loss: 0.314069\n",
      "Iteration 162475 | Loss: 0.314015\n",
      "Iteration 162500 | Loss: 0.313962\n",
      "Iteration 162525 | Loss: 0.313909\n",
      "Iteration 162550 | Loss: 0.313856\n",
      "Iteration 162575 | Loss: 0.313803\n",
      "Iteration 162600 | Loss: 0.313750\n",
      "Iteration 162625 | Loss: 0.313697\n",
      "Iteration 162650 | Loss: 0.313644\n",
      "Iteration 162675 | Loss: 0.313592\n",
      "Iteration 162700 | Loss: 0.313539\n",
      "Iteration 162725 | Loss: 0.313486\n",
      "Iteration 162750 | Loss: 0.313433\n",
      "Iteration 162775 | Loss: 0.313381\n",
      "Iteration 162800 | Loss: 0.313328\n",
      "Iteration 162825 | Loss: 0.313276\n",
      "Iteration 162850 | Loss: 0.313223\n",
      "Iteration 162875 | Loss: 0.313171\n",
      "Iteration 162900 | Loss: 0.313119\n",
      "Iteration 162925 | Loss: 0.313067\n",
      "Iteration 162950 | Loss: 0.313014\n",
      "Iteration 162975 | Loss: 0.312962\n",
      "Iteration 163000 | Loss: 0.312910\n",
      "Iteration 163025 | Loss: 0.312858\n",
      "Iteration 163050 | Loss: 0.312806\n",
      "Iteration 163075 | Loss: 0.312754\n",
      "Iteration 163100 | Loss: 0.312702\n",
      "Iteration 163125 | Loss: 0.312651\n",
      "Iteration 163150 | Loss: 0.312599\n",
      "Iteration 163175 | Loss: 0.312547\n",
      "Iteration 163200 | Loss: 0.312495\n",
      "Iteration 163225 | Loss: 0.312444\n",
      "Iteration 163250 | Loss: 0.312392\n",
      "Iteration 163275 | Loss: 0.312341\n",
      "Iteration 163300 | Loss: 0.312289\n",
      "Iteration 163325 | Loss: 0.312238\n",
      "Iteration 163350 | Loss: 0.312187\n",
      "Iteration 163375 | Loss: 0.312135\n",
      "Iteration 163400 | Loss: 0.312084\n",
      "Iteration 163425 | Loss: 0.312033\n",
      "Iteration 163450 | Loss: 0.311982\n",
      "Iteration 163475 | Loss: 0.311931\n",
      "Iteration 163500 | Loss: 0.311880\n",
      "Iteration 163525 | Loss: 0.311829\n",
      "Iteration 163550 | Loss: 0.311778\n",
      "Iteration 163575 | Loss: 0.311727\n",
      "Iteration 163600 | Loss: 0.311676\n",
      "Iteration 163625 | Loss: 0.311625\n",
      "Iteration 163650 | Loss: 0.311575\n",
      "Iteration 163675 | Loss: 0.311524\n",
      "Iteration 163700 | Loss: 0.311474\n",
      "Iteration 163725 | Loss: 0.311423\n",
      "Iteration 163750 | Loss: 0.311373\n",
      "Iteration 163775 | Loss: 0.311322\n",
      "Iteration 163800 | Loss: 0.311272\n",
      "Iteration 163825 | Loss: 0.311221\n",
      "Iteration 163850 | Loss: 0.311171\n",
      "Iteration 163875 | Loss: 0.311121\n",
      "Iteration 163900 | Loss: 0.311071\n",
      "Iteration 163925 | Loss: 0.311021\n",
      "Iteration 163950 | Loss: 0.310971\n",
      "Iteration 163975 | Loss: 0.310921\n",
      "Iteration 164000 | Loss: 0.310871\n",
      "Iteration 164025 | Loss: 0.310821\n",
      "Iteration 164050 | Loss: 0.310771\n",
      "Iteration 164075 | Loss: 0.310721\n",
      "Iteration 164100 | Loss: 0.310671\n",
      "Iteration 164125 | Loss: 0.310622\n",
      "Iteration 164150 | Loss: 0.310572\n",
      "Iteration 164175 | Loss: 0.310522\n",
      "Iteration 164200 | Loss: 0.310473\n",
      "Iteration 164225 | Loss: 0.310423\n",
      "Iteration 164250 | Loss: 0.310374\n",
      "Iteration 164275 | Loss: 0.310324\n",
      "Iteration 164300 | Loss: 0.310275\n",
      "Iteration 164325 | Loss: 0.310226\n",
      "Iteration 164350 | Loss: 0.310177\n",
      "Iteration 164375 | Loss: 0.310127\n",
      "Iteration 164400 | Loss: 0.310078\n",
      "Iteration 164425 | Loss: 0.310029\n",
      "Iteration 164450 | Loss: 0.309980\n",
      "Iteration 164475 | Loss: 0.309931\n",
      "Iteration 164500 | Loss: 0.309882\n",
      "Iteration 164525 | Loss: 0.309833\n",
      "Iteration 164550 | Loss: 0.309785\n",
      "Iteration 164575 | Loss: 0.309736\n",
      "Iteration 164600 | Loss: 0.309687\n",
      "Iteration 164625 | Loss: 0.309638\n",
      "Iteration 164650 | Loss: 0.309590\n",
      "Iteration 164675 | Loss: 0.309541\n",
      "Iteration 164700 | Loss: 0.309493\n",
      "Iteration 164725 | Loss: 0.309444\n",
      "Iteration 164750 | Loss: 0.309396\n",
      "Iteration 164775 | Loss: 0.309347\n",
      "Iteration 164800 | Loss: 0.309299\n",
      "Iteration 164825 | Loss: 0.309251\n",
      "Iteration 164850 | Loss: 0.309203\n",
      "Iteration 164875 | Loss: 0.309154\n",
      "Iteration 164900 | Loss: 0.309106\n",
      "Iteration 164925 | Loss: 0.309058\n",
      "Iteration 164950 | Loss: 0.309010\n",
      "Iteration 164975 | Loss: 0.308962\n",
      "Iteration 165000 | Loss: 0.308914\n",
      "Iteration 165025 | Loss: 0.308866\n",
      "Iteration 165050 | Loss: 0.308819\n",
      "Iteration 165075 | Loss: 0.308771\n",
      "Iteration 165100 | Loss: 0.308723\n",
      "Iteration 165125 | Loss: 0.308675\n",
      "Iteration 165150 | Loss: 0.308628\n",
      "Iteration 165175 | Loss: 0.308580\n",
      "Iteration 165200 | Loss: 0.308533\n",
      "Iteration 165225 | Loss: 0.308485\n",
      "Iteration 165250 | Loss: 0.308438\n",
      "Iteration 165275 | Loss: 0.308390\n",
      "Iteration 165300 | Loss: 0.308343\n",
      "Iteration 165325 | Loss: 0.308296\n",
      "Iteration 165350 | Loss: 0.308249\n",
      "Iteration 165375 | Loss: 0.308201\n",
      "Iteration 165400 | Loss: 0.308154\n",
      "Iteration 165425 | Loss: 0.308107\n",
      "Iteration 165450 | Loss: 0.308060\n",
      "Iteration 165475 | Loss: 0.308013\n",
      "Iteration 165500 | Loss: 0.307966\n",
      "Iteration 165525 | Loss: 0.307919\n",
      "Iteration 165550 | Loss: 0.307873\n",
      "Iteration 165575 | Loss: 0.307826\n",
      "Iteration 165600 | Loss: 0.307779\n",
      "Iteration 165625 | Loss: 0.307732\n",
      "Iteration 165650 | Loss: 0.307686\n",
      "Iteration 165675 | Loss: 0.307639\n",
      "Iteration 165700 | Loss: 0.307593\n",
      "Iteration 165725 | Loss: 0.307546\n",
      "Iteration 165750 | Loss: 0.307500\n",
      "Iteration 165775 | Loss: 0.307453\n",
      "Iteration 165800 | Loss: 0.307407\n",
      "Iteration 165825 | Loss: 0.307361\n",
      "Iteration 165850 | Loss: 0.307314\n",
      "Iteration 165875 | Loss: 0.307268\n",
      "Iteration 165900 | Loss: 0.307222\n",
      "Iteration 165925 | Loss: 0.307176\n",
      "Iteration 165950 | Loss: 0.307130\n",
      "Iteration 165975 | Loss: 0.307084\n",
      "Iteration 166000 | Loss: 0.307038\n",
      "Iteration 166025 | Loss: 0.306992\n",
      "Iteration 166050 | Loss: 0.306946\n",
      "Iteration 166075 | Loss: 0.306900\n",
      "Iteration 166100 | Loss: 0.306854\n",
      "Iteration 166125 | Loss: 0.306809\n",
      "Iteration 166150 | Loss: 0.306763\n",
      "Iteration 166175 | Loss: 0.306717\n",
      "Iteration 166200 | Loss: 0.306672\n",
      "Iteration 166225 | Loss: 0.306626\n",
      "Iteration 166250 | Loss: 0.306581\n",
      "Iteration 166275 | Loss: 0.306535\n",
      "Iteration 166300 | Loss: 0.306490\n",
      "Iteration 166325 | Loss: 0.306445\n",
      "Iteration 166350 | Loss: 0.306399\n",
      "Iteration 166375 | Loss: 0.306354\n",
      "Iteration 166400 | Loss: 0.306309\n",
      "Iteration 166425 | Loss: 0.306264\n",
      "Iteration 166450 | Loss: 0.306219\n",
      "Iteration 166475 | Loss: 0.306174\n",
      "Iteration 166500 | Loss: 0.306129\n",
      "Iteration 166525 | Loss: 0.306084\n",
      "Iteration 166550 | Loss: 0.306039\n",
      "Iteration 166575 | Loss: 0.305994\n",
      "Iteration 166600 | Loss: 0.305949\n",
      "Iteration 166625 | Loss: 0.305904\n",
      "Iteration 166650 | Loss: 0.305859\n",
      "Iteration 166675 | Loss: 0.305815\n",
      "Iteration 166700 | Loss: 0.305770\n",
      "Iteration 166725 | Loss: 0.305725\n",
      "Iteration 166750 | Loss: 0.305681\n",
      "Iteration 166775 | Loss: 0.305636\n",
      "Iteration 166800 | Loss: 0.305592\n",
      "Iteration 166825 | Loss: 0.305548\n",
      "Iteration 166850 | Loss: 0.305503\n",
      "Iteration 166875 | Loss: 0.305459\n",
      "Iteration 166900 | Loss: 0.305415\n",
      "Iteration 166925 | Loss: 0.305370\n",
      "Iteration 166950 | Loss: 0.305326\n",
      "Iteration 166975 | Loss: 0.305282\n",
      "Iteration 167000 | Loss: 0.305238\n",
      "Iteration 167025 | Loss: 0.305194\n",
      "Iteration 167050 | Loss: 0.305150\n",
      "Iteration 167075 | Loss: 0.305106\n",
      "Iteration 167100 | Loss: 0.305062\n",
      "Iteration 167125 | Loss: 0.305018\n",
      "Iteration 167150 | Loss: 0.304974\n",
      "Iteration 167175 | Loss: 0.304931\n",
      "Iteration 167200 | Loss: 0.304887\n",
      "Iteration 167225 | Loss: 0.304843\n",
      "Iteration 167250 | Loss: 0.304800\n",
      "Iteration 167275 | Loss: 0.304756\n",
      "Iteration 167300 | Loss: 0.304712\n",
      "Iteration 167325 | Loss: 0.304669\n",
      "Iteration 167350 | Loss: 0.304626\n",
      "Iteration 167375 | Loss: 0.304582\n",
      "Iteration 167400 | Loss: 0.304539\n",
      "Iteration 167425 | Loss: 0.304495\n",
      "Iteration 167450 | Loss: 0.304452\n",
      "Iteration 167475 | Loss: 0.304409\n",
      "Iteration 167500 | Loss: 0.304366\n",
      "Iteration 167525 | Loss: 0.304323\n",
      "Iteration 167550 | Loss: 0.304280\n",
      "Iteration 167575 | Loss: 0.304236\n",
      "Iteration 167600 | Loss: 0.304193\n",
      "Iteration 167625 | Loss: 0.304151\n",
      "Iteration 167650 | Loss: 0.304108\n",
      "Iteration 167675 | Loss: 0.304065\n",
      "Iteration 167700 | Loss: 0.304022\n",
      "Iteration 167725 | Loss: 0.303979\n",
      "Iteration 167750 | Loss: 0.303936\n",
      "Iteration 167775 | Loss: 0.303894\n",
      "Iteration 167800 | Loss: 0.303851\n",
      "Iteration 167825 | Loss: 0.303808\n",
      "Iteration 167850 | Loss: 0.303766\n",
      "Iteration 167875 | Loss: 0.303723\n",
      "Iteration 167900 | Loss: 0.303681\n",
      "Iteration 167925 | Loss: 0.303639\n",
      "Iteration 167950 | Loss: 0.303596\n",
      "Iteration 167975 | Loss: 0.303554\n",
      "Iteration 168000 | Loss: 0.303512\n",
      "Iteration 168025 | Loss: 0.303469\n",
      "Iteration 168050 | Loss: 0.303427\n",
      "Iteration 168075 | Loss: 0.303385\n",
      "Iteration 168100 | Loss: 0.303343\n",
      "Iteration 168125 | Loss: 0.303301\n",
      "Iteration 168150 | Loss: 0.303259\n",
      "Iteration 168175 | Loss: 0.303217\n",
      "Iteration 168200 | Loss: 0.303175\n",
      "Iteration 168225 | Loss: 0.303133\n",
      "Iteration 168250 | Loss: 0.303091\n",
      "Iteration 168275 | Loss: 0.303049\n",
      "Iteration 168300 | Loss: 0.303007\n",
      "Iteration 168325 | Loss: 0.302966\n",
      "Iteration 168350 | Loss: 0.302924\n",
      "Iteration 168375 | Loss: 0.302882\n",
      "Iteration 168400 | Loss: 0.302841\n",
      "Iteration 168425 | Loss: 0.302799\n",
      "Iteration 168450 | Loss: 0.302758\n",
      "Iteration 168475 | Loss: 0.302716\n",
      "Iteration 168500 | Loss: 0.302675\n",
      "Iteration 168525 | Loss: 0.302634\n",
      "Iteration 168550 | Loss: 0.302592\n",
      "Iteration 168575 | Loss: 0.302551\n",
      "Iteration 168600 | Loss: 0.302510\n",
      "Iteration 168625 | Loss: 0.302469\n",
      "Iteration 168650 | Loss: 0.302427\n",
      "Iteration 168675 | Loss: 0.302386\n",
      "Iteration 168700 | Loss: 0.302345\n",
      "Iteration 168725 | Loss: 0.302304\n",
      "Iteration 168750 | Loss: 0.302263\n",
      "Iteration 168775 | Loss: 0.302222\n",
      "Iteration 168800 | Loss: 0.302181\n",
      "Iteration 168825 | Loss: 0.302140\n",
      "Iteration 168850 | Loss: 0.302100\n",
      "Iteration 168875 | Loss: 0.302059\n",
      "Iteration 168900 | Loss: 0.302018\n",
      "Iteration 168925 | Loss: 0.301977\n",
      "Iteration 168950 | Loss: 0.301937\n",
      "Iteration 168975 | Loss: 0.301896\n",
      "Iteration 169000 | Loss: 0.301856\n",
      "Iteration 169025 | Loss: 0.301815\n",
      "Iteration 169050 | Loss: 0.301775\n",
      "Iteration 169075 | Loss: 0.301734\n",
      "Iteration 169100 | Loss: 0.301694\n",
      "Iteration 169125 | Loss: 0.301653\n",
      "Iteration 169150 | Loss: 0.301613\n",
      "Iteration 169175 | Loss: 0.301573\n",
      "Iteration 169200 | Loss: 0.301533\n",
      "Iteration 169225 | Loss: 0.301492\n",
      "Iteration 169250 | Loss: 0.301452\n",
      "Iteration 169275 | Loss: 0.301412\n",
      "Iteration 169300 | Loss: 0.301372\n",
      "Iteration 169325 | Loss: 0.301332\n",
      "Iteration 169350 | Loss: 0.301292\n",
      "Iteration 169375 | Loss: 0.301252\n",
      "Iteration 169400 | Loss: 0.301212\n",
      "Iteration 169425 | Loss: 0.301172\n",
      "Iteration 169450 | Loss: 0.301133\n",
      "Iteration 169475 | Loss: 0.301093\n",
      "Iteration 169500 | Loss: 0.301053\n",
      "Iteration 169525 | Loss: 0.301013\n",
      "Iteration 169550 | Loss: 0.300974\n",
      "Iteration 169575 | Loss: 0.300934\n",
      "Iteration 169600 | Loss: 0.300895\n",
      "Iteration 169625 | Loss: 0.300855\n",
      "Iteration 169650 | Loss: 0.300816\n",
      "Iteration 169675 | Loss: 0.300776\n",
      "Iteration 169700 | Loss: 0.300737\n",
      "Iteration 169725 | Loss: 0.300697\n",
      "Iteration 169750 | Loss: 0.300658\n",
      "Iteration 169775 | Loss: 0.300619\n",
      "Iteration 169800 | Loss: 0.300580\n",
      "Iteration 169825 | Loss: 0.300540\n",
      "Iteration 169850 | Loss: 0.300501\n",
      "Iteration 169875 | Loss: 0.300462\n",
      "Iteration 169900 | Loss: 0.300423\n",
      "Iteration 169925 | Loss: 0.300384\n",
      "Iteration 169950 | Loss: 0.300345\n",
      "Iteration 169975 | Loss: 0.300306\n",
      "Iteration 170000 | Loss: 0.300267\n",
      "Iteration 170025 | Loss: 0.300228\n",
      "Iteration 170050 | Loss: 0.300190\n",
      "Iteration 170075 | Loss: 0.300151\n",
      "Iteration 170100 | Loss: 0.300112\n",
      "Iteration 170125 | Loss: 0.300073\n",
      "Iteration 170150 | Loss: 0.300035\n",
      "Iteration 170175 | Loss: 0.299996\n",
      "Iteration 170200 | Loss: 0.299957\n",
      "Iteration 170225 | Loss: 0.299919\n",
      "Iteration 170250 | Loss: 0.299880\n",
      "Iteration 170275 | Loss: 0.299842\n",
      "Iteration 170300 | Loss: 0.299803\n",
      "Iteration 170325 | Loss: 0.299765\n",
      "Iteration 170350 | Loss: 0.299727\n",
      "Iteration 170375 | Loss: 0.299688\n",
      "Iteration 170400 | Loss: 0.299650\n",
      "Iteration 170425 | Loss: 0.299612\n",
      "Iteration 170450 | Loss: 0.299574\n",
      "Iteration 170475 | Loss: 0.299536\n",
      "Iteration 170500 | Loss: 0.299497\n",
      "Iteration 170525 | Loss: 0.299459\n",
      "Iteration 170550 | Loss: 0.299421\n",
      "Iteration 170575 | Loss: 0.299383\n",
      "Iteration 170600 | Loss: 0.299345\n",
      "Iteration 170625 | Loss: 0.299308\n",
      "Iteration 170650 | Loss: 0.299270\n",
      "Iteration 170675 | Loss: 0.299232\n",
      "Iteration 170700 | Loss: 0.299194\n",
      "Iteration 170725 | Loss: 0.299156\n",
      "Iteration 170750 | Loss: 0.299119\n",
      "Iteration 170775 | Loss: 0.299081\n",
      "Iteration 170800 | Loss: 0.299043\n",
      "Iteration 170825 | Loss: 0.299006\n",
      "Iteration 170850 | Loss: 0.298968\n",
      "Iteration 170875 | Loss: 0.298931\n",
      "Iteration 170900 | Loss: 0.298893\n",
      "Iteration 170925 | Loss: 0.298856\n",
      "Iteration 170950 | Loss: 0.298818\n",
      "Iteration 170975 | Loss: 0.298781\n",
      "Iteration 171000 | Loss: 0.298744\n",
      "Iteration 171025 | Loss: 0.298706\n",
      "Iteration 171050 | Loss: 0.298669\n",
      "Iteration 171075 | Loss: 0.298632\n",
      "Iteration 171100 | Loss: 0.298595\n",
      "Iteration 171125 | Loss: 0.298558\n",
      "Iteration 171150 | Loss: 0.298521\n",
      "Iteration 171175 | Loss: 0.298483\n",
      "Iteration 171200 | Loss: 0.298446\n",
      "Iteration 171225 | Loss: 0.298409\n",
      "Iteration 171250 | Loss: 0.298373\n",
      "Iteration 171275 | Loss: 0.298336\n",
      "Iteration 171300 | Loss: 0.298299\n",
      "Iteration 171325 | Loss: 0.298262\n",
      "Iteration 171350 | Loss: 0.298225\n",
      "Iteration 171375 | Loss: 0.298188\n",
      "Iteration 171400 | Loss: 0.298152\n",
      "Iteration 171425 | Loss: 0.298115\n",
      "Iteration 171450 | Loss: 0.298078\n",
      "Iteration 171475 | Loss: 0.298042\n",
      "Iteration 171500 | Loss: 0.298005\n",
      "Iteration 171525 | Loss: 0.297969\n",
      "Iteration 171550 | Loss: 0.297932\n",
      "Iteration 171575 | Loss: 0.297896\n",
      "Iteration 171600 | Loss: 0.297859\n",
      "Iteration 171625 | Loss: 0.297823\n",
      "Iteration 171650 | Loss: 0.297787\n",
      "Iteration 171675 | Loss: 0.297751\n",
      "Iteration 171700 | Loss: 0.297714\n",
      "Iteration 171725 | Loss: 0.297678\n",
      "Iteration 171750 | Loss: 0.297642\n",
      "Iteration 171775 | Loss: 0.297606\n",
      "Iteration 171800 | Loss: 0.297570\n",
      "Iteration 171825 | Loss: 0.297534\n",
      "Iteration 171850 | Loss: 0.297498\n",
      "Iteration 171875 | Loss: 0.297462\n",
      "Iteration 171900 | Loss: 0.297426\n",
      "Iteration 171925 | Loss: 0.297390\n",
      "Iteration 171950 | Loss: 0.297354\n",
      "Iteration 171975 | Loss: 0.297318\n",
      "Iteration 172000 | Loss: 0.297282\n",
      "Iteration 172025 | Loss: 0.297246\n",
      "Iteration 172050 | Loss: 0.297211\n",
      "Iteration 172075 | Loss: 0.297175\n",
      "Iteration 172100 | Loss: 0.297139\n",
      "Iteration 172125 | Loss: 0.297104\n",
      "Iteration 172150 | Loss: 0.297068\n",
      "Iteration 172175 | Loss: 0.297033\n",
      "Iteration 172200 | Loss: 0.296997\n",
      "Iteration 172225 | Loss: 0.296962\n",
      "Iteration 172250 | Loss: 0.296926\n",
      "Iteration 172275 | Loss: 0.296891\n",
      "Iteration 172300 | Loss: 0.296856\n",
      "Iteration 172325 | Loss: 0.296820\n",
      "Iteration 172350 | Loss: 0.296785\n",
      "Iteration 172375 | Loss: 0.296750\n",
      "Iteration 172400 | Loss: 0.296714\n",
      "Iteration 172425 | Loss: 0.296679\n",
      "Iteration 172450 | Loss: 0.296644\n",
      "Iteration 172475 | Loss: 0.296609\n",
      "Iteration 172500 | Loss: 0.296574\n",
      "Iteration 172525 | Loss: 0.296539\n",
      "Iteration 172550 | Loss: 0.296504\n",
      "Iteration 172575 | Loss: 0.296469\n",
      "Iteration 172600 | Loss: 0.296434\n",
      "Iteration 172625 | Loss: 0.296399\n",
      "Iteration 172650 | Loss: 0.296364\n",
      "Iteration 172675 | Loss: 0.296330\n",
      "Iteration 172700 | Loss: 0.296295\n",
      "Iteration 172725 | Loss: 0.296260\n",
      "Iteration 172750 | Loss: 0.296225\n",
      "Iteration 172775 | Loss: 0.296191\n",
      "Iteration 172800 | Loss: 0.296156\n",
      "Iteration 172825 | Loss: 0.296122\n",
      "Iteration 172850 | Loss: 0.296087\n",
      "Iteration 172875 | Loss: 0.296052\n",
      "Iteration 172900 | Loss: 0.296018\n",
      "Iteration 172925 | Loss: 0.295984\n",
      "Iteration 172950 | Loss: 0.295949\n",
      "Iteration 172975 | Loss: 0.295915\n",
      "Iteration 173000 | Loss: 0.295880\n",
      "Iteration 173025 | Loss: 0.295846\n",
      "Iteration 173050 | Loss: 0.295812\n",
      "Iteration 173075 | Loss: 0.295778\n",
      "Iteration 173100 | Loss: 0.295743\n",
      "Iteration 173125 | Loss: 0.295709\n",
      "Iteration 173150 | Loss: 0.295675\n",
      "Iteration 173175 | Loss: 0.295641\n",
      "Iteration 173200 | Loss: 0.295607\n",
      "Iteration 173225 | Loss: 0.295573\n",
      "Iteration 173250 | Loss: 0.295539\n",
      "Iteration 173275 | Loss: 0.295505\n",
      "Iteration 173300 | Loss: 0.295471\n",
      "Iteration 173325 | Loss: 0.295437\n",
      "Iteration 173350 | Loss: 0.295403\n",
      "Iteration 173375 | Loss: 0.295370\n",
      "Iteration 173400 | Loss: 0.295336\n",
      "Iteration 173425 | Loss: 0.295302\n",
      "Iteration 173450 | Loss: 0.295268\n",
      "Iteration 173475 | Loss: 0.295235\n",
      "Iteration 173500 | Loss: 0.295201\n",
      "Iteration 173525 | Loss: 0.295168\n",
      "Iteration 173550 | Loss: 0.295134\n",
      "Iteration 173575 | Loss: 0.295100\n",
      "Iteration 173600 | Loss: 0.295067\n",
      "Iteration 173625 | Loss: 0.295033\n",
      "Iteration 173650 | Loss: 0.295000\n",
      "Iteration 173675 | Loss: 0.294967\n",
      "Iteration 173700 | Loss: 0.294933\n",
      "Iteration 173725 | Loss: 0.294900\n",
      "Iteration 173750 | Loss: 0.294867\n",
      "Iteration 173775 | Loss: 0.294833\n",
      "Iteration 173800 | Loss: 0.294800\n",
      "Iteration 173825 | Loss: 0.294767\n",
      "Iteration 173850 | Loss: 0.294734\n",
      "Iteration 173875 | Loss: 0.294701\n",
      "Iteration 173900 | Loss: 0.294668\n",
      "Iteration 173925 | Loss: 0.294635\n",
      "Iteration 173950 | Loss: 0.294602\n",
      "Iteration 173975 | Loss: 0.294569\n",
      "Iteration 174000 | Loss: 0.294536\n",
      "Iteration 174025 | Loss: 0.294503\n",
      "Iteration 174050 | Loss: 0.294470\n",
      "Iteration 174075 | Loss: 0.294437\n",
      "Iteration 174100 | Loss: 0.294404\n",
      "Iteration 174125 | Loss: 0.294372\n",
      "Iteration 174150 | Loss: 0.294339\n",
      "Iteration 174175 | Loss: 0.294306\n",
      "Iteration 174200 | Loss: 0.294274\n",
      "Iteration 174225 | Loss: 0.294241\n",
      "Iteration 174250 | Loss: 0.294208\n",
      "Iteration 174275 | Loss: 0.294176\n",
      "Iteration 174300 | Loss: 0.294143\n",
      "Iteration 174325 | Loss: 0.294111\n",
      "Iteration 174350 | Loss: 0.294078\n",
      "Iteration 174375 | Loss: 0.294046\n",
      "Iteration 174400 | Loss: 0.294013\n",
      "Iteration 174425 | Loss: 0.293981\n",
      "Iteration 174450 | Loss: 0.293949\n",
      "Iteration 174475 | Loss: 0.293916\n",
      "Iteration 174500 | Loss: 0.293884\n",
      "Iteration 174525 | Loss: 0.293852\n",
      "Iteration 174550 | Loss: 0.293820\n",
      "Iteration 174575 | Loss: 0.293788\n",
      "Iteration 174600 | Loss: 0.293756\n",
      "Iteration 174625 | Loss: 0.293723\n",
      "Iteration 174650 | Loss: 0.293691\n",
      "Iteration 174675 | Loss: 0.293659\n",
      "Iteration 174700 | Loss: 0.293627\n",
      "Iteration 174725 | Loss: 0.293595\n",
      "Iteration 174750 | Loss: 0.293563\n",
      "Iteration 174775 | Loss: 0.293532\n",
      "Iteration 174800 | Loss: 0.293500\n",
      "Iteration 174825 | Loss: 0.293468\n",
      "Iteration 174850 | Loss: 0.293436\n",
      "Iteration 174875 | Loss: 0.293404\n",
      "Iteration 174900 | Loss: 0.293373\n",
      "Iteration 174925 | Loss: 0.293341\n",
      "Iteration 174950 | Loss: 0.293309\n",
      "Iteration 174975 | Loss: 0.293278\n",
      "Iteration 175000 | Loss: 0.293246\n",
      "Iteration 175025 | Loss: 0.293215\n",
      "Iteration 175050 | Loss: 0.293183\n",
      "Iteration 175075 | Loss: 0.293151\n",
      "Iteration 175100 | Loss: 0.293120\n",
      "Iteration 175125 | Loss: 0.293089\n",
      "Iteration 175150 | Loss: 0.293057\n",
      "Iteration 175175 | Loss: 0.293026\n",
      "Iteration 175200 | Loss: 0.292994\n",
      "Iteration 175225 | Loss: 0.292963\n",
      "Iteration 175250 | Loss: 0.292932\n",
      "Iteration 175275 | Loss: 0.292901\n",
      "Iteration 175300 | Loss: 0.292869\n",
      "Iteration 175325 | Loss: 0.292838\n",
      "Iteration 175350 | Loss: 0.292807\n",
      "Iteration 175375 | Loss: 0.292776\n",
      "Iteration 175400 | Loss: 0.292745\n",
      "Iteration 175425 | Loss: 0.292714\n",
      "Iteration 175450 | Loss: 0.292683\n",
      "Iteration 175475 | Loss: 0.292652\n",
      "Iteration 175500 | Loss: 0.292621\n",
      "Iteration 175525 | Loss: 0.292590\n",
      "Iteration 175550 | Loss: 0.292559\n",
      "Iteration 175575 | Loss: 0.292528\n",
      "Iteration 175600 | Loss: 0.292498\n",
      "Iteration 175625 | Loss: 0.292467\n",
      "Iteration 175650 | Loss: 0.292436\n",
      "Iteration 175675 | Loss: 0.292405\n",
      "Iteration 175700 | Loss: 0.292375\n",
      "Iteration 175725 | Loss: 0.292344\n",
      "Iteration 175750 | Loss: 0.292313\n",
      "Iteration 175775 | Loss: 0.292283\n",
      "Iteration 175800 | Loss: 0.292252\n",
      "Iteration 175825 | Loss: 0.292222\n",
      "Iteration 175850 | Loss: 0.292191\n",
      "Iteration 175875 | Loss: 0.292161\n",
      "Iteration 175900 | Loss: 0.292130\n",
      "Iteration 175925 | Loss: 0.292100\n",
      "Iteration 175950 | Loss: 0.292070\n",
      "Iteration 175975 | Loss: 0.292039\n",
      "Iteration 176000 | Loss: 0.292009\n",
      "Iteration 176025 | Loss: 0.291979\n",
      "Iteration 176050 | Loss: 0.291948\n",
      "Iteration 176075 | Loss: 0.291918\n",
      "Iteration 176100 | Loss: 0.291888\n",
      "Iteration 176125 | Loss: 0.291858\n",
      "Iteration 176150 | Loss: 0.291828\n",
      "Iteration 176175 | Loss: 0.291798\n",
      "Iteration 176200 | Loss: 0.291768\n",
      "Iteration 176225 | Loss: 0.291738\n",
      "Iteration 176250 | Loss: 0.291708\n",
      "Iteration 176275 | Loss: 0.291678\n",
      "Iteration 176300 | Loss: 0.291648\n",
      "Iteration 176325 | Loss: 0.291618\n",
      "Iteration 176350 | Loss: 0.291588\n",
      "Iteration 176375 | Loss: 0.291558\n",
      "Iteration 176400 | Loss: 0.291528\n",
      "Iteration 176425 | Loss: 0.291499\n",
      "Iteration 176450 | Loss: 0.291469\n",
      "Iteration 176475 | Loss: 0.291439\n",
      "Iteration 176500 | Loss: 0.291409\n",
      "Iteration 176525 | Loss: 0.291380\n",
      "Iteration 176550 | Loss: 0.291350\n",
      "Iteration 176575 | Loss: 0.291321\n",
      "Iteration 176600 | Loss: 0.291291\n",
      "Iteration 176625 | Loss: 0.291261\n",
      "Iteration 176650 | Loss: 0.291232\n",
      "Iteration 176675 | Loss: 0.291202\n",
      "Iteration 176700 | Loss: 0.291173\n",
      "Iteration 176725 | Loss: 0.291144\n",
      "Iteration 176750 | Loss: 0.291114\n",
      "Iteration 176775 | Loss: 0.291085\n",
      "Iteration 176800 | Loss: 0.291056\n",
      "Iteration 176825 | Loss: 0.291026\n",
      "Iteration 176850 | Loss: 0.290997\n",
      "Iteration 176875 | Loss: 0.290968\n",
      "Iteration 176900 | Loss: 0.290939\n",
      "Iteration 176925 | Loss: 0.290910\n",
      "Iteration 176950 | Loss: 0.290880\n",
      "Iteration 176975 | Loss: 0.290851\n",
      "Iteration 177000 | Loss: 0.290822\n",
      "Iteration 177025 | Loss: 0.290793\n",
      "Iteration 177050 | Loss: 0.290764\n",
      "Iteration 177075 | Loss: 0.290735\n",
      "Iteration 177100 | Loss: 0.290706\n",
      "Iteration 177125 | Loss: 0.290677\n",
      "Iteration 177150 | Loss: 0.290648\n",
      "Iteration 177175 | Loss: 0.290620\n",
      "Iteration 177200 | Loss: 0.290591\n",
      "Iteration 177225 | Loss: 0.290562\n",
      "Iteration 177250 | Loss: 0.290533\n",
      "Iteration 177275 | Loss: 0.290504\n",
      "Iteration 177300 | Loss: 0.290476\n",
      "Iteration 177325 | Loss: 0.290447\n",
      "Iteration 177350 | Loss: 0.290418\n",
      "Iteration 177375 | Loss: 0.290390\n",
      "Iteration 177400 | Loss: 0.290361\n",
      "Iteration 177425 | Loss: 0.290333\n",
      "Iteration 177450 | Loss: 0.290304\n",
      "Iteration 177475 | Loss: 0.290276\n",
      "Iteration 177500 | Loss: 0.290247\n",
      "Iteration 177525 | Loss: 0.290219\n",
      "Iteration 177550 | Loss: 0.290190\n",
      "Iteration 177575 | Loss: 0.290162\n",
      "Iteration 177600 | Loss: 0.290134\n",
      "Iteration 177625 | Loss: 0.290105\n",
      "Iteration 177650 | Loss: 0.290077\n",
      "Iteration 177675 | Loss: 0.290049\n",
      "Iteration 177700 | Loss: 0.290020\n",
      "Iteration 177725 | Loss: 0.289992\n",
      "Iteration 177750 | Loss: 0.289964\n",
      "Iteration 177775 | Loss: 0.289936\n",
      "Iteration 177800 | Loss: 0.289908\n",
      "Iteration 177825 | Loss: 0.289880\n",
      "Iteration 177850 | Loss: 0.289852\n",
      "Iteration 177875 | Loss: 0.289824\n",
      "Iteration 177900 | Loss: 0.289796\n",
      "Iteration 177925 | Loss: 0.289768\n",
      "Iteration 177950 | Loss: 0.289740\n",
      "Iteration 177975 | Loss: 0.289712\n",
      "Iteration 178000 | Loss: 0.289684\n",
      "Iteration 178025 | Loss: 0.289656\n",
      "Iteration 178050 | Loss: 0.289628\n",
      "Iteration 178075 | Loss: 0.289600\n",
      "Iteration 178100 | Loss: 0.289573\n",
      "Iteration 178125 | Loss: 0.289545\n",
      "Iteration 178150 | Loss: 0.289517\n",
      "Iteration 178175 | Loss: 0.289490\n",
      "Iteration 178200 | Loss: 0.289462\n",
      "Iteration 178225 | Loss: 0.289434\n",
      "Iteration 178250 | Loss: 0.289407\n",
      "Iteration 178275 | Loss: 0.289379\n",
      "Iteration 178300 | Loss: 0.289352\n",
      "Iteration 178325 | Loss: 0.289324\n",
      "Iteration 178350 | Loss: 0.289297\n",
      "Iteration 178375 | Loss: 0.289269\n",
      "Iteration 178400 | Loss: 0.289242\n",
      "Iteration 178425 | Loss: 0.289214\n",
      "Iteration 178450 | Loss: 0.289187\n",
      "Iteration 178475 | Loss: 0.289160\n",
      "Iteration 178500 | Loss: 0.289132\n",
      "Iteration 178525 | Loss: 0.289105\n",
      "Iteration 178550 | Loss: 0.289078\n",
      "Iteration 178575 | Loss: 0.289051\n",
      "Iteration 178600 | Loss: 0.289023\n",
      "Iteration 178625 | Loss: 0.288996\n",
      "Iteration 178650 | Loss: 0.288969\n",
      "Iteration 178675 | Loss: 0.288942\n",
      "Iteration 178700 | Loss: 0.288915\n",
      "Iteration 178725 | Loss: 0.288888\n",
      "Iteration 178750 | Loss: 0.288861\n",
      "Iteration 178775 | Loss: 0.288834\n",
      "Iteration 178800 | Loss: 0.288807\n",
      "Iteration 178825 | Loss: 0.288780\n",
      "Iteration 178850 | Loss: 0.288753\n",
      "Iteration 178875 | Loss: 0.288726\n",
      "Iteration 178900 | Loss: 0.288699\n",
      "Iteration 178925 | Loss: 0.288672\n",
      "Iteration 178950 | Loss: 0.288646\n",
      "Iteration 178975 | Loss: 0.288619\n",
      "Iteration 179000 | Loss: 0.288592\n",
      "Iteration 179025 | Loss: 0.288565\n",
      "Iteration 179050 | Loss: 0.288539\n",
      "Iteration 179075 | Loss: 0.288512\n",
      "Iteration 179100 | Loss: 0.288485\n",
      "Iteration 179125 | Loss: 0.288459\n",
      "Iteration 179150 | Loss: 0.288432\n",
      "Iteration 179175 | Loss: 0.288406\n",
      "Iteration 179200 | Loss: 0.288379\n",
      "Iteration 179225 | Loss: 0.288353\n",
      "Iteration 179250 | Loss: 0.288326\n",
      "Iteration 179275 | Loss: 0.288300\n",
      "Iteration 179300 | Loss: 0.288273\n",
      "Iteration 179325 | Loss: 0.288247\n",
      "Iteration 179350 | Loss: 0.288221\n",
      "Iteration 179375 | Loss: 0.288194\n",
      "Iteration 179400 | Loss: 0.288168\n",
      "Iteration 179425 | Loss: 0.288142\n",
      "Iteration 179450 | Loss: 0.288115\n",
      "Iteration 179475 | Loss: 0.288089\n",
      "Iteration 179500 | Loss: 0.288063\n",
      "Iteration 179525 | Loss: 0.288037\n",
      "Iteration 179550 | Loss: 0.288011\n",
      "Iteration 179575 | Loss: 0.287985\n",
      "Iteration 179600 | Loss: 0.287958\n",
      "Iteration 179625 | Loss: 0.287932\n",
      "Iteration 179650 | Loss: 0.287906\n",
      "Iteration 179675 | Loss: 0.287880\n",
      "Iteration 179700 | Loss: 0.287854\n",
      "Iteration 179725 | Loss: 0.287828\n",
      "Iteration 179750 | Loss: 0.287802\n",
      "Iteration 179775 | Loss: 0.287777\n",
      "Iteration 179800 | Loss: 0.287751\n",
      "Iteration 179825 | Loss: 0.287725\n",
      "Iteration 179850 | Loss: 0.287699\n",
      "Iteration 179875 | Loss: 0.287673\n",
      "Iteration 179900 | Loss: 0.287647\n",
      "Iteration 179925 | Loss: 0.287622\n",
      "Iteration 179950 | Loss: 0.287596\n",
      "Iteration 179975 | Loss: 0.287570\n",
      "Iteration 180000 | Loss: 0.287545\n",
      "Iteration 180025 | Loss: 0.287519\n",
      "Iteration 180050 | Loss: 0.287493\n",
      "Iteration 180075 | Loss: 0.287468\n",
      "Iteration 180100 | Loss: 0.287442\n",
      "Iteration 180125 | Loss: 0.287417\n",
      "Iteration 180150 | Loss: 0.287391\n",
      "Iteration 180175 | Loss: 0.287366\n",
      "Iteration 180200 | Loss: 0.287340\n",
      "Iteration 180225 | Loss: 0.287315\n",
      "Iteration 180250 | Loss: 0.287290\n",
      "Iteration 180275 | Loss: 0.287264\n",
      "Iteration 180300 | Loss: 0.287239\n",
      "Iteration 180325 | Loss: 0.287214\n",
      "Iteration 180350 | Loss: 0.287188\n",
      "Iteration 180375 | Loss: 0.287163\n",
      "Iteration 180400 | Loss: 0.287138\n",
      "Iteration 180425 | Loss: 0.287113\n",
      "Iteration 180450 | Loss: 0.287087\n",
      "Iteration 180475 | Loss: 0.287062\n",
      "Iteration 180500 | Loss: 0.287037\n",
      "Iteration 180525 | Loss: 0.287012\n",
      "Iteration 180550 | Loss: 0.286987\n",
      "Iteration 180575 | Loss: 0.286962\n",
      "Iteration 180600 | Loss: 0.286937\n",
      "Iteration 180625 | Loss: 0.286912\n",
      "Iteration 180650 | Loss: 0.286887\n",
      "Iteration 180675 | Loss: 0.286862\n",
      "Iteration 180700 | Loss: 0.286837\n",
      "Iteration 180725 | Loss: 0.286812\n",
      "Iteration 180750 | Loss: 0.286787\n",
      "Iteration 180775 | Loss: 0.286762\n",
      "Iteration 180800 | Loss: 0.286738\n",
      "Iteration 180825 | Loss: 0.286713\n",
      "Iteration 180850 | Loss: 0.286688\n",
      "Iteration 180875 | Loss: 0.286663\n",
      "Iteration 180900 | Loss: 0.286639\n",
      "Iteration 180925 | Loss: 0.286614\n",
      "Iteration 180950 | Loss: 0.286589\n",
      "Iteration 180975 | Loss: 0.286565\n",
      "Iteration 181000 | Loss: 0.286540\n",
      "Iteration 181025 | Loss: 0.286515\n",
      "Iteration 181050 | Loss: 0.286491\n",
      "Iteration 181075 | Loss: 0.286466\n",
      "Iteration 181100 | Loss: 0.286442\n",
      "Iteration 181125 | Loss: 0.286417\n",
      "Iteration 181150 | Loss: 0.286393\n",
      "Iteration 181175 | Loss: 0.286369\n",
      "Iteration 181200 | Loss: 0.286344\n",
      "Iteration 181225 | Loss: 0.286320\n",
      "Iteration 181250 | Loss: 0.286295\n",
      "Iteration 181275 | Loss: 0.286271\n",
      "Iteration 181300 | Loss: 0.286247\n",
      "Iteration 181325 | Loss: 0.286222\n",
      "Iteration 181350 | Loss: 0.286198\n",
      "Iteration 181375 | Loss: 0.286174\n",
      "Iteration 181400 | Loss: 0.286150\n",
      "Iteration 181425 | Loss: 0.286126\n",
      "Iteration 181450 | Loss: 0.286101\n",
      "Iteration 181475 | Loss: 0.286077\n",
      "Iteration 181500 | Loss: 0.286053\n",
      "Iteration 181525 | Loss: 0.286029\n",
      "Iteration 181550 | Loss: 0.286005\n",
      "Iteration 181575 | Loss: 0.285981\n",
      "Iteration 181600 | Loss: 0.285957\n",
      "Iteration 181625 | Loss: 0.285933\n",
      "Iteration 181650 | Loss: 0.285909\n",
      "Iteration 181675 | Loss: 0.285885\n",
      "Iteration 181700 | Loss: 0.285861\n",
      "Iteration 181725 | Loss: 0.285837\n",
      "Iteration 181750 | Loss: 0.285814\n",
      "Iteration 181775 | Loss: 0.285790\n",
      "Iteration 181800 | Loss: 0.285766\n",
      "Iteration 181825 | Loss: 0.285742\n",
      "Iteration 181850 | Loss: 0.285718\n",
      "Iteration 181875 | Loss: 0.285695\n",
      "Iteration 181900 | Loss: 0.285671\n",
      "Iteration 181925 | Loss: 0.285647\n",
      "Iteration 181950 | Loss: 0.285624\n",
      "Iteration 181975 | Loss: 0.285600\n",
      "Iteration 182000 | Loss: 0.285576\n",
      "Iteration 182025 | Loss: 0.285553\n",
      "Iteration 182050 | Loss: 0.285529\n",
      "Iteration 182075 | Loss: 0.285506\n",
      "Iteration 182100 | Loss: 0.285482\n",
      "Iteration 182125 | Loss: 0.285459\n",
      "Iteration 182150 | Loss: 0.285435\n",
      "Iteration 182175 | Loss: 0.285412\n",
      "Iteration 182200 | Loss: 0.285388\n",
      "Iteration 182225 | Loss: 0.285365\n",
      "Iteration 182250 | Loss: 0.285342\n",
      "Iteration 182275 | Loss: 0.285318\n",
      "Iteration 182300 | Loss: 0.285295\n",
      "Iteration 182325 | Loss: 0.285272\n",
      "Iteration 182350 | Loss: 0.285248\n",
      "Iteration 182375 | Loss: 0.285225\n",
      "Iteration 182400 | Loss: 0.285202\n",
      "Iteration 182425 | Loss: 0.285179\n",
      "Iteration 182450 | Loss: 0.285156\n",
      "Iteration 182475 | Loss: 0.285132\n",
      "Iteration 182500 | Loss: 0.285109\n",
      "Iteration 182525 | Loss: 0.285086\n",
      "Iteration 182550 | Loss: 0.285063\n",
      "Iteration 182575 | Loss: 0.285040\n",
      "Iteration 182600 | Loss: 0.285017\n",
      "Iteration 182625 | Loss: 0.284994\n",
      "Iteration 182650 | Loss: 0.284971\n",
      "Iteration 182675 | Loss: 0.284948\n",
      "Iteration 182700 | Loss: 0.284925\n",
      "Iteration 182725 | Loss: 0.284902\n",
      "Iteration 182750 | Loss: 0.284879\n",
      "Iteration 182775 | Loss: 0.284857\n",
      "Iteration 182800 | Loss: 0.284834\n",
      "Iteration 182825 | Loss: 0.284811\n",
      "Iteration 182850 | Loss: 0.284788\n",
      "Iteration 182875 | Loss: 0.284765\n",
      "Iteration 182900 | Loss: 0.284743\n",
      "Iteration 182925 | Loss: 0.284720\n",
      "Iteration 182950 | Loss: 0.284697\n",
      "Iteration 182975 | Loss: 0.284675\n",
      "Iteration 183000 | Loss: 0.284652\n",
      "Iteration 183025 | Loss: 0.284629\n",
      "Iteration 183050 | Loss: 0.284607\n",
      "Iteration 183075 | Loss: 0.284584\n",
      "Iteration 183100 | Loss: 0.284562\n",
      "Iteration 183125 | Loss: 0.284539\n",
      "Iteration 183150 | Loss: 0.284517\n",
      "Iteration 183175 | Loss: 0.284494\n",
      "Iteration 183200 | Loss: 0.284472\n",
      "Iteration 183225 | Loss: 0.284449\n",
      "Iteration 183250 | Loss: 0.284427\n",
      "Iteration 183275 | Loss: 0.284404\n",
      "Iteration 183300 | Loss: 0.284382\n",
      "Iteration 183325 | Loss: 0.284360\n",
      "Iteration 183350 | Loss: 0.284337\n",
      "Iteration 183375 | Loss: 0.284315\n",
      "Iteration 183400 | Loss: 0.284293\n",
      "Iteration 183425 | Loss: 0.284271\n",
      "Iteration 183450 | Loss: 0.284248\n",
      "Iteration 183475 | Loss: 0.284226\n",
      "Iteration 183500 | Loss: 0.284204\n",
      "Iteration 183525 | Loss: 0.284182\n",
      "Iteration 183550 | Loss: 0.284160\n",
      "Iteration 183575 | Loss: 0.284138\n",
      "Iteration 183600 | Loss: 0.284116\n",
      "Iteration 183625 | Loss: 0.284093\n",
      "Iteration 183650 | Loss: 0.284071\n",
      "Iteration 183675 | Loss: 0.284049\n",
      "Iteration 183700 | Loss: 0.284027\n",
      "Iteration 183725 | Loss: 0.284005\n",
      "Iteration 183750 | Loss: 0.283983\n",
      "Iteration 183775 | Loss: 0.283962\n",
      "Iteration 183800 | Loss: 0.283940\n",
      "Iteration 183825 | Loss: 0.283918\n",
      "Iteration 183850 | Loss: 0.283896\n",
      "Iteration 183875 | Loss: 0.283874\n",
      "Iteration 183900 | Loss: 0.283852\n",
      "Iteration 183925 | Loss: 0.283830\n",
      "Iteration 183950 | Loss: 0.283809\n",
      "Iteration 183975 | Loss: 0.283787\n",
      "Iteration 184000 | Loss: 0.283765\n",
      "Iteration 184025 | Loss: 0.283744\n",
      "Iteration 184050 | Loss: 0.283722\n",
      "Iteration 184075 | Loss: 0.283700\n",
      "Iteration 184100 | Loss: 0.283679\n",
      "Iteration 184125 | Loss: 0.283657\n",
      "Iteration 184150 | Loss: 0.283635\n",
      "Iteration 184175 | Loss: 0.283614\n",
      "Iteration 184200 | Loss: 0.283592\n",
      "Iteration 184225 | Loss: 0.283571\n",
      "Iteration 184250 | Loss: 0.283549\n",
      "Iteration 184275 | Loss: 0.283528\n",
      "Iteration 184300 | Loss: 0.283506\n",
      "Iteration 184325 | Loss: 0.283485\n",
      "Iteration 184350 | Loss: 0.283464\n",
      "Iteration 184375 | Loss: 0.283442\n",
      "Iteration 184400 | Loss: 0.283421\n",
      "Iteration 184425 | Loss: 0.283399\n",
      "Iteration 184450 | Loss: 0.283378\n",
      "Iteration 184475 | Loss: 0.283357\n",
      "Iteration 184500 | Loss: 0.283336\n",
      "Iteration 184525 | Loss: 0.283314\n",
      "Iteration 184550 | Loss: 0.283293\n",
      "Iteration 184575 | Loss: 0.283272\n",
      "Iteration 184600 | Loss: 0.283251\n",
      "Iteration 184625 | Loss: 0.283230\n",
      "Iteration 184650 | Loss: 0.283208\n",
      "Iteration 184675 | Loss: 0.283187\n",
      "Iteration 184700 | Loss: 0.283166\n",
      "Iteration 184725 | Loss: 0.283145\n",
      "Iteration 184750 | Loss: 0.283124\n",
      "Iteration 184775 | Loss: 0.283103\n",
      "Iteration 184800 | Loss: 0.283082\n",
      "Iteration 184825 | Loss: 0.283061\n",
      "Iteration 184850 | Loss: 0.283040\n",
      "Iteration 184875 | Loss: 0.283019\n",
      "Iteration 184900 | Loss: 0.282998\n",
      "Iteration 184925 | Loss: 0.282977\n",
      "Iteration 184950 | Loss: 0.282956\n",
      "Iteration 184975 | Loss: 0.282936\n",
      "Iteration 185000 | Loss: 0.282915\n",
      "Iteration 185025 | Loss: 0.282894\n",
      "Iteration 185050 | Loss: 0.282873\n",
      "Iteration 185075 | Loss: 0.282852\n",
      "Iteration 185100 | Loss: 0.282832\n",
      "Iteration 185125 | Loss: 0.282811\n",
      "Iteration 185150 | Loss: 0.282790\n",
      "Iteration 185175 | Loss: 0.282769\n",
      "Iteration 185200 | Loss: 0.282749\n",
      "Iteration 185225 | Loss: 0.282728\n",
      "Iteration 185250 | Loss: 0.282708\n",
      "Iteration 185275 | Loss: 0.282687\n",
      "Iteration 185300 | Loss: 0.282666\n",
      "Iteration 185325 | Loss: 0.282646\n",
      "Iteration 185350 | Loss: 0.282625\n",
      "Iteration 185375 | Loss: 0.282605\n",
      "Iteration 185400 | Loss: 0.282584\n",
      "Iteration 185425 | Loss: 0.282564\n",
      "Iteration 185450 | Loss: 0.282543\n",
      "Iteration 185475 | Loss: 0.282523\n",
      "Iteration 185500 | Loss: 0.282502\n",
      "Iteration 185525 | Loss: 0.282482\n",
      "Iteration 185550 | Loss: 0.282462\n",
      "Iteration 185575 | Loss: 0.282441\n",
      "Iteration 185600 | Loss: 0.282421\n",
      "Iteration 185625 | Loss: 0.282401\n",
      "Iteration 185650 | Loss: 0.282381\n",
      "Iteration 185675 | Loss: 0.282360\n",
      "Iteration 185700 | Loss: 0.282340\n",
      "Iteration 185725 | Loss: 0.282320\n",
      "Iteration 185750 | Loss: 0.282300\n",
      "Iteration 185775 | Loss: 0.282279\n",
      "Iteration 185800 | Loss: 0.282259\n",
      "Iteration 185825 | Loss: 0.282239\n",
      "Iteration 185850 | Loss: 0.282219\n",
      "Iteration 185875 | Loss: 0.282199\n",
      "Iteration 185900 | Loss: 0.282179\n",
      "Iteration 185925 | Loss: 0.282159\n",
      "Iteration 185950 | Loss: 0.282139\n",
      "Iteration 185975 | Loss: 0.282119\n",
      "Iteration 186000 | Loss: 0.282099\n",
      "Iteration 186025 | Loss: 0.282079\n",
      "Iteration 186050 | Loss: 0.282059\n",
      "Iteration 186075 | Loss: 0.282039\n",
      "Iteration 186100 | Loss: 0.282019\n",
      "Iteration 186125 | Loss: 0.281999\n",
      "Iteration 186150 | Loss: 0.281979\n",
      "Iteration 186175 | Loss: 0.281959\n",
      "Iteration 186200 | Loss: 0.281940\n",
      "Iteration 186225 | Loss: 0.281920\n",
      "Iteration 186250 | Loss: 0.281900\n",
      "Iteration 186275 | Loss: 0.281880\n",
      "Iteration 186300 | Loss: 0.281861\n",
      "Iteration 186325 | Loss: 0.281841\n",
      "Iteration 186350 | Loss: 0.281821\n",
      "Iteration 186375 | Loss: 0.281801\n",
      "Iteration 186400 | Loss: 0.281782\n",
      "Iteration 186425 | Loss: 0.281762\n",
      "Iteration 186450 | Loss: 0.281743\n",
      "Iteration 186475 | Loss: 0.281723\n",
      "Iteration 186500 | Loss: 0.281703\n",
      "Iteration 186525 | Loss: 0.281684\n",
      "Iteration 186550 | Loss: 0.281664\n",
      "Iteration 186575 | Loss: 0.281645\n",
      "Iteration 186600 | Loss: 0.281625\n",
      "Iteration 186625 | Loss: 0.281606\n",
      "Iteration 186650 | Loss: 0.281586\n",
      "Iteration 186675 | Loss: 0.281567\n",
      "Iteration 186700 | Loss: 0.281548\n",
      "Iteration 186725 | Loss: 0.281528\n",
      "Iteration 186750 | Loss: 0.281509\n",
      "Iteration 186775 | Loss: 0.281489\n",
      "Iteration 186800 | Loss: 0.281470\n",
      "Iteration 186825 | Loss: 0.281451\n",
      "Iteration 186850 | Loss: 0.281431\n",
      "Iteration 186875 | Loss: 0.281412\n",
      "Iteration 186900 | Loss: 0.281393\n",
      "Iteration 186925 | Loss: 0.281374\n",
      "Iteration 186950 | Loss: 0.281355\n",
      "Iteration 186975 | Loss: 0.281335\n",
      "Iteration 187000 | Loss: 0.281316\n",
      "Iteration 187025 | Loss: 0.281297\n",
      "Iteration 187050 | Loss: 0.281278\n",
      "Iteration 187075 | Loss: 0.281259\n",
      "Iteration 187100 | Loss: 0.281240\n",
      "Iteration 187125 | Loss: 0.281221\n",
      "Iteration 187150 | Loss: 0.281202\n",
      "Iteration 187175 | Loss: 0.281182\n",
      "Iteration 187200 | Loss: 0.281163\n",
      "Iteration 187225 | Loss: 0.281144\n",
      "Iteration 187250 | Loss: 0.281126\n",
      "Iteration 187275 | Loss: 0.281107\n",
      "Iteration 187300 | Loss: 0.281088\n",
      "Iteration 187325 | Loss: 0.281069\n",
      "Iteration 187350 | Loss: 0.281050\n",
      "Iteration 187375 | Loss: 0.281031\n",
      "Iteration 187400 | Loss: 0.281012\n",
      "Iteration 187425 | Loss: 0.280993\n",
      "Iteration 187450 | Loss: 0.280974\n",
      "Iteration 187475 | Loss: 0.280956\n",
      "Iteration 187500 | Loss: 0.280937\n",
      "Iteration 187525 | Loss: 0.280918\n",
      "Iteration 187550 | Loss: 0.280899\n",
      "Iteration 187575 | Loss: 0.280881\n",
      "Iteration 187600 | Loss: 0.280862\n",
      "Iteration 187625 | Loss: 0.280843\n",
      "Iteration 187650 | Loss: 0.280825\n",
      "Iteration 187675 | Loss: 0.280806\n",
      "Iteration 187700 | Loss: 0.280787\n",
      "Iteration 187725 | Loss: 0.280769\n",
      "Iteration 187750 | Loss: 0.280750\n",
      "Iteration 187775 | Loss: 0.280732\n",
      "Iteration 187800 | Loss: 0.280713\n",
      "Iteration 187825 | Loss: 0.280695\n",
      "Iteration 187850 | Loss: 0.280676\n",
      "Iteration 187875 | Loss: 0.280658\n",
      "Iteration 187900 | Loss: 0.280639\n",
      "Iteration 187925 | Loss: 0.280621\n",
      "Iteration 187950 | Loss: 0.280602\n",
      "Iteration 187975 | Loss: 0.280584\n",
      "Iteration 188000 | Loss: 0.280565\n",
      "Iteration 188025 | Loss: 0.280547\n",
      "Iteration 188050 | Loss: 0.280529\n",
      "Iteration 188075 | Loss: 0.280510\n",
      "Iteration 188100 | Loss: 0.280492\n",
      "Iteration 188125 | Loss: 0.280474\n",
      "Iteration 188150 | Loss: 0.280455\n",
      "Iteration 188175 | Loss: 0.280437\n",
      "Iteration 188200 | Loss: 0.280419\n",
      "Iteration 188225 | Loss: 0.280401\n",
      "Iteration 188250 | Loss: 0.280383\n",
      "Iteration 188275 | Loss: 0.280364\n",
      "Iteration 188300 | Loss: 0.280346\n",
      "Iteration 188325 | Loss: 0.280328\n",
      "Iteration 188350 | Loss: 0.280310\n",
      "Iteration 188375 | Loss: 0.280292\n",
      "Iteration 188400 | Loss: 0.280274\n",
      "Iteration 188425 | Loss: 0.280256\n",
      "Iteration 188450 | Loss: 0.280238\n",
      "Iteration 188475 | Loss: 0.280220\n",
      "Iteration 188500 | Loss: 0.280202\n",
      "Iteration 188525 | Loss: 0.280184\n",
      "Iteration 188550 | Loss: 0.280166\n",
      "Iteration 188575 | Loss: 0.280148\n",
      "Iteration 188600 | Loss: 0.280130\n",
      "Iteration 188625 | Loss: 0.280112\n",
      "Iteration 188650 | Loss: 0.280094\n",
      "Iteration 188675 | Loss: 0.280076\n",
      "Iteration 188700 | Loss: 0.280058\n",
      "Iteration 188725 | Loss: 0.280040\n",
      "Iteration 188750 | Loss: 0.280022\n",
      "Iteration 188775 | Loss: 0.280005\n",
      "Iteration 188800 | Loss: 0.279987\n",
      "Iteration 188825 | Loss: 0.279969\n",
      "Iteration 188850 | Loss: 0.279951\n",
      "Iteration 188875 | Loss: 0.279934\n",
      "Iteration 188900 | Loss: 0.279916\n",
      "Iteration 188925 | Loss: 0.279898\n",
      "Iteration 188950 | Loss: 0.279881\n",
      "Iteration 188975 | Loss: 0.279863\n",
      "Iteration 189000 | Loss: 0.279845\n",
      "Iteration 189025 | Loss: 0.279828\n",
      "Iteration 189050 | Loss: 0.279810\n",
      "Iteration 189075 | Loss: 0.279792\n",
      "Iteration 189100 | Loss: 0.279775\n",
      "Iteration 189125 | Loss: 0.279757\n",
      "Iteration 189150 | Loss: 0.279740\n",
      "Iteration 189175 | Loss: 0.279722\n",
      "Iteration 189200 | Loss: 0.279705\n",
      "Iteration 189225 | Loss: 0.279687\n",
      "Iteration 189250 | Loss: 0.279670\n",
      "Iteration 189275 | Loss: 0.279652\n",
      "Iteration 189300 | Loss: 0.279635\n",
      "Iteration 189325 | Loss: 0.279618\n",
      "Iteration 189350 | Loss: 0.279600\n",
      "Iteration 189375 | Loss: 0.279583\n",
      "Iteration 189400 | Loss: 0.279565\n",
      "Iteration 189425 | Loss: 0.279548\n",
      "Iteration 189450 | Loss: 0.279531\n",
      "Iteration 189475 | Loss: 0.279514\n",
      "Iteration 189500 | Loss: 0.279496\n",
      "Iteration 189525 | Loss: 0.279479\n",
      "Iteration 189550 | Loss: 0.279462\n",
      "Iteration 189575 | Loss: 0.279445\n",
      "Iteration 189600 | Loss: 0.279427\n",
      "Iteration 189625 | Loss: 0.279410\n",
      "Iteration 189650 | Loss: 0.279393\n",
      "Iteration 189675 | Loss: 0.279376\n",
      "Iteration 189700 | Loss: 0.279359\n",
      "Iteration 189725 | Loss: 0.279342\n",
      "Iteration 189750 | Loss: 0.279324\n",
      "Iteration 189775 | Loss: 0.279307\n",
      "Iteration 189800 | Loss: 0.279290\n",
      "Iteration 189825 | Loss: 0.279273\n",
      "Iteration 189850 | Loss: 0.279256\n",
      "Iteration 189875 | Loss: 0.279239\n",
      "Iteration 189900 | Loss: 0.279222\n",
      "Iteration 189925 | Loss: 0.279205\n",
      "Iteration 189950 | Loss: 0.279188\n",
      "Iteration 189975 | Loss: 0.279171\n",
      "Iteration 190000 | Loss: 0.279154\n",
      "Iteration 190025 | Loss: 0.279138\n",
      "Iteration 190050 | Loss: 0.279121\n",
      "Iteration 190075 | Loss: 0.279104\n",
      "Iteration 190100 | Loss: 0.279087\n",
      "Iteration 190125 | Loss: 0.279070\n",
      "Iteration 190150 | Loss: 0.279053\n",
      "Iteration 190175 | Loss: 0.279036\n",
      "Iteration 190200 | Loss: 0.279020\n",
      "Iteration 190225 | Loss: 0.279003\n",
      "Iteration 190250 | Loss: 0.278986\n",
      "Iteration 190275 | Loss: 0.278969\n",
      "Iteration 190300 | Loss: 0.278953\n",
      "Iteration 190325 | Loss: 0.278936\n",
      "Iteration 190350 | Loss: 0.278919\n",
      "Iteration 190375 | Loss: 0.278903\n",
      "Iteration 190400 | Loss: 0.278886\n",
      "Iteration 190425 | Loss: 0.278869\n",
      "Iteration 190450 | Loss: 0.278853\n",
      "Iteration 190475 | Loss: 0.278836\n",
      "Iteration 190500 | Loss: 0.278820\n",
      "Iteration 190525 | Loss: 0.278803\n",
      "Iteration 190550 | Loss: 0.278787\n",
      "Iteration 190575 | Loss: 0.278770\n",
      "Iteration 190600 | Loss: 0.278754\n",
      "Iteration 190625 | Loss: 0.278737\n",
      "Iteration 190650 | Loss: 0.278721\n",
      "Iteration 190675 | Loss: 0.278704\n",
      "Iteration 190700 | Loss: 0.278688\n",
      "Iteration 190725 | Loss: 0.278671\n",
      "Iteration 190750 | Loss: 0.278655\n",
      "Iteration 190775 | Loss: 0.278638\n",
      "Iteration 190800 | Loss: 0.278622\n",
      "Iteration 190825 | Loss: 0.278606\n",
      "Iteration 190850 | Loss: 0.278589\n",
      "Iteration 190875 | Loss: 0.278573\n",
      "Iteration 190900 | Loss: 0.278557\n",
      "Iteration 190925 | Loss: 0.278540\n",
      "Iteration 190950 | Loss: 0.278524\n",
      "Iteration 190975 | Loss: 0.278508\n",
      "Iteration 191000 | Loss: 0.278492\n",
      "Iteration 191025 | Loss: 0.278476\n",
      "Iteration 191050 | Loss: 0.278459\n",
      "Iteration 191075 | Loss: 0.278443\n",
      "Iteration 191100 | Loss: 0.278427\n",
      "Iteration 191125 | Loss: 0.278411\n",
      "Iteration 191150 | Loss: 0.278395\n",
      "Iteration 191175 | Loss: 0.278379\n",
      "Iteration 191200 | Loss: 0.278362\n",
      "Iteration 191225 | Loss: 0.278346\n",
      "Iteration 191250 | Loss: 0.278330\n",
      "Iteration 191275 | Loss: 0.278314\n",
      "Iteration 191300 | Loss: 0.278298\n",
      "Iteration 191325 | Loss: 0.278282\n",
      "Iteration 191350 | Loss: 0.278266\n",
      "Iteration 191375 | Loss: 0.278250\n",
      "Iteration 191400 | Loss: 0.278234\n",
      "Iteration 191425 | Loss: 0.278218\n",
      "Iteration 191450 | Loss: 0.278202\n",
      "Iteration 191475 | Loss: 0.278186\n",
      "Iteration 191500 | Loss: 0.278171\n",
      "Iteration 191525 | Loss: 0.278155\n",
      "Iteration 191550 | Loss: 0.278139\n",
      "Iteration 191575 | Loss: 0.278123\n",
      "Iteration 191600 | Loss: 0.278107\n",
      "Iteration 191625 | Loss: 0.278091\n",
      "Iteration 191650 | Loss: 0.278076\n",
      "Iteration 191675 | Loss: 0.278060\n",
      "Iteration 191700 | Loss: 0.278044\n",
      "Iteration 191725 | Loss: 0.278028\n",
      "Iteration 191750 | Loss: 0.278013\n",
      "Iteration 191775 | Loss: 0.277997\n",
      "Iteration 191800 | Loss: 0.277981\n",
      "Iteration 191825 | Loss: 0.277965\n",
      "Iteration 191850 | Loss: 0.277950\n",
      "Iteration 191875 | Loss: 0.277934\n",
      "Iteration 191900 | Loss: 0.277918\n",
      "Iteration 191925 | Loss: 0.277903\n",
      "Iteration 191950 | Loss: 0.277887\n",
      "Iteration 191975 | Loss: 0.277872\n",
      "Iteration 192000 | Loss: 0.277856\n",
      "Iteration 192025 | Loss: 0.277841\n",
      "Iteration 192050 | Loss: 0.277825\n",
      "Iteration 192075 | Loss: 0.277809\n",
      "Iteration 192100 | Loss: 0.277794\n",
      "Iteration 192125 | Loss: 0.277778\n",
      "Iteration 192150 | Loss: 0.277763\n",
      "Iteration 192175 | Loss: 0.277748\n",
      "Iteration 192200 | Loss: 0.277732\n",
      "Iteration 192225 | Loss: 0.277717\n",
      "Iteration 192250 | Loss: 0.277701\n",
      "Iteration 192275 | Loss: 0.277686\n",
      "Iteration 192300 | Loss: 0.277670\n",
      "Iteration 192325 | Loss: 0.277655\n",
      "Iteration 192350 | Loss: 0.277640\n",
      "Iteration 192375 | Loss: 0.277624\n",
      "Iteration 192400 | Loss: 0.277609\n",
      "Iteration 192425 | Loss: 0.277594\n",
      "Iteration 192450 | Loss: 0.277579\n",
      "Iteration 192475 | Loss: 0.277563\n",
      "Iteration 192500 | Loss: 0.277548\n",
      "Iteration 192525 | Loss: 0.277533\n",
      "Iteration 192550 | Loss: 0.277518\n",
      "Iteration 192575 | Loss: 0.277502\n",
      "Iteration 192600 | Loss: 0.277487\n",
      "Iteration 192625 | Loss: 0.277472\n",
      "Iteration 192650 | Loss: 0.277457\n",
      "Iteration 192675 | Loss: 0.277442\n",
      "Iteration 192700 | Loss: 0.277427\n",
      "Iteration 192725 | Loss: 0.277411\n",
      "Iteration 192750 | Loss: 0.277396\n",
      "Iteration 192775 | Loss: 0.277381\n",
      "Iteration 192800 | Loss: 0.277366\n",
      "Iteration 192825 | Loss: 0.277351\n",
      "Iteration 192850 | Loss: 0.277336\n",
      "Iteration 192875 | Loss: 0.277321\n",
      "Iteration 192900 | Loss: 0.277306\n",
      "Iteration 192925 | Loss: 0.277291\n",
      "Iteration 192950 | Loss: 0.277276\n",
      "Iteration 192975 | Loss: 0.277261\n",
      "Iteration 193000 | Loss: 0.277246\n",
      "Iteration 193025 | Loss: 0.277231\n",
      "Iteration 193050 | Loss: 0.277216\n",
      "Iteration 193075 | Loss: 0.277202\n",
      "Iteration 193100 | Loss: 0.277187\n",
      "Iteration 193125 | Loss: 0.277172\n",
      "Iteration 193150 | Loss: 0.277157\n",
      "Iteration 193175 | Loss: 0.277142\n",
      "Iteration 193200 | Loss: 0.277127\n",
      "Iteration 193225 | Loss: 0.277113\n",
      "Iteration 193250 | Loss: 0.277098\n",
      "Iteration 193275 | Loss: 0.277083\n",
      "Iteration 193300 | Loss: 0.277068\n",
      "Iteration 193325 | Loss: 0.277054\n",
      "Iteration 193350 | Loss: 0.277039\n",
      "Iteration 193375 | Loss: 0.277024\n",
      "Iteration 193400 | Loss: 0.277009\n",
      "Iteration 193425 | Loss: 0.276995\n",
      "Iteration 193450 | Loss: 0.276980\n",
      "Iteration 193475 | Loss: 0.276965\n",
      "Iteration 193500 | Loss: 0.276951\n",
      "Iteration 193525 | Loss: 0.276936\n",
      "Iteration 193550 | Loss: 0.276922\n",
      "Iteration 193575 | Loss: 0.276907\n",
      "Iteration 193600 | Loss: 0.276892\n",
      "Iteration 193625 | Loss: 0.276878\n",
      "Iteration 193650 | Loss: 0.276863\n",
      "Iteration 193675 | Loss: 0.276849\n",
      "Iteration 193700 | Loss: 0.276834\n",
      "Iteration 193725 | Loss: 0.276820\n",
      "Iteration 193750 | Loss: 0.276805\n",
      "Iteration 193775 | Loss: 0.276791\n",
      "Iteration 193800 | Loss: 0.276776\n",
      "Iteration 193825 | Loss: 0.276762\n",
      "Iteration 193850 | Loss: 0.276748\n",
      "Iteration 193875 | Loss: 0.276733\n",
      "Iteration 193900 | Loss: 0.276719\n",
      "Iteration 193925 | Loss: 0.276704\n",
      "Iteration 193950 | Loss: 0.276690\n",
      "Iteration 193975 | Loss: 0.276676\n",
      "Iteration 194000 | Loss: 0.276661\n",
      "Iteration 194025 | Loss: 0.276647\n",
      "Iteration 194050 | Loss: 0.276633\n",
      "Iteration 194075 | Loss: 0.276618\n",
      "Iteration 194100 | Loss: 0.276604\n",
      "Iteration 194125 | Loss: 0.276590\n",
      "Iteration 194150 | Loss: 0.276576\n",
      "Iteration 194175 | Loss: 0.276561\n",
      "Iteration 194200 | Loss: 0.276547\n",
      "Iteration 194225 | Loss: 0.276533\n",
      "Iteration 194250 | Loss: 0.276519\n",
      "Iteration 194275 | Loss: 0.276505\n",
      "Iteration 194300 | Loss: 0.276491\n",
      "Iteration 194325 | Loss: 0.276476\n",
      "Iteration 194350 | Loss: 0.276462\n",
      "Iteration 194375 | Loss: 0.276448\n",
      "Iteration 194400 | Loss: 0.276434\n",
      "Iteration 194425 | Loss: 0.276420\n",
      "Iteration 194450 | Loss: 0.276406\n",
      "Iteration 194475 | Loss: 0.276392\n",
      "Iteration 194500 | Loss: 0.276378\n",
      "Iteration 194525 | Loss: 0.276364\n",
      "Iteration 194550 | Loss: 0.276350\n",
      "Iteration 194575 | Loss: 0.276336\n",
      "Iteration 194600 | Loss: 0.276322\n",
      "Iteration 194625 | Loss: 0.276308\n",
      "Iteration 194650 | Loss: 0.276294\n",
      "Iteration 194675 | Loss: 0.276280\n",
      "Iteration 194700 | Loss: 0.276266\n",
      "Iteration 194725 | Loss: 0.276252\n",
      "Iteration 194750 | Loss: 0.276238\n",
      "Iteration 194775 | Loss: 0.276224\n",
      "Iteration 194800 | Loss: 0.276211\n",
      "Iteration 194825 | Loss: 0.276197\n",
      "Iteration 194850 | Loss: 0.276183\n",
      "Iteration 194875 | Loss: 0.276169\n",
      "Iteration 194900 | Loss: 0.276155\n",
      "Iteration 194925 | Loss: 0.276142\n",
      "Iteration 194950 | Loss: 0.276128\n",
      "Iteration 194975 | Loss: 0.276114\n",
      "Iteration 195000 | Loss: 0.276100\n",
      "Iteration 195025 | Loss: 0.276087\n",
      "Iteration 195050 | Loss: 0.276073\n",
      "Iteration 195075 | Loss: 0.276059\n",
      "Iteration 195100 | Loss: 0.276045\n",
      "Iteration 195125 | Loss: 0.276032\n",
      "Iteration 195150 | Loss: 0.276018\n",
      "Iteration 195175 | Loss: 0.276004\n",
      "Iteration 195200 | Loss: 0.275991\n",
      "Iteration 195225 | Loss: 0.275977\n",
      "Iteration 195250 | Loss: 0.275964\n",
      "Iteration 195275 | Loss: 0.275950\n",
      "Iteration 195300 | Loss: 0.275936\n",
      "Iteration 195325 | Loss: 0.275923\n",
      "Iteration 195350 | Loss: 0.275909\n",
      "Iteration 195375 | Loss: 0.275896\n",
      "Iteration 195400 | Loss: 0.275882\n",
      "Iteration 195425 | Loss: 0.275869\n",
      "Iteration 195450 | Loss: 0.275855\n",
      "Iteration 195475 | Loss: 0.275842\n",
      "Iteration 195500 | Loss: 0.275828\n",
      "Iteration 195525 | Loss: 0.275815\n",
      "Iteration 195550 | Loss: 0.275801\n",
      "Iteration 195575 | Loss: 0.275788\n",
      "Iteration 195600 | Loss: 0.275775\n",
      "Iteration 195625 | Loss: 0.275761\n",
      "Iteration 195650 | Loss: 0.275748\n",
      "Iteration 195675 | Loss: 0.275734\n",
      "Iteration 195700 | Loss: 0.275721\n",
      "Iteration 195725 | Loss: 0.275708\n",
      "Iteration 195750 | Loss: 0.275694\n",
      "Iteration 195775 | Loss: 0.275681\n",
      "Iteration 195800 | Loss: 0.275668\n",
      "Iteration 195825 | Loss: 0.275655\n",
      "Iteration 195850 | Loss: 0.275641\n",
      "Iteration 195875 | Loss: 0.275628\n",
      "Iteration 195900 | Loss: 0.275615\n",
      "Iteration 195925 | Loss: 0.275602\n",
      "Iteration 195950 | Loss: 0.275588\n",
      "Iteration 195975 | Loss: 0.275575\n",
      "Iteration 196000 | Loss: 0.275562\n",
      "Iteration 196025 | Loss: 0.275549\n",
      "Iteration 196050 | Loss: 0.275536\n",
      "Iteration 196075 | Loss: 0.275523\n",
      "Iteration 196100 | Loss: 0.275509\n",
      "Iteration 196125 | Loss: 0.275496\n",
      "Iteration 196150 | Loss: 0.275483\n",
      "Iteration 196175 | Loss: 0.275470\n",
      "Iteration 196200 | Loss: 0.275457\n",
      "Iteration 196225 | Loss: 0.275444\n",
      "Iteration 196250 | Loss: 0.275431\n",
      "Iteration 196275 | Loss: 0.275418\n",
      "Iteration 196300 | Loss: 0.275405\n",
      "Iteration 196325 | Loss: 0.275392\n",
      "Iteration 196350 | Loss: 0.275379\n",
      "Iteration 196375 | Loss: 0.275366\n",
      "Iteration 196400 | Loss: 0.275353\n",
      "Iteration 196425 | Loss: 0.275340\n",
      "Iteration 196450 | Loss: 0.275327\n",
      "Iteration 196475 | Loss: 0.275314\n",
      "Iteration 196500 | Loss: 0.275301\n",
      "Iteration 196525 | Loss: 0.275288\n",
      "Iteration 196550 | Loss: 0.275275\n",
      "Iteration 196575 | Loss: 0.275262\n",
      "Iteration 196600 | Loss: 0.275250\n",
      "Iteration 196625 | Loss: 0.275237\n",
      "Iteration 196650 | Loss: 0.275224\n",
      "Iteration 196675 | Loss: 0.275211\n",
      "Iteration 196700 | Loss: 0.275198\n",
      "Iteration 196725 | Loss: 0.275185\n",
      "Iteration 196750 | Loss: 0.275173\n",
      "Iteration 196775 | Loss: 0.275160\n",
      "Iteration 196800 | Loss: 0.275147\n",
      "Iteration 196825 | Loss: 0.275134\n",
      "Iteration 196850 | Loss: 0.275122\n",
      "Iteration 196875 | Loss: 0.275109\n",
      "Iteration 196900 | Loss: 0.275096\n",
      "Iteration 196925 | Loss: 0.275084\n",
      "Iteration 196950 | Loss: 0.275071\n",
      "Iteration 196975 | Loss: 0.275058\n",
      "Iteration 197000 | Loss: 0.275046\n",
      "Iteration 197025 | Loss: 0.275033\n",
      "Iteration 197050 | Loss: 0.275020\n",
      "Iteration 197075 | Loss: 0.275008\n",
      "Iteration 197100 | Loss: 0.274995\n",
      "Iteration 197125 | Loss: 0.274983\n",
      "Iteration 197150 | Loss: 0.274970\n",
      "Iteration 197175 | Loss: 0.274957\n",
      "Iteration 197200 | Loss: 0.274945\n",
      "Iteration 197225 | Loss: 0.274932\n",
      "Iteration 197250 | Loss: 0.274920\n",
      "Iteration 197275 | Loss: 0.274907\n",
      "Iteration 197300 | Loss: 0.274895\n",
      "Iteration 197325 | Loss: 0.274882\n",
      "Iteration 197350 | Loss: 0.274870\n",
      "Iteration 197375 | Loss: 0.274857\n",
      "Iteration 197400 | Loss: 0.274845\n",
      "Iteration 197425 | Loss: 0.274833\n",
      "Iteration 197450 | Loss: 0.274820\n",
      "Iteration 197475 | Loss: 0.274808\n",
      "Iteration 197500 | Loss: 0.274795\n",
      "Iteration 197525 | Loss: 0.274783\n",
      "Iteration 197550 | Loss: 0.274771\n",
      "Iteration 197575 | Loss: 0.274758\n",
      "Iteration 197600 | Loss: 0.274746\n",
      "Iteration 197625 | Loss: 0.274734\n",
      "Iteration 197650 | Loss: 0.274721\n",
      "Iteration 197675 | Loss: 0.274709\n",
      "Iteration 197700 | Loss: 0.274697\n",
      "Iteration 197725 | Loss: 0.274684\n",
      "Iteration 197750 | Loss: 0.274672\n",
      "Iteration 197775 | Loss: 0.274660\n",
      "Iteration 197800 | Loss: 0.274648\n",
      "Iteration 197825 | Loss: 0.274636\n",
      "Iteration 197850 | Loss: 0.274623\n",
      "Iteration 197875 | Loss: 0.274611\n",
      "Iteration 197900 | Loss: 0.274599\n",
      "Iteration 197925 | Loss: 0.274587\n",
      "Iteration 197950 | Loss: 0.274575\n",
      "Iteration 197975 | Loss: 0.274562\n",
      "Iteration 198000 | Loss: 0.274550\n",
      "Iteration 198025 | Loss: 0.274538\n",
      "Iteration 198050 | Loss: 0.274526\n",
      "Iteration 198075 | Loss: 0.274514\n",
      "Iteration 198100 | Loss: 0.274502\n",
      "Iteration 198125 | Loss: 0.274490\n",
      "Iteration 198150 | Loss: 0.274478\n",
      "Iteration 198175 | Loss: 0.274466\n",
      "Iteration 198200 | Loss: 0.274454\n",
      "Iteration 198225 | Loss: 0.274442\n",
      "Iteration 198250 | Loss: 0.274430\n",
      "Iteration 198275 | Loss: 0.274418\n",
      "Iteration 198300 | Loss: 0.274406\n",
      "Iteration 198325 | Loss: 0.274394\n",
      "Iteration 198350 | Loss: 0.274382\n",
      "Iteration 198375 | Loss: 0.274370\n",
      "Iteration 198400 | Loss: 0.274358\n",
      "Iteration 198425 | Loss: 0.274346\n",
      "Iteration 198450 | Loss: 0.274334\n",
      "Iteration 198475 | Loss: 0.274322\n",
      "Iteration 198500 | Loss: 0.274310\n",
      "Iteration 198525 | Loss: 0.274298\n",
      "Iteration 198550 | Loss: 0.274287\n",
      "Iteration 198575 | Loss: 0.274275\n",
      "Iteration 198600 | Loss: 0.274263\n",
      "Iteration 198625 | Loss: 0.274251\n",
      "Iteration 198650 | Loss: 0.274239\n",
      "Iteration 198675 | Loss: 0.274227\n",
      "Iteration 198700 | Loss: 0.274216\n",
      "Iteration 198725 | Loss: 0.274204\n",
      "Iteration 198750 | Loss: 0.274192\n",
      "Iteration 198775 | Loss: 0.274180\n",
      "Iteration 198800 | Loss: 0.274169\n",
      "Iteration 198825 | Loss: 0.274157\n",
      "Iteration 198850 | Loss: 0.274145\n",
      "Iteration 198875 | Loss: 0.274133\n",
      "Iteration 198900 | Loss: 0.274122\n",
      "Iteration 198925 | Loss: 0.274110\n",
      "Iteration 198950 | Loss: 0.274098\n",
      "Iteration 198975 | Loss: 0.274087\n",
      "Iteration 199000 | Loss: 0.274075\n",
      "Iteration 199025 | Loss: 0.274064\n",
      "Iteration 199050 | Loss: 0.274052\n",
      "Iteration 199075 | Loss: 0.274040\n",
      "Iteration 199100 | Loss: 0.274029\n",
      "Iteration 199125 | Loss: 0.274017\n",
      "Iteration 199150 | Loss: 0.274006\n",
      "Iteration 199175 | Loss: 0.273994\n",
      "Iteration 199200 | Loss: 0.273982\n",
      "Iteration 199225 | Loss: 0.273971\n",
      "Iteration 199250 | Loss: 0.273959\n",
      "Iteration 199275 | Loss: 0.273948\n",
      "Iteration 199300 | Loss: 0.273936\n",
      "Iteration 199325 | Loss: 0.273925\n",
      "Iteration 199350 | Loss: 0.273913\n",
      "Iteration 199375 | Loss: 0.273902\n",
      "Iteration 199400 | Loss: 0.273891\n",
      "Iteration 199425 | Loss: 0.273879\n",
      "Iteration 199450 | Loss: 0.273868\n",
      "Iteration 199475 | Loss: 0.273856\n",
      "Iteration 199500 | Loss: 0.273845\n",
      "Iteration 199525 | Loss: 0.273834\n",
      "Iteration 199550 | Loss: 0.273822\n",
      "Iteration 199575 | Loss: 0.273811\n",
      "Iteration 199600 | Loss: 0.273799\n",
      "Iteration 199625 | Loss: 0.273788\n",
      "Iteration 199650 | Loss: 0.273777\n",
      "Iteration 199675 | Loss: 0.273765\n",
      "Iteration 199700 | Loss: 0.273754\n",
      "Iteration 199725 | Loss: 0.273743\n",
      "Iteration 199750 | Loss: 0.273732\n",
      "Iteration 199775 | Loss: 0.273720\n",
      "Iteration 199800 | Loss: 0.273709\n",
      "Iteration 199825 | Loss: 0.273698\n",
      "Iteration 199850 | Loss: 0.273687\n",
      "Iteration 199875 | Loss: 0.273675\n",
      "Iteration 199900 | Loss: 0.273664\n",
      "Iteration 199925 | Loss: 0.273653\n",
      "Iteration 199950 | Loss: 0.273642\n",
      "Iteration 199975 | Loss: 0.273631\n",
      "Iteration 200000 | Loss: 0.273619\n",
      "Iteration 200025 | Loss: 0.273608\n",
      "Iteration 200050 | Loss: 0.273597\n",
      "Iteration 200075 | Loss: 0.273586\n",
      "Iteration 200100 | Loss: 0.273575\n",
      "Iteration 200125 | Loss: 0.273564\n",
      "Iteration 200150 | Loss: 0.273553\n",
      "Iteration 200175 | Loss: 0.273542\n",
      "Iteration 200200 | Loss: 0.273530\n",
      "Iteration 200225 | Loss: 0.273519\n",
      "Iteration 200250 | Loss: 0.273508\n",
      "Iteration 200275 | Loss: 0.273497\n",
      "Iteration 200300 | Loss: 0.273486\n",
      "Iteration 200325 | Loss: 0.273475\n",
      "Iteration 200350 | Loss: 0.273464\n",
      "Iteration 200375 | Loss: 0.273453\n",
      "Iteration 200400 | Loss: 0.273442\n",
      "Iteration 200425 | Loss: 0.273431\n",
      "Iteration 200450 | Loss: 0.273420\n",
      "Iteration 200475 | Loss: 0.273409\n",
      "Iteration 200500 | Loss: 0.273398\n",
      "Iteration 200525 | Loss: 0.273388\n",
      "Iteration 200550 | Loss: 0.273377\n",
      "Iteration 200575 | Loss: 0.273366\n",
      "Iteration 200600 | Loss: 0.273355\n",
      "Iteration 200625 | Loss: 0.273344\n",
      "Iteration 200650 | Loss: 0.273333\n",
      "Iteration 200675 | Loss: 0.273322\n",
      "Iteration 200700 | Loss: 0.273311\n",
      "Iteration 200725 | Loss: 0.273301\n",
      "Iteration 200750 | Loss: 0.273290\n",
      "Iteration 200775 | Loss: 0.273279\n",
      "Iteration 200800 | Loss: 0.273268\n",
      "Iteration 200825 | Loss: 0.273257\n",
      "Iteration 200850 | Loss: 0.273247\n",
      "Iteration 200875 | Loss: 0.273236\n",
      "Iteration 200900 | Loss: 0.273225\n",
      "Iteration 200925 | Loss: 0.273214\n",
      "Iteration 200950 | Loss: 0.273204\n",
      "Iteration 200975 | Loss: 0.273193\n",
      "Iteration 201000 | Loss: 0.273182\n",
      "Iteration 201025 | Loss: 0.273171\n",
      "Iteration 201050 | Loss: 0.273161\n",
      "Iteration 201075 | Loss: 0.273150\n",
      "Iteration 201100 | Loss: 0.273139\n",
      "Iteration 201125 | Loss: 0.273129\n",
      "Iteration 201150 | Loss: 0.273118\n",
      "Iteration 201175 | Loss: 0.273107\n",
      "Iteration 201200 | Loss: 0.273097\n",
      "Iteration 201225 | Loss: 0.273086\n",
      "Iteration 201250 | Loss: 0.273076\n",
      "Iteration 201275 | Loss: 0.273065\n",
      "Iteration 201300 | Loss: 0.273054\n",
      "Iteration 201325 | Loss: 0.273044\n",
      "Iteration 201350 | Loss: 0.273033\n",
      "Iteration 201375 | Loss: 0.273023\n",
      "Iteration 201400 | Loss: 0.273012\n",
      "Iteration 201425 | Loss: 0.273002\n",
      "Iteration 201450 | Loss: 0.272991\n",
      "Iteration 201475 | Loss: 0.272981\n",
      "Iteration 201500 | Loss: 0.272970\n",
      "Iteration 201525 | Loss: 0.272960\n",
      "Iteration 201550 | Loss: 0.272949\n",
      "Iteration 201575 | Loss: 0.272939\n",
      "Iteration 201600 | Loss: 0.272928\n",
      "Iteration 201625 | Loss: 0.272918\n",
      "Iteration 201650 | Loss: 0.272908\n",
      "Iteration 201675 | Loss: 0.272897\n",
      "Iteration 201700 | Loss: 0.272887\n",
      "Iteration 201725 | Loss: 0.272876\n",
      "Iteration 201750 | Loss: 0.272866\n",
      "Iteration 201775 | Loss: 0.272856\n",
      "Iteration 201800 | Loss: 0.272845\n",
      "Iteration 201825 | Loss: 0.272835\n",
      "Iteration 201850 | Loss: 0.272825\n",
      "Iteration 201875 | Loss: 0.272814\n",
      "Iteration 201900 | Loss: 0.272804\n",
      "Iteration 201925 | Loss: 0.272794\n",
      "Iteration 201950 | Loss: 0.272783\n",
      "Iteration 201975 | Loss: 0.272773\n",
      "Iteration 202000 | Loss: 0.272763\n",
      "Iteration 202025 | Loss: 0.272752\n",
      "Iteration 202050 | Loss: 0.272742\n",
      "Iteration 202075 | Loss: 0.272732\n",
      "Iteration 202100 | Loss: 0.272722\n",
      "Iteration 202125 | Loss: 0.272711\n",
      "Iteration 202150 | Loss: 0.272701\n",
      "Iteration 202175 | Loss: 0.272691\n",
      "Iteration 202200 | Loss: 0.272681\n",
      "Iteration 202225 | Loss: 0.272671\n",
      "Iteration 202250 | Loss: 0.272661\n",
      "Iteration 202275 | Loss: 0.272650\n",
      "Iteration 202300 | Loss: 0.272640\n",
      "Iteration 202325 | Loss: 0.272630\n",
      "Iteration 202350 | Loss: 0.272620\n",
      "Iteration 202375 | Loss: 0.272610\n",
      "Iteration 202400 | Loss: 0.272600\n",
      "Iteration 202425 | Loss: 0.272590\n",
      "Iteration 202450 | Loss: 0.272580\n",
      "Iteration 202475 | Loss: 0.272569\n",
      "Iteration 202500 | Loss: 0.272559\n",
      "Iteration 202525 | Loss: 0.272549\n",
      "Iteration 202550 | Loss: 0.272539\n",
      "Iteration 202575 | Loss: 0.272529\n",
      "Iteration 202600 | Loss: 0.272519\n",
      "Iteration 202625 | Loss: 0.272509\n",
      "Iteration 202650 | Loss: 0.272499\n",
      "Iteration 202675 | Loss: 0.272489\n",
      "Iteration 202700 | Loss: 0.272479\n",
      "Iteration 202725 | Loss: 0.272469\n",
      "Iteration 202750 | Loss: 0.272459\n",
      "Iteration 202775 | Loss: 0.272449\n",
      "Iteration 202800 | Loss: 0.272439\n",
      "Iteration 202825 | Loss: 0.272430\n",
      "Iteration 202850 | Loss: 0.272420\n",
      "Iteration 202875 | Loss: 0.272410\n",
      "Iteration 202900 | Loss: 0.272400\n",
      "Iteration 202925 | Loss: 0.272390\n",
      "Iteration 202950 | Loss: 0.272380\n",
      "Iteration 202975 | Loss: 0.272370\n",
      "Iteration 203000 | Loss: 0.272360\n",
      "Iteration 203025 | Loss: 0.272350\n",
      "Iteration 203050 | Loss: 0.272341\n",
      "Iteration 203075 | Loss: 0.272331\n",
      "Iteration 203100 | Loss: 0.272321\n",
      "Iteration 203125 | Loss: 0.272311\n",
      "Iteration 203150 | Loss: 0.272301\n",
      "Iteration 203175 | Loss: 0.272292\n",
      "Iteration 203200 | Loss: 0.272282\n",
      "Iteration 203225 | Loss: 0.272272\n",
      "Iteration 203250 | Loss: 0.272262\n",
      "Iteration 203275 | Loss: 0.272253\n",
      "Iteration 203300 | Loss: 0.272243\n",
      "Iteration 203325 | Loss: 0.272233\n",
      "Iteration 203350 | Loss: 0.272223\n",
      "Iteration 203375 | Loss: 0.272214\n",
      "Iteration 203400 | Loss: 0.272204\n",
      "Iteration 203425 | Loss: 0.272194\n",
      "Iteration 203450 | Loss: 0.272185\n",
      "Iteration 203475 | Loss: 0.272175\n",
      "Iteration 203500 | Loss: 0.272165\n",
      "Iteration 203525 | Loss: 0.272156\n",
      "Iteration 203550 | Loss: 0.272146\n",
      "Iteration 203575 | Loss: 0.272136\n",
      "Iteration 203600 | Loss: 0.272127\n",
      "Iteration 203625 | Loss: 0.272117\n",
      "Iteration 203650 | Loss: 0.272108\n",
      "Iteration 203675 | Loss: 0.272098\n",
      "Iteration 203700 | Loss: 0.272088\n",
      "Iteration 203725 | Loss: 0.272079\n",
      "Iteration 203750 | Loss: 0.272069\n",
      "Iteration 203775 | Loss: 0.272060\n",
      "Iteration 203800 | Loss: 0.272050\n",
      "Iteration 203825 | Loss: 0.272041\n",
      "Iteration 203850 | Loss: 0.272031\n",
      "Iteration 203875 | Loss: 0.272022\n",
      "Iteration 203900 | Loss: 0.272012\n",
      "Iteration 203925 | Loss: 0.272003\n",
      "Iteration 203950 | Loss: 0.271993\n",
      "Iteration 203975 | Loss: 0.271984\n",
      "Iteration 204000 | Loss: 0.271974\n",
      "Iteration 204025 | Loss: 0.271965\n",
      "Iteration 204050 | Loss: 0.271955\n",
      "Iteration 204075 | Loss: 0.271946\n",
      "Iteration 204100 | Loss: 0.271937\n",
      "Iteration 204125 | Loss: 0.271927\n",
      "Iteration 204150 | Loss: 0.271918\n",
      "Iteration 204175 | Loss: 0.271908\n",
      "Iteration 204200 | Loss: 0.271899\n",
      "Iteration 204225 | Loss: 0.271890\n",
      "Iteration 204250 | Loss: 0.271880\n",
      "Iteration 204275 | Loss: 0.271871\n",
      "Iteration 204300 | Loss: 0.271862\n",
      "Iteration 204325 | Loss: 0.271852\n",
      "Iteration 204350 | Loss: 0.271843\n",
      "Iteration 204375 | Loss: 0.271834\n",
      "Iteration 204400 | Loss: 0.271824\n",
      "Iteration 204425 | Loss: 0.271815\n",
      "Iteration 204450 | Loss: 0.271806\n",
      "Iteration 204475 | Loss: 0.271797\n",
      "Iteration 204500 | Loss: 0.271787\n",
      "Iteration 204525 | Loss: 0.271778\n",
      "Iteration 204550 | Loss: 0.271769\n",
      "Iteration 204575 | Loss: 0.271760\n",
      "Iteration 204600 | Loss: 0.271750\n",
      "Iteration 204625 | Loss: 0.271741\n",
      "Iteration 204650 | Loss: 0.271732\n",
      "Iteration 204675 | Loss: 0.271723\n",
      "Iteration 204700 | Loss: 0.271714\n",
      "Iteration 204725 | Loss: 0.271704\n",
      "Iteration 204750 | Loss: 0.271695\n",
      "Iteration 204775 | Loss: 0.271686\n",
      "Iteration 204800 | Loss: 0.271677\n",
      "Iteration 204825 | Loss: 0.271668\n",
      "Iteration 204850 | Loss: 0.271659\n",
      "Iteration 204875 | Loss: 0.271649\n",
      "Iteration 204900 | Loss: 0.271640\n",
      "Iteration 204925 | Loss: 0.271631\n",
      "Iteration 204950 | Loss: 0.271622\n",
      "Iteration 204975 | Loss: 0.271613\n",
      "Iteration 205000 | Loss: 0.271604\n",
      "Iteration 205025 | Loss: 0.271595\n",
      "Iteration 205050 | Loss: 0.271586\n",
      "Iteration 205075 | Loss: 0.271577\n",
      "Iteration 205100 | Loss: 0.271568\n",
      "Iteration 205125 | Loss: 0.271559\n",
      "Iteration 205150 | Loss: 0.271550\n",
      "Iteration 205175 | Loss: 0.271541\n",
      "Iteration 205200 | Loss: 0.271532\n",
      "Iteration 205225 | Loss: 0.271523\n",
      "Iteration 205250 | Loss: 0.271514\n",
      "Iteration 205275 | Loss: 0.271505\n",
      "Iteration 205300 | Loss: 0.271496\n",
      "Iteration 205325 | Loss: 0.271487\n",
      "Iteration 205350 | Loss: 0.271478\n",
      "Iteration 205375 | Loss: 0.271469\n",
      "Iteration 205400 | Loss: 0.271460\n",
      "Iteration 205425 | Loss: 0.271451\n",
      "Iteration 205450 | Loss: 0.271442\n",
      "Iteration 205475 | Loss: 0.271433\n",
      "Iteration 205500 | Loss: 0.271425\n",
      "Iteration 205525 | Loss: 0.271416\n",
      "Iteration 205550 | Loss: 0.271407\n",
      "Iteration 205575 | Loss: 0.271398\n",
      "Iteration 205600 | Loss: 0.271389\n",
      "Iteration 205625 | Loss: 0.271380\n",
      "Iteration 205650 | Loss: 0.271371\n",
      "Iteration 205675 | Loss: 0.271363\n",
      "Iteration 205700 | Loss: 0.271354\n",
      "Iteration 205725 | Loss: 0.271345\n",
      "Iteration 205750 | Loss: 0.271336\n",
      "Iteration 205775 | Loss: 0.271327\n",
      "Iteration 205800 | Loss: 0.271319\n",
      "Iteration 205825 | Loss: 0.271310\n",
      "Iteration 205850 | Loss: 0.271301\n",
      "Iteration 205875 | Loss: 0.271292\n",
      "Iteration 205900 | Loss: 0.271284\n",
      "Iteration 205925 | Loss: 0.271275\n",
      "Iteration 205950 | Loss: 0.271266\n",
      "Iteration 205975 | Loss: 0.271258\n",
      "Iteration 206000 | Loss: 0.271249\n",
      "Iteration 206025 | Loss: 0.271240\n",
      "Iteration 206050 | Loss: 0.271231\n",
      "Iteration 206075 | Loss: 0.271223\n",
      "Iteration 206100 | Loss: 0.271214\n",
      "Iteration 206125 | Loss: 0.271205\n",
      "Iteration 206150 | Loss: 0.271197\n",
      "Iteration 206175 | Loss: 0.271188\n",
      "Iteration 206200 | Loss: 0.271180\n",
      "Iteration 206225 | Loss: 0.271171\n",
      "Iteration 206250 | Loss: 0.271162\n",
      "Iteration 206275 | Loss: 0.271154\n",
      "Iteration 206300 | Loss: 0.271145\n",
      "Iteration 206325 | Loss: 0.271137\n",
      "Iteration 206350 | Loss: 0.271128\n",
      "Iteration 206375 | Loss: 0.271119\n",
      "Iteration 206400 | Loss: 0.271111\n",
      "Iteration 206425 | Loss: 0.271102\n",
      "Iteration 206450 | Loss: 0.271094\n",
      "Iteration 206475 | Loss: 0.271085\n",
      "Iteration 206500 | Loss: 0.271077\n",
      "Iteration 206525 | Loss: 0.271068\n",
      "Iteration 206550 | Loss: 0.271060\n",
      "Iteration 206575 | Loss: 0.271051\n",
      "Iteration 206600 | Loss: 0.271043\n",
      "Iteration 206625 | Loss: 0.271034\n",
      "Iteration 206650 | Loss: 0.271026\n",
      "Iteration 206675 | Loss: 0.271017\n",
      "Iteration 206700 | Loss: 0.271009\n",
      "Iteration 206725 | Loss: 0.271000\n",
      "Iteration 206750 | Loss: 0.270992\n",
      "Iteration 206775 | Loss: 0.270984\n",
      "Iteration 206800 | Loss: 0.270975\n",
      "Iteration 206825 | Loss: 0.270967\n",
      "Iteration 206850 | Loss: 0.270958\n",
      "Iteration 206875 | Loss: 0.270950\n",
      "Iteration 206900 | Loss: 0.270942\n",
      "Iteration 206925 | Loss: 0.270933\n",
      "Iteration 206950 | Loss: 0.270925\n",
      "Iteration 206975 | Loss: 0.270916\n",
      "Iteration 207000 | Loss: 0.270908\n",
      "Iteration 207025 | Loss: 0.270900\n",
      "Iteration 207050 | Loss: 0.270891\n",
      "Iteration 207075 | Loss: 0.270883\n",
      "Iteration 207100 | Loss: 0.270875\n",
      "Iteration 207125 | Loss: 0.270866\n",
      "Iteration 207150 | Loss: 0.270858\n",
      "Iteration 207175 | Loss: 0.270850\n",
      "Iteration 207200 | Loss: 0.270842\n",
      "Iteration 207225 | Loss: 0.270833\n",
      "Iteration 207250 | Loss: 0.270825\n",
      "Iteration 207275 | Loss: 0.270817\n",
      "Iteration 207300 | Loss: 0.270809\n",
      "Iteration 207325 | Loss: 0.270800\n",
      "Iteration 207350 | Loss: 0.270792\n",
      "Iteration 207375 | Loss: 0.270784\n",
      "Iteration 207400 | Loss: 0.270776\n",
      "Iteration 207425 | Loss: 0.270767\n",
      "Iteration 207450 | Loss: 0.270759\n",
      "Iteration 207475 | Loss: 0.270751\n",
      "Iteration 207500 | Loss: 0.270743\n",
      "Iteration 207525 | Loss: 0.270735\n",
      "Iteration 207550 | Loss: 0.270727\n",
      "Iteration 207575 | Loss: 0.270718\n",
      "Iteration 207600 | Loss: 0.270710\n",
      "Iteration 207625 | Loss: 0.270702\n",
      "Iteration 207650 | Loss: 0.270694\n",
      "Iteration 207675 | Loss: 0.270686\n",
      "Iteration 207700 | Loss: 0.270678\n",
      "Iteration 207725 | Loss: 0.270670\n",
      "Iteration 207750 | Loss: 0.270662\n",
      "Iteration 207775 | Loss: 0.270654\n",
      "Iteration 207800 | Loss: 0.270645\n",
      "Iteration 207825 | Loss: 0.270637\n",
      "Iteration 207850 | Loss: 0.270629\n",
      "Iteration 207875 | Loss: 0.270621\n",
      "Iteration 207900 | Loss: 0.270613\n",
      "Iteration 207925 | Loss: 0.270605\n",
      "Iteration 207950 | Loss: 0.270597\n",
      "Iteration 207975 | Loss: 0.270589\n",
      "Iteration 208000 | Loss: 0.270581\n",
      "Iteration 208025 | Loss: 0.270573\n",
      "Iteration 208050 | Loss: 0.270565\n",
      "Iteration 208075 | Loss: 0.270557\n",
      "Iteration 208100 | Loss: 0.270549\n",
      "Iteration 208125 | Loss: 0.270541\n",
      "Iteration 208150 | Loss: 0.270533\n",
      "Iteration 208175 | Loss: 0.270525\n",
      "Iteration 208200 | Loss: 0.270517\n",
      "Iteration 208225 | Loss: 0.270509\n",
      "Iteration 208250 | Loss: 0.270502\n",
      "Iteration 208275 | Loss: 0.270494\n",
      "Iteration 208300 | Loss: 0.270486\n",
      "Iteration 208325 | Loss: 0.270478\n",
      "Iteration 208350 | Loss: 0.270470\n",
      "Iteration 208375 | Loss: 0.270462\n",
      "Iteration 208400 | Loss: 0.270454\n",
      "Iteration 208425 | Loss: 0.270446\n",
      "Iteration 208450 | Loss: 0.270438\n",
      "Iteration 208475 | Loss: 0.270431\n",
      "Iteration 208500 | Loss: 0.270423\n",
      "Iteration 208525 | Loss: 0.270415\n",
      "Iteration 208550 | Loss: 0.270407\n",
      "Iteration 208575 | Loss: 0.270399\n",
      "Iteration 208600 | Loss: 0.270391\n",
      "Iteration 208625 | Loss: 0.270384\n",
      "Iteration 208650 | Loss: 0.270376\n",
      "Iteration 208675 | Loss: 0.270368\n",
      "Iteration 208700 | Loss: 0.270360\n",
      "Iteration 208725 | Loss: 0.270353\n",
      "Iteration 208750 | Loss: 0.270345\n",
      "Iteration 208775 | Loss: 0.270337\n",
      "Iteration 208800 | Loss: 0.270329\n",
      "Iteration 208825 | Loss: 0.270322\n",
      "Iteration 208850 | Loss: 0.270314\n",
      "Iteration 208875 | Loss: 0.270306\n",
      "Iteration 208900 | Loss: 0.270298\n",
      "Iteration 208925 | Loss: 0.270291\n",
      "Iteration 208950 | Loss: 0.270283\n",
      "Iteration 208975 | Loss: 0.270275\n",
      "Iteration 209000 | Loss: 0.270268\n",
      "Iteration 209025 | Loss: 0.270260\n",
      "Iteration 209050 | Loss: 0.270252\n",
      "Iteration 209075 | Loss: 0.270245\n",
      "Iteration 209100 | Loss: 0.270237\n",
      "Iteration 209125 | Loss: 0.270229\n",
      "Iteration 209150 | Loss: 0.270222\n",
      "Iteration 209175 | Loss: 0.270214\n",
      "Iteration 209200 | Loss: 0.270206\n",
      "Iteration 209225 | Loss: 0.270199\n",
      "Iteration 209250 | Loss: 0.270191\n",
      "Iteration 209275 | Loss: 0.270184\n",
      "Iteration 209300 | Loss: 0.270176\n",
      "Iteration 209325 | Loss: 0.270168\n",
      "Iteration 209350 | Loss: 0.270161\n",
      "Iteration 209375 | Loss: 0.270153\n",
      "Iteration 209400 | Loss: 0.270146\n",
      "Iteration 209425 | Loss: 0.270138\n",
      "Iteration 209450 | Loss: 0.270131\n",
      "Iteration 209475 | Loss: 0.270123\n",
      "Iteration 209500 | Loss: 0.270116\n",
      "Iteration 209525 | Loss: 0.270108\n",
      "Iteration 209550 | Loss: 0.270101\n",
      "Iteration 209575 | Loss: 0.270093\n",
      "Iteration 209600 | Loss: 0.270086\n",
      "Iteration 209625 | Loss: 0.270078\n",
      "Iteration 209650 | Loss: 0.270071\n",
      "Iteration 209675 | Loss: 0.270063\n",
      "Iteration 209700 | Loss: 0.270056\n",
      "Iteration 209725 | Loss: 0.270048\n",
      "Iteration 209750 | Loss: 0.270041\n",
      "Iteration 209775 | Loss: 0.270033\n",
      "Iteration 209800 | Loss: 0.270026\n",
      "Iteration 209825 | Loss: 0.270018\n",
      "Iteration 209850 | Loss: 0.270011\n",
      "Iteration 209875 | Loss: 0.270004\n",
      "Iteration 209900 | Loss: 0.269996\n",
      "Iteration 209925 | Loss: 0.269989\n",
      "Iteration 209950 | Loss: 0.269981\n",
      "Iteration 209975 | Loss: 0.269974\n",
      "Iteration 210000 | Loss: 0.269967\n",
      "Iteration 210025 | Loss: 0.269959\n",
      "Iteration 210050 | Loss: 0.269952\n",
      "Iteration 210075 | Loss: 0.269945\n",
      "Iteration 210100 | Loss: 0.269937\n",
      "Iteration 210125 | Loss: 0.269930\n",
      "Iteration 210150 | Loss: 0.269923\n",
      "Iteration 210175 | Loss: 0.269915\n",
      "Iteration 210200 | Loss: 0.269908\n",
      "Iteration 210225 | Loss: 0.269901\n",
      "Iteration 210250 | Loss: 0.269893\n",
      "Iteration 210275 | Loss: 0.269886\n",
      "Iteration 210300 | Loss: 0.269879\n",
      "Iteration 210325 | Loss: 0.269872\n",
      "Iteration 210350 | Loss: 0.269864\n",
      "Iteration 210375 | Loss: 0.269857\n",
      "Iteration 210400 | Loss: 0.269850\n",
      "Iteration 210425 | Loss: 0.269843\n",
      "Iteration 210450 | Loss: 0.269835\n",
      "Iteration 210475 | Loss: 0.269828\n",
      "Iteration 210500 | Loss: 0.269821\n",
      "Iteration 210525 | Loss: 0.269814\n",
      "Iteration 210550 | Loss: 0.269807\n",
      "Iteration 210575 | Loss: 0.269799\n",
      "Iteration 210600 | Loss: 0.269792\n",
      "Iteration 210625 | Loss: 0.269785\n",
      "Iteration 210650 | Loss: 0.269778\n",
      "Iteration 210675 | Loss: 0.269771\n",
      "Iteration 210700 | Loss: 0.269763\n",
      "Iteration 210725 | Loss: 0.269756\n",
      "Iteration 210750 | Loss: 0.269749\n",
      "Iteration 210775 | Loss: 0.269742\n",
      "Iteration 210800 | Loss: 0.269735\n",
      "Iteration 210825 | Loss: 0.269728\n",
      "Iteration 210850 | Loss: 0.269721\n",
      "Iteration 210875 | Loss: 0.269714\n",
      "Iteration 210900 | Loss: 0.269706\n",
      "Iteration 210925 | Loss: 0.269699\n",
      "Iteration 210950 | Loss: 0.269692\n",
      "Iteration 210975 | Loss: 0.269685\n",
      "Iteration 211000 | Loss: 0.269678\n",
      "Iteration 211025 | Loss: 0.269671\n",
      "Iteration 211050 | Loss: 0.269664\n",
      "Iteration 211075 | Loss: 0.269657\n",
      "Iteration 211100 | Loss: 0.269650\n",
      "Iteration 211125 | Loss: 0.269643\n",
      "Iteration 211150 | Loss: 0.269636\n",
      "Iteration 211175 | Loss: 0.269629\n",
      "Iteration 211200 | Loss: 0.269622\n",
      "Iteration 211225 | Loss: 0.269615\n",
      "Iteration 211250 | Loss: 0.269608\n",
      "Iteration 211275 | Loss: 0.269601\n",
      "Iteration 211300 | Loss: 0.269594\n",
      "Iteration 211325 | Loss: 0.269587\n",
      "Iteration 211350 | Loss: 0.269580\n",
      "Iteration 211375 | Loss: 0.269573\n",
      "Iteration 211400 | Loss: 0.269566\n",
      "Iteration 211425 | Loss: 0.269559\n",
      "Iteration 211450 | Loss: 0.269552\n",
      "Iteration 211475 | Loss: 0.269545\n",
      "Iteration 211500 | Loss: 0.269538\n",
      "Iteration 211525 | Loss: 0.269531\n",
      "Iteration 211550 | Loss: 0.269524\n",
      "Iteration 211575 | Loss: 0.269518\n",
      "Iteration 211600 | Loss: 0.269511\n",
      "Iteration 211625 | Loss: 0.269504\n",
      "Iteration 211650 | Loss: 0.269497\n",
      "Iteration 211675 | Loss: 0.269490\n",
      "Iteration 211700 | Loss: 0.269483\n",
      "Iteration 211725 | Loss: 0.269476\n",
      "Iteration 211750 | Loss: 0.269469\n",
      "Iteration 211775 | Loss: 0.269463\n",
      "Iteration 211800 | Loss: 0.269456\n",
      "Iteration 211825 | Loss: 0.269449\n",
      "Iteration 211850 | Loss: 0.269442\n",
      "Iteration 211875 | Loss: 0.269435\n",
      "Iteration 211900 | Loss: 0.269428\n",
      "Iteration 211925 | Loss: 0.269422\n",
      "Iteration 211950 | Loss: 0.269415\n",
      "Iteration 211975 | Loss: 0.269408\n",
      "Iteration 212000 | Loss: 0.269401\n",
      "Iteration 212025 | Loss: 0.269394\n",
      "Iteration 212050 | Loss: 0.269388\n",
      "Iteration 212075 | Loss: 0.269381\n",
      "Iteration 212100 | Loss: 0.269374\n",
      "Iteration 212125 | Loss: 0.269367\n",
      "Iteration 212150 | Loss: 0.269361\n",
      "Iteration 212175 | Loss: 0.269354\n",
      "Iteration 212200 | Loss: 0.269347\n",
      "Iteration 212225 | Loss: 0.269341\n",
      "Iteration 212250 | Loss: 0.269334\n",
      "Iteration 212275 | Loss: 0.269327\n",
      "Iteration 212300 | Loss: 0.269320\n",
      "Iteration 212325 | Loss: 0.269314\n",
      "Iteration 212350 | Loss: 0.269307\n",
      "Iteration 212375 | Loss: 0.269300\n",
      "Iteration 212400 | Loss: 0.269294\n",
      "Iteration 212425 | Loss: 0.269287\n",
      "Iteration 212450 | Loss: 0.269280\n",
      "Iteration 212475 | Loss: 0.269274\n",
      "Iteration 212500 | Loss: 0.269267\n",
      "Iteration 212525 | Loss: 0.269260\n",
      "Iteration 212550 | Loss: 0.269254\n",
      "Iteration 212575 | Loss: 0.269247\n",
      "Iteration 212600 | Loss: 0.269241\n",
      "Iteration 212625 | Loss: 0.269234\n",
      "Iteration 212650 | Loss: 0.269227\n",
      "Iteration 212675 | Loss: 0.269221\n",
      "Iteration 212700 | Loss: 0.269214\n",
      "Iteration 212725 | Loss: 0.269208\n",
      "Iteration 212750 | Loss: 0.269201\n",
      "Iteration 212775 | Loss: 0.269194\n",
      "Iteration 212800 | Loss: 0.269188\n",
      "Iteration 212825 | Loss: 0.269181\n",
      "Iteration 212850 | Loss: 0.269175\n",
      "Iteration 212875 | Loss: 0.269168\n",
      "Iteration 212900 | Loss: 0.269162\n",
      "Iteration 212925 | Loss: 0.269155\n",
      "Iteration 212950 | Loss: 0.269149\n",
      "Iteration 212975 | Loss: 0.269142\n",
      "Iteration 213000 | Loss: 0.269136\n",
      "Iteration 213025 | Loss: 0.269129\n",
      "Iteration 213050 | Loss: 0.269123\n",
      "Iteration 213075 | Loss: 0.269116\n",
      "Iteration 213100 | Loss: 0.269110\n",
      "Iteration 213125 | Loss: 0.269103\n",
      "Iteration 213150 | Loss: 0.269097\n",
      "Iteration 213175 | Loss: 0.269090\n",
      "Iteration 213200 | Loss: 0.269084\n",
      "Iteration 213225 | Loss: 0.269077\n",
      "Iteration 213250 | Loss: 0.269071\n",
      "Iteration 213275 | Loss: 0.269065\n",
      "Iteration 213300 | Loss: 0.269058\n",
      "Iteration 213325 | Loss: 0.269052\n",
      "Iteration 213350 | Loss: 0.269045\n",
      "Iteration 213375 | Loss: 0.269039\n",
      "Iteration 213400 | Loss: 0.269032\n",
      "Iteration 213425 | Loss: 0.269026\n",
      "Iteration 213450 | Loss: 0.269020\n",
      "Iteration 213475 | Loss: 0.269013\n",
      "Iteration 213500 | Loss: 0.269007\n",
      "Iteration 213525 | Loss: 0.269001\n",
      "Iteration 213550 | Loss: 0.268994\n",
      "Iteration 213575 | Loss: 0.268988\n",
      "Iteration 213600 | Loss: 0.268982\n",
      "Iteration 213625 | Loss: 0.268975\n",
      "Iteration 213650 | Loss: 0.268969\n",
      "Iteration 213675 | Loss: 0.268963\n",
      "Iteration 213700 | Loss: 0.268956\n",
      "Iteration 213725 | Loss: 0.268950\n",
      "Iteration 213750 | Loss: 0.268944\n",
      "Iteration 213775 | Loss: 0.268937\n",
      "Iteration 213800 | Loss: 0.268931\n",
      "Iteration 213825 | Loss: 0.268925\n",
      "Iteration 213850 | Loss: 0.268918\n",
      "Iteration 213875 | Loss: 0.268912\n",
      "Iteration 213900 | Loss: 0.268906\n",
      "Iteration 213925 | Loss: 0.268900\n",
      "Iteration 213950 | Loss: 0.268893\n",
      "Iteration 213975 | Loss: 0.268887\n",
      "Iteration 214000 | Loss: 0.268881\n",
      "Iteration 214025 | Loss: 0.268875\n",
      "Iteration 214050 | Loss: 0.268868\n",
      "Iteration 214075 | Loss: 0.268862\n",
      "Iteration 214100 | Loss: 0.268856\n",
      "Iteration 214125 | Loss: 0.268850\n",
      "Iteration 214150 | Loss: 0.268844\n",
      "Iteration 214175 | Loss: 0.268837\n",
      "Iteration 214200 | Loss: 0.268831\n",
      "Iteration 214225 | Loss: 0.268825\n",
      "Iteration 214250 | Loss: 0.268819\n",
      "Iteration 214275 | Loss: 0.268813\n",
      "Iteration 214300 | Loss: 0.268806\n",
      "Iteration 214325 | Loss: 0.268800\n",
      "Iteration 214350 | Loss: 0.268794\n",
      "Iteration 214375 | Loss: 0.268788\n",
      "Iteration 214400 | Loss: 0.268782\n",
      "Iteration 214425 | Loss: 0.268776\n",
      "Iteration 214450 | Loss: 0.268770\n",
      "Iteration 214475 | Loss: 0.268763\n",
      "Iteration 214500 | Loss: 0.268757\n",
      "Iteration 214525 | Loss: 0.268751\n",
      "Iteration 214550 | Loss: 0.268745\n",
      "Iteration 214575 | Loss: 0.268739\n",
      "Iteration 214600 | Loss: 0.268733\n",
      "Iteration 214625 | Loss: 0.268727\n",
      "Iteration 214650 | Loss: 0.268721\n",
      "Iteration 214675 | Loss: 0.268715\n",
      "Iteration 214700 | Loss: 0.268709\n",
      "Iteration 214725 | Loss: 0.268703\n",
      "Iteration 214750 | Loss: 0.268697\n",
      "Iteration 214775 | Loss: 0.268691\n",
      "Iteration 214800 | Loss: 0.268685\n",
      "Iteration 214825 | Loss: 0.268678\n",
      "Iteration 214850 | Loss: 0.268672\n",
      "Iteration 214875 | Loss: 0.268666\n",
      "Iteration 214900 | Loss: 0.268660\n",
      "Iteration 214925 | Loss: 0.268654\n",
      "Iteration 214950 | Loss: 0.268648\n",
      "Iteration 214975 | Loss: 0.268642\n",
      "Iteration 215000 | Loss: 0.268636\n",
      "Iteration 215025 | Loss: 0.268630\n",
      "Iteration 215050 | Loss: 0.268624\n",
      "Iteration 215075 | Loss: 0.268618\n",
      "Iteration 215100 | Loss: 0.268613\n",
      "Iteration 215125 | Loss: 0.268607\n",
      "Iteration 215150 | Loss: 0.268601\n",
      "Iteration 215175 | Loss: 0.268595\n",
      "Iteration 215200 | Loss: 0.268589\n",
      "Iteration 215225 | Loss: 0.268583\n",
      "Iteration 215250 | Loss: 0.268577\n",
      "Iteration 215275 | Loss: 0.268571\n",
      "Iteration 215300 | Loss: 0.268565\n",
      "Iteration 215325 | Loss: 0.268559\n",
      "Iteration 215350 | Loss: 0.268553\n",
      "Iteration 215375 | Loss: 0.268547\n",
      "Iteration 215400 | Loss: 0.268541\n",
      "Iteration 215425 | Loss: 0.268536\n",
      "Iteration 215450 | Loss: 0.268530\n",
      "Iteration 215475 | Loss: 0.268524\n",
      "Iteration 215500 | Loss: 0.268518\n",
      "Iteration 215525 | Loss: 0.268512\n",
      "Iteration 215550 | Loss: 0.268506\n",
      "Iteration 215575 | Loss: 0.268500\n",
      "Iteration 215600 | Loss: 0.268495\n",
      "Iteration 215625 | Loss: 0.268489\n",
      "Iteration 215650 | Loss: 0.268483\n",
      "Iteration 215675 | Loss: 0.268477\n",
      "Iteration 215700 | Loss: 0.268471\n",
      "Iteration 215725 | Loss: 0.268465\n",
      "Iteration 215750 | Loss: 0.268460\n",
      "Iteration 215775 | Loss: 0.268454\n",
      "Iteration 215800 | Loss: 0.268448\n",
      "Iteration 215825 | Loss: 0.268442\n",
      "Iteration 215850 | Loss: 0.268436\n",
      "Iteration 215875 | Loss: 0.268431\n",
      "Iteration 215900 | Loss: 0.268425\n",
      "Iteration 215925 | Loss: 0.268419\n",
      "Iteration 215950 | Loss: 0.268413\n",
      "Iteration 215975 | Loss: 0.268408\n",
      "Iteration 216000 | Loss: 0.268402\n",
      "Iteration 216025 | Loss: 0.268396\n",
      "Iteration 216050 | Loss: 0.268390\n",
      "Iteration 216075 | Loss: 0.268385\n",
      "Iteration 216100 | Loss: 0.268379\n",
      "Iteration 216125 | Loss: 0.268373\n",
      "Iteration 216150 | Loss: 0.268368\n",
      "Iteration 216175 | Loss: 0.268362\n",
      "Iteration 216200 | Loss: 0.268356\n",
      "Iteration 216225 | Loss: 0.268350\n",
      "Iteration 216250 | Loss: 0.268345\n",
      "Iteration 216275 | Loss: 0.268339\n",
      "Iteration 216300 | Loss: 0.268333\n",
      "Iteration 216325 | Loss: 0.268328\n",
      "Iteration 216350 | Loss: 0.268322\n",
      "Iteration 216375 | Loss: 0.268316\n",
      "Iteration 216400 | Loss: 0.268311\n",
      "Iteration 216425 | Loss: 0.268305\n",
      "Iteration 216450 | Loss: 0.268300\n",
      "Iteration 216475 | Loss: 0.268294\n",
      "Iteration 216500 | Loss: 0.268288\n",
      "Iteration 216525 | Loss: 0.268283\n",
      "Iteration 216550 | Loss: 0.268277\n",
      "Iteration 216575 | Loss: 0.268271\n",
      "Iteration 216600 | Loss: 0.268266\n",
      "Iteration 216625 | Loss: 0.268260\n",
      "Iteration 216650 | Loss: 0.268255\n",
      "Iteration 216675 | Loss: 0.268249\n",
      "Iteration 216700 | Loss: 0.268243\n",
      "Iteration 216725 | Loss: 0.268238\n",
      "Iteration 216750 | Loss: 0.268232\n",
      "Iteration 216775 | Loss: 0.268227\n",
      "Iteration 216800 | Loss: 0.268221\n",
      "Iteration 216825 | Loss: 0.268216\n",
      "Iteration 216850 | Loss: 0.268210\n",
      "Iteration 216875 | Loss: 0.268205\n",
      "Iteration 216900 | Loss: 0.268199\n",
      "Iteration 216925 | Loss: 0.268194\n",
      "Iteration 216950 | Loss: 0.268188\n",
      "Iteration 216975 | Loss: 0.268182\n",
      "Iteration 217000 | Loss: 0.268177\n",
      "Iteration 217025 | Loss: 0.268171\n",
      "Iteration 217050 | Loss: 0.268166\n",
      "Iteration 217075 | Loss: 0.268160\n",
      "Iteration 217100 | Loss: 0.268155\n",
      "Iteration 217125 | Loss: 0.268149\n",
      "Iteration 217150 | Loss: 0.268144\n",
      "Iteration 217175 | Loss: 0.268139\n",
      "Iteration 217200 | Loss: 0.268133\n",
      "Iteration 217225 | Loss: 0.268128\n",
      "Iteration 217250 | Loss: 0.268122\n",
      "Iteration 217275 | Loss: 0.268117\n",
      "Iteration 217300 | Loss: 0.268111\n",
      "Iteration 217325 | Loss: 0.268106\n",
      "Iteration 217350 | Loss: 0.268100\n",
      "Iteration 217375 | Loss: 0.268095\n",
      "Iteration 217400 | Loss: 0.268090\n",
      "Iteration 217425 | Loss: 0.268084\n",
      "Iteration 217450 | Loss: 0.268079\n",
      "Iteration 217475 | Loss: 0.268073\n",
      "Iteration 217500 | Loss: 0.268068\n",
      "Iteration 217525 | Loss: 0.268063\n",
      "Iteration 217550 | Loss: 0.268057\n",
      "Iteration 217575 | Loss: 0.268052\n",
      "Iteration 217600 | Loss: 0.268046\n",
      "Iteration 217625 | Loss: 0.268041\n",
      "Iteration 217650 | Loss: 0.268036\n",
      "Iteration 217675 | Loss: 0.268030\n",
      "Iteration 217700 | Loss: 0.268025\n",
      "Iteration 217725 | Loss: 0.268020\n",
      "Iteration 217750 | Loss: 0.268014\n",
      "Iteration 217775 | Loss: 0.268009\n",
      "Iteration 217800 | Loss: 0.268004\n",
      "Iteration 217825 | Loss: 0.267998\n",
      "Iteration 217850 | Loss: 0.267993\n",
      "Iteration 217875 | Loss: 0.267988\n",
      "Iteration 217900 | Loss: 0.267982\n",
      "Iteration 217925 | Loss: 0.267977\n",
      "Iteration 217950 | Loss: 0.267972\n",
      "Iteration 217975 | Loss: 0.267966\n",
      "Iteration 218000 | Loss: 0.267961\n",
      "Iteration 218025 | Loss: 0.267956\n",
      "Iteration 218050 | Loss: 0.267951\n",
      "Iteration 218075 | Loss: 0.267945\n",
      "Iteration 218100 | Loss: 0.267940\n",
      "Iteration 218125 | Loss: 0.267935\n",
      "Iteration 218150 | Loss: 0.267930\n",
      "Iteration 218175 | Loss: 0.267924\n",
      "Iteration 218200 | Loss: 0.267919\n",
      "Iteration 218225 | Loss: 0.267914\n",
      "Iteration 218250 | Loss: 0.267909\n",
      "Iteration 218275 | Loss: 0.267903\n",
      "Iteration 218300 | Loss: 0.267898\n",
      "Iteration 218325 | Loss: 0.267893\n",
      "Iteration 218350 | Loss: 0.267888\n",
      "Iteration 218375 | Loss: 0.267882\n",
      "Iteration 218400 | Loss: 0.267877\n",
      "Iteration 218425 | Loss: 0.267872\n",
      "Iteration 218450 | Loss: 0.267867\n",
      "Iteration 218475 | Loss: 0.267862\n",
      "Iteration 218500 | Loss: 0.267857\n",
      "Iteration 218525 | Loss: 0.267851\n",
      "Iteration 218550 | Loss: 0.267846\n",
      "Iteration 218575 | Loss: 0.267841\n",
      "Iteration 218600 | Loss: 0.267836\n",
      "Iteration 218625 | Loss: 0.267831\n",
      "Iteration 218650 | Loss: 0.267826\n",
      "Iteration 218675 | Loss: 0.267820\n",
      "Iteration 218700 | Loss: 0.267815\n",
      "Iteration 218725 | Loss: 0.267810\n",
      "Iteration 218750 | Loss: 0.267805\n",
      "Iteration 218775 | Loss: 0.267800\n",
      "Iteration 218800 | Loss: 0.267795\n",
      "Iteration 218825 | Loss: 0.267790\n",
      "Iteration 218850 | Loss: 0.267785\n",
      "Iteration 218875 | Loss: 0.267779\n",
      "Iteration 218900 | Loss: 0.267774\n",
      "Iteration 218925 | Loss: 0.267769\n",
      "Iteration 218950 | Loss: 0.267764\n",
      "Iteration 218975 | Loss: 0.267759\n",
      "Iteration 219000 | Loss: 0.267754\n",
      "Iteration 219025 | Loss: 0.267749\n",
      "Iteration 219050 | Loss: 0.267744\n",
      "Iteration 219075 | Loss: 0.267739\n",
      "Iteration 219100 | Loss: 0.267734\n",
      "Iteration 219125 | Loss: 0.267729\n",
      "Iteration 219150 | Loss: 0.267724\n",
      "Iteration 219175 | Loss: 0.267719\n",
      "Iteration 219200 | Loss: 0.267714\n",
      "Iteration 219225 | Loss: 0.267709\n",
      "Iteration 219250 | Loss: 0.267704\n",
      "Iteration 219275 | Loss: 0.267699\n",
      "Iteration 219300 | Loss: 0.267694\n",
      "Iteration 219325 | Loss: 0.267689\n",
      "Iteration 219350 | Loss: 0.267684\n",
      "Iteration 219375 | Loss: 0.267679\n",
      "Iteration 219400 | Loss: 0.267674\n",
      "Iteration 219425 | Loss: 0.267669\n",
      "Iteration 219450 | Loss: 0.267664\n",
      "Iteration 219475 | Loss: 0.267659\n",
      "Iteration 219500 | Loss: 0.267654\n",
      "Iteration 219525 | Loss: 0.267649\n",
      "Iteration 219550 | Loss: 0.267644\n",
      "Iteration 219575 | Loss: 0.267639\n",
      "Iteration 219600 | Loss: 0.267634\n",
      "Iteration 219625 | Loss: 0.267629\n",
      "Iteration 219650 | Loss: 0.267624\n",
      "Iteration 219675 | Loss: 0.267619\n",
      "Iteration 219700 | Loss: 0.267614\n",
      "Iteration 219725 | Loss: 0.267609\n",
      "Iteration 219750 | Loss: 0.267604\n",
      "Iteration 219775 | Loss: 0.267599\n",
      "Iteration 219800 | Loss: 0.267595\n",
      "Iteration 219825 | Loss: 0.267590\n",
      "Iteration 219850 | Loss: 0.267585\n",
      "Iteration 219875 | Loss: 0.267580\n",
      "Iteration 219900 | Loss: 0.267575\n",
      "Iteration 219925 | Loss: 0.267570\n",
      "Iteration 219950 | Loss: 0.267565\n",
      "Iteration 219975 | Loss: 0.267560\n",
      "Iteration 220000 | Loss: 0.267555\n",
      "Iteration 220025 | Loss: 0.267551\n",
      "Iteration 220050 | Loss: 0.267546\n",
      "Iteration 220075 | Loss: 0.267541\n",
      "Iteration 220100 | Loss: 0.267536\n",
      "Iteration 220125 | Loss: 0.267531\n",
      "Iteration 220150 | Loss: 0.267526\n",
      "Iteration 220175 | Loss: 0.267522\n",
      "Iteration 220200 | Loss: 0.267517\n",
      "Iteration 220225 | Loss: 0.267512\n",
      "Iteration 220250 | Loss: 0.267507\n",
      "Iteration 220275 | Loss: 0.267502\n",
      "Iteration 220300 | Loss: 0.267497\n",
      "Iteration 220325 | Loss: 0.267493\n",
      "Iteration 220350 | Loss: 0.267488\n",
      "Iteration 220375 | Loss: 0.267483\n",
      "Iteration 220400 | Loss: 0.267478\n",
      "Iteration 220425 | Loss: 0.267473\n",
      "Iteration 220450 | Loss: 0.267469\n",
      "Iteration 220475 | Loss: 0.267464\n",
      "Iteration 220500 | Loss: 0.267459\n",
      "Iteration 220525 | Loss: 0.267454\n",
      "Iteration 220550 | Loss: 0.267450\n",
      "Iteration 220575 | Loss: 0.267445\n",
      "Iteration 220600 | Loss: 0.267440\n",
      "Iteration 220625 | Loss: 0.267435\n",
      "Iteration 220650 | Loss: 0.267431\n",
      "Iteration 220675 | Loss: 0.267426\n",
      "Iteration 220700 | Loss: 0.267421\n",
      "Iteration 220725 | Loss: 0.267417\n",
      "Iteration 220750 | Loss: 0.267412\n",
      "Iteration 220775 | Loss: 0.267407\n",
      "Iteration 220800 | Loss: 0.267402\n",
      "Iteration 220825 | Loss: 0.267398\n",
      "Iteration 220850 | Loss: 0.267393\n",
      "Iteration 220875 | Loss: 0.267388\n",
      "Iteration 220900 | Loss: 0.267384\n",
      "Iteration 220925 | Loss: 0.267379\n",
      "Iteration 220950 | Loss: 0.267374\n",
      "Iteration 220975 | Loss: 0.267370\n",
      "Iteration 221000 | Loss: 0.267365\n",
      "Iteration 221025 | Loss: 0.267360\n",
      "Iteration 221050 | Loss: 0.267356\n",
      "Iteration 221075 | Loss: 0.267351\n",
      "Iteration 221100 | Loss: 0.267346\n",
      "Iteration 221125 | Loss: 0.267342\n",
      "Iteration 221150 | Loss: 0.267337\n",
      "Iteration 221175 | Loss: 0.267332\n",
      "Iteration 221200 | Loss: 0.267328\n",
      "Iteration 221225 | Loss: 0.267323\n",
      "Iteration 221250 | Loss: 0.267318\n",
      "Iteration 221275 | Loss: 0.267314\n",
      "Iteration 221300 | Loss: 0.267309\n",
      "Iteration 221325 | Loss: 0.267305\n",
      "Iteration 221350 | Loss: 0.267300\n",
      "Iteration 221375 | Loss: 0.267295\n",
      "Iteration 221400 | Loss: 0.267291\n",
      "Iteration 221425 | Loss: 0.267286\n",
      "Iteration 221450 | Loss: 0.267282\n",
      "Iteration 221475 | Loss: 0.267277\n",
      "Iteration 221500 | Loss: 0.267273\n",
      "Iteration 221525 | Loss: 0.267268\n",
      "Iteration 221550 | Loss: 0.267263\n",
      "Iteration 221575 | Loss: 0.267259\n",
      "Iteration 221600 | Loss: 0.267254\n",
      "Iteration 221625 | Loss: 0.267250\n",
      "Iteration 221650 | Loss: 0.267245\n",
      "Iteration 221675 | Loss: 0.267241\n",
      "Iteration 221700 | Loss: 0.267236\n",
      "Iteration 221725 | Loss: 0.267232\n",
      "Iteration 221750 | Loss: 0.267227\n",
      "Iteration 221775 | Loss: 0.267223\n",
      "Iteration 221800 | Loss: 0.267218\n",
      "Iteration 221825 | Loss: 0.267214\n",
      "Iteration 221850 | Loss: 0.267209\n",
      "Iteration 221875 | Loss: 0.267205\n",
      "Iteration 221900 | Loss: 0.267200\n",
      "Iteration 221925 | Loss: 0.267196\n",
      "Iteration 221950 | Loss: 0.267191\n",
      "Iteration 221975 | Loss: 0.267187\n",
      "Iteration 222000 | Loss: 0.267182\n",
      "Iteration 222025 | Loss: 0.267178\n",
      "Iteration 222050 | Loss: 0.267173\n",
      "Iteration 222075 | Loss: 0.267169\n",
      "Iteration 222100 | Loss: 0.267164\n",
      "Iteration 222125 | Loss: 0.267160\n",
      "Iteration 222150 | Loss: 0.267155\n",
      "Iteration 222175 | Loss: 0.267151\n",
      "Iteration 222200 | Loss: 0.267146\n",
      "Iteration 222225 | Loss: 0.267142\n",
      "Iteration 222250 | Loss: 0.267138\n",
      "Iteration 222275 | Loss: 0.267133\n",
      "Iteration 222300 | Loss: 0.267129\n",
      "Iteration 222325 | Loss: 0.267124\n",
      "Iteration 222350 | Loss: 0.267120\n",
      "Iteration 222375 | Loss: 0.267115\n",
      "Iteration 222400 | Loss: 0.267111\n",
      "Iteration 222425 | Loss: 0.267107\n",
      "Iteration 222450 | Loss: 0.267102\n",
      "Iteration 222475 | Loss: 0.267098\n",
      "Iteration 222500 | Loss: 0.267093\n",
      "Iteration 222525 | Loss: 0.267089\n",
      "Iteration 222550 | Loss: 0.267085\n",
      "Iteration 222575 | Loss: 0.267080\n",
      "Iteration 222600 | Loss: 0.267076\n",
      "Iteration 222625 | Loss: 0.267072\n",
      "Iteration 222650 | Loss: 0.267067\n",
      "Iteration 222675 | Loss: 0.267063\n",
      "Iteration 222700 | Loss: 0.267059\n",
      "Iteration 222725 | Loss: 0.267054\n",
      "Iteration 222750 | Loss: 0.267050\n",
      "Iteration 222775 | Loss: 0.267046\n",
      "Iteration 222800 | Loss: 0.267041\n",
      "Iteration 222825 | Loss: 0.267037\n",
      "Iteration 222850 | Loss: 0.267033\n",
      "Iteration 222875 | Loss: 0.267028\n",
      "Iteration 222900 | Loss: 0.267024\n",
      "Iteration 222925 | Loss: 0.267020\n",
      "Iteration 222950 | Loss: 0.267015\n",
      "Iteration 222975 | Loss: 0.267011\n",
      "Iteration 223000 | Loss: 0.267007\n",
      "Iteration 223025 | Loss: 0.267002\n",
      "Iteration 223050 | Loss: 0.266998\n",
      "Iteration 223075 | Loss: 0.266994\n",
      "Iteration 223100 | Loss: 0.266990\n",
      "Iteration 223125 | Loss: 0.266985\n",
      "Iteration 223150 | Loss: 0.266981\n",
      "Iteration 223175 | Loss: 0.266977\n",
      "Iteration 223200 | Loss: 0.266972\n",
      "Iteration 223225 | Loss: 0.266968\n",
      "Iteration 223250 | Loss: 0.266964\n",
      "Iteration 223275 | Loss: 0.266960\n",
      "Iteration 223300 | Loss: 0.266955\n",
      "Iteration 223325 | Loss: 0.266951\n",
      "Iteration 223350 | Loss: 0.266947\n",
      "Iteration 223375 | Loss: 0.266943\n",
      "Iteration 223400 | Loss: 0.266939\n",
      "Iteration 223425 | Loss: 0.266934\n",
      "Iteration 223450 | Loss: 0.266930\n",
      "Iteration 223475 | Loss: 0.266926\n",
      "Iteration 223500 | Loss: 0.266922\n",
      "Iteration 223525 | Loss: 0.266917\n",
      "Iteration 223550 | Loss: 0.266913\n",
      "Iteration 223575 | Loss: 0.266909\n",
      "Iteration 223600 | Loss: 0.266905\n",
      "Iteration 223625 | Loss: 0.266901\n",
      "Iteration 223650 | Loss: 0.266897\n",
      "Iteration 223675 | Loss: 0.266892\n",
      "Iteration 223700 | Loss: 0.266888\n",
      "Iteration 223725 | Loss: 0.266884\n",
      "Iteration 223750 | Loss: 0.266880\n",
      "Iteration 223775 | Loss: 0.266876\n",
      "Iteration 223800 | Loss: 0.266872\n",
      "Iteration 223825 | Loss: 0.266867\n",
      "Iteration 223850 | Loss: 0.266863\n",
      "Iteration 223875 | Loss: 0.266859\n",
      "Iteration 223900 | Loss: 0.266855\n",
      "Iteration 223925 | Loss: 0.266851\n",
      "Iteration 223950 | Loss: 0.266847\n",
      "Iteration 223975 | Loss: 0.266843\n",
      "Iteration 224000 | Loss: 0.266838\n",
      "Iteration 224025 | Loss: 0.266834\n",
      "Iteration 224050 | Loss: 0.266830\n",
      "Iteration 224075 | Loss: 0.266826\n",
      "Iteration 224100 | Loss: 0.266822\n",
      "Iteration 224125 | Loss: 0.266818\n",
      "Iteration 224150 | Loss: 0.266814\n",
      "Iteration 224175 | Loss: 0.266810\n",
      "Iteration 224200 | Loss: 0.266806\n",
      "Iteration 224225 | Loss: 0.266802\n",
      "Iteration 224250 | Loss: 0.266797\n",
      "Iteration 224275 | Loss: 0.266793\n",
      "Iteration 224300 | Loss: 0.266789\n",
      "Iteration 224325 | Loss: 0.266785\n",
      "Iteration 224350 | Loss: 0.266781\n",
      "Iteration 224375 | Loss: 0.266777\n",
      "Iteration 224400 | Loss: 0.266773\n",
      "Iteration 224425 | Loss: 0.266769\n",
      "Iteration 224450 | Loss: 0.266765\n",
      "Iteration 224475 | Loss: 0.266761\n",
      "Iteration 224500 | Loss: 0.266757\n",
      "Iteration 224525 | Loss: 0.266753\n",
      "Iteration 224550 | Loss: 0.266749\n",
      "Iteration 224575 | Loss: 0.266745\n",
      "Iteration 224600 | Loss: 0.266741\n",
      "Iteration 224625 | Loss: 0.266737\n",
      "Iteration 224650 | Loss: 0.266733\n",
      "Iteration 224675 | Loss: 0.266729\n",
      "Iteration 224700 | Loss: 0.266725\n",
      "Iteration 224725 | Loss: 0.266721\n",
      "Iteration 224750 | Loss: 0.266717\n",
      "Iteration 224775 | Loss: 0.266713\n",
      "Iteration 224800 | Loss: 0.266709\n",
      "Iteration 224825 | Loss: 0.266705\n",
      "Iteration 224850 | Loss: 0.266701\n",
      "Iteration 224875 | Loss: 0.266697\n",
      "Iteration 224900 | Loss: 0.266693\n",
      "Iteration 224925 | Loss: 0.266689\n",
      "Iteration 224950 | Loss: 0.266685\n",
      "Iteration 224975 | Loss: 0.266681\n",
      "Iteration 225000 | Loss: 0.266677\n",
      "Iteration 225025 | Loss: 0.266673\n",
      "Iteration 225050 | Loss: 0.266669\n",
      "Iteration 225075 | Loss: 0.266665\n",
      "Iteration 225100 | Loss: 0.266661\n",
      "Iteration 225125 | Loss: 0.266657\n",
      "Iteration 225150 | Loss: 0.266653\n",
      "Iteration 225175 | Loss: 0.266649\n",
      "Iteration 225200 | Loss: 0.266646\n",
      "Iteration 225225 | Loss: 0.266642\n",
      "Iteration 225250 | Loss: 0.266638\n",
      "Iteration 225275 | Loss: 0.266634\n",
      "Iteration 225300 | Loss: 0.266630\n",
      "Iteration 225325 | Loss: 0.266626\n",
      "Iteration 225350 | Loss: 0.266622\n",
      "Iteration 225375 | Loss: 0.266618\n",
      "Iteration 225400 | Loss: 0.266614\n",
      "Iteration 225425 | Loss: 0.266610\n",
      "Iteration 225450 | Loss: 0.266607\n",
      "Iteration 225475 | Loss: 0.266603\n",
      "Iteration 225500 | Loss: 0.266599\n",
      "Iteration 225525 | Loss: 0.266595\n",
      "Iteration 225550 | Loss: 0.266591\n",
      "Iteration 225575 | Loss: 0.266587\n",
      "Iteration 225600 | Loss: 0.266583\n",
      "Iteration 225625 | Loss: 0.266579\n",
      "Iteration 225650 | Loss: 0.266576\n",
      "Iteration 225675 | Loss: 0.266572\n",
      "Iteration 225700 | Loss: 0.266568\n",
      "Iteration 225725 | Loss: 0.266564\n",
      "Iteration 225750 | Loss: 0.266560\n",
      "Iteration 225775 | Loss: 0.266556\n",
      "Iteration 225800 | Loss: 0.266553\n",
      "Iteration 225825 | Loss: 0.266549\n",
      "Iteration 225850 | Loss: 0.266545\n",
      "Iteration 225875 | Loss: 0.266541\n",
      "Iteration 225900 | Loss: 0.266537\n",
      "Iteration 225925 | Loss: 0.266534\n",
      "Iteration 225950 | Loss: 0.266530\n",
      "Iteration 225975 | Loss: 0.266526\n",
      "Iteration 226000 | Loss: 0.266522\n",
      "Iteration 226025 | Loss: 0.266518\n",
      "Iteration 226050 | Loss: 0.266515\n",
      "Iteration 226075 | Loss: 0.266511\n",
      "Iteration 226100 | Loss: 0.266507\n",
      "Iteration 226125 | Loss: 0.266503\n",
      "Iteration 226150 | Loss: 0.266499\n",
      "Iteration 226175 | Loss: 0.266496\n",
      "Iteration 226200 | Loss: 0.266492\n",
      "Iteration 226225 | Loss: 0.266488\n",
      "Iteration 226250 | Loss: 0.266484\n",
      "Iteration 226275 | Loss: 0.266481\n",
      "Iteration 226300 | Loss: 0.266477\n",
      "Iteration 226325 | Loss: 0.266473\n",
      "Iteration 226350 | Loss: 0.266469\n",
      "Iteration 226375 | Loss: 0.266466\n",
      "Iteration 226400 | Loss: 0.266462\n",
      "Iteration 226425 | Loss: 0.266458\n",
      "Iteration 226450 | Loss: 0.266455\n",
      "Iteration 226475 | Loss: 0.266451\n",
      "Iteration 226500 | Loss: 0.266447\n",
      "Iteration 226525 | Loss: 0.266443\n",
      "Iteration 226550 | Loss: 0.266440\n",
      "Iteration 226575 | Loss: 0.266436\n",
      "Iteration 226600 | Loss: 0.266432\n",
      "Iteration 226625 | Loss: 0.266429\n",
      "Iteration 226650 | Loss: 0.266425\n",
      "Iteration 226675 | Loss: 0.266421\n",
      "Iteration 226700 | Loss: 0.266417\n",
      "Iteration 226725 | Loss: 0.266414\n",
      "Iteration 226750 | Loss: 0.266410\n",
      "Iteration 226775 | Loss: 0.266406\n",
      "Iteration 226800 | Loss: 0.266403\n",
      "Iteration 226825 | Loss: 0.266399\n",
      "Iteration 226850 | Loss: 0.266395\n",
      "Iteration 226875 | Loss: 0.266392\n",
      "Iteration 226900 | Loss: 0.266388\n",
      "Iteration 226925 | Loss: 0.266385\n",
      "Iteration 226950 | Loss: 0.266381\n",
      "Iteration 226975 | Loss: 0.266377\n",
      "Iteration 227000 | Loss: 0.266374\n",
      "Iteration 227025 | Loss: 0.266370\n",
      "Iteration 227050 | Loss: 0.266366\n",
      "Iteration 227075 | Loss: 0.266363\n",
      "Iteration 227100 | Loss: 0.266359\n",
      "Iteration 227125 | Loss: 0.266355\n",
      "Iteration 227150 | Loss: 0.266352\n",
      "Iteration 227175 | Loss: 0.266348\n",
      "Iteration 227200 | Loss: 0.266345\n",
      "Iteration 227225 | Loss: 0.266341\n",
      "Iteration 227250 | Loss: 0.266337\n",
      "Iteration 227275 | Loss: 0.266334\n",
      "Iteration 227300 | Loss: 0.266330\n",
      "Iteration 227325 | Loss: 0.266327\n",
      "Iteration 227350 | Loss: 0.266323\n",
      "Iteration 227375 | Loss: 0.266319\n",
      "Iteration 227400 | Loss: 0.266316\n",
      "Iteration 227425 | Loss: 0.266312\n",
      "Iteration 227450 | Loss: 0.266309\n",
      "Iteration 227475 | Loss: 0.266305\n",
      "Iteration 227500 | Loss: 0.266302\n",
      "Iteration 227525 | Loss: 0.266298\n",
      "Iteration 227550 | Loss: 0.266294\n",
      "Iteration 227575 | Loss: 0.266291\n",
      "Iteration 227600 | Loss: 0.266287\n",
      "Iteration 227625 | Loss: 0.266284\n",
      "Iteration 227650 | Loss: 0.266280\n",
      "Iteration 227675 | Loss: 0.266277\n",
      "Iteration 227700 | Loss: 0.266273\n",
      "Iteration 227725 | Loss: 0.266270\n",
      "Iteration 227750 | Loss: 0.266266\n",
      "Iteration 227775 | Loss: 0.266263\n",
      "Iteration 227800 | Loss: 0.266259\n",
      "Iteration 227825 | Loss: 0.266256\n",
      "Iteration 227850 | Loss: 0.266252\n",
      "Iteration 227875 | Loss: 0.266249\n",
      "Iteration 227900 | Loss: 0.266245\n",
      "Iteration 227925 | Loss: 0.266242\n",
      "Iteration 227950 | Loss: 0.266238\n",
      "Iteration 227975 | Loss: 0.266235\n",
      "Iteration 228000 | Loss: 0.266231\n",
      "Iteration 228025 | Loss: 0.266228\n",
      "Iteration 228050 | Loss: 0.266224\n",
      "Iteration 228075 | Loss: 0.266221\n",
      "Iteration 228100 | Loss: 0.266217\n",
      "Iteration 228125 | Loss: 0.266214\n",
      "Iteration 228150 | Loss: 0.266210\n",
      "Iteration 228175 | Loss: 0.266207\n",
      "Iteration 228200 | Loss: 0.266203\n",
      "Iteration 228225 | Loss: 0.266200\n",
      "Iteration 228250 | Loss: 0.266196\n",
      "Iteration 228275 | Loss: 0.266193\n",
      "Iteration 228300 | Loss: 0.266189\n",
      "Iteration 228325 | Loss: 0.266186\n",
      "Iteration 228350 | Loss: 0.266183\n",
      "Iteration 228375 | Loss: 0.266179\n",
      "Iteration 228400 | Loss: 0.266176\n",
      "Iteration 228425 | Loss: 0.266172\n",
      "Iteration 228450 | Loss: 0.266169\n",
      "Iteration 228475 | Loss: 0.266165\n",
      "Iteration 228500 | Loss: 0.266162\n",
      "Iteration 228525 | Loss: 0.266159\n",
      "Iteration 228550 | Loss: 0.266155\n",
      "Iteration 228575 | Loss: 0.266152\n",
      "Iteration 228600 | Loss: 0.266148\n",
      "Iteration 228625 | Loss: 0.266145\n",
      "Iteration 228650 | Loss: 0.266141\n",
      "Iteration 228675 | Loss: 0.266138\n",
      "Iteration 228700 | Loss: 0.266135\n",
      "Iteration 228725 | Loss: 0.266131\n",
      "Iteration 228750 | Loss: 0.266128\n",
      "Iteration 228775 | Loss: 0.266125\n",
      "Iteration 228800 | Loss: 0.266121\n",
      "Iteration 228825 | Loss: 0.266118\n",
      "Iteration 228850 | Loss: 0.266114\n",
      "Iteration 228875 | Loss: 0.266111\n",
      "Iteration 228900 | Loss: 0.266108\n",
      "Iteration 228925 | Loss: 0.266104\n",
      "Iteration 228950 | Loss: 0.266101\n",
      "Iteration 228975 | Loss: 0.266098\n",
      "Iteration 229000 | Loss: 0.266094\n",
      "Iteration 229025 | Loss: 0.266091\n",
      "Iteration 229050 | Loss: 0.266088\n",
      "Iteration 229075 | Loss: 0.266084\n",
      "Iteration 229100 | Loss: 0.266081\n",
      "Iteration 229125 | Loss: 0.266078\n",
      "Iteration 229150 | Loss: 0.266074\n",
      "Iteration 229175 | Loss: 0.266071\n",
      "Iteration 229200 | Loss: 0.266068\n",
      "Iteration 229225 | Loss: 0.266064\n",
      "Iteration 229250 | Loss: 0.266061\n",
      "Iteration 229275 | Loss: 0.266058\n",
      "Iteration 229300 | Loss: 0.266054\n",
      "Iteration 229325 | Loss: 0.266051\n",
      "Iteration 229350 | Loss: 0.266048\n",
      "Iteration 229375 | Loss: 0.266044\n",
      "Iteration 229400 | Loss: 0.266041\n",
      "Iteration 229425 | Loss: 0.266038\n",
      "Iteration 229450 | Loss: 0.266035\n",
      "Iteration 229475 | Loss: 0.266031\n",
      "Iteration 229500 | Loss: 0.266028\n",
      "Iteration 229525 | Loss: 0.266025\n",
      "Iteration 229550 | Loss: 0.266021\n",
      "Iteration 229575 | Loss: 0.266018\n",
      "Iteration 229600 | Loss: 0.266015\n",
      "Iteration 229625 | Loss: 0.266012\n",
      "Iteration 229650 | Loss: 0.266008\n",
      "Iteration 229675 | Loss: 0.266005\n",
      "Iteration 229700 | Loss: 0.266002\n",
      "Iteration 229725 | Loss: 0.265999\n",
      "Iteration 229750 | Loss: 0.265995\n",
      "Iteration 229775 | Loss: 0.265992\n",
      "Iteration 229800 | Loss: 0.265989\n",
      "Iteration 229825 | Loss: 0.265986\n",
      "Iteration 229850 | Loss: 0.265982\n",
      "Iteration 229875 | Loss: 0.265979\n",
      "Iteration 229900 | Loss: 0.265976\n",
      "Iteration 229925 | Loss: 0.265973\n",
      "Iteration 229950 | Loss: 0.265970\n",
      "Iteration 229975 | Loss: 0.265966\n",
      "Iteration 230000 | Loss: 0.265963\n",
      "Iteration 230025 | Loss: 0.265960\n",
      "Iteration 230050 | Loss: 0.265957\n",
      "Iteration 230075 | Loss: 0.265953\n",
      "Iteration 230100 | Loss: 0.265950\n",
      "Iteration 230125 | Loss: 0.265947\n",
      "Iteration 230150 | Loss: 0.265944\n",
      "Iteration 230175 | Loss: 0.265941\n",
      "Iteration 230200 | Loss: 0.265937\n",
      "Iteration 230225 | Loss: 0.265934\n",
      "Iteration 230250 | Loss: 0.265931\n",
      "Iteration 230275 | Loss: 0.265928\n",
      "Iteration 230300 | Loss: 0.265925\n",
      "Iteration 230325 | Loss: 0.265922\n",
      "Iteration 230350 | Loss: 0.265918\n",
      "Iteration 230375 | Loss: 0.265915\n",
      "Iteration 230400 | Loss: 0.265912\n",
      "Iteration 230425 | Loss: 0.265909\n",
      "Iteration 230450 | Loss: 0.265906\n",
      "Iteration 230475 | Loss: 0.265903\n",
      "Iteration 230500 | Loss: 0.265899\n",
      "Iteration 230525 | Loss: 0.265896\n",
      "Iteration 230550 | Loss: 0.265893\n",
      "Iteration 230575 | Loss: 0.265890\n",
      "Iteration 230600 | Loss: 0.265887\n",
      "Iteration 230625 | Loss: 0.265884\n",
      "Iteration 230650 | Loss: 0.265881\n",
      "Iteration 230675 | Loss: 0.265878\n",
      "Iteration 230700 | Loss: 0.265874\n",
      "Iteration 230725 | Loss: 0.265871\n",
      "Iteration 230750 | Loss: 0.265868\n",
      "Iteration 230775 | Loss: 0.265865\n",
      "Iteration 230800 | Loss: 0.265862\n",
      "Iteration 230825 | Loss: 0.265859\n",
      "Iteration 230850 | Loss: 0.265856\n",
      "Iteration 230875 | Loss: 0.265853\n",
      "Iteration 230900 | Loss: 0.265850\n",
      "Iteration 230925 | Loss: 0.265846\n",
      "Iteration 230950 | Loss: 0.265843\n",
      "Iteration 230975 | Loss: 0.265840\n",
      "Iteration 231000 | Loss: 0.265837\n",
      "Iteration 231025 | Loss: 0.265834\n",
      "Iteration 231050 | Loss: 0.265831\n",
      "Iteration 231075 | Loss: 0.265828\n",
      "Iteration 231100 | Loss: 0.265825\n",
      "Iteration 231125 | Loss: 0.265822\n",
      "Iteration 231150 | Loss: 0.265819\n",
      "Iteration 231175 | Loss: 0.265816\n",
      "Iteration 231200 | Loss: 0.265813\n",
      "Iteration 231225 | Loss: 0.265810\n",
      "Iteration 231250 | Loss: 0.265807\n",
      "Iteration 231275 | Loss: 0.265804\n",
      "Iteration 231300 | Loss: 0.265800\n",
      "Iteration 231325 | Loss: 0.265797\n",
      "Iteration 231350 | Loss: 0.265794\n",
      "Iteration 231375 | Loss: 0.265791\n",
      "Iteration 231400 | Loss: 0.265788\n",
      "Iteration 231425 | Loss: 0.265785\n",
      "Iteration 231450 | Loss: 0.265782\n",
      "Iteration 231475 | Loss: 0.265779\n",
      "Iteration 231500 | Loss: 0.265776\n",
      "Iteration 231525 | Loss: 0.265773\n",
      "Iteration 231550 | Loss: 0.265770\n",
      "Iteration 231575 | Loss: 0.265767\n",
      "Iteration 231600 | Loss: 0.265764\n",
      "Iteration 231625 | Loss: 0.265761\n",
      "Iteration 231650 | Loss: 0.265758\n",
      "Iteration 231675 | Loss: 0.265755\n",
      "Iteration 231700 | Loss: 0.265752\n",
      "Iteration 231725 | Loss: 0.265749\n",
      "Iteration 231750 | Loss: 0.265746\n",
      "Iteration 231775 | Loss: 0.265743\n",
      "Iteration 231800 | Loss: 0.265740\n",
      "Iteration 231825 | Loss: 0.265737\n",
      "Iteration 231850 | Loss: 0.265734\n",
      "Iteration 231875 | Loss: 0.265731\n",
      "Iteration 231900 | Loss: 0.265728\n",
      "Iteration 231925 | Loss: 0.265725\n",
      "Iteration 231950 | Loss: 0.265722\n",
      "Iteration 231975 | Loss: 0.265719\n",
      "Iteration 232000 | Loss: 0.265716\n",
      "Iteration 232025 | Loss: 0.265713\n",
      "Iteration 232050 | Loss: 0.265711\n",
      "Iteration 232075 | Loss: 0.265708\n",
      "Iteration 232100 | Loss: 0.265705\n",
      "Iteration 232125 | Loss: 0.265702\n",
      "Iteration 232150 | Loss: 0.265699\n",
      "Iteration 232175 | Loss: 0.265696\n",
      "Iteration 232200 | Loss: 0.265693\n",
      "Iteration 232225 | Loss: 0.265690\n",
      "Iteration 232250 | Loss: 0.265687\n",
      "Iteration 232275 | Loss: 0.265684\n",
      "Iteration 232300 | Loss: 0.265681\n",
      "Iteration 232325 | Loss: 0.265678\n",
      "Iteration 232350 | Loss: 0.265675\n",
      "Iteration 232375 | Loss: 0.265672\n",
      "Iteration 232400 | Loss: 0.265670\n",
      "Iteration 232425 | Loss: 0.265667\n",
      "Iteration 232450 | Loss: 0.265664\n",
      "Iteration 232475 | Loss: 0.265661\n",
      "Iteration 232500 | Loss: 0.265658\n",
      "Iteration 232525 | Loss: 0.265655\n",
      "Iteration 232550 | Loss: 0.265652\n",
      "Iteration 232575 | Loss: 0.265649\n",
      "Iteration 232600 | Loss: 0.265646\n",
      "Iteration 232625 | Loss: 0.265643\n",
      "Iteration 232650 | Loss: 0.265641\n",
      "Iteration 232675 | Loss: 0.265638\n",
      "Iteration 232700 | Loss: 0.265635\n",
      "Iteration 232725 | Loss: 0.265632\n",
      "Iteration 232750 | Loss: 0.265629\n",
      "Iteration 232775 | Loss: 0.265626\n",
      "Iteration 232800 | Loss: 0.265623\n",
      "Iteration 232825 | Loss: 0.265621\n",
      "Iteration 232850 | Loss: 0.265618\n",
      "Iteration 232875 | Loss: 0.265615\n",
      "Iteration 232900 | Loss: 0.265612\n",
      "Iteration 232925 | Loss: 0.265609\n",
      "Iteration 232950 | Loss: 0.265606\n",
      "Iteration 232975 | Loss: 0.265603\n",
      "Iteration 233000 | Loss: 0.265601\n",
      "Iteration 233025 | Loss: 0.265598\n",
      "Iteration 233050 | Loss: 0.265595\n",
      "Iteration 233075 | Loss: 0.265592\n",
      "Iteration 233100 | Loss: 0.265589\n",
      "Iteration 233125 | Loss: 0.265586\n",
      "Iteration 233150 | Loss: 0.265584\n",
      "Iteration 233175 | Loss: 0.265581\n",
      "Iteration 233200 | Loss: 0.265578\n",
      "Iteration 233225 | Loss: 0.265575\n",
      "Iteration 233250 | Loss: 0.265572\n",
      "Iteration 233275 | Loss: 0.265570\n",
      "Iteration 233300 | Loss: 0.265567\n",
      "Iteration 233325 | Loss: 0.265564\n",
      "Iteration 233350 | Loss: 0.265561\n",
      "Iteration 233375 | Loss: 0.265558\n",
      "Iteration 233400 | Loss: 0.265556\n",
      "Iteration 233425 | Loss: 0.265553\n",
      "Iteration 233450 | Loss: 0.265550\n",
      "Iteration 233475 | Loss: 0.265547\n",
      "Iteration 233500 | Loss: 0.265544\n",
      "Iteration 233525 | Loss: 0.265542\n",
      "Iteration 233550 | Loss: 0.265539\n",
      "Iteration 233575 | Loss: 0.265536\n",
      "Iteration 233600 | Loss: 0.265533\n",
      "Iteration 233625 | Loss: 0.265531\n",
      "Iteration 233650 | Loss: 0.265528\n",
      "Iteration 233675 | Loss: 0.265525\n",
      "Iteration 233700 | Loss: 0.265522\n",
      "Iteration 233725 | Loss: 0.265520\n",
      "Iteration 233750 | Loss: 0.265517\n",
      "Iteration 233775 | Loss: 0.265514\n",
      "Iteration 233800 | Loss: 0.265511\n",
      "Iteration 233825 | Loss: 0.265509\n",
      "Iteration 233850 | Loss: 0.265506\n",
      "Iteration 233875 | Loss: 0.265503\n",
      "Iteration 233900 | Loss: 0.265500\n",
      "Iteration 233925 | Loss: 0.265498\n",
      "Iteration 233950 | Loss: 0.265495\n",
      "Iteration 233975 | Loss: 0.265492\n",
      "Iteration 234000 | Loss: 0.265489\n",
      "Iteration 234025 | Loss: 0.265487\n",
      "Iteration 234050 | Loss: 0.265484\n",
      "Iteration 234075 | Loss: 0.265481\n",
      "Iteration 234100 | Loss: 0.265479\n",
      "Iteration 234125 | Loss: 0.265476\n",
      "Iteration 234150 | Loss: 0.265473\n",
      "Iteration 234175 | Loss: 0.265470\n",
      "Iteration 234200 | Loss: 0.265468\n",
      "Iteration 234225 | Loss: 0.265465\n",
      "Iteration 234250 | Loss: 0.265462\n",
      "Iteration 234275 | Loss: 0.265460\n",
      "Iteration 234300 | Loss: 0.265457\n",
      "Iteration 234325 | Loss: 0.265454\n",
      "Iteration 234350 | Loss: 0.265452\n",
      "Iteration 234375 | Loss: 0.265449\n",
      "Iteration 234400 | Loss: 0.265446\n",
      "Iteration 234425 | Loss: 0.265444\n",
      "Iteration 234450 | Loss: 0.265441\n",
      "Iteration 234475 | Loss: 0.265438\n",
      "Iteration 234500 | Loss: 0.265436\n",
      "Iteration 234525 | Loss: 0.265433\n",
      "Iteration 234550 | Loss: 0.265430\n",
      "Iteration 234575 | Loss: 0.265428\n",
      "Iteration 234600 | Loss: 0.265425\n",
      "Iteration 234625 | Loss: 0.265422\n",
      "Iteration 234650 | Loss: 0.265420\n",
      "Iteration 234675 | Loss: 0.265417\n",
      "Iteration 234700 | Loss: 0.265414\n",
      "Iteration 234725 | Loss: 0.265412\n",
      "Iteration 234750 | Loss: 0.265409\n",
      "Iteration 234775 | Loss: 0.265406\n",
      "Iteration 234800 | Loss: 0.265404\n",
      "Iteration 234825 | Loss: 0.265401\n",
      "Iteration 234850 | Loss: 0.265399\n",
      "Iteration 234875 | Loss: 0.265396\n",
      "Iteration 234900 | Loss: 0.265393\n",
      "Iteration 234925 | Loss: 0.265391\n",
      "Iteration 234950 | Loss: 0.265388\n",
      "Iteration 234975 | Loss: 0.265385\n",
      "Iteration 235000 | Loss: 0.265383\n",
      "Iteration 235025 | Loss: 0.265380\n",
      "Iteration 235050 | Loss: 0.265378\n",
      "Iteration 235075 | Loss: 0.265375\n",
      "Iteration 235100 | Loss: 0.265372\n",
      "Iteration 235125 | Loss: 0.265370\n",
      "Iteration 235150 | Loss: 0.265367\n",
      "Iteration 235175 | Loss: 0.265365\n",
      "Iteration 235200 | Loss: 0.265362\n",
      "Iteration 235225 | Loss: 0.265359\n",
      "Iteration 235250 | Loss: 0.265357\n",
      "Iteration 235275 | Loss: 0.265354\n",
      "Iteration 235300 | Loss: 0.265352\n",
      "Iteration 235325 | Loss: 0.265349\n",
      "Iteration 235350 | Loss: 0.265347\n",
      "Iteration 235375 | Loss: 0.265344\n",
      "Iteration 235400 | Loss: 0.265341\n",
      "Iteration 235425 | Loss: 0.265339\n",
      "Iteration 235450 | Loss: 0.265336\n",
      "Iteration 235475 | Loss: 0.265334\n",
      "Iteration 235500 | Loss: 0.265331\n",
      "Iteration 235525 | Loss: 0.265329\n",
      "Iteration 235550 | Loss: 0.265326\n",
      "Iteration 235575 | Loss: 0.265323\n",
      "Iteration 235600 | Loss: 0.265321\n",
      "Iteration 235625 | Loss: 0.265318\n",
      "Iteration 235650 | Loss: 0.265316\n",
      "Iteration 235675 | Loss: 0.265313\n",
      "Iteration 235700 | Loss: 0.265311\n",
      "Iteration 235725 | Loss: 0.265308\n",
      "Iteration 235750 | Loss: 0.265306\n",
      "Iteration 235775 | Loss: 0.265303\n",
      "Iteration 235800 | Loss: 0.265301\n",
      "Iteration 235825 | Loss: 0.265298\n",
      "Iteration 235850 | Loss: 0.265296\n",
      "Iteration 235875 | Loss: 0.265293\n",
      "Iteration 235900 | Loss: 0.265291\n",
      "Iteration 235925 | Loss: 0.265288\n",
      "Iteration 235950 | Loss: 0.265286\n",
      "Iteration 235975 | Loss: 0.265283\n",
      "Iteration 236000 | Loss: 0.265280\n",
      "Iteration 236025 | Loss: 0.265278\n",
      "Iteration 236050 | Loss: 0.265275\n",
      "Iteration 236075 | Loss: 0.265273\n",
      "Iteration 236100 | Loss: 0.265270\n",
      "Iteration 236125 | Loss: 0.265268\n",
      "Iteration 236150 | Loss: 0.265266\n",
      "Iteration 236175 | Loss: 0.265263\n",
      "Iteration 236200 | Loss: 0.265261\n",
      "Iteration 236225 | Loss: 0.265258\n",
      "Iteration 236250 | Loss: 0.265256\n",
      "Iteration 236275 | Loss: 0.265253\n",
      "Iteration 236300 | Loss: 0.265251\n",
      "Iteration 236325 | Loss: 0.265248\n",
      "Iteration 236350 | Loss: 0.265246\n",
      "Iteration 236375 | Loss: 0.265243\n",
      "Iteration 236400 | Loss: 0.265241\n",
      "Iteration 236425 | Loss: 0.265238\n",
      "Iteration 236450 | Loss: 0.265236\n",
      "Iteration 236475 | Loss: 0.265233\n",
      "Iteration 236500 | Loss: 0.265231\n",
      "Iteration 236525 | Loss: 0.265228\n",
      "Iteration 236550 | Loss: 0.265226\n",
      "Iteration 236575 | Loss: 0.265224\n",
      "Iteration 236600 | Loss: 0.265221\n",
      "Iteration 236625 | Loss: 0.265219\n",
      "Iteration 236650 | Loss: 0.265216\n",
      "Iteration 236675 | Loss: 0.265214\n",
      "Iteration 236700 | Loss: 0.265211\n",
      "Iteration 236725 | Loss: 0.265209\n",
      "Iteration 236750 | Loss: 0.265206\n",
      "Iteration 236775 | Loss: 0.265204\n",
      "Iteration 236800 | Loss: 0.265202\n",
      "Iteration 236825 | Loss: 0.265199\n",
      "Iteration 236850 | Loss: 0.265197\n",
      "Iteration 236875 | Loss: 0.265194\n",
      "Iteration 236900 | Loss: 0.265192\n",
      "Iteration 236925 | Loss: 0.265190\n",
      "Iteration 236950 | Loss: 0.265187\n",
      "Iteration 236975 | Loss: 0.265185\n",
      "Iteration 237000 | Loss: 0.265182\n",
      "Iteration 237025 | Loss: 0.265180\n",
      "Iteration 237050 | Loss: 0.265178\n",
      "Iteration 237075 | Loss: 0.265175\n",
      "Iteration 237100 | Loss: 0.265173\n",
      "Iteration 237125 | Loss: 0.265170\n",
      "Iteration 237150 | Loss: 0.265168\n",
      "Iteration 237175 | Loss: 0.265166\n",
      "Iteration 237200 | Loss: 0.265163\n",
      "Iteration 237225 | Loss: 0.265161\n",
      "Iteration 237250 | Loss: 0.265158\n",
      "Iteration 237275 | Loss: 0.265156\n",
      "Iteration 237300 | Loss: 0.265154\n",
      "Iteration 237325 | Loss: 0.265151\n",
      "Iteration 237350 | Loss: 0.265149\n",
      "Iteration 237375 | Loss: 0.265147\n",
      "Iteration 237400 | Loss: 0.265144\n",
      "Iteration 237425 | Loss: 0.265142\n",
      "Iteration 237450 | Loss: 0.265139\n",
      "Iteration 237475 | Loss: 0.265137\n",
      "Iteration 237500 | Loss: 0.265135\n",
      "Iteration 237525 | Loss: 0.265132\n",
      "Iteration 237550 | Loss: 0.265130\n",
      "Iteration 237575 | Loss: 0.265128\n",
      "Iteration 237600 | Loss: 0.265125\n",
      "Iteration 237625 | Loss: 0.265123\n",
      "Iteration 237650 | Loss: 0.265121\n",
      "Iteration 237675 | Loss: 0.265118\n",
      "Iteration 237700 | Loss: 0.265116\n",
      "Iteration 237725 | Loss: 0.265114\n",
      "Iteration 237750 | Loss: 0.265111\n",
      "Iteration 237775 | Loss: 0.265109\n",
      "Iteration 237800 | Loss: 0.265107\n",
      "Iteration 237825 | Loss: 0.265104\n",
      "Iteration 237850 | Loss: 0.265102\n",
      "Iteration 237875 | Loss: 0.265100\n",
      "Iteration 237900 | Loss: 0.265097\n",
      "Iteration 237925 | Loss: 0.265095\n",
      "Iteration 237950 | Loss: 0.265093\n",
      "Iteration 237975 | Loss: 0.265090\n",
      "Iteration 238000 | Loss: 0.265088\n",
      "Iteration 238025 | Loss: 0.265086\n",
      "Iteration 238050 | Loss: 0.265084\n",
      "Iteration 238075 | Loss: 0.265081\n",
      "Iteration 238100 | Loss: 0.265079\n",
      "Iteration 238125 | Loss: 0.265077\n",
      "Iteration 238150 | Loss: 0.265074\n",
      "Iteration 238175 | Loss: 0.265072\n",
      "Iteration 238200 | Loss: 0.265070\n",
      "Iteration 238225 | Loss: 0.265067\n",
      "Iteration 238250 | Loss: 0.265065\n",
      "Iteration 238275 | Loss: 0.265063\n",
      "Iteration 238300 | Loss: 0.265061\n",
      "Iteration 238325 | Loss: 0.265058\n",
      "Iteration 238350 | Loss: 0.265056\n",
      "Iteration 238375 | Loss: 0.265054\n",
      "Iteration 238400 | Loss: 0.265052\n",
      "Iteration 238425 | Loss: 0.265049\n",
      "Iteration 238450 | Loss: 0.265047\n",
      "Iteration 238475 | Loss: 0.265045\n",
      "Iteration 238500 | Loss: 0.265043\n",
      "Iteration 238525 | Loss: 0.265040\n",
      "Iteration 238550 | Loss: 0.265038\n",
      "Iteration 238575 | Loss: 0.265036\n",
      "Iteration 238600 | Loss: 0.265033\n",
      "Iteration 238625 | Loss: 0.265031\n",
      "Iteration 238650 | Loss: 0.265029\n",
      "Iteration 238675 | Loss: 0.265027\n",
      "Iteration 238700 | Loss: 0.265025\n",
      "Iteration 238725 | Loss: 0.265022\n",
      "Iteration 238750 | Loss: 0.265020\n",
      "Iteration 238775 | Loss: 0.265018\n",
      "Iteration 238800 | Loss: 0.265016\n",
      "Iteration 238825 | Loss: 0.265013\n",
      "Iteration 238850 | Loss: 0.265011\n",
      "Iteration 238875 | Loss: 0.265009\n",
      "Iteration 238900 | Loss: 0.265007\n",
      "Iteration 238925 | Loss: 0.265004\n",
      "Iteration 238950 | Loss: 0.265002\n",
      "Iteration 238975 | Loss: 0.265000\n",
      "Iteration 239000 | Loss: 0.264998\n",
      "Iteration 239025 | Loss: 0.264996\n",
      "Iteration 239050 | Loss: 0.264993\n",
      "Iteration 239075 | Loss: 0.264991\n",
      "Iteration 239100 | Loss: 0.264989\n",
      "Iteration 239125 | Loss: 0.264987\n",
      "Iteration 239150 | Loss: 0.264985\n",
      "Iteration 239175 | Loss: 0.264982\n",
      "Iteration 239200 | Loss: 0.264980\n",
      "Iteration 239225 | Loss: 0.264978\n",
      "Iteration 239250 | Loss: 0.264976\n",
      "Iteration 239275 | Loss: 0.264974\n",
      "Iteration 239300 | Loss: 0.264971\n",
      "Iteration 239325 | Loss: 0.264969\n",
      "Iteration 239350 | Loss: 0.264967\n",
      "Iteration 239375 | Loss: 0.264965\n",
      "Iteration 239400 | Loss: 0.264963\n",
      "Iteration 239425 | Loss: 0.264961\n",
      "Iteration 239450 | Loss: 0.264958\n",
      "Iteration 239475 | Loss: 0.264956\n",
      "Iteration 239500 | Loss: 0.264954\n",
      "Iteration 239525 | Loss: 0.264952\n",
      "Iteration 239550 | Loss: 0.264950\n",
      "Iteration 239575 | Loss: 0.264948\n",
      "Iteration 239600 | Loss: 0.264945\n",
      "Iteration 239625 | Loss: 0.264943\n",
      "Iteration 239650 | Loss: 0.264941\n",
      "Iteration 239675 | Loss: 0.264939\n",
      "Iteration 239700 | Loss: 0.264937\n",
      "Iteration 239725 | Loss: 0.264935\n",
      "Iteration 239750 | Loss: 0.264932\n",
      "Iteration 239775 | Loss: 0.264930\n",
      "Iteration 239800 | Loss: 0.264928\n",
      "Iteration 239825 | Loss: 0.264926\n",
      "Iteration 239850 | Loss: 0.264924\n",
      "Iteration 239875 | Loss: 0.264922\n",
      "Iteration 239900 | Loss: 0.264920\n",
      "Iteration 239925 | Loss: 0.264917\n",
      "Iteration 239950 | Loss: 0.264915\n",
      "Iteration 239975 | Loss: 0.264913\n",
      "Iteration 240000 | Loss: 0.264911\n",
      "Iteration 240025 | Loss: 0.264909\n",
      "Iteration 240050 | Loss: 0.264907\n",
      "Iteration 240075 | Loss: 0.264905\n",
      "Iteration 240100 | Loss: 0.264903\n",
      "Iteration 240125 | Loss: 0.264901\n",
      "Iteration 240150 | Loss: 0.264898\n",
      "Iteration 240175 | Loss: 0.264896\n",
      "Iteration 240200 | Loss: 0.264894\n",
      "Iteration 240225 | Loss: 0.264892\n",
      "Iteration 240250 | Loss: 0.264890\n",
      "Iteration 240275 | Loss: 0.264888\n",
      "Iteration 240300 | Loss: 0.264886\n",
      "Iteration 240325 | Loss: 0.264884\n",
      "Iteration 240350 | Loss: 0.264882\n",
      "Iteration 240375 | Loss: 0.264880\n",
      "Iteration 240400 | Loss: 0.264877\n",
      "Iteration 240425 | Loss: 0.264875\n",
      "Iteration 240450 | Loss: 0.264873\n",
      "Iteration 240475 | Loss: 0.264871\n",
      "Iteration 240500 | Loss: 0.264869\n",
      "Iteration 240525 | Loss: 0.264867\n",
      "Iteration 240550 | Loss: 0.264865\n",
      "Iteration 240575 | Loss: 0.264863\n",
      "Iteration 240600 | Loss: 0.264861\n",
      "Iteration 240625 | Loss: 0.264859\n",
      "Iteration 240650 | Loss: 0.264857\n",
      "Iteration 240675 | Loss: 0.264855\n",
      "Iteration 240700 | Loss: 0.264853\n",
      "Iteration 240725 | Loss: 0.264850\n",
      "Iteration 240750 | Loss: 0.264848\n",
      "Iteration 240775 | Loss: 0.264846\n",
      "Iteration 240800 | Loss: 0.264844\n",
      "Iteration 240825 | Loss: 0.264842\n",
      "Iteration 240850 | Loss: 0.264840\n",
      "Iteration 240875 | Loss: 0.264838\n",
      "Iteration 240900 | Loss: 0.264836\n",
      "Iteration 240925 | Loss: 0.264834\n",
      "Iteration 240950 | Loss: 0.264832\n",
      "Iteration 240975 | Loss: 0.264830\n",
      "Iteration 241000 | Loss: 0.264828\n",
      "Iteration 241025 | Loss: 0.264826\n",
      "Iteration 241050 | Loss: 0.264824\n",
      "Iteration 241075 | Loss: 0.264822\n",
      "Iteration 241100 | Loss: 0.264820\n",
      "Iteration 241125 | Loss: 0.264818\n",
      "Iteration 241150 | Loss: 0.264816\n",
      "Iteration 241175 | Loss: 0.264814\n",
      "Iteration 241200 | Loss: 0.264812\n",
      "Iteration 241225 | Loss: 0.264810\n",
      "Iteration 241250 | Loss: 0.264808\n",
      "Iteration 241275 | Loss: 0.264806\n",
      "Iteration 241300 | Loss: 0.264804\n",
      "Iteration 241325 | Loss: 0.264802\n",
      "Iteration 241350 | Loss: 0.264800\n",
      "Iteration 241375 | Loss: 0.264798\n",
      "Iteration 241400 | Loss: 0.264796\n",
      "Iteration 241425 | Loss: 0.264794\n",
      "Iteration 241450 | Loss: 0.264792\n",
      "Iteration 241475 | Loss: 0.264790\n",
      "Iteration 241500 | Loss: 0.264788\n",
      "Iteration 241525 | Loss: 0.264786\n",
      "Iteration 241550 | Loss: 0.264784\n",
      "Iteration 241575 | Loss: 0.264782\n",
      "Iteration 241600 | Loss: 0.264780\n",
      "Iteration 241625 | Loss: 0.264778\n",
      "Iteration 241650 | Loss: 0.264776\n",
      "Iteration 241675 | Loss: 0.264774\n",
      "Iteration 241700 | Loss: 0.264772\n",
      "Iteration 241725 | Loss: 0.264770\n",
      "Iteration 241750 | Loss: 0.264768\n",
      "Iteration 241775 | Loss: 0.264766\n",
      "Iteration 241800 | Loss: 0.264764\n",
      "Iteration 241825 | Loss: 0.264762\n",
      "Iteration 241850 | Loss: 0.264760\n",
      "Iteration 241875 | Loss: 0.264758\n",
      "Iteration 241900 | Loss: 0.264756\n",
      "Iteration 241925 | Loss: 0.264754\n",
      "Iteration 241950 | Loss: 0.264752\n",
      "Iteration 241975 | Loss: 0.264750\n",
      "Iteration 242000 | Loss: 0.264748\n",
      "Iteration 242025 | Loss: 0.264746\n",
      "Iteration 242050 | Loss: 0.264744\n",
      "Iteration 242075 | Loss: 0.264742\n",
      "Iteration 242100 | Loss: 0.264740\n",
      "Iteration 242125 | Loss: 0.264738\n",
      "Iteration 242150 | Loss: 0.264736\n",
      "Iteration 242175 | Loss: 0.264734\n",
      "Iteration 242200 | Loss: 0.264733\n",
      "Iteration 242225 | Loss: 0.264731\n",
      "Iteration 242250 | Loss: 0.264729\n",
      "Iteration 242275 | Loss: 0.264727\n",
      "Iteration 242300 | Loss: 0.264725\n",
      "Iteration 242325 | Loss: 0.264723\n",
      "Iteration 242350 | Loss: 0.264721\n",
      "Iteration 242375 | Loss: 0.264719\n",
      "Iteration 242400 | Loss: 0.264717\n",
      "Iteration 242425 | Loss: 0.264715\n",
      "Iteration 242450 | Loss: 0.264713\n",
      "Iteration 242475 | Loss: 0.264711\n",
      "Iteration 242500 | Loss: 0.264709\n",
      "Iteration 242525 | Loss: 0.264708\n",
      "Iteration 242550 | Loss: 0.264706\n",
      "Iteration 242575 | Loss: 0.264704\n",
      "Iteration 242600 | Loss: 0.264702\n",
      "Iteration 242625 | Loss: 0.264700\n",
      "Iteration 242650 | Loss: 0.264698\n",
      "Iteration 242675 | Loss: 0.264696\n",
      "Iteration 242700 | Loss: 0.264694\n",
      "Iteration 242725 | Loss: 0.264692\n",
      "Iteration 242750 | Loss: 0.264690\n",
      "Iteration 242775 | Loss: 0.264689\n",
      "Iteration 242800 | Loss: 0.264687\n",
      "Iteration 242825 | Loss: 0.264685\n",
      "Iteration 242850 | Loss: 0.264683\n",
      "Iteration 242875 | Loss: 0.264681\n",
      "Iteration 242900 | Loss: 0.264679\n",
      "Iteration 242925 | Loss: 0.264677\n",
      "Iteration 242950 | Loss: 0.264675\n",
      "Iteration 242975 | Loss: 0.264673\n",
      "Iteration 243000 | Loss: 0.264672\n",
      "Iteration 243025 | Loss: 0.264670\n",
      "Iteration 243050 | Loss: 0.264668\n",
      "Iteration 243075 | Loss: 0.264666\n",
      "Iteration 243100 | Loss: 0.264664\n",
      "Iteration 243125 | Loss: 0.264662\n",
      "Iteration 243150 | Loss: 0.264660\n",
      "Iteration 243175 | Loss: 0.264658\n",
      "Iteration 243200 | Loss: 0.264657\n",
      "Iteration 243225 | Loss: 0.264655\n",
      "Iteration 243250 | Loss: 0.264653\n",
      "Iteration 243275 | Loss: 0.264651\n",
      "Iteration 243300 | Loss: 0.264649\n",
      "Iteration 243325 | Loss: 0.264647\n",
      "Iteration 243350 | Loss: 0.264646\n",
      "Iteration 243375 | Loss: 0.264644\n",
      "Iteration 243400 | Loss: 0.264642\n",
      "Iteration 243425 | Loss: 0.264640\n",
      "Iteration 243450 | Loss: 0.264638\n",
      "Iteration 243475 | Loss: 0.264636\n",
      "Iteration 243500 | Loss: 0.264634\n",
      "Iteration 243525 | Loss: 0.264633\n",
      "Iteration 243550 | Loss: 0.264631\n",
      "Iteration 243575 | Loss: 0.264629\n",
      "Iteration 243600 | Loss: 0.264627\n",
      "Iteration 243625 | Loss: 0.264625\n",
      "Iteration 243650 | Loss: 0.264623\n",
      "Iteration 243675 | Loss: 0.264622\n",
      "Iteration 243700 | Loss: 0.264620\n",
      "Iteration 243725 | Loss: 0.264618\n",
      "Iteration 243750 | Loss: 0.264616\n",
      "Iteration 243775 | Loss: 0.264614\n",
      "Iteration 243800 | Loss: 0.264613\n",
      "Iteration 243825 | Loss: 0.264611\n",
      "Iteration 243850 | Loss: 0.264609\n",
      "Iteration 243875 | Loss: 0.264607\n",
      "Iteration 243900 | Loss: 0.264605\n",
      "Iteration 243925 | Loss: 0.264604\n",
      "Iteration 243950 | Loss: 0.264602\n",
      "Iteration 243975 | Loss: 0.264600\n",
      "Iteration 244000 | Loss: 0.264598\n",
      "Iteration 244025 | Loss: 0.264596\n",
      "Iteration 244050 | Loss: 0.264595\n",
      "Iteration 244075 | Loss: 0.264593\n",
      "Iteration 244100 | Loss: 0.264591\n",
      "Iteration 244125 | Loss: 0.264589\n",
      "Iteration 244150 | Loss: 0.264587\n",
      "Iteration 244175 | Loss: 0.264586\n",
      "Iteration 244200 | Loss: 0.264584\n",
      "Iteration 244225 | Loss: 0.264582\n",
      "Iteration 244250 | Loss: 0.264580\n",
      "Iteration 244275 | Loss: 0.264578\n",
      "Iteration 244300 | Loss: 0.264577\n",
      "Iteration 244325 | Loss: 0.264575\n",
      "Iteration 244350 | Loss: 0.264573\n",
      "Iteration 244375 | Loss: 0.264571\n",
      "Iteration 244400 | Loss: 0.264570\n",
      "Iteration 244425 | Loss: 0.264568\n",
      "Iteration 244450 | Loss: 0.264566\n",
      "Iteration 244475 | Loss: 0.264564\n",
      "Iteration 244500 | Loss: 0.264563\n",
      "Iteration 244525 | Loss: 0.264561\n",
      "Iteration 244550 | Loss: 0.264559\n",
      "Iteration 244575 | Loss: 0.264557\n",
      "Iteration 244600 | Loss: 0.264555\n",
      "Iteration 244625 | Loss: 0.264554\n",
      "Iteration 244650 | Loss: 0.264552\n",
      "Iteration 244675 | Loss: 0.264550\n",
      "Iteration 244700 | Loss: 0.264548\n",
      "Iteration 244725 | Loss: 0.264547\n",
      "Iteration 244750 | Loss: 0.264545\n",
      "Iteration 244775 | Loss: 0.264543\n",
      "Iteration 244800 | Loss: 0.264541\n",
      "Iteration 244825 | Loss: 0.264540\n",
      "Iteration 244850 | Loss: 0.264538\n",
      "Iteration 244875 | Loss: 0.264536\n",
      "Iteration 244900 | Loss: 0.264535\n",
      "Iteration 244925 | Loss: 0.264533\n",
      "Iteration 244950 | Loss: 0.264531\n",
      "Iteration 244975 | Loss: 0.264529\n",
      "Iteration 245000 | Loss: 0.264528\n",
      "Iteration 245025 | Loss: 0.264526\n",
      "Iteration 245050 | Loss: 0.264524\n",
      "Iteration 245075 | Loss: 0.264522\n",
      "Iteration 245100 | Loss: 0.264521\n",
      "Iteration 245125 | Loss: 0.264519\n",
      "Iteration 245150 | Loss: 0.264517\n",
      "Iteration 245175 | Loss: 0.264516\n",
      "Iteration 245200 | Loss: 0.264514\n",
      "Iteration 245225 | Loss: 0.264512\n",
      "Iteration 245250 | Loss: 0.264510\n",
      "Iteration 245275 | Loss: 0.264509\n",
      "Iteration 245300 | Loss: 0.264507\n",
      "Iteration 245325 | Loss: 0.264505\n",
      "Iteration 245350 | Loss: 0.264504\n",
      "Iteration 245375 | Loss: 0.264502\n",
      "Iteration 245400 | Loss: 0.264500\n",
      "Iteration 245425 | Loss: 0.264499\n",
      "Iteration 245450 | Loss: 0.264497\n",
      "Iteration 245475 | Loss: 0.264495\n",
      "Iteration 245500 | Loss: 0.264493\n",
      "Iteration 245525 | Loss: 0.264492\n",
      "Iteration 245550 | Loss: 0.264490\n",
      "Iteration 245575 | Loss: 0.264488\n",
      "Iteration 245600 | Loss: 0.264487\n",
      "Iteration 245625 | Loss: 0.264485\n",
      "Iteration 245650 | Loss: 0.264483\n",
      "Iteration 245675 | Loss: 0.264482\n",
      "Iteration 245700 | Loss: 0.264480\n",
      "Iteration 245725 | Loss: 0.264478\n",
      "Iteration 245750 | Loss: 0.264477\n",
      "Iteration 245775 | Loss: 0.264475\n",
      "Iteration 245800 | Loss: 0.264473\n",
      "Iteration 245825 | Loss: 0.264472\n",
      "Iteration 245850 | Loss: 0.264470\n",
      "Iteration 245875 | Loss: 0.264468\n",
      "Iteration 245900 | Loss: 0.264467\n",
      "Iteration 245925 | Loss: 0.264465\n",
      "Iteration 245950 | Loss: 0.264463\n",
      "Iteration 245975 | Loss: 0.264462\n",
      "Iteration 246000 | Loss: 0.264460\n",
      "Iteration 246025 | Loss: 0.264458\n",
      "Iteration 246050 | Loss: 0.264457\n",
      "Iteration 246075 | Loss: 0.264455\n",
      "Iteration 246100 | Loss: 0.264453\n",
      "Iteration 246125 | Loss: 0.264452\n",
      "Iteration 246150 | Loss: 0.264450\n",
      "Iteration 246175 | Loss: 0.264448\n",
      "Iteration 246200 | Loss: 0.264447\n",
      "Iteration 246225 | Loss: 0.264445\n",
      "Iteration 246250 | Loss: 0.264444\n",
      "Iteration 246275 | Loss: 0.264442\n",
      "Iteration 246300 | Loss: 0.264440\n",
      "Iteration 246325 | Loss: 0.264439\n",
      "Iteration 246350 | Loss: 0.264437\n",
      "Iteration 246375 | Loss: 0.264435\n",
      "Iteration 246400 | Loss: 0.264434\n",
      "Iteration 246425 | Loss: 0.264432\n",
      "Iteration 246450 | Loss: 0.264430\n",
      "Iteration 246475 | Loss: 0.264429\n",
      "Iteration 246500 | Loss: 0.264427\n",
      "Iteration 246525 | Loss: 0.264426\n",
      "Iteration 246550 | Loss: 0.264424\n",
      "Iteration 246575 | Loss: 0.264422\n",
      "Iteration 246600 | Loss: 0.264421\n",
      "Iteration 246625 | Loss: 0.264419\n",
      "Iteration 246650 | Loss: 0.264418\n",
      "Iteration 246675 | Loss: 0.264416\n",
      "Iteration 246700 | Loss: 0.264414\n",
      "Iteration 246725 | Loss: 0.264413\n",
      "Iteration 246750 | Loss: 0.264411\n",
      "Iteration 246775 | Loss: 0.264409\n",
      "Iteration 246800 | Loss: 0.264408\n",
      "Iteration 246825 | Loss: 0.264406\n",
      "Iteration 246850 | Loss: 0.264405\n",
      "Iteration 246875 | Loss: 0.264403\n",
      "Iteration 246900 | Loss: 0.264401\n",
      "Iteration 246925 | Loss: 0.264400\n",
      "Iteration 246950 | Loss: 0.264398\n",
      "Iteration 246975 | Loss: 0.264397\n",
      "Iteration 247000 | Loss: 0.264395\n",
      "Iteration 247025 | Loss: 0.264394\n",
      "Iteration 247050 | Loss: 0.264392\n",
      "Iteration 247075 | Loss: 0.264390\n",
      "Iteration 247100 | Loss: 0.264389\n",
      "Iteration 247125 | Loss: 0.264387\n",
      "Iteration 247150 | Loss: 0.264386\n",
      "Iteration 247175 | Loss: 0.264384\n",
      "Iteration 247200 | Loss: 0.264382\n",
      "Iteration 247225 | Loss: 0.264381\n",
      "Iteration 247250 | Loss: 0.264379\n",
      "Iteration 247275 | Loss: 0.264378\n",
      "Iteration 247300 | Loss: 0.264376\n",
      "Iteration 247325 | Loss: 0.264375\n",
      "Iteration 247350 | Loss: 0.264373\n",
      "Iteration 247375 | Loss: 0.264371\n",
      "Iteration 247400 | Loss: 0.264370\n",
      "Iteration 247425 | Loss: 0.264368\n",
      "Iteration 247450 | Loss: 0.264367\n",
      "Iteration 247475 | Loss: 0.264365\n",
      "Iteration 247500 | Loss: 0.264364\n",
      "Iteration 247525 | Loss: 0.264362\n",
      "Iteration 247550 | Loss: 0.264361\n",
      "Iteration 247575 | Loss: 0.264359\n",
      "Iteration 247600 | Loss: 0.264357\n",
      "Iteration 247625 | Loss: 0.264356\n",
      "Iteration 247650 | Loss: 0.264354\n",
      "Iteration 247675 | Loss: 0.264353\n",
      "Iteration 247700 | Loss: 0.264351\n",
      "Iteration 247725 | Loss: 0.264350\n",
      "Iteration 247750 | Loss: 0.264348\n",
      "Iteration 247775 | Loss: 0.264347\n",
      "Iteration 247800 | Loss: 0.264345\n",
      "Iteration 247825 | Loss: 0.264344\n",
      "Iteration 247850 | Loss: 0.264342\n",
      "Iteration 247875 | Loss: 0.264341\n",
      "Iteration 247900 | Loss: 0.264339\n",
      "Iteration 247925 | Loss: 0.264337\n",
      "Iteration 247950 | Loss: 0.264336\n",
      "Iteration 247975 | Loss: 0.264334\n",
      "Iteration 248000 | Loss: 0.264333\n",
      "Iteration 248025 | Loss: 0.264331\n",
      "Iteration 248050 | Loss: 0.264330\n",
      "Iteration 248075 | Loss: 0.264328\n",
      "Iteration 248100 | Loss: 0.264327\n",
      "Iteration 248125 | Loss: 0.264325\n",
      "Iteration 248150 | Loss: 0.264324\n",
      "Iteration 248175 | Loss: 0.264322\n",
      "Iteration 248200 | Loss: 0.264321\n",
      "Iteration 248225 | Loss: 0.264319\n",
      "Iteration 248250 | Loss: 0.264318\n",
      "Iteration 248275 | Loss: 0.264316\n",
      "Iteration 248300 | Loss: 0.264315\n",
      "Iteration 248325 | Loss: 0.264313\n",
      "Iteration 248350 | Loss: 0.264312\n",
      "Iteration 248375 | Loss: 0.264310\n",
      "Iteration 248400 | Loss: 0.264309\n",
      "Iteration 248425 | Loss: 0.264307\n",
      "Iteration 248450 | Loss: 0.264306\n",
      "Iteration 248475 | Loss: 0.264304\n",
      "Iteration 248500 | Loss: 0.264303\n",
      "Iteration 248525 | Loss: 0.264301\n",
      "Iteration 248550 | Loss: 0.264300\n",
      "Iteration 248575 | Loss: 0.264298\n",
      "Iteration 248600 | Loss: 0.264297\n",
      "Iteration 248625 | Loss: 0.264295\n",
      "Iteration 248650 | Loss: 0.264294\n",
      "Iteration 248675 | Loss: 0.264292\n",
      "Iteration 248700 | Loss: 0.264291\n",
      "Iteration 248725 | Loss: 0.264289\n",
      "Iteration 248750 | Loss: 0.264288\n",
      "Iteration 248775 | Loss: 0.264286\n",
      "Iteration 248800 | Loss: 0.264285\n",
      "Iteration 248825 | Loss: 0.264283\n",
      "Iteration 248850 | Loss: 0.264282\n",
      "Iteration 248875 | Loss: 0.264280\n",
      "Iteration 248900 | Loss: 0.264279\n",
      "Iteration 248925 | Loss: 0.264278\n",
      "Iteration 248950 | Loss: 0.264276\n",
      "Iteration 248975 | Loss: 0.264275\n",
      "Iteration 249000 | Loss: 0.264273\n",
      "Iteration 249025 | Loss: 0.264272\n",
      "Iteration 249050 | Loss: 0.264270\n",
      "Iteration 249075 | Loss: 0.264269\n",
      "Iteration 249100 | Loss: 0.264267\n",
      "Iteration 249125 | Loss: 0.264266\n",
      "Iteration 249150 | Loss: 0.264264\n",
      "Iteration 249175 | Loss: 0.264263\n",
      "Iteration 249200 | Loss: 0.264261\n",
      "Iteration 249225 | Loss: 0.264260\n",
      "Iteration 249250 | Loss: 0.264259\n",
      "Iteration 249275 | Loss: 0.264257\n",
      "Iteration 249300 | Loss: 0.264256\n",
      "Iteration 249325 | Loss: 0.264254\n",
      "Iteration 249350 | Loss: 0.264253\n",
      "Iteration 249375 | Loss: 0.264251\n",
      "Iteration 249400 | Loss: 0.264250\n",
      "Iteration 249425 | Loss: 0.264249\n",
      "Iteration 249450 | Loss: 0.264247\n",
      "Iteration 249475 | Loss: 0.264246\n",
      "Iteration 249500 | Loss: 0.264244\n",
      "Iteration 249525 | Loss: 0.264243\n",
      "Iteration 249550 | Loss: 0.264241\n",
      "Iteration 249575 | Loss: 0.264240\n",
      "Iteration 249600 | Loss: 0.264238\n",
      "Iteration 249625 | Loss: 0.264237\n",
      "Iteration 249650 | Loss: 0.264236\n",
      "Iteration 249675 | Loss: 0.264234\n",
      "Iteration 249700 | Loss: 0.264233\n",
      "Iteration 249725 | Loss: 0.264231\n",
      "Iteration 249750 | Loss: 0.264230\n",
      "Iteration 249775 | Loss: 0.264229\n",
      "Iteration 249800 | Loss: 0.264227\n",
      "Iteration 249825 | Loss: 0.264226\n",
      "Iteration 249850 | Loss: 0.264224\n",
      "Iteration 249875 | Loss: 0.264223\n",
      "Iteration 249900 | Loss: 0.264221\n",
      "Iteration 249925 | Loss: 0.264220\n",
      "Iteration 249950 | Loss: 0.264219\n",
      "Iteration 249975 | Loss: 0.264217\n",
      "Iteration 250000 | Loss: 0.264216\n",
      "Iteration 250025 | Loss: 0.264214\n",
      "Iteration 250050 | Loss: 0.264213\n",
      "Iteration 250075 | Loss: 0.264212\n",
      "Iteration 250100 | Loss: 0.264210\n",
      "Iteration 250125 | Loss: 0.264209\n",
      "Iteration 250150 | Loss: 0.264207\n",
      "Iteration 250175 | Loss: 0.264206\n",
      "Iteration 250200 | Loss: 0.264205\n",
      "Iteration 250225 | Loss: 0.264203\n",
      "Iteration 250250 | Loss: 0.264202\n",
      "Iteration 250275 | Loss: 0.264200\n",
      "Iteration 250300 | Loss: 0.264199\n",
      "Iteration 250325 | Loss: 0.264198\n",
      "Iteration 250350 | Loss: 0.264196\n",
      "Iteration 250375 | Loss: 0.264195\n",
      "Iteration 250400 | Loss: 0.264194\n",
      "Iteration 250425 | Loss: 0.264192\n",
      "Iteration 250450 | Loss: 0.264191\n",
      "Iteration 250475 | Loss: 0.264189\n",
      "Iteration 250500 | Loss: 0.264188\n",
      "Iteration 250525 | Loss: 0.264187\n",
      "Iteration 250550 | Loss: 0.264185\n",
      "Iteration 250575 | Loss: 0.264184\n",
      "Iteration 250600 | Loss: 0.264183\n",
      "Iteration 250625 | Loss: 0.264181\n",
      "Iteration 250650 | Loss: 0.264180\n",
      "Iteration 250675 | Loss: 0.264178\n",
      "Iteration 250700 | Loss: 0.264177\n",
      "Iteration 250725 | Loss: 0.264176\n",
      "Iteration 250750 | Loss: 0.264174\n",
      "Iteration 250775 | Loss: 0.264173\n",
      "Iteration 250800 | Loss: 0.264172\n",
      "Iteration 250825 | Loss: 0.264170\n",
      "Iteration 250850 | Loss: 0.264169\n",
      "Iteration 250875 | Loss: 0.264168\n",
      "Iteration 250900 | Loss: 0.264166\n",
      "Iteration 250925 | Loss: 0.264165\n",
      "Iteration 250950 | Loss: 0.264164\n",
      "Iteration 250975 | Loss: 0.264162\n",
      "Iteration 251000 | Loss: 0.264161\n",
      "Iteration 251025 | Loss: 0.264160\n",
      "Iteration 251050 | Loss: 0.264158\n",
      "Iteration 251075 | Loss: 0.264157\n",
      "Iteration 251100 | Loss: 0.264155\n",
      "Iteration 251125 | Loss: 0.264154\n",
      "Iteration 251150 | Loss: 0.264153\n",
      "Iteration 251175 | Loss: 0.264151\n",
      "Iteration 251200 | Loss: 0.264150\n",
      "Iteration 251225 | Loss: 0.264149\n",
      "Iteration 251250 | Loss: 0.264147\n",
      "Iteration 251275 | Loss: 0.264146\n",
      "Iteration 251300 | Loss: 0.264145\n",
      "Iteration 251325 | Loss: 0.264143\n",
      "Iteration 251350 | Loss: 0.264142\n",
      "Iteration 251375 | Loss: 0.264141\n",
      "Iteration 251400 | Loss: 0.264139\n",
      "Iteration 251425 | Loss: 0.264138\n",
      "Iteration 251450 | Loss: 0.264137\n",
      "Iteration 251475 | Loss: 0.264136\n",
      "Iteration 251500 | Loss: 0.264134\n",
      "Iteration 251525 | Loss: 0.264133\n",
      "Iteration 251550 | Loss: 0.264132\n",
      "Iteration 251575 | Loss: 0.264130\n",
      "Iteration 251600 | Loss: 0.264129\n",
      "Iteration 251625 | Loss: 0.264128\n",
      "Iteration 251650 | Loss: 0.264126\n",
      "Iteration 251675 | Loss: 0.264125\n",
      "Iteration 251700 | Loss: 0.264124\n",
      "Iteration 251725 | Loss: 0.264122\n",
      "Iteration 251750 | Loss: 0.264121\n",
      "Iteration 251775 | Loss: 0.264120\n",
      "Iteration 251800 | Loss: 0.264118\n",
      "Iteration 251825 | Loss: 0.264117\n",
      "Iteration 251850 | Loss: 0.264116\n",
      "Iteration 251875 | Loss: 0.264115\n",
      "Iteration 251900 | Loss: 0.264113\n",
      "Iteration 251925 | Loss: 0.264112\n",
      "Iteration 251950 | Loss: 0.264111\n",
      "Iteration 251975 | Loss: 0.264109\n",
      "Iteration 252000 | Loss: 0.264108\n",
      "Iteration 252025 | Loss: 0.264107\n",
      "Iteration 252050 | Loss: 0.264106\n",
      "Iteration 252075 | Loss: 0.264104\n",
      "Iteration 252100 | Loss: 0.264103\n",
      "Iteration 252125 | Loss: 0.264102\n",
      "Iteration 252150 | Loss: 0.264100\n",
      "Iteration 252175 | Loss: 0.264099\n",
      "Iteration 252200 | Loss: 0.264098\n",
      "Iteration 252225 | Loss: 0.264097\n",
      "Iteration 252250 | Loss: 0.264095\n",
      "Iteration 252275 | Loss: 0.264094\n",
      "Iteration 252300 | Loss: 0.264093\n",
      "Iteration 252325 | Loss: 0.264091\n",
      "Iteration 252350 | Loss: 0.264090\n",
      "Iteration 252375 | Loss: 0.264089\n",
      "Iteration 252400 | Loss: 0.264088\n",
      "Iteration 252425 | Loss: 0.264086\n",
      "Iteration 252450 | Loss: 0.264085\n",
      "Iteration 252475 | Loss: 0.264084\n",
      "Iteration 252500 | Loss: 0.264083\n",
      "Iteration 252525 | Loss: 0.264081\n",
      "Iteration 252550 | Loss: 0.264080\n",
      "Iteration 252575 | Loss: 0.264079\n",
      "Iteration 252600 | Loss: 0.264077\n",
      "Iteration 252625 | Loss: 0.264076\n",
      "Iteration 252650 | Loss: 0.264075\n",
      "Iteration 252675 | Loss: 0.264074\n",
      "Iteration 252700 | Loss: 0.264072\n",
      "Iteration 252725 | Loss: 0.264071\n",
      "Iteration 252750 | Loss: 0.264070\n",
      "Iteration 252775 | Loss: 0.264069\n",
      "Iteration 252800 | Loss: 0.264067\n",
      "Iteration 252825 | Loss: 0.264066\n",
      "Iteration 252850 | Loss: 0.264065\n",
      "Iteration 252875 | Loss: 0.264064\n",
      "Iteration 252900 | Loss: 0.264062\n",
      "Iteration 252925 | Loss: 0.264061\n",
      "Iteration 252950 | Loss: 0.264060\n",
      "Iteration 252975 | Loss: 0.264059\n",
      "Iteration 253000 | Loss: 0.264057\n",
      "Iteration 253025 | Loss: 0.264056\n",
      "Iteration 253050 | Loss: 0.264055\n",
      "Iteration 253075 | Loss: 0.264054\n",
      "Iteration 253100 | Loss: 0.264053\n",
      "Iteration 253125 | Loss: 0.264051\n",
      "Iteration 253150 | Loss: 0.264050\n",
      "Iteration 253175 | Loss: 0.264049\n",
      "Iteration 253200 | Loss: 0.264048\n",
      "Iteration 253225 | Loss: 0.264046\n",
      "Iteration 253250 | Loss: 0.264045\n",
      "Iteration 253275 | Loss: 0.264044\n",
      "Iteration 253300 | Loss: 0.264043\n",
      "Iteration 253325 | Loss: 0.264041\n",
      "Iteration 253350 | Loss: 0.264040\n",
      "Iteration 253375 | Loss: 0.264039\n",
      "Iteration 253400 | Loss: 0.264038\n",
      "Iteration 253425 | Loss: 0.264037\n",
      "Iteration 253450 | Loss: 0.264035\n",
      "Iteration 253475 | Loss: 0.264034\n",
      "Iteration 253500 | Loss: 0.264033\n",
      "Iteration 253525 | Loss: 0.264032\n",
      "Iteration 253550 | Loss: 0.264031\n",
      "Iteration 253575 | Loss: 0.264029\n",
      "Iteration 253600 | Loss: 0.264028\n",
      "Iteration 253625 | Loss: 0.264027\n",
      "Iteration 253650 | Loss: 0.264026\n",
      "Iteration 253675 | Loss: 0.264024\n",
      "Iteration 253700 | Loss: 0.264023\n",
      "Iteration 253725 | Loss: 0.264022\n",
      "Iteration 253750 | Loss: 0.264021\n",
      "Iteration 253775 | Loss: 0.264020\n",
      "Iteration 253800 | Loss: 0.264018\n",
      "Iteration 253825 | Loss: 0.264017\n",
      "Iteration 253850 | Loss: 0.264016\n",
      "Iteration 253875 | Loss: 0.264015\n",
      "Iteration 253900 | Loss: 0.264014\n",
      "Iteration 253925 | Loss: 0.264012\n",
      "Iteration 253950 | Loss: 0.264011\n",
      "Iteration 253975 | Loss: 0.264010\n",
      "Iteration 254000 | Loss: 0.264009\n",
      "Iteration 254025 | Loss: 0.264008\n",
      "Iteration 254050 | Loss: 0.264007\n",
      "Iteration 254075 | Loss: 0.264005\n",
      "Iteration 254100 | Loss: 0.264004\n",
      "Iteration 254125 | Loss: 0.264003\n",
      "Iteration 254150 | Loss: 0.264002\n",
      "Iteration 254175 | Loss: 0.264001\n",
      "Iteration 254200 | Loss: 0.263999\n",
      "Iteration 254225 | Loss: 0.263998\n",
      "Iteration 254250 | Loss: 0.263997\n",
      "Iteration 254275 | Loss: 0.263996\n",
      "Iteration 254300 | Loss: 0.263995\n",
      "Iteration 254325 | Loss: 0.263994\n",
      "Iteration 254350 | Loss: 0.263992\n",
      "Iteration 254375 | Loss: 0.263991\n",
      "Iteration 254400 | Loss: 0.263990\n",
      "Iteration 254425 | Loss: 0.263989\n",
      "Iteration 254450 | Loss: 0.263988\n",
      "Iteration 254475 | Loss: 0.263987\n",
      "Iteration 254500 | Loss: 0.263985\n",
      "Iteration 254525 | Loss: 0.263984\n",
      "Iteration 254550 | Loss: 0.263983\n",
      "Iteration 254575 | Loss: 0.263982\n",
      "Iteration 254600 | Loss: 0.263981\n",
      "Iteration 254625 | Loss: 0.263980\n",
      "Iteration 254650 | Loss: 0.263978\n",
      "Iteration 254675 | Loss: 0.263977\n",
      "Iteration 254700 | Loss: 0.263976\n",
      "Iteration 254725 | Loss: 0.263975\n",
      "Iteration 254750 | Loss: 0.263974\n",
      "Iteration 254775 | Loss: 0.263973\n",
      "Iteration 254800 | Loss: 0.263971\n",
      "Iteration 254825 | Loss: 0.263970\n",
      "Iteration 254850 | Loss: 0.263969\n",
      "Iteration 254875 | Loss: 0.263968\n",
      "Iteration 254900 | Loss: 0.263967\n",
      "Iteration 254925 | Loss: 0.263966\n",
      "Iteration 254950 | Loss: 0.263965\n",
      "Iteration 254975 | Loss: 0.263963\n",
      "Iteration 255000 | Loss: 0.263962\n",
      "Iteration 255025 | Loss: 0.263961\n",
      "Iteration 255050 | Loss: 0.263960\n",
      "Iteration 255075 | Loss: 0.263959\n",
      "Iteration 255100 | Loss: 0.263958\n",
      "Iteration 255125 | Loss: 0.263957\n",
      "Iteration 255150 | Loss: 0.263955\n",
      "Iteration 255175 | Loss: 0.263954\n",
      "Iteration 255200 | Loss: 0.263953\n",
      "Iteration 255225 | Loss: 0.263952\n",
      "Iteration 255250 | Loss: 0.263951\n",
      "Iteration 255275 | Loss: 0.263950\n",
      "Iteration 255300 | Loss: 0.263949\n",
      "Iteration 255325 | Loss: 0.263948\n",
      "Iteration 255350 | Loss: 0.263946\n",
      "Iteration 255375 | Loss: 0.263945\n",
      "Iteration 255400 | Loss: 0.263944\n",
      "Iteration 255425 | Loss: 0.263943\n",
      "Iteration 255450 | Loss: 0.263942\n",
      "Iteration 255475 | Loss: 0.263941\n",
      "Iteration 255500 | Loss: 0.263940\n",
      "Iteration 255525 | Loss: 0.263939\n",
      "Iteration 255550 | Loss: 0.263938\n",
      "Iteration 255575 | Loss: 0.263936\n",
      "Iteration 255600 | Loss: 0.263935\n",
      "Iteration 255625 | Loss: 0.263934\n",
      "Iteration 255650 | Loss: 0.263933\n",
      "Iteration 255675 | Loss: 0.263932\n",
      "Iteration 255700 | Loss: 0.263931\n",
      "Iteration 255725 | Loss: 0.263930\n",
      "Iteration 255750 | Loss: 0.263929\n",
      "Iteration 255775 | Loss: 0.263928\n",
      "Iteration 255800 | Loss: 0.263926\n",
      "Iteration 255825 | Loss: 0.263925\n",
      "Iteration 255850 | Loss: 0.263924\n",
      "Iteration 255875 | Loss: 0.263923\n",
      "Iteration 255900 | Loss: 0.263922\n",
      "Iteration 255925 | Loss: 0.263921\n",
      "Iteration 255950 | Loss: 0.263920\n",
      "Iteration 255975 | Loss: 0.263919\n",
      "Iteration 256000 | Loss: 0.263918\n",
      "Iteration 256025 | Loss: 0.263917\n",
      "Iteration 256050 | Loss: 0.263915\n",
      "Iteration 256075 | Loss: 0.263914\n",
      "Iteration 256100 | Loss: 0.263913\n",
      "Iteration 256125 | Loss: 0.263912\n",
      "Iteration 256150 | Loss: 0.263911\n",
      "Iteration 256175 | Loss: 0.263910\n",
      "Iteration 256200 | Loss: 0.263909\n",
      "Iteration 256225 | Loss: 0.263908\n",
      "Iteration 256250 | Loss: 0.263907\n",
      "Iteration 256275 | Loss: 0.263906\n",
      "Iteration 256300 | Loss: 0.263905\n",
      "Iteration 256325 | Loss: 0.263903\n",
      "Iteration 256350 | Loss: 0.263902\n",
      "Iteration 256375 | Loss: 0.263901\n",
      "Iteration 256400 | Loss: 0.263900\n",
      "Iteration 256425 | Loss: 0.263899\n",
      "Iteration 256450 | Loss: 0.263898\n",
      "Iteration 256475 | Loss: 0.263897\n",
      "Iteration 256500 | Loss: 0.263896\n",
      "Iteration 256525 | Loss: 0.263895\n",
      "Iteration 256550 | Loss: 0.263894\n",
      "Iteration 256575 | Loss: 0.263893\n",
      "Iteration 256600 | Loss: 0.263892\n",
      "Iteration 256625 | Loss: 0.263891\n",
      "Iteration 256650 | Loss: 0.263890\n",
      "Iteration 256675 | Loss: 0.263888\n",
      "Iteration 256700 | Loss: 0.263887\n",
      "Iteration 256725 | Loss: 0.263886\n",
      "Iteration 256750 | Loss: 0.263885\n",
      "Iteration 256775 | Loss: 0.263884\n",
      "Iteration 256800 | Loss: 0.263883\n",
      "Iteration 256825 | Loss: 0.263882\n",
      "Iteration 256850 | Loss: 0.263881\n",
      "Iteration 256875 | Loss: 0.263880\n",
      "Iteration 256900 | Loss: 0.263879\n",
      "Iteration 256925 | Loss: 0.263878\n",
      "Iteration 256950 | Loss: 0.263877\n",
      "Iteration 256975 | Loss: 0.263876\n",
      "Iteration 257000 | Loss: 0.263875\n",
      "Iteration 257025 | Loss: 0.263874\n",
      "Iteration 257050 | Loss: 0.263873\n",
      "Iteration 257075 | Loss: 0.263872\n",
      "Iteration 257100 | Loss: 0.263871\n",
      "Iteration 257125 | Loss: 0.263869\n",
      "Iteration 257150 | Loss: 0.263868\n",
      "Iteration 257175 | Loss: 0.263867\n",
      "Iteration 257200 | Loss: 0.263866\n",
      "Iteration 257225 | Loss: 0.263865\n",
      "Iteration 257250 | Loss: 0.263864\n",
      "Iteration 257275 | Loss: 0.263863\n",
      "Iteration 257300 | Loss: 0.263862\n",
      "Iteration 257325 | Loss: 0.263861\n",
      "Iteration 257350 | Loss: 0.263860\n",
      "Iteration 257375 | Loss: 0.263859\n",
      "Iteration 257400 | Loss: 0.263858\n",
      "Iteration 257425 | Loss: 0.263857\n",
      "Iteration 257450 | Loss: 0.263856\n",
      "Iteration 257475 | Loss: 0.263855\n",
      "Iteration 257500 | Loss: 0.263854\n",
      "Iteration 257525 | Loss: 0.263853\n",
      "Iteration 257550 | Loss: 0.263852\n",
      "Iteration 257575 | Loss: 0.263851\n",
      "Iteration 257600 | Loss: 0.263850\n",
      "Iteration 257625 | Loss: 0.263849\n",
      "Iteration 257650 | Loss: 0.263848\n",
      "Iteration 257675 | Loss: 0.263847\n",
      "Iteration 257700 | Loss: 0.263846\n",
      "Iteration 257725 | Loss: 0.263845\n",
      "Iteration 257750 | Loss: 0.263844\n",
      "Iteration 257775 | Loss: 0.263843\n",
      "Iteration 257800 | Loss: 0.263842\n",
      "Iteration 257825 | Loss: 0.263841\n",
      "Iteration 257850 | Loss: 0.263840\n",
      "Iteration 257875 | Loss: 0.263839\n",
      "Iteration 257900 | Loss: 0.263838\n",
      "Iteration 257925 | Loss: 0.263837\n",
      "Iteration 257950 | Loss: 0.263836\n",
      "Iteration 257975 | Loss: 0.263835\n",
      "Iteration 258000 | Loss: 0.263834\n",
      "Iteration 258025 | Loss: 0.263833\n",
      "Iteration 258050 | Loss: 0.263832\n",
      "Iteration 258075 | Loss: 0.263831\n",
      "Iteration 258100 | Loss: 0.263830\n",
      "Iteration 258125 | Loss: 0.263829\n",
      "Iteration 258150 | Loss: 0.263828\n",
      "Iteration 258175 | Loss: 0.263827\n",
      "Iteration 258200 | Loss: 0.263826\n",
      "Iteration 258225 | Loss: 0.263825\n",
      "Iteration 258250 | Loss: 0.263824\n",
      "Iteration 258275 | Loss: 0.263823\n",
      "Iteration 258300 | Loss: 0.263822\n",
      "Iteration 258325 | Loss: 0.263821\n",
      "Iteration 258350 | Loss: 0.263820\n",
      "Iteration 258375 | Loss: 0.263819\n",
      "Iteration 258400 | Loss: 0.263818\n",
      "Iteration 258425 | Loss: 0.263817\n",
      "Iteration 258450 | Loss: 0.263816\n",
      "Iteration 258475 | Loss: 0.263815\n",
      "Iteration 258500 | Loss: 0.263814\n",
      "Iteration 258525 | Loss: 0.263813\n",
      "Iteration 258550 | Loss: 0.263812\n",
      "Iteration 258575 | Loss: 0.263811\n",
      "Iteration 258600 | Loss: 0.263810\n",
      "Iteration 258625 | Loss: 0.263809\n",
      "Iteration 258650 | Loss: 0.263808\n",
      "Iteration 258675 | Loss: 0.263807\n",
      "Iteration 258700 | Loss: 0.263806\n",
      "Iteration 258725 | Loss: 0.263805\n",
      "Iteration 258750 | Loss: 0.263804\n",
      "Iteration 258775 | Loss: 0.263803\n",
      "Iteration 258800 | Loss: 0.263802\n",
      "Iteration 258825 | Loss: 0.263801\n",
      "Iteration 258850 | Loss: 0.263800\n",
      "Iteration 258875 | Loss: 0.263799\n",
      "Iteration 258900 | Loss: 0.263798\n",
      "Iteration 258925 | Loss: 0.263797\n",
      "Iteration 258950 | Loss: 0.263796\n",
      "Iteration 258975 | Loss: 0.263795\n",
      "Iteration 259000 | Loss: 0.263794\n",
      "Iteration 259025 | Loss: 0.263793\n",
      "Iteration 259050 | Loss: 0.263792\n",
      "Iteration 259075 | Loss: 0.263791\n",
      "Iteration 259100 | Loss: 0.263790\n",
      "Iteration 259125 | Loss: 0.263789\n",
      "Iteration 259150 | Loss: 0.263788\n",
      "Iteration 259175 | Loss: 0.263787\n",
      "Iteration 259200 | Loss: 0.263786\n",
      "Iteration 259225 | Loss: 0.263785\n",
      "Iteration 259250 | Loss: 0.263784\n",
      "Iteration 259275 | Loss: 0.263783\n",
      "Iteration 259300 | Loss: 0.263783\n",
      "Iteration 259325 | Loss: 0.263782\n",
      "Iteration 259350 | Loss: 0.263781\n",
      "Iteration 259375 | Loss: 0.263780\n",
      "Iteration 259400 | Loss: 0.263779\n",
      "Iteration 259425 | Loss: 0.263778\n",
      "Iteration 259450 | Loss: 0.263777\n",
      "Iteration 259475 | Loss: 0.263776\n",
      "Iteration 259500 | Loss: 0.263775\n",
      "Iteration 259525 | Loss: 0.263774\n",
      "Iteration 259550 | Loss: 0.263773\n",
      "Iteration 259575 | Loss: 0.263772\n",
      "Iteration 259600 | Loss: 0.263771\n",
      "Iteration 259625 | Loss: 0.263770\n",
      "Iteration 259650 | Loss: 0.263769\n",
      "Iteration 259675 | Loss: 0.263768\n",
      "Iteration 259700 | Loss: 0.263767\n",
      "Iteration 259725 | Loss: 0.263766\n",
      "Iteration 259750 | Loss: 0.263765\n",
      "Iteration 259775 | Loss: 0.263765\n",
      "Iteration 259800 | Loss: 0.263764\n",
      "Iteration 259825 | Loss: 0.263763\n",
      "Iteration 259850 | Loss: 0.263762\n",
      "Iteration 259875 | Loss: 0.263761\n",
      "Iteration 259900 | Loss: 0.263760\n",
      "Iteration 259925 | Loss: 0.263759\n",
      "Iteration 259950 | Loss: 0.263758\n",
      "Iteration 259975 | Loss: 0.263757\n",
      "Iteration 260000 | Loss: 0.263756\n",
      "Iteration 260025 | Loss: 0.263755\n",
      "Iteration 260050 | Loss: 0.263754\n",
      "Iteration 260075 | Loss: 0.263753\n",
      "Iteration 260100 | Loss: 0.263752\n",
      "Iteration 260125 | Loss: 0.263752\n",
      "Iteration 260150 | Loss: 0.263751\n",
      "Iteration 260175 | Loss: 0.263750\n",
      "Iteration 260200 | Loss: 0.263749\n",
      "Iteration 260225 | Loss: 0.263748\n",
      "Iteration 260250 | Loss: 0.263747\n",
      "Iteration 260275 | Loss: 0.263746\n",
      "Iteration 260300 | Loss: 0.263745\n",
      "Iteration 260325 | Loss: 0.263744\n",
      "Iteration 260350 | Loss: 0.263743\n",
      "Iteration 260375 | Loss: 0.263742\n",
      "Iteration 260400 | Loss: 0.263741\n",
      "Iteration 260425 | Loss: 0.263741\n",
      "Iteration 260450 | Loss: 0.263740\n",
      "Iteration 260475 | Loss: 0.263739\n",
      "Iteration 260500 | Loss: 0.263738\n",
      "Iteration 260525 | Loss: 0.263737\n",
      "Iteration 260550 | Loss: 0.263736\n",
      "Iteration 260575 | Loss: 0.263735\n",
      "Iteration 260600 | Loss: 0.263734\n",
      "Iteration 260625 | Loss: 0.263733\n",
      "Iteration 260650 | Loss: 0.263732\n",
      "Iteration 260675 | Loss: 0.263731\n",
      "Iteration 260700 | Loss: 0.263731\n",
      "Iteration 260725 | Loss: 0.263730\n",
      "Iteration 260750 | Loss: 0.263729\n",
      "Iteration 260775 | Loss: 0.263728\n",
      "Iteration 260800 | Loss: 0.263727\n",
      "Iteration 260825 | Loss: 0.263726\n",
      "Iteration 260850 | Loss: 0.263725\n",
      "Iteration 260875 | Loss: 0.263724\n",
      "Iteration 260900 | Loss: 0.263723\n",
      "Iteration 260925 | Loss: 0.263722\n",
      "Iteration 260950 | Loss: 0.263722\n",
      "Iteration 260975 | Loss: 0.263721\n",
      "Iteration 261000 | Loss: 0.263720\n",
      "Iteration 261025 | Loss: 0.263719\n",
      "Iteration 261050 | Loss: 0.263718\n",
      "Iteration 261075 | Loss: 0.263717\n",
      "Iteration 261100 | Loss: 0.263716\n",
      "Iteration 261125 | Loss: 0.263715\n",
      "Iteration 261150 | Loss: 0.263714\n",
      "Iteration 261175 | Loss: 0.263714\n",
      "Iteration 261200 | Loss: 0.263713\n",
      "Iteration 261225 | Loss: 0.263712\n",
      "Iteration 261250 | Loss: 0.263711\n",
      "Iteration 261275 | Loss: 0.263710\n",
      "Iteration 261300 | Loss: 0.263709\n",
      "Iteration 261325 | Loss: 0.263708\n",
      "Iteration 261350 | Loss: 0.263707\n",
      "Iteration 261375 | Loss: 0.263707\n",
      "Iteration 261400 | Loss: 0.263706\n",
      "Iteration 261425 | Loss: 0.263705\n",
      "Iteration 261450 | Loss: 0.263704\n",
      "Iteration 261475 | Loss: 0.263703\n",
      "Iteration 261500 | Loss: 0.263702\n",
      "Iteration 261525 | Loss: 0.263701\n",
      "Iteration 261550 | Loss: 0.263700\n",
      "Iteration 261575 | Loss: 0.263700\n",
      "Iteration 261600 | Loss: 0.263699\n",
      "Iteration 261625 | Loss: 0.263698\n",
      "Iteration 261650 | Loss: 0.263697\n",
      "Iteration 261675 | Loss: 0.263696\n",
      "Iteration 261700 | Loss: 0.263695\n",
      "Iteration 261725 | Loss: 0.263694\n",
      "Iteration 261750 | Loss: 0.263693\n",
      "Iteration 261775 | Loss: 0.263693\n",
      "Iteration 261800 | Loss: 0.263692\n",
      "Iteration 261825 | Loss: 0.263691\n",
      "Iteration 261850 | Loss: 0.263690\n",
      "Iteration 261875 | Loss: 0.263689\n",
      "Iteration 261900 | Loss: 0.263688\n",
      "Iteration 261925 | Loss: 0.263687\n",
      "Iteration 261950 | Loss: 0.263687\n",
      "Iteration 261975 | Loss: 0.263686\n",
      "Iteration 262000 | Loss: 0.263685\n",
      "Iteration 262025 | Loss: 0.263684\n",
      "Iteration 262050 | Loss: 0.263683\n",
      "Iteration 262075 | Loss: 0.263682\n",
      "Iteration 262100 | Loss: 0.263681\n",
      "Iteration 262125 | Loss: 0.263681\n",
      "Iteration 262150 | Loss: 0.263680\n",
      "Iteration 262175 | Loss: 0.263679\n",
      "Iteration 262200 | Loss: 0.263678\n",
      "Iteration 262225 | Loss: 0.263677\n",
      "Iteration 262250 | Loss: 0.263676\n",
      "Iteration 262275 | Loss: 0.263676\n",
      "Iteration 262300 | Loss: 0.263675\n",
      "Iteration 262325 | Loss: 0.263674\n",
      "Iteration 262350 | Loss: 0.263673\n",
      "Iteration 262375 | Loss: 0.263672\n",
      "Iteration 262400 | Loss: 0.263671\n",
      "Iteration 262425 | Loss: 0.263671\n",
      "Iteration 262450 | Loss: 0.263670\n",
      "Iteration 262475 | Loss: 0.263669\n",
      "Iteration 262500 | Loss: 0.263668\n",
      "Iteration 262525 | Loss: 0.263667\n",
      "Iteration 262550 | Loss: 0.263666\n",
      "Iteration 262575 | Loss: 0.263665\n",
      "Iteration 262600 | Loss: 0.263665\n",
      "Iteration 262625 | Loss: 0.263664\n",
      "Iteration 262650 | Loss: 0.263663\n",
      "Iteration 262675 | Loss: 0.263662\n",
      "Iteration 262700 | Loss: 0.263661\n",
      "Iteration 262725 | Loss: 0.263660\n",
      "Iteration 262750 | Loss: 0.263660\n",
      "Iteration 262775 | Loss: 0.263659\n",
      "Iteration 262800 | Loss: 0.263658\n",
      "Iteration 262825 | Loss: 0.263657\n",
      "Iteration 262850 | Loss: 0.263656\n",
      "Iteration 262875 | Loss: 0.263656\n",
      "Iteration 262900 | Loss: 0.263655\n",
      "Iteration 262925 | Loss: 0.263654\n",
      "Iteration 262950 | Loss: 0.263653\n",
      "Iteration 262975 | Loss: 0.263652\n",
      "Iteration 263000 | Loss: 0.263651\n",
      "Iteration 263025 | Loss: 0.263651\n",
      "Iteration 263050 | Loss: 0.263650\n",
      "Iteration 263075 | Loss: 0.263649\n",
      "Iteration 263100 | Loss: 0.263648\n",
      "Iteration 263125 | Loss: 0.263647\n",
      "Iteration 263150 | Loss: 0.263647\n",
      "Iteration 263175 | Loss: 0.263646\n",
      "Iteration 263200 | Loss: 0.263645\n",
      "Iteration 263225 | Loss: 0.263644\n",
      "Iteration 263250 | Loss: 0.263643\n",
      "Iteration 263275 | Loss: 0.263642\n",
      "Iteration 263300 | Loss: 0.263642\n",
      "Iteration 263325 | Loss: 0.263641\n",
      "Iteration 263350 | Loss: 0.263640\n",
      "Iteration 263375 | Loss: 0.263639\n",
      "Iteration 263400 | Loss: 0.263638\n",
      "Iteration 263425 | Loss: 0.263638\n",
      "Iteration 263450 | Loss: 0.263637\n",
      "Iteration 263475 | Loss: 0.263636\n",
      "Iteration 263500 | Loss: 0.263635\n",
      "Iteration 263525 | Loss: 0.263634\n",
      "Iteration 263550 | Loss: 0.263634\n",
      "Iteration 263575 | Loss: 0.263633\n",
      "Iteration 263600 | Loss: 0.263632\n",
      "Iteration 263625 | Loss: 0.263631\n",
      "Iteration 263650 | Loss: 0.263630\n",
      "Iteration 263675 | Loss: 0.263630\n",
      "Iteration 263700 | Loss: 0.263629\n",
      "Iteration 263725 | Loss: 0.263628\n",
      "Iteration 263750 | Loss: 0.263627\n",
      "Iteration 263775 | Loss: 0.263626\n",
      "Iteration 263800 | Loss: 0.263626\n",
      "Iteration 263825 | Loss: 0.263625\n",
      "Iteration 263850 | Loss: 0.263624\n",
      "Iteration 263875 | Loss: 0.263623\n",
      "Iteration 263900 | Loss: 0.263622\n",
      "Iteration 263925 | Loss: 0.263622\n",
      "Iteration 263950 | Loss: 0.263621\n",
      "Iteration 263975 | Loss: 0.263620\n",
      "Iteration 264000 | Loss: 0.263619\n",
      "Iteration 264025 | Loss: 0.263619\n",
      "Iteration 264050 | Loss: 0.263618\n",
      "Iteration 264075 | Loss: 0.263617\n",
      "Iteration 264100 | Loss: 0.263616\n",
      "Iteration 264125 | Loss: 0.263615\n",
      "Iteration 264150 | Loss: 0.263615\n",
      "Iteration 264175 | Loss: 0.263614\n",
      "Iteration 264200 | Loss: 0.263613\n",
      "Iteration 264225 | Loss: 0.263612\n",
      "Iteration 264250 | Loss: 0.263611\n",
      "Iteration 264275 | Loss: 0.263611\n",
      "Iteration 264300 | Loss: 0.263610\n",
      "Iteration 264325 | Loss: 0.263609\n",
      "Iteration 264350 | Loss: 0.263608\n",
      "Iteration 264375 | Loss: 0.263608\n",
      "Iteration 264400 | Loss: 0.263607\n",
      "Iteration 264425 | Loss: 0.263606\n",
      "Iteration 264450 | Loss: 0.263605\n",
      "Iteration 264475 | Loss: 0.263605\n",
      "Iteration 264500 | Loss: 0.263604\n",
      "Iteration 264525 | Loss: 0.263603\n",
      "Iteration 264550 | Loss: 0.263602\n",
      "Iteration 264575 | Loss: 0.263601\n",
      "Iteration 264600 | Loss: 0.263601\n",
      "Iteration 264625 | Loss: 0.263600\n",
      "Iteration 264650 | Loss: 0.263599\n",
      "Iteration 264675 | Loss: 0.263598\n",
      "Iteration 264700 | Loss: 0.263598\n",
      "Iteration 264725 | Loss: 0.263597\n",
      "Iteration 264750 | Loss: 0.263596\n",
      "Iteration 264775 | Loss: 0.263595\n",
      "Iteration 264800 | Loss: 0.263595\n",
      "Iteration 264825 | Loss: 0.263594\n",
      "Iteration 264850 | Loss: 0.263593\n",
      "Iteration 264875 | Loss: 0.263592\n",
      "Iteration 264900 | Loss: 0.263592\n",
      "Iteration 264925 | Loss: 0.263591\n",
      "Iteration 264950 | Loss: 0.263590\n",
      "Iteration 264975 | Loss: 0.263589\n",
      "Iteration 265000 | Loss: 0.263588\n",
      "Iteration 265025 | Loss: 0.263588\n",
      "Iteration 265050 | Loss: 0.263587\n",
      "Iteration 265075 | Loss: 0.263586\n",
      "Iteration 265100 | Loss: 0.263585\n",
      "Iteration 265125 | Loss: 0.263585\n",
      "Iteration 265150 | Loss: 0.263584\n",
      "Iteration 265175 | Loss: 0.263583\n",
      "Iteration 265200 | Loss: 0.263582\n",
      "Iteration 265225 | Loss: 0.263582\n",
      "Iteration 265250 | Loss: 0.263581\n",
      "Iteration 265275 | Loss: 0.263580\n",
      "Iteration 265300 | Loss: 0.263579\n",
      "Iteration 265325 | Loss: 0.263579\n",
      "Iteration 265350 | Loss: 0.263578\n",
      "Iteration 265375 | Loss: 0.263577\n",
      "Iteration 265400 | Loss: 0.263577\n",
      "Iteration 265425 | Loss: 0.263576\n",
      "Iteration 265450 | Loss: 0.263575\n",
      "Iteration 265475 | Loss: 0.263574\n",
      "Iteration 265500 | Loss: 0.263574\n",
      "Iteration 265525 | Loss: 0.263573\n",
      "Iteration 265550 | Loss: 0.263572\n",
      "Iteration 265575 | Loss: 0.263571\n",
      "Iteration 265600 | Loss: 0.263571\n",
      "Iteration 265625 | Loss: 0.263570\n",
      "Iteration 265650 | Loss: 0.263569\n",
      "Iteration 265675 | Loss: 0.263568\n",
      "Iteration 265700 | Loss: 0.263568\n",
      "Iteration 265725 | Loss: 0.263567\n",
      "Iteration 265750 | Loss: 0.263566\n",
      "Iteration 265775 | Loss: 0.263565\n",
      "Iteration 265800 | Loss: 0.263565\n",
      "Iteration 265825 | Loss: 0.263564\n",
      "Iteration 265850 | Loss: 0.263563\n",
      "Iteration 265875 | Loss: 0.263563\n",
      "Iteration 265900 | Loss: 0.263562\n",
      "Iteration 265925 | Loss: 0.263561\n",
      "Iteration 265950 | Loss: 0.263560\n",
      "Iteration 265975 | Loss: 0.263560\n",
      "Iteration 266000 | Loss: 0.263559\n",
      "Iteration 266025 | Loss: 0.263558\n",
      "Iteration 266050 | Loss: 0.263557\n",
      "Iteration 266075 | Loss: 0.263557\n",
      "Iteration 266100 | Loss: 0.263556\n",
      "Iteration 266125 | Loss: 0.263555\n",
      "Iteration 266150 | Loss: 0.263555\n",
      "Iteration 266175 | Loss: 0.263554\n",
      "Iteration 266200 | Loss: 0.263553\n",
      "Iteration 266225 | Loss: 0.263552\n",
      "Iteration 266250 | Loss: 0.263552\n",
      "Iteration 266275 | Loss: 0.263551\n",
      "Iteration 266300 | Loss: 0.263550\n",
      "Iteration 266325 | Loss: 0.263550\n",
      "Iteration 266350 | Loss: 0.263549\n",
      "Iteration 266375 | Loss: 0.263548\n",
      "Iteration 266400 | Loss: 0.263547\n",
      "Iteration 266425 | Loss: 0.263547\n",
      "Iteration 266450 | Loss: 0.263546\n",
      "Iteration 266475 | Loss: 0.263545\n",
      "Iteration 266500 | Loss: 0.263545\n",
      "Iteration 266525 | Loss: 0.263544\n",
      "Iteration 266550 | Loss: 0.263543\n",
      "Iteration 266575 | Loss: 0.263542\n",
      "Iteration 266600 | Loss: 0.263542\n",
      "Iteration 266625 | Loss: 0.263541\n",
      "Iteration 266650 | Loss: 0.263540\n",
      "Iteration 266675 | Loss: 0.263540\n",
      "Iteration 266700 | Loss: 0.263539\n",
      "Iteration 266725 | Loss: 0.263538\n",
      "Iteration 266750 | Loss: 0.263538\n",
      "Iteration 266775 | Loss: 0.263537\n",
      "Iteration 266800 | Loss: 0.263536\n",
      "Iteration 266825 | Loss: 0.263535\n",
      "Iteration 266850 | Loss: 0.263535\n",
      "Iteration 266875 | Loss: 0.263534\n",
      "Iteration 266900 | Loss: 0.263533\n",
      "Iteration 266925 | Loss: 0.263533\n",
      "Iteration 266950 | Loss: 0.263532\n",
      "Iteration 266975 | Loss: 0.263531\n",
      "Iteration 267000 | Loss: 0.263531\n",
      "Iteration 267025 | Loss: 0.263530\n",
      "Iteration 267050 | Loss: 0.263529\n",
      "Iteration 267075 | Loss: 0.263528\n",
      "Iteration 267100 | Loss: 0.263528\n",
      "Iteration 267125 | Loss: 0.263527\n",
      "Iteration 267150 | Loss: 0.263526\n",
      "Iteration 267175 | Loss: 0.263526\n",
      "Iteration 267200 | Loss: 0.263525\n",
      "Iteration 267225 | Loss: 0.263524\n",
      "Iteration 267250 | Loss: 0.263524\n",
      "Iteration 267275 | Loss: 0.263523\n",
      "Iteration 267300 | Loss: 0.263522\n",
      "Iteration 267325 | Loss: 0.263522\n",
      "Iteration 267350 | Loss: 0.263521\n",
      "Iteration 267375 | Loss: 0.263520\n",
      "Iteration 267400 | Loss: 0.263519\n",
      "Iteration 267425 | Loss: 0.263519\n",
      "Iteration 267450 | Loss: 0.263518\n",
      "Iteration 267475 | Loss: 0.263517\n",
      "Iteration 267500 | Loss: 0.263517\n",
      "Iteration 267525 | Loss: 0.263516\n",
      "Iteration 267550 | Loss: 0.263515\n",
      "Iteration 267575 | Loss: 0.263515\n",
      "Iteration 267600 | Loss: 0.263514\n",
      "Iteration 267625 | Loss: 0.263513\n",
      "Iteration 267650 | Loss: 0.263513\n",
      "Iteration 267675 | Loss: 0.263512\n",
      "Iteration 267700 | Loss: 0.263511\n",
      "Iteration 267725 | Loss: 0.263511\n",
      "Iteration 267750 | Loss: 0.263510\n",
      "Iteration 267775 | Loss: 0.263509\n",
      "Iteration 267800 | Loss: 0.263509\n",
      "Iteration 267825 | Loss: 0.263508\n",
      "Iteration 267850 | Loss: 0.263507\n",
      "Iteration 267875 | Loss: 0.263507\n",
      "Iteration 267900 | Loss: 0.263506\n",
      "Iteration 267925 | Loss: 0.263505\n",
      "Iteration 267950 | Loss: 0.263505\n",
      "Iteration 267975 | Loss: 0.263504\n",
      "Iteration 268000 | Loss: 0.263503\n",
      "Iteration 268025 | Loss: 0.263503\n",
      "Iteration 268050 | Loss: 0.263502\n",
      "Iteration 268075 | Loss: 0.263501\n",
      "Iteration 268100 | Loss: 0.263501\n",
      "Iteration 268125 | Loss: 0.263500\n",
      "Iteration 268150 | Loss: 0.263499\n",
      "Iteration 268175 | Loss: 0.263499\n",
      "Iteration 268200 | Loss: 0.263498\n",
      "Iteration 268225 | Loss: 0.263497\n",
      "Iteration 268250 | Loss: 0.263497\n",
      "Iteration 268275 | Loss: 0.263496\n",
      "Iteration 268300 | Loss: 0.263495\n",
      "Iteration 268325 | Loss: 0.263495\n",
      "Iteration 268350 | Loss: 0.263494\n",
      "Iteration 268375 | Loss: 0.263493\n",
      "Iteration 268400 | Loss: 0.263493\n",
      "Iteration 268425 | Loss: 0.263492\n",
      "Iteration 268450 | Loss: 0.263491\n",
      "Iteration 268475 | Loss: 0.263491\n",
      "Iteration 268500 | Loss: 0.263490\n",
      "Iteration 268525 | Loss: 0.263489\n",
      "Iteration 268550 | Loss: 0.263489\n",
      "Iteration 268575 | Loss: 0.263488\n",
      "Iteration 268600 | Loss: 0.263487\n",
      "Iteration 268625 | Loss: 0.263487\n",
      "Iteration 268650 | Loss: 0.263486\n",
      "Iteration 268675 | Loss: 0.263486\n",
      "Iteration 268700 | Loss: 0.263485\n",
      "Iteration 268725 | Loss: 0.263484\n",
      "Iteration 268750 | Loss: 0.263484\n",
      "Iteration 268775 | Loss: 0.263483\n",
      "Iteration 268800 | Loss: 0.263482\n",
      "Iteration 268825 | Loss: 0.263482\n",
      "Iteration 268850 | Loss: 0.263481\n",
      "Iteration 268875 | Loss: 0.263480\n",
      "Iteration 268900 | Loss: 0.263480\n",
      "Iteration 268925 | Loss: 0.263479\n",
      "Iteration 268950 | Loss: 0.263478\n",
      "Iteration 268975 | Loss: 0.263478\n",
      "Iteration 269000 | Loss: 0.263477\n",
      "Iteration 269025 | Loss: 0.263477\n",
      "Iteration 269050 | Loss: 0.263476\n",
      "Iteration 269075 | Loss: 0.263475\n",
      "Iteration 269100 | Loss: 0.263475\n",
      "Iteration 269125 | Loss: 0.263474\n",
      "Iteration 269150 | Loss: 0.263473\n",
      "Iteration 269175 | Loss: 0.263473\n",
      "Iteration 269200 | Loss: 0.263472\n",
      "Iteration 269225 | Loss: 0.263471\n",
      "Iteration 269250 | Loss: 0.263471\n",
      "Iteration 269275 | Loss: 0.263470\n",
      "Iteration 269300 | Loss: 0.263470\n",
      "Iteration 269325 | Loss: 0.263469\n",
      "Iteration 269350 | Loss: 0.263468\n",
      "Iteration 269375 | Loss: 0.263468\n",
      "Iteration 269400 | Loss: 0.263467\n",
      "Iteration 269425 | Loss: 0.263466\n",
      "Iteration 269450 | Loss: 0.263466\n",
      "Iteration 269475 | Loss: 0.263465\n",
      "Iteration 269500 | Loss: 0.263464\n",
      "Iteration 269525 | Loss: 0.263464\n",
      "Iteration 269550 | Loss: 0.263463\n",
      "Iteration 269575 | Loss: 0.263463\n",
      "Iteration 269600 | Loss: 0.263462\n",
      "Iteration 269625 | Loss: 0.263461\n",
      "Iteration 269650 | Loss: 0.263461\n",
      "Iteration 269675 | Loss: 0.263460\n",
      "Iteration 269700 | Loss: 0.263460\n",
      "Iteration 269725 | Loss: 0.263459\n",
      "Iteration 269750 | Loss: 0.263458\n",
      "Iteration 269775 | Loss: 0.263458\n",
      "Iteration 269800 | Loss: 0.263457\n",
      "Iteration 269825 | Loss: 0.263456\n",
      "Iteration 269850 | Loss: 0.263456\n",
      "Iteration 269875 | Loss: 0.263455\n",
      "Iteration 269900 | Loss: 0.263455\n",
      "Iteration 269925 | Loss: 0.263454\n",
      "Iteration 269950 | Loss: 0.263453\n",
      "Iteration 269975 | Loss: 0.263453\n",
      "Iteration 270000 | Loss: 0.263452\n",
      "Iteration 270025 | Loss: 0.263451\n",
      "Iteration 270050 | Loss: 0.263451\n",
      "Iteration 270075 | Loss: 0.263450\n",
      "Iteration 270100 | Loss: 0.263450\n",
      "Iteration 270125 | Loss: 0.263449\n",
      "Iteration 270150 | Loss: 0.263448\n",
      "Iteration 270175 | Loss: 0.263448\n",
      "Iteration 270200 | Loss: 0.263447\n",
      "Iteration 270225 | Loss: 0.263447\n",
      "Iteration 270250 | Loss: 0.263446\n",
      "Iteration 270275 | Loss: 0.263445\n",
      "Iteration 270300 | Loss: 0.263445\n",
      "Iteration 270325 | Loss: 0.263444\n",
      "Iteration 270350 | Loss: 0.263444\n",
      "Iteration 270375 | Loss: 0.263443\n",
      "Iteration 270400 | Loss: 0.263442\n",
      "Iteration 270425 | Loss: 0.263442\n",
      "Iteration 270450 | Loss: 0.263441\n",
      "Iteration 270475 | Loss: 0.263441\n",
      "Iteration 270500 | Loss: 0.263440\n",
      "Iteration 270525 | Loss: 0.263439\n",
      "Iteration 270550 | Loss: 0.263439\n",
      "Iteration 270575 | Loss: 0.263438\n",
      "Iteration 270600 | Loss: 0.263438\n",
      "Iteration 270625 | Loss: 0.263437\n",
      "Iteration 270650 | Loss: 0.263436\n",
      "Iteration 270675 | Loss: 0.263436\n",
      "Iteration 270700 | Loss: 0.263435\n",
      "Iteration 270725 | Loss: 0.263435\n",
      "Iteration 270750 | Loss: 0.263434\n",
      "Iteration 270775 | Loss: 0.263433\n",
      "Iteration 270800 | Loss: 0.263433\n",
      "Iteration 270825 | Loss: 0.263432\n",
      "Iteration 270850 | Loss: 0.263432\n",
      "Iteration 270875 | Loss: 0.263431\n",
      "Iteration 270900 | Loss: 0.263430\n",
      "Iteration 270925 | Loss: 0.263430\n",
      "Iteration 270950 | Loss: 0.263429\n",
      "Iteration 270975 | Loss: 0.263429\n",
      "Iteration 271000 | Loss: 0.263428\n",
      "Iteration 271025 | Loss: 0.263427\n",
      "Iteration 271050 | Loss: 0.263427\n",
      "Iteration 271075 | Loss: 0.263426\n",
      "Iteration 271100 | Loss: 0.263426\n",
      "Iteration 271125 | Loss: 0.263425\n",
      "Iteration 271150 | Loss: 0.263425\n",
      "Iteration 271175 | Loss: 0.263424\n",
      "Iteration 271200 | Loss: 0.263423\n",
      "Iteration 271225 | Loss: 0.263423\n",
      "Iteration 271250 | Loss: 0.263422\n",
      "Iteration 271275 | Loss: 0.263422\n",
      "Iteration 271300 | Loss: 0.263421\n",
      "Iteration 271325 | Loss: 0.263420\n",
      "Iteration 271350 | Loss: 0.263420\n",
      "Iteration 271375 | Loss: 0.263419\n",
      "Iteration 271400 | Loss: 0.263419\n",
      "Iteration 271425 | Loss: 0.263418\n",
      "Iteration 271450 | Loss: 0.263418\n",
      "Iteration 271475 | Loss: 0.263417\n",
      "Iteration 271500 | Loss: 0.263416\n",
      "Iteration 271525 | Loss: 0.263416\n",
      "Iteration 271550 | Loss: 0.263415\n",
      "Iteration 271575 | Loss: 0.263415\n",
      "Iteration 271600 | Loss: 0.263414\n",
      "Iteration 271625 | Loss: 0.263413\n",
      "Iteration 271650 | Loss: 0.263413\n",
      "Iteration 271675 | Loss: 0.263412\n",
      "Iteration 271700 | Loss: 0.263412\n",
      "Iteration 271725 | Loss: 0.263411\n",
      "Iteration 271750 | Loss: 0.263411\n",
      "Iteration 271775 | Loss: 0.263410\n",
      "Iteration 271800 | Loss: 0.263409\n",
      "Iteration 271825 | Loss: 0.263409\n",
      "Iteration 271850 | Loss: 0.263408\n",
      "Iteration 271875 | Loss: 0.263408\n",
      "Iteration 271900 | Loss: 0.263407\n",
      "Iteration 271925 | Loss: 0.263407\n",
      "Iteration 271950 | Loss: 0.263406\n",
      "Iteration 271975 | Loss: 0.263406\n",
      "Iteration 272000 | Loss: 0.263405\n",
      "Iteration 272025 | Loss: 0.263404\n",
      "Iteration 272050 | Loss: 0.263404\n",
      "Iteration 272075 | Loss: 0.263403\n",
      "Iteration 272100 | Loss: 0.263403\n",
      "Iteration 272125 | Loss: 0.263402\n",
      "Iteration 272150 | Loss: 0.263402\n",
      "Iteration 272175 | Loss: 0.263401\n",
      "Iteration 272200 | Loss: 0.263400\n",
      "Iteration 272225 | Loss: 0.263400\n",
      "Iteration 272250 | Loss: 0.263399\n",
      "Iteration 272275 | Loss: 0.263399\n",
      "Iteration 272300 | Loss: 0.263398\n",
      "Iteration 272325 | Loss: 0.263398\n",
      "Iteration 272350 | Loss: 0.263397\n",
      "Iteration 272375 | Loss: 0.263397\n",
      "Iteration 272400 | Loss: 0.263396\n",
      "Iteration 272425 | Loss: 0.263395\n",
      "Iteration 272450 | Loss: 0.263395\n",
      "Iteration 272475 | Loss: 0.263394\n",
      "Iteration 272500 | Loss: 0.263394\n",
      "Iteration 272525 | Loss: 0.263393\n",
      "Iteration 272550 | Loss: 0.263393\n",
      "Iteration 272575 | Loss: 0.263392\n",
      "Iteration 272600 | Loss: 0.263392\n",
      "Iteration 272625 | Loss: 0.263391\n",
      "Iteration 272650 | Loss: 0.263390\n",
      "Iteration 272675 | Loss: 0.263390\n",
      "Iteration 272700 | Loss: 0.263389\n",
      "Iteration 272725 | Loss: 0.263389\n",
      "Iteration 272750 | Loss: 0.263388\n",
      "Iteration 272775 | Loss: 0.263388\n",
      "Iteration 272800 | Loss: 0.263387\n",
      "Iteration 272825 | Loss: 0.263387\n",
      "Iteration 272850 | Loss: 0.263386\n",
      "Iteration 272875 | Loss: 0.263386\n",
      "Iteration 272900 | Loss: 0.263385\n",
      "Iteration 272925 | Loss: 0.263384\n",
      "Iteration 272950 | Loss: 0.263384\n",
      "Iteration 272975 | Loss: 0.263383\n",
      "Iteration 273000 | Loss: 0.263383\n",
      "Iteration 273025 | Loss: 0.263382\n",
      "Iteration 273050 | Loss: 0.263382\n",
      "Iteration 273075 | Loss: 0.263381\n",
      "Iteration 273100 | Loss: 0.263381\n",
      "Iteration 273125 | Loss: 0.263380\n",
      "Iteration 273150 | Loss: 0.263380\n",
      "Iteration 273175 | Loss: 0.263379\n",
      "Iteration 273200 | Loss: 0.263378\n",
      "Iteration 273225 | Loss: 0.263378\n",
      "Iteration 273250 | Loss: 0.263377\n",
      "Iteration 273275 | Loss: 0.263377\n",
      "Iteration 273300 | Loss: 0.263376\n",
      "Iteration 273325 | Loss: 0.263376\n",
      "Iteration 273350 | Loss: 0.263375\n",
      "Iteration 273375 | Loss: 0.263375\n",
      "Iteration 273400 | Loss: 0.263374\n",
      "Iteration 273425 | Loss: 0.263374\n",
      "Iteration 273450 | Loss: 0.263373\n",
      "Iteration 273475 | Loss: 0.263373\n",
      "Iteration 273500 | Loss: 0.263372\n",
      "Iteration 273525 | Loss: 0.263372\n",
      "Iteration 273550 | Loss: 0.263371\n",
      "Iteration 273575 | Loss: 0.263370\n",
      "Iteration 273600 | Loss: 0.263370\n",
      "Iteration 273625 | Loss: 0.263369\n",
      "Iteration 273650 | Loss: 0.263369\n",
      "Iteration 273675 | Loss: 0.263368\n",
      "Iteration 273700 | Loss: 0.263368\n",
      "Iteration 273725 | Loss: 0.263367\n",
      "Iteration 273750 | Loss: 0.263367\n",
      "Iteration 273775 | Loss: 0.263366\n",
      "Iteration 273800 | Loss: 0.263366\n",
      "Iteration 273825 | Loss: 0.263365\n",
      "Iteration 273850 | Loss: 0.263365\n",
      "Iteration 273875 | Loss: 0.263364\n",
      "Iteration 273900 | Loss: 0.263364\n",
      "Iteration 273925 | Loss: 0.263363\n",
      "Iteration 273950 | Loss: 0.263363\n",
      "Iteration 273975 | Loss: 0.263362\n",
      "Iteration 274000 | Loss: 0.263362\n",
      "Iteration 274025 | Loss: 0.263361\n",
      "Iteration 274050 | Loss: 0.263360\n",
      "Iteration 274075 | Loss: 0.263360\n",
      "Iteration 274100 | Loss: 0.263359\n",
      "Iteration 274125 | Loss: 0.263359\n",
      "Iteration 274150 | Loss: 0.263358\n",
      "Iteration 274175 | Loss: 0.263358\n",
      "Iteration 274200 | Loss: 0.263357\n",
      "Iteration 274225 | Loss: 0.263357\n",
      "Iteration 274250 | Loss: 0.263356\n",
      "Iteration 274275 | Loss: 0.263356\n",
      "Iteration 274300 | Loss: 0.263355\n",
      "Iteration 274325 | Loss: 0.263355\n",
      "Iteration 274350 | Loss: 0.263354\n",
      "Iteration 274375 | Loss: 0.263354\n",
      "Iteration 274400 | Loss: 0.263353\n",
      "Iteration 274425 | Loss: 0.263353\n",
      "Iteration 274450 | Loss: 0.263352\n",
      "Iteration 274475 | Loss: 0.263352\n",
      "Iteration 274500 | Loss: 0.263351\n",
      "Iteration 274525 | Loss: 0.263351\n",
      "Iteration 274550 | Loss: 0.263350\n",
      "Iteration 274575 | Loss: 0.263350\n",
      "Iteration 274600 | Loss: 0.263349\n",
      "Iteration 274625 | Loss: 0.263349\n",
      "Iteration 274650 | Loss: 0.263348\n",
      "Iteration 274675 | Loss: 0.263348\n",
      "Iteration 274700 | Loss: 0.263347\n",
      "Iteration 274725 | Loss: 0.263347\n",
      "Iteration 274750 | Loss: 0.263346\n",
      "Iteration 274775 | Loss: 0.263346\n",
      "Iteration 274800 | Loss: 0.263345\n",
      "Iteration 274825 | Loss: 0.263345\n",
      "Iteration 274850 | Loss: 0.263344\n",
      "Iteration 274875 | Loss: 0.263344\n",
      "Iteration 274900 | Loss: 0.263343\n",
      "Iteration 274925 | Loss: 0.263343\n",
      "Iteration 274950 | Loss: 0.263342\n",
      "Iteration 274975 | Loss: 0.263342\n",
      "Iteration 275000 | Loss: 0.263341\n",
      "Iteration 275025 | Loss: 0.263341\n",
      "Iteration 275050 | Loss: 0.263340\n",
      "Iteration 275075 | Loss: 0.263340\n",
      "Iteration 275100 | Loss: 0.263339\n",
      "Iteration 275125 | Loss: 0.263339\n",
      "Iteration 275150 | Loss: 0.263338\n",
      "Iteration 275175 | Loss: 0.263338\n",
      "Iteration 275200 | Loss: 0.263337\n",
      "Iteration 275225 | Loss: 0.263337\n",
      "Iteration 275250 | Loss: 0.263336\n",
      "Iteration 275275 | Loss: 0.263336\n",
      "Iteration 275300 | Loss: 0.263335\n",
      "Iteration 275325 | Loss: 0.263335\n",
      "Iteration 275350 | Loss: 0.263334\n",
      "Iteration 275375 | Loss: 0.263334\n",
      "Iteration 275400 | Loss: 0.263333\n",
      "Iteration 275425 | Loss: 0.263333\n",
      "Iteration 275450 | Loss: 0.263332\n",
      "Iteration 275475 | Loss: 0.263332\n",
      "Iteration 275500 | Loss: 0.263331\n",
      "Iteration 275525 | Loss: 0.263331\n",
      "Iteration 275550 | Loss: 0.263330\n",
      "Iteration 275575 | Loss: 0.263330\n",
      "Iteration 275600 | Loss: 0.263329\n",
      "Iteration 275625 | Loss: 0.263329\n",
      "Iteration 275650 | Loss: 0.263328\n",
      "Iteration 275675 | Loss: 0.263328\n",
      "Iteration 275700 | Loss: 0.263327\n",
      "Iteration 275725 | Loss: 0.263327\n",
      "Iteration 275750 | Loss: 0.263326\n",
      "Iteration 275775 | Loss: 0.263326\n",
      "Iteration 275800 | Loss: 0.263325\n",
      "Iteration 275825 | Loss: 0.263325\n",
      "Iteration 275850 | Loss: 0.263324\n",
      "Iteration 275875 | Loss: 0.263324\n",
      "Iteration 275900 | Loss: 0.263323\n",
      "Iteration 275925 | Loss: 0.263323\n",
      "Iteration 275950 | Loss: 0.263323\n",
      "Iteration 275975 | Loss: 0.263322\n",
      "Iteration 276000 | Loss: 0.263322\n",
      "Iteration 276025 | Loss: 0.263321\n",
      "Iteration 276050 | Loss: 0.263321\n",
      "Iteration 276075 | Loss: 0.263320\n",
      "Iteration 276100 | Loss: 0.263320\n",
      "Iteration 276125 | Loss: 0.263319\n",
      "Iteration 276150 | Loss: 0.263319\n",
      "Iteration 276175 | Loss: 0.263318\n",
      "Iteration 276200 | Loss: 0.263318\n",
      "Iteration 276225 | Loss: 0.263317\n",
      "Iteration 276250 | Loss: 0.263317\n",
      "Iteration 276275 | Loss: 0.263316\n",
      "Iteration 276300 | Loss: 0.263316\n",
      "Iteration 276325 | Loss: 0.263315\n",
      "Iteration 276350 | Loss: 0.263315\n",
      "Iteration 276375 | Loss: 0.263314\n",
      "Iteration 276400 | Loss: 0.263314\n",
      "Iteration 276425 | Loss: 0.263313\n",
      "Iteration 276450 | Loss: 0.263313\n",
      "Iteration 276475 | Loss: 0.263313\n",
      "Iteration 276500 | Loss: 0.263312\n",
      "Iteration 276525 | Loss: 0.263312\n",
      "Iteration 276550 | Loss: 0.263311\n",
      "Iteration 276575 | Loss: 0.263311\n",
      "Iteration 276600 | Loss: 0.263310\n",
      "Iteration 276625 | Loss: 0.263310\n",
      "Iteration 276650 | Loss: 0.263309\n",
      "Iteration 276675 | Loss: 0.263309\n",
      "Iteration 276700 | Loss: 0.263308\n",
      "Iteration 276725 | Loss: 0.263308\n",
      "Iteration 276750 | Loss: 0.263307\n",
      "Iteration 276775 | Loss: 0.263307\n",
      "Iteration 276800 | Loss: 0.263306\n",
      "Iteration 276825 | Loss: 0.263306\n",
      "Iteration 276850 | Loss: 0.263306\n",
      "Iteration 276875 | Loss: 0.263305\n",
      "Iteration 276900 | Loss: 0.263305\n",
      "Iteration 276925 | Loss: 0.263304\n",
      "Iteration 276950 | Loss: 0.263304\n",
      "Iteration 276975 | Loss: 0.263303\n",
      "Iteration 277000 | Loss: 0.263303\n",
      "Iteration 277025 | Loss: 0.263302\n",
      "Iteration 277050 | Loss: 0.263302\n",
      "Iteration 277075 | Loss: 0.263301\n",
      "Iteration 277100 | Loss: 0.263301\n",
      "Iteration 277125 | Loss: 0.263300\n",
      "Iteration 277150 | Loss: 0.263300\n",
      "Iteration 277175 | Loss: 0.263300\n",
      "Iteration 277200 | Loss: 0.263299\n",
      "Iteration 277225 | Loss: 0.263299\n",
      "Iteration 277250 | Loss: 0.263298\n",
      "Iteration 277275 | Loss: 0.263298\n",
      "Iteration 277300 | Loss: 0.263297\n",
      "Iteration 277325 | Loss: 0.263297\n",
      "Iteration 277350 | Loss: 0.263296\n",
      "Iteration 277375 | Loss: 0.263296\n",
      "Iteration 277400 | Loss: 0.263295\n",
      "Iteration 277425 | Loss: 0.263295\n",
      "Iteration 277450 | Loss: 0.263295\n",
      "Iteration 277475 | Loss: 0.263294\n",
      "Iteration 277500 | Loss: 0.263294\n",
      "Iteration 277525 | Loss: 0.263293\n",
      "Iteration 277550 | Loss: 0.263293\n",
      "Iteration 277575 | Loss: 0.263292\n",
      "Iteration 277600 | Loss: 0.263292\n",
      "Iteration 277625 | Loss: 0.263291\n",
      "Iteration 277650 | Loss: 0.263291\n",
      "Iteration 277675 | Loss: 0.263291\n",
      "Iteration 277700 | Loss: 0.263290\n",
      "Iteration 277725 | Loss: 0.263290\n",
      "Iteration 277750 | Loss: 0.263289\n",
      "Iteration 277775 | Loss: 0.263289\n",
      "Iteration 277800 | Loss: 0.263288\n",
      "Iteration 277825 | Loss: 0.263288\n",
      "Iteration 277850 | Loss: 0.263287\n",
      "Iteration 277875 | Loss: 0.263287\n",
      "Iteration 277900 | Loss: 0.263287\n",
      "Iteration 277925 | Loss: 0.263286\n",
      "Iteration 277950 | Loss: 0.263286\n",
      "Iteration 277975 | Loss: 0.263285\n",
      "Iteration 278000 | Loss: 0.263285\n",
      "Iteration 278025 | Loss: 0.263284\n",
      "Iteration 278050 | Loss: 0.263284\n",
      "Iteration 278075 | Loss: 0.263283\n",
      "Iteration 278100 | Loss: 0.263283\n",
      "Iteration 278125 | Loss: 0.263283\n",
      "Iteration 278150 | Loss: 0.263282\n",
      "Iteration 278175 | Loss: 0.263282\n",
      "Iteration 278200 | Loss: 0.263281\n",
      "Iteration 278225 | Loss: 0.263281\n",
      "Iteration 278250 | Loss: 0.263280\n",
      "Iteration 278275 | Loss: 0.263280\n",
      "Iteration 278300 | Loss: 0.263279\n",
      "Iteration 278325 | Loss: 0.263279\n",
      "Iteration 278350 | Loss: 0.263279\n",
      "Iteration 278375 | Loss: 0.263278\n",
      "Iteration 278400 | Loss: 0.263278\n",
      "Iteration 278425 | Loss: 0.263277\n",
      "Iteration 278450 | Loss: 0.263277\n",
      "Iteration 278475 | Loss: 0.263276\n",
      "Iteration 278500 | Loss: 0.263276\n",
      "Iteration 278525 | Loss: 0.263276\n",
      "Iteration 278550 | Loss: 0.263275\n",
      "Iteration 278575 | Loss: 0.263275\n",
      "Iteration 278600 | Loss: 0.263274\n",
      "Iteration 278625 | Loss: 0.263274\n",
      "Iteration 278650 | Loss: 0.263273\n",
      "Iteration 278675 | Loss: 0.263273\n",
      "Iteration 278700 | Loss: 0.263273\n",
      "Iteration 278725 | Loss: 0.263272\n",
      "Iteration 278750 | Loss: 0.263272\n",
      "Iteration 278775 | Loss: 0.263271\n",
      "Iteration 278800 | Loss: 0.263271\n",
      "Iteration 278825 | Loss: 0.263270\n",
      "Iteration 278850 | Loss: 0.263270\n",
      "Iteration 278875 | Loss: 0.263270\n",
      "Iteration 278900 | Loss: 0.263269\n",
      "Iteration 278925 | Loss: 0.263269\n",
      "Iteration 278950 | Loss: 0.263268\n",
      "Iteration 278975 | Loss: 0.263268\n",
      "Iteration 279000 | Loss: 0.263267\n",
      "Iteration 279025 | Loss: 0.263267\n",
      "Iteration 279050 | Loss: 0.263267\n",
      "Iteration 279075 | Loss: 0.263266\n",
      "Iteration 279100 | Loss: 0.263266\n",
      "Iteration 279125 | Loss: 0.263265\n",
      "Iteration 279150 | Loss: 0.263265\n",
      "Iteration 279175 | Loss: 0.263265\n",
      "Iteration 279200 | Loss: 0.263264\n",
      "Iteration 279225 | Loss: 0.263264\n",
      "Iteration 279250 | Loss: 0.263263\n",
      "Iteration 279275 | Loss: 0.263263\n",
      "Iteration 279300 | Loss: 0.263262\n",
      "Iteration 279325 | Loss: 0.263262\n",
      "Iteration 279350 | Loss: 0.263262\n",
      "Iteration 279375 | Loss: 0.263261\n",
      "Iteration 279400 | Loss: 0.263261\n",
      "Iteration 279425 | Loss: 0.263260\n",
      "Iteration 279450 | Loss: 0.263260\n",
      "Iteration 279475 | Loss: 0.263259\n",
      "Iteration 279500 | Loss: 0.263259\n",
      "Iteration 279525 | Loss: 0.263259\n",
      "Iteration 279550 | Loss: 0.263258\n",
      "Iteration 279575 | Loss: 0.263258\n",
      "Iteration 279600 | Loss: 0.263257\n",
      "Iteration 279625 | Loss: 0.263257\n",
      "Iteration 279650 | Loss: 0.263257\n",
      "Iteration 279675 | Loss: 0.263256\n",
      "Iteration 279700 | Loss: 0.263256\n",
      "Iteration 279725 | Loss: 0.263255\n",
      "Iteration 279750 | Loss: 0.263255\n",
      "Iteration 279775 | Loss: 0.263255\n",
      "Iteration 279800 | Loss: 0.263254\n",
      "Iteration 279825 | Loss: 0.263254\n",
      "Iteration 279850 | Loss: 0.263253\n",
      "Iteration 279875 | Loss: 0.263253\n",
      "Iteration 279900 | Loss: 0.263252\n",
      "Iteration 279925 | Loss: 0.263252\n",
      "Iteration 279950 | Loss: 0.263252\n",
      "Iteration 279975 | Loss: 0.263251\n",
      "Iteration 280000 | Loss: 0.263251\n",
      "Iteration 280025 | Loss: 0.263250\n",
      "Iteration 280050 | Loss: 0.263250\n",
      "Iteration 280075 | Loss: 0.263250\n",
      "Iteration 280100 | Loss: 0.263249\n",
      "Iteration 280125 | Loss: 0.263249\n",
      "Iteration 280150 | Loss: 0.263248\n",
      "Iteration 280175 | Loss: 0.263248\n",
      "Iteration 280200 | Loss: 0.263248\n",
      "Iteration 280225 | Loss: 0.263247\n",
      "Iteration 280250 | Loss: 0.263247\n",
      "Iteration 280275 | Loss: 0.263246\n",
      "Iteration 280300 | Loss: 0.263246\n",
      "Iteration 280325 | Loss: 0.263246\n",
      "Iteration 280350 | Loss: 0.263245\n",
      "Iteration 280375 | Loss: 0.263245\n",
      "Iteration 280400 | Loss: 0.263244\n",
      "Iteration 280425 | Loss: 0.263244\n",
      "Iteration 280450 | Loss: 0.263244\n",
      "Iteration 280475 | Loss: 0.263243\n",
      "Iteration 280500 | Loss: 0.263243\n",
      "Iteration 280525 | Loss: 0.263242\n",
      "Iteration 280550 | Loss: 0.263242\n",
      "Iteration 280575 | Loss: 0.263242\n",
      "Iteration 280600 | Loss: 0.263241\n",
      "Iteration 280625 | Loss: 0.263241\n",
      "Iteration 280650 | Loss: 0.263240\n",
      "Iteration 280675 | Loss: 0.263240\n",
      "Iteration 280700 | Loss: 0.263240\n",
      "Iteration 280725 | Loss: 0.263239\n",
      "Iteration 280750 | Loss: 0.263239\n",
      "Iteration 280775 | Loss: 0.263238\n",
      "Iteration 280800 | Loss: 0.263238\n",
      "Iteration 280825 | Loss: 0.263238\n",
      "Iteration 280850 | Loss: 0.263237\n",
      "Iteration 280875 | Loss: 0.263237\n",
      "Iteration 280900 | Loss: 0.263237\n",
      "Iteration 280925 | Loss: 0.263236\n",
      "Iteration 280950 | Loss: 0.263236\n",
      "Iteration 280975 | Loss: 0.263235\n",
      "Iteration 281000 | Loss: 0.263235\n",
      "Iteration 281025 | Loss: 0.263235\n",
      "Iteration 281050 | Loss: 0.263234\n",
      "Iteration 281075 | Loss: 0.263234\n",
      "Iteration 281100 | Loss: 0.263233\n",
      "Iteration 281125 | Loss: 0.263233\n",
      "Iteration 281150 | Loss: 0.263233\n",
      "Iteration 281175 | Loss: 0.263232\n",
      "Iteration 281200 | Loss: 0.263232\n",
      "Iteration 281225 | Loss: 0.263231\n",
      "Iteration 281250 | Loss: 0.263231\n",
      "Iteration 281275 | Loss: 0.263231\n",
      "Iteration 281300 | Loss: 0.263230\n",
      "Iteration 281325 | Loss: 0.263230\n",
      "Iteration 281350 | Loss: 0.263230\n",
      "Iteration 281375 | Loss: 0.263229\n",
      "Iteration 281400 | Loss: 0.263229\n",
      "Iteration 281425 | Loss: 0.263228\n",
      "Iteration 281450 | Loss: 0.263228\n",
      "Iteration 281475 | Loss: 0.263228\n",
      "Iteration 281500 | Loss: 0.263227\n",
      "Iteration 281525 | Loss: 0.263227\n",
      "Iteration 281550 | Loss: 0.263226\n",
      "Iteration 281575 | Loss: 0.263226\n",
      "Iteration 281600 | Loss: 0.263226\n",
      "Iteration 281625 | Loss: 0.263225\n",
      "Iteration 281650 | Loss: 0.263225\n",
      "Iteration 281675 | Loss: 0.263225\n",
      "Iteration 281700 | Loss: 0.263224\n",
      "Iteration 281725 | Loss: 0.263224\n",
      "Iteration 281750 | Loss: 0.263223\n",
      "Iteration 281775 | Loss: 0.263223\n",
      "Iteration 281800 | Loss: 0.263223\n",
      "Iteration 281825 | Loss: 0.263222\n",
      "Iteration 281850 | Loss: 0.263222\n",
      "Iteration 281875 | Loss: 0.263222\n",
      "Iteration 281900 | Loss: 0.263221\n",
      "Iteration 281925 | Loss: 0.263221\n",
      "Iteration 281950 | Loss: 0.263220\n",
      "Iteration 281975 | Loss: 0.263220\n",
      "Iteration 282000 | Loss: 0.263220\n",
      "Iteration 282025 | Loss: 0.263219\n",
      "Iteration 282050 | Loss: 0.263219\n",
      "Iteration 282075 | Loss: 0.263219\n",
      "Iteration 282100 | Loss: 0.263218\n",
      "Iteration 282125 | Loss: 0.263218\n",
      "Iteration 282150 | Loss: 0.263217\n",
      "Iteration 282175 | Loss: 0.263217\n",
      "Iteration 282200 | Loss: 0.263217\n",
      "Iteration 282225 | Loss: 0.263216\n",
      "Iteration 282250 | Loss: 0.263216\n",
      "Iteration 282275 | Loss: 0.263216\n",
      "Iteration 282300 | Loss: 0.263215\n",
      "Iteration 282325 | Loss: 0.263215\n",
      "Iteration 282350 | Loss: 0.263214\n",
      "Iteration 282375 | Loss: 0.263214\n",
      "Iteration 282400 | Loss: 0.263214\n",
      "Iteration 282425 | Loss: 0.263213\n",
      "Iteration 282450 | Loss: 0.263213\n",
      "Iteration 282475 | Loss: 0.263213\n",
      "Iteration 282500 | Loss: 0.263212\n",
      "Iteration 282525 | Loss: 0.263212\n",
      "Iteration 282550 | Loss: 0.263212\n",
      "Iteration 282575 | Loss: 0.263211\n",
      "Iteration 282600 | Loss: 0.263211\n",
      "Iteration 282625 | Loss: 0.263210\n",
      "Iteration 282650 | Loss: 0.263210\n",
      "Iteration 282675 | Loss: 0.263210\n",
      "Iteration 282700 | Loss: 0.263209\n",
      "Iteration 282725 | Loss: 0.263209\n",
      "Iteration 282750 | Loss: 0.263209\n",
      "Iteration 282775 | Loss: 0.263208\n",
      "Iteration 282800 | Loss: 0.263208\n",
      "Iteration 282825 | Loss: 0.263208\n",
      "Iteration 282850 | Loss: 0.263207\n",
      "Iteration 282875 | Loss: 0.263207\n",
      "Iteration 282900 | Loss: 0.263206\n",
      "Iteration 282925 | Loss: 0.263206\n",
      "Iteration 282950 | Loss: 0.263206\n",
      "Iteration 282975 | Loss: 0.263205\n",
      "Iteration 283000 | Loss: 0.263205\n",
      "Iteration 283025 | Loss: 0.263205\n",
      "Iteration 283050 | Loss: 0.263204\n",
      "Iteration 283075 | Loss: 0.263204\n",
      "Iteration 283100 | Loss: 0.263204\n",
      "Iteration 283125 | Loss: 0.263203\n",
      "Iteration 283150 | Loss: 0.263203\n",
      "Iteration 283175 | Loss: 0.263202\n",
      "Iteration 283200 | Loss: 0.263202\n",
      "Iteration 283225 | Loss: 0.263202\n",
      "Iteration 283250 | Loss: 0.263201\n",
      "Iteration 283275 | Loss: 0.263201\n",
      "Iteration 283300 | Loss: 0.263201\n",
      "Iteration 283325 | Loss: 0.263200\n",
      "Iteration 283350 | Loss: 0.263200\n",
      "Iteration 283375 | Loss: 0.263200\n",
      "Iteration 283400 | Loss: 0.263199\n",
      "Iteration 283425 | Loss: 0.263199\n",
      "Iteration 283450 | Loss: 0.263199\n",
      "Iteration 283475 | Loss: 0.263198\n",
      "Iteration 283500 | Loss: 0.263198\n",
      "Iteration 283525 | Loss: 0.263198\n",
      "Iteration 283550 | Loss: 0.263197\n",
      "Iteration 283575 | Loss: 0.263197\n",
      "Iteration 283600 | Loss: 0.263196\n",
      "Iteration 283625 | Loss: 0.263196\n",
      "Iteration 283650 | Loss: 0.263196\n",
      "Iteration 283675 | Loss: 0.263195\n",
      "Iteration 283700 | Loss: 0.263195\n",
      "Iteration 283725 | Loss: 0.263195\n",
      "Iteration 283750 | Loss: 0.263194\n",
      "Iteration 283775 | Loss: 0.263194\n",
      "Iteration 283800 | Loss: 0.263194\n",
      "Iteration 283825 | Loss: 0.263193\n",
      "Iteration 283850 | Loss: 0.263193\n",
      "Iteration 283875 | Loss: 0.263193\n",
      "Iteration 283900 | Loss: 0.263192\n",
      "Iteration 283925 | Loss: 0.263192\n",
      "Iteration 283950 | Loss: 0.263192\n",
      "Iteration 283975 | Loss: 0.263191\n",
      "Iteration 284000 | Loss: 0.263191\n",
      "Iteration 284025 | Loss: 0.263191\n",
      "Iteration 284050 | Loss: 0.263190\n",
      "Iteration 284075 | Loss: 0.263190\n",
      "Iteration 284100 | Loss: 0.263190\n",
      "Iteration 284125 | Loss: 0.263189\n",
      "Iteration 284150 | Loss: 0.263189\n",
      "Iteration 284175 | Loss: 0.263189\n",
      "Iteration 284200 | Loss: 0.263188\n",
      "Iteration 284225 | Loss: 0.263188\n",
      "Iteration 284250 | Loss: 0.263187\n",
      "Iteration 284275 | Loss: 0.263187\n",
      "Iteration 284300 | Loss: 0.263187\n",
      "Iteration 284325 | Loss: 0.263186\n",
      "Iteration 284350 | Loss: 0.263186\n",
      "Iteration 284375 | Loss: 0.263186\n",
      "Iteration 284400 | Loss: 0.263185\n",
      "Iteration 284425 | Loss: 0.263185\n",
      "Iteration 284450 | Loss: 0.263185\n",
      "Iteration 284475 | Loss: 0.263184\n",
      "Iteration 284500 | Loss: 0.263184\n",
      "Iteration 284525 | Loss: 0.263184\n",
      "Iteration 284550 | Loss: 0.263183\n",
      "Iteration 284575 | Loss: 0.263183\n",
      "Iteration 284600 | Loss: 0.263183\n",
      "Iteration 284625 | Loss: 0.263182\n",
      "Iteration 284650 | Loss: 0.263182\n",
      "Iteration 284675 | Loss: 0.263182\n",
      "Iteration 284700 | Loss: 0.263181\n",
      "Iteration 284725 | Loss: 0.263181\n",
      "Iteration 284750 | Loss: 0.263181\n",
      "Iteration 284775 | Loss: 0.263180\n",
      "Iteration 284800 | Loss: 0.263180\n",
      "Iteration 284825 | Loss: 0.263180\n",
      "Iteration 284850 | Loss: 0.263179\n",
      "Iteration 284875 | Loss: 0.263179\n",
      "Iteration 284900 | Loss: 0.263179\n",
      "Iteration 284925 | Loss: 0.263178\n",
      "Iteration 284950 | Loss: 0.263178\n",
      "Iteration 284975 | Loss: 0.263178\n",
      "Iteration 285000 | Loss: 0.263177\n",
      "Iteration 285025 | Loss: 0.263177\n",
      "Iteration 285050 | Loss: 0.263177\n",
      "Iteration 285075 | Loss: 0.263176\n",
      "Iteration 285100 | Loss: 0.263176\n",
      "Iteration 285125 | Loss: 0.263176\n",
      "Iteration 285150 | Loss: 0.263175\n",
      "Iteration 285175 | Loss: 0.263175\n",
      "Iteration 285200 | Loss: 0.263175\n",
      "Iteration 285225 | Loss: 0.263174\n",
      "Iteration 285250 | Loss: 0.263174\n",
      "Iteration 285275 | Loss: 0.263174\n",
      "Iteration 285300 | Loss: 0.263173\n",
      "Iteration 285325 | Loss: 0.263173\n",
      "Iteration 285350 | Loss: 0.263173\n",
      "Iteration 285375 | Loss: 0.263172\n",
      "Iteration 285400 | Loss: 0.263172\n",
      "Iteration 285425 | Loss: 0.263172\n",
      "Iteration 285450 | Loss: 0.263172\n",
      "Iteration 285475 | Loss: 0.263171\n",
      "Iteration 285500 | Loss: 0.263171\n",
      "Iteration 285525 | Loss: 0.263171\n",
      "Iteration 285550 | Loss: 0.263170\n",
      "Iteration 285575 | Loss: 0.263170\n",
      "Iteration 285600 | Loss: 0.263170\n",
      "Iteration 285625 | Loss: 0.263169\n",
      "Iteration 285650 | Loss: 0.263169\n",
      "Iteration 285675 | Loss: 0.263169\n",
      "Iteration 285700 | Loss: 0.263168\n",
      "Iteration 285725 | Loss: 0.263168\n",
      "Iteration 285750 | Loss: 0.263168\n",
      "Iteration 285775 | Loss: 0.263167\n",
      "Iteration 285800 | Loss: 0.263167\n",
      "Iteration 285825 | Loss: 0.263167\n",
      "Iteration 285850 | Loss: 0.263166\n",
      "Iteration 285875 | Loss: 0.263166\n",
      "Iteration 285900 | Loss: 0.263166\n",
      "Iteration 285925 | Loss: 0.263165\n",
      "Iteration 285950 | Loss: 0.263165\n",
      "Iteration 285975 | Loss: 0.263165\n",
      "Iteration 286000 | Loss: 0.263164\n",
      "Iteration 286025 | Loss: 0.263164\n",
      "Iteration 286050 | Loss: 0.263164\n",
      "Iteration 286075 | Loss: 0.263164\n",
      "Iteration 286100 | Loss: 0.263163\n",
      "Iteration 286125 | Loss: 0.263163\n",
      "Iteration 286150 | Loss: 0.263163\n",
      "Iteration 286175 | Loss: 0.263162\n",
      "Iteration 286200 | Loss: 0.263162\n",
      "Iteration 286225 | Loss: 0.263162\n",
      "Iteration 286250 | Loss: 0.263161\n",
      "Iteration 286275 | Loss: 0.263161\n",
      "Iteration 286300 | Loss: 0.263161\n",
      "Iteration 286325 | Loss: 0.263160\n",
      "Iteration 286350 | Loss: 0.263160\n",
      "Iteration 286375 | Loss: 0.263160\n",
      "Iteration 286400 | Loss: 0.263159\n",
      "Iteration 286425 | Loss: 0.263159\n",
      "Iteration 286450 | Loss: 0.263159\n",
      "Iteration 286475 | Loss: 0.263158\n",
      "Iteration 286500 | Loss: 0.263158\n",
      "Iteration 286525 | Loss: 0.263158\n",
      "Iteration 286550 | Loss: 0.263158\n",
      "Iteration 286575 | Loss: 0.263157\n",
      "Iteration 286600 | Loss: 0.263157\n",
      "Iteration 286625 | Loss: 0.263157\n",
      "Iteration 286650 | Loss: 0.263156\n",
      "Iteration 286675 | Loss: 0.263156\n",
      "Iteration 286700 | Loss: 0.263156\n",
      "Iteration 286725 | Loss: 0.263155\n",
      "Iteration 286750 | Loss: 0.263155\n",
      "Iteration 286775 | Loss: 0.263155\n",
      "Iteration 286800 | Loss: 0.263154\n",
      "Iteration 286825 | Loss: 0.263154\n",
      "Iteration 286850 | Loss: 0.263154\n",
      "Iteration 286875 | Loss: 0.263154\n",
      "Iteration 286900 | Loss: 0.263153\n",
      "Iteration 286925 | Loss: 0.263153\n",
      "Iteration 286950 | Loss: 0.263153\n",
      "Iteration 286975 | Loss: 0.263152\n",
      "Iteration 287000 | Loss: 0.263152\n",
      "Iteration 287025 | Loss: 0.263152\n",
      "Iteration 287050 | Loss: 0.263151\n",
      "Iteration 287075 | Loss: 0.263151\n",
      "Iteration 287100 | Loss: 0.263151\n",
      "Iteration 287125 | Loss: 0.263151\n",
      "Iteration 287150 | Loss: 0.263150\n",
      "Iteration 287175 | Loss: 0.263150\n",
      "Iteration 287200 | Loss: 0.263150\n",
      "Iteration 287225 | Loss: 0.263149\n",
      "Iteration 287250 | Loss: 0.263149\n",
      "Iteration 287275 | Loss: 0.263149\n",
      "Iteration 287300 | Loss: 0.263148\n",
      "Iteration 287325 | Loss: 0.263148\n",
      "Iteration 287350 | Loss: 0.263148\n",
      "Iteration 287375 | Loss: 0.263147\n",
      "Iteration 287400 | Loss: 0.263147\n",
      "Iteration 287425 | Loss: 0.263147\n",
      "Iteration 287450 | Loss: 0.263147\n",
      "Iteration 287475 | Loss: 0.263146\n",
      "Iteration 287500 | Loss: 0.263146\n",
      "Iteration 287525 | Loss: 0.263146\n",
      "Iteration 287550 | Loss: 0.263145\n",
      "Iteration 287575 | Loss: 0.263145\n",
      "Iteration 287600 | Loss: 0.263145\n",
      "Iteration 287625 | Loss: 0.263145\n",
      "Iteration 287650 | Loss: 0.263144\n",
      "Iteration 287675 | Loss: 0.263144\n",
      "Iteration 287700 | Loss: 0.263144\n",
      "Iteration 287725 | Loss: 0.263143\n",
      "Iteration 287750 | Loss: 0.263143\n",
      "Iteration 287775 | Loss: 0.263143\n",
      "Iteration 287800 | Loss: 0.263142\n",
      "Iteration 287825 | Loss: 0.263142\n",
      "Iteration 287850 | Loss: 0.263142\n",
      "Iteration 287875 | Loss: 0.263142\n",
      "Iteration 287900 | Loss: 0.263141\n",
      "Iteration 287925 | Loss: 0.263141\n",
      "Iteration 287950 | Loss: 0.263141\n",
      "Iteration 287975 | Loss: 0.263140\n",
      "Iteration 288000 | Loss: 0.263140\n",
      "Iteration 288025 | Loss: 0.263140\n",
      "Iteration 288050 | Loss: 0.263140\n",
      "Iteration 288075 | Loss: 0.263139\n",
      "Iteration 288100 | Loss: 0.263139\n",
      "Iteration 288125 | Loss: 0.263139\n",
      "Iteration 288150 | Loss: 0.263138\n",
      "Iteration 288175 | Loss: 0.263138\n",
      "Iteration 288200 | Loss: 0.263138\n",
      "Iteration 288225 | Loss: 0.263137\n",
      "Iteration 288250 | Loss: 0.263137\n",
      "Iteration 288275 | Loss: 0.263137\n",
      "Iteration 288300 | Loss: 0.263137\n",
      "Iteration 288325 | Loss: 0.263136\n",
      "Iteration 288350 | Loss: 0.263136\n",
      "Iteration 288375 | Loss: 0.263136\n",
      "Iteration 288400 | Loss: 0.263135\n",
      "Iteration 288425 | Loss: 0.263135\n",
      "Iteration 288450 | Loss: 0.263135\n",
      "Iteration 288475 | Loss: 0.263135\n",
      "Iteration 288500 | Loss: 0.263134\n",
      "Iteration 288525 | Loss: 0.263134\n",
      "Iteration 288550 | Loss: 0.263134\n",
      "Iteration 288575 | Loss: 0.263133\n",
      "Iteration 288600 | Loss: 0.263133\n",
      "Iteration 288625 | Loss: 0.263133\n",
      "Iteration 288650 | Loss: 0.263133\n",
      "Iteration 288675 | Loss: 0.263132\n",
      "Iteration 288700 | Loss: 0.263132\n",
      "Iteration 288725 | Loss: 0.263132\n",
      "Iteration 288750 | Loss: 0.263131\n",
      "Iteration 288775 | Loss: 0.263131\n",
      "Iteration 288800 | Loss: 0.263131\n",
      "Iteration 288825 | Loss: 0.263131\n",
      "Iteration 288850 | Loss: 0.263130\n",
      "Iteration 288875 | Loss: 0.263130\n",
      "Iteration 288900 | Loss: 0.263130\n",
      "Iteration 288925 | Loss: 0.263129\n",
      "Iteration 288950 | Loss: 0.263129\n",
      "Iteration 288975 | Loss: 0.263129\n",
      "Iteration 289000 | Loss: 0.263129\n",
      "Iteration 289025 | Loss: 0.263128\n",
      "Iteration 289050 | Loss: 0.263128\n",
      "Iteration 289075 | Loss: 0.263128\n",
      "Iteration 289100 | Loss: 0.263128\n",
      "Iteration 289125 | Loss: 0.263127\n",
      "Iteration 289150 | Loss: 0.263127\n",
      "Iteration 289175 | Loss: 0.263127\n",
      "Iteration 289200 | Loss: 0.263126\n",
      "Iteration 289225 | Loss: 0.263126\n",
      "Iteration 289250 | Loss: 0.263126\n",
      "Iteration 289275 | Loss: 0.263126\n",
      "Iteration 289300 | Loss: 0.263125\n",
      "Iteration 289325 | Loss: 0.263125\n",
      "Iteration 289350 | Loss: 0.263125\n",
      "Iteration 289375 | Loss: 0.263124\n",
      "Iteration 289400 | Loss: 0.263124\n",
      "Iteration 289425 | Loss: 0.263124\n",
      "Iteration 289450 | Loss: 0.263124\n",
      "Iteration 289475 | Loss: 0.263123\n",
      "Iteration 289500 | Loss: 0.263123\n",
      "Iteration 289525 | Loss: 0.263123\n",
      "Iteration 289550 | Loss: 0.263123\n",
      "Iteration 289575 | Loss: 0.263122\n",
      "Iteration 289600 | Loss: 0.263122\n",
      "Iteration 289625 | Loss: 0.263122\n",
      "Iteration 289650 | Loss: 0.263121\n",
      "Iteration 289675 | Loss: 0.263121\n",
      "Iteration 289700 | Loss: 0.263121\n",
      "Iteration 289725 | Loss: 0.263121\n",
      "Iteration 289750 | Loss: 0.263120\n",
      "Iteration 289775 | Loss: 0.263120\n",
      "Iteration 289800 | Loss: 0.263120\n",
      "Iteration 289825 | Loss: 0.263120\n",
      "Iteration 289850 | Loss: 0.263119\n",
      "Iteration 289875 | Loss: 0.263119\n",
      "Iteration 289900 | Loss: 0.263119\n",
      "Iteration 289925 | Loss: 0.263118\n",
      "Iteration 289950 | Loss: 0.263118\n",
      "Iteration 289975 | Loss: 0.263118\n",
      "Iteration 290000 | Loss: 0.263118\n",
      "Iteration 290025 | Loss: 0.263117\n",
      "Iteration 290050 | Loss: 0.263117\n",
      "Iteration 290075 | Loss: 0.263117\n",
      "Iteration 290100 | Loss: 0.263117\n",
      "Iteration 290125 | Loss: 0.263116\n",
      "Iteration 290150 | Loss: 0.263116\n",
      "Iteration 290175 | Loss: 0.263116\n",
      "Iteration 290200 | Loss: 0.263116\n",
      "Iteration 290225 | Loss: 0.263115\n",
      "Iteration 290250 | Loss: 0.263115\n",
      "Iteration 290275 | Loss: 0.263115\n",
      "Iteration 290300 | Loss: 0.263114\n",
      "Iteration 290325 | Loss: 0.263114\n",
      "Iteration 290350 | Loss: 0.263114\n",
      "Iteration 290375 | Loss: 0.263114\n",
      "Iteration 290400 | Loss: 0.263113\n",
      "Iteration 290425 | Loss: 0.263113\n",
      "Iteration 290450 | Loss: 0.263113\n",
      "Iteration 290475 | Loss: 0.263113\n",
      "Iteration 290500 | Loss: 0.263112\n",
      "Iteration 290525 | Loss: 0.263112\n",
      "Iteration 290550 | Loss: 0.263112\n",
      "Iteration 290575 | Loss: 0.263112\n",
      "Iteration 290600 | Loss: 0.263111\n",
      "Iteration 290625 | Loss: 0.263111\n",
      "Iteration 290650 | Loss: 0.263111\n",
      "Iteration 290675 | Loss: 0.263110\n",
      "Iteration 290700 | Loss: 0.263110\n",
      "Iteration 290725 | Loss: 0.263110\n",
      "Iteration 290750 | Loss: 0.263110\n",
      "Iteration 290775 | Loss: 0.263109\n",
      "Iteration 290800 | Loss: 0.263109\n",
      "Iteration 290825 | Loss: 0.263109\n",
      "Iteration 290850 | Loss: 0.263109\n",
      "Iteration 290875 | Loss: 0.263108\n",
      "Iteration 290900 | Loss: 0.263108\n",
      "Iteration 290925 | Loss: 0.263108\n",
      "Iteration 290950 | Loss: 0.263108\n",
      "Iteration 290975 | Loss: 0.263107\n",
      "Iteration 291000 | Loss: 0.263107\n",
      "Iteration 291025 | Loss: 0.263107\n",
      "Iteration 291050 | Loss: 0.263107\n",
      "Iteration 291075 | Loss: 0.263106\n",
      "Iteration 291100 | Loss: 0.263106\n",
      "Iteration 291125 | Loss: 0.263106\n",
      "Iteration 291150 | Loss: 0.263106\n",
      "Iteration 291175 | Loss: 0.263105\n",
      "Iteration 291200 | Loss: 0.263105\n",
      "Iteration 291225 | Loss: 0.263105\n",
      "Iteration 291250 | Loss: 0.263105\n",
      "Iteration 291275 | Loss: 0.263104\n",
      "Iteration 291300 | Loss: 0.263104\n",
      "Iteration 291325 | Loss: 0.263104\n",
      "Iteration 291350 | Loss: 0.263104\n",
      "Iteration 291375 | Loss: 0.263103\n",
      "Iteration 291400 | Loss: 0.263103\n",
      "Iteration 291425 | Loss: 0.263103\n",
      "Iteration 291450 | Loss: 0.263102\n",
      "Iteration 291475 | Loss: 0.263102\n",
      "Iteration 291500 | Loss: 0.263102\n",
      "Iteration 291525 | Loss: 0.263102\n",
      "Iteration 291550 | Loss: 0.263101\n",
      "Iteration 291575 | Loss: 0.263101\n",
      "Iteration 291600 | Loss: 0.263101\n",
      "Iteration 291625 | Loss: 0.263101\n",
      "Iteration 291650 | Loss: 0.263100\n",
      "Iteration 291675 | Loss: 0.263100\n",
      "Iteration 291700 | Loss: 0.263100\n",
      "Iteration 291725 | Loss: 0.263100\n",
      "Iteration 291750 | Loss: 0.263099\n",
      "Iteration 291775 | Loss: 0.263099\n",
      "Iteration 291800 | Loss: 0.263099\n",
      "Iteration 291825 | Loss: 0.263099\n",
      "Iteration 291850 | Loss: 0.263098\n",
      "Iteration 291875 | Loss: 0.263098\n",
      "Iteration 291900 | Loss: 0.263098\n",
      "Iteration 291925 | Loss: 0.263098\n",
      "Iteration 291950 | Loss: 0.263097\n",
      "Iteration 291975 | Loss: 0.263097\n",
      "Iteration 292000 | Loss: 0.263097\n",
      "Iteration 292025 | Loss: 0.263097\n",
      "Iteration 292050 | Loss: 0.263096\n",
      "Iteration 292075 | Loss: 0.263096\n",
      "Iteration 292100 | Loss: 0.263096\n",
      "Iteration 292125 | Loss: 0.263096\n",
      "Iteration 292150 | Loss: 0.263095\n",
      "Iteration 292175 | Loss: 0.263095\n",
      "Iteration 292200 | Loss: 0.263095\n",
      "Iteration 292225 | Loss: 0.263095\n",
      "Iteration 292250 | Loss: 0.263095\n",
      "Iteration 292275 | Loss: 0.263094\n",
      "Iteration 292300 | Loss: 0.263094\n",
      "Iteration 292325 | Loss: 0.263094\n",
      "Iteration 292350 | Loss: 0.263094\n",
      "Iteration 292375 | Loss: 0.263093\n",
      "Iteration 292400 | Loss: 0.263093\n",
      "Iteration 292425 | Loss: 0.263093\n",
      "Iteration 292450 | Loss: 0.263093\n",
      "Iteration 292475 | Loss: 0.263092\n",
      "Iteration 292500 | Loss: 0.263092\n",
      "Iteration 292525 | Loss: 0.263092\n",
      "Iteration 292550 | Loss: 0.263092\n",
      "Iteration 292575 | Loss: 0.263091\n",
      "Iteration 292600 | Loss: 0.263091\n",
      "Iteration 292625 | Loss: 0.263091\n",
      "Iteration 292650 | Loss: 0.263091\n",
      "Iteration 292675 | Loss: 0.263090\n",
      "Iteration 292700 | Loss: 0.263090\n",
      "Iteration 292725 | Loss: 0.263090\n",
      "Iteration 292750 | Loss: 0.263090\n",
      "Iteration 292775 | Loss: 0.263089\n",
      "Iteration 292800 | Loss: 0.263089\n",
      "Iteration 292825 | Loss: 0.263089\n",
      "Iteration 292850 | Loss: 0.263089\n",
      "Iteration 292875 | Loss: 0.263088\n",
      "Iteration 292900 | Loss: 0.263088\n",
      "Iteration 292925 | Loss: 0.263088\n",
      "Iteration 292950 | Loss: 0.263088\n",
      "Iteration 292975 | Loss: 0.263088\n",
      "Iteration 293000 | Loss: 0.263087\n",
      "Iteration 293025 | Loss: 0.263087\n",
      "Iteration 293050 | Loss: 0.263087\n",
      "Iteration 293075 | Loss: 0.263087\n",
      "Iteration 293100 | Loss: 0.263086\n",
      "Iteration 293125 | Loss: 0.263086\n",
      "Iteration 293150 | Loss: 0.263086\n",
      "Iteration 293175 | Loss: 0.263086\n",
      "Iteration 293200 | Loss: 0.263085\n",
      "Iteration 293225 | Loss: 0.263085\n",
      "Iteration 293250 | Loss: 0.263085\n",
      "Iteration 293275 | Loss: 0.263085\n",
      "Iteration 293300 | Loss: 0.263084\n",
      "Iteration 293325 | Loss: 0.263084\n",
      "Iteration 293350 | Loss: 0.263084\n",
      "Iteration 293375 | Loss: 0.263084\n",
      "Iteration 293400 | Loss: 0.263083\n",
      "Iteration 293425 | Loss: 0.263083\n",
      "Iteration 293450 | Loss: 0.263083\n",
      "Iteration 293475 | Loss: 0.263083\n",
      "Iteration 293500 | Loss: 0.263083\n",
      "Iteration 293525 | Loss: 0.263082\n",
      "Iteration 293550 | Loss: 0.263082\n",
      "Iteration 293575 | Loss: 0.263082\n",
      "Iteration 293600 | Loss: 0.263082\n",
      "Iteration 293625 | Loss: 0.263081\n",
      "Iteration 293650 | Loss: 0.263081\n",
      "Iteration 293675 | Loss: 0.263081\n",
      "Iteration 293700 | Loss: 0.263081\n",
      "Iteration 293725 | Loss: 0.263080\n",
      "Iteration 293750 | Loss: 0.263080\n",
      "Iteration 293775 | Loss: 0.263080\n",
      "Iteration 293800 | Loss: 0.263080\n",
      "Iteration 293825 | Loss: 0.263080\n",
      "Iteration 293850 | Loss: 0.263079\n",
      "Iteration 293875 | Loss: 0.263079\n",
      "Iteration 293900 | Loss: 0.263079\n",
      "Iteration 293925 | Loss: 0.263079\n",
      "Iteration 293950 | Loss: 0.263078\n",
      "Iteration 293975 | Loss: 0.263078\n",
      "Iteration 294000 | Loss: 0.263078\n",
      "Iteration 294025 | Loss: 0.263078\n",
      "Iteration 294050 | Loss: 0.263077\n",
      "Iteration 294075 | Loss: 0.263077\n",
      "Iteration 294100 | Loss: 0.263077\n",
      "Iteration 294125 | Loss: 0.263077\n",
      "Iteration 294150 | Loss: 0.263077\n",
      "Iteration 294175 | Loss: 0.263076\n",
      "Iteration 294200 | Loss: 0.263076\n",
      "Iteration 294225 | Loss: 0.263076\n",
      "Iteration 294250 | Loss: 0.263076\n",
      "Iteration 294275 | Loss: 0.263075\n",
      "Iteration 294300 | Loss: 0.263075\n",
      "Iteration 294325 | Loss: 0.263075\n",
      "Iteration 294350 | Loss: 0.263075\n",
      "Iteration 294375 | Loss: 0.263075\n",
      "Iteration 294400 | Loss: 0.263074\n",
      "Iteration 294425 | Loss: 0.263074\n",
      "Iteration 294450 | Loss: 0.263074\n",
      "Iteration 294475 | Loss: 0.263074\n",
      "Iteration 294500 | Loss: 0.263073\n",
      "Iteration 294525 | Loss: 0.263073\n",
      "Iteration 294550 | Loss: 0.263073\n",
      "Iteration 294575 | Loss: 0.263073\n",
      "Iteration 294600 | Loss: 0.263073\n",
      "Iteration 294625 | Loss: 0.263072\n",
      "Iteration 294650 | Loss: 0.263072\n",
      "Iteration 294675 | Loss: 0.263072\n",
      "Iteration 294700 | Loss: 0.263072\n",
      "Iteration 294725 | Loss: 0.263071\n",
      "Iteration 294750 | Loss: 0.263071\n",
      "Iteration 294775 | Loss: 0.263071\n",
      "Iteration 294800 | Loss: 0.263071\n",
      "Iteration 294825 | Loss: 0.263071\n",
      "Iteration 294850 | Loss: 0.263070\n",
      "Iteration 294875 | Loss: 0.263070\n",
      "Iteration 294900 | Loss: 0.263070\n",
      "Iteration 294925 | Loss: 0.263070\n",
      "Iteration 294950 | Loss: 0.263069\n",
      "Iteration 294975 | Loss: 0.263069\n",
      "Iteration 295000 | Loss: 0.263069\n",
      "Iteration 295025 | Loss: 0.263069\n",
      "Iteration 295050 | Loss: 0.263069\n",
      "Iteration 295075 | Loss: 0.263068\n",
      "Iteration 295100 | Loss: 0.263068\n",
      "Iteration 295125 | Loss: 0.263068\n",
      "Iteration 295150 | Loss: 0.263068\n",
      "Iteration 295175 | Loss: 0.263067\n",
      "Iteration 295200 | Loss: 0.263067\n",
      "Iteration 295225 | Loss: 0.263067\n",
      "Iteration 295250 | Loss: 0.263067\n",
      "Iteration 295275 | Loss: 0.263067\n",
      "Iteration 295300 | Loss: 0.263066\n",
      "Iteration 295325 | Loss: 0.263066\n",
      "Iteration 295350 | Loss: 0.263066\n",
      "Iteration 295375 | Loss: 0.263066\n",
      "Iteration 295400 | Loss: 0.263066\n",
      "Iteration 295425 | Loss: 0.263065\n",
      "Iteration 295450 | Loss: 0.263065\n",
      "Iteration 295475 | Loss: 0.263065\n",
      "Iteration 295500 | Loss: 0.263065\n",
      "Iteration 295525 | Loss: 0.263064\n",
      "Iteration 295550 | Loss: 0.263064\n",
      "Iteration 295575 | Loss: 0.263064\n",
      "Iteration 295600 | Loss: 0.263064\n",
      "Iteration 295625 | Loss: 0.263064\n",
      "Iteration 295650 | Loss: 0.263063\n",
      "Iteration 295675 | Loss: 0.263063\n",
      "Iteration 295700 | Loss: 0.263063\n",
      "Iteration 295725 | Loss: 0.263063\n",
      "Iteration 295750 | Loss: 0.263063\n",
      "Iteration 295775 | Loss: 0.263062\n",
      "Iteration 295800 | Loss: 0.263062\n",
      "Iteration 295825 | Loss: 0.263062\n",
      "Iteration 295850 | Loss: 0.263062\n",
      "Iteration 295875 | Loss: 0.263061\n",
      "Iteration 295900 | Loss: 0.263061\n",
      "Iteration 295925 | Loss: 0.263061\n",
      "Iteration 295950 | Loss: 0.263061\n",
      "Iteration 295975 | Loss: 0.263061\n",
      "Iteration 296000 | Loss: 0.263060\n",
      "Iteration 296025 | Loss: 0.263060\n",
      "Iteration 296050 | Loss: 0.263060\n",
      "Iteration 296075 | Loss: 0.263060\n",
      "Iteration 296100 | Loss: 0.263060\n",
      "Iteration 296125 | Loss: 0.263059\n",
      "Iteration 296150 | Loss: 0.263059\n",
      "Iteration 296175 | Loss: 0.263059\n",
      "Iteration 296200 | Loss: 0.263059\n",
      "Iteration 296225 | Loss: 0.263059\n",
      "Iteration 296250 | Loss: 0.263058\n",
      "Iteration 296275 | Loss: 0.263058\n",
      "Iteration 296300 | Loss: 0.263058\n",
      "Iteration 296325 | Loss: 0.263058\n",
      "Iteration 296350 | Loss: 0.263057\n",
      "Iteration 296375 | Loss: 0.263057\n",
      "Iteration 296400 | Loss: 0.263057\n",
      "Iteration 296425 | Loss: 0.263057\n",
      "Iteration 296450 | Loss: 0.263057\n",
      "Iteration 296475 | Loss: 0.263056\n",
      "Iteration 296500 | Loss: 0.263056\n",
      "Iteration 296525 | Loss: 0.263056\n",
      "Iteration 296550 | Loss: 0.263056\n",
      "Iteration 296575 | Loss: 0.263056\n",
      "Iteration 296600 | Loss: 0.263055\n",
      "Iteration 296625 | Loss: 0.263055\n",
      "Iteration 296650 | Loss: 0.263055\n",
      "Iteration 296675 | Loss: 0.263055\n",
      "Iteration 296700 | Loss: 0.263055\n",
      "Iteration 296725 | Loss: 0.263054\n",
      "Iteration 296750 | Loss: 0.263054\n",
      "Iteration 296775 | Loss: 0.263054\n",
      "Iteration 296800 | Loss: 0.263054\n",
      "Iteration 296825 | Loss: 0.263054\n",
      "Iteration 296850 | Loss: 0.263053\n",
      "Iteration 296875 | Loss: 0.263053\n",
      "Iteration 296900 | Loss: 0.263053\n",
      "Iteration 296925 | Loss: 0.263053\n",
      "Iteration 296950 | Loss: 0.263053\n",
      "Iteration 296975 | Loss: 0.263052\n",
      "Iteration 297000 | Loss: 0.263052\n",
      "Iteration 297025 | Loss: 0.263052\n",
      "Iteration 297050 | Loss: 0.263052\n",
      "Iteration 297075 | Loss: 0.263052\n",
      "Iteration 297100 | Loss: 0.263051\n",
      "Iteration 297125 | Loss: 0.263051\n",
      "Iteration 297150 | Loss: 0.263051\n",
      "Iteration 297175 | Loss: 0.263051\n",
      "Iteration 297200 | Loss: 0.263051\n",
      "Iteration 297225 | Loss: 0.263050\n",
      "Iteration 297250 | Loss: 0.263050\n",
      "Iteration 297275 | Loss: 0.263050\n",
      "Iteration 297300 | Loss: 0.263050\n",
      "Iteration 297325 | Loss: 0.263050\n",
      "Iteration 297350 | Loss: 0.263049\n",
      "Iteration 297375 | Loss: 0.263049\n",
      "Iteration 297400 | Loss: 0.263049\n",
      "Iteration 297425 | Loss: 0.263049\n",
      "Iteration 297450 | Loss: 0.263049\n",
      "Iteration 297475 | Loss: 0.263048\n",
      "Iteration 297500 | Loss: 0.263048\n",
      "Iteration 297525 | Loss: 0.263048\n",
      "Iteration 297550 | Loss: 0.263048\n",
      "Iteration 297575 | Loss: 0.263048\n",
      "Iteration 297600 | Loss: 0.263047\n",
      "Iteration 297625 | Loss: 0.263047\n",
      "Iteration 297650 | Loss: 0.263047\n",
      "Iteration 297675 | Loss: 0.263047\n",
      "Iteration 297700 | Loss: 0.263047\n",
      "Iteration 297725 | Loss: 0.263046\n",
      "Iteration 297750 | Loss: 0.263046\n",
      "Iteration 297775 | Loss: 0.263046\n",
      "Iteration 297800 | Loss: 0.263046\n",
      "Iteration 297825 | Loss: 0.263046\n",
      "Iteration 297850 | Loss: 0.263045\n",
      "Iteration 297875 | Loss: 0.263045\n",
      "Iteration 297900 | Loss: 0.263045\n",
      "Iteration 297925 | Loss: 0.263045\n",
      "Iteration 297950 | Loss: 0.263045\n",
      "Iteration 297975 | Loss: 0.263044\n",
      "Iteration 298000 | Loss: 0.263044\n",
      "Iteration 298025 | Loss: 0.263044\n",
      "Iteration 298050 | Loss: 0.263044\n",
      "Iteration 298075 | Loss: 0.263044\n",
      "Iteration 298100 | Loss: 0.263043\n",
      "Iteration 298125 | Loss: 0.263043\n",
      "Iteration 298150 | Loss: 0.263043\n",
      "Iteration 298175 | Loss: 0.263043\n",
      "Iteration 298200 | Loss: 0.263043\n",
      "Iteration 298225 | Loss: 0.263043\n",
      "Iteration 298250 | Loss: 0.263042\n",
      "Iteration 298275 | Loss: 0.263042\n",
      "Iteration 298300 | Loss: 0.263042\n",
      "Iteration 298325 | Loss: 0.263042\n",
      "Iteration 298350 | Loss: 0.263042\n",
      "Iteration 298375 | Loss: 0.263041\n",
      "Iteration 298400 | Loss: 0.263041\n",
      "Iteration 298425 | Loss: 0.263041\n",
      "Iteration 298450 | Loss: 0.263041\n",
      "Iteration 298475 | Loss: 0.263041\n",
      "Iteration 298500 | Loss: 0.263040\n",
      "Iteration 298525 | Loss: 0.263040\n",
      "Iteration 298550 | Loss: 0.263040\n",
      "Iteration 298575 | Loss: 0.263040\n",
      "Iteration 298600 | Loss: 0.263040\n",
      "Iteration 298625 | Loss: 0.263039\n",
      "Iteration 298650 | Loss: 0.263039\n",
      "Iteration 298675 | Loss: 0.263039\n",
      "Iteration 298700 | Loss: 0.263039\n",
      "Iteration 298725 | Loss: 0.263039\n",
      "Iteration 298750 | Loss: 0.263039\n",
      "Iteration 298775 | Loss: 0.263038\n",
      "Iteration 298800 | Loss: 0.263038\n",
      "Iteration 298825 | Loss: 0.263038\n",
      "Iteration 298850 | Loss: 0.263038\n",
      "Iteration 298875 | Loss: 0.263038\n",
      "Iteration 298900 | Loss: 0.263037\n",
      "Iteration 298925 | Loss: 0.263037\n",
      "Iteration 298950 | Loss: 0.263037\n",
      "Iteration 298975 | Loss: 0.263037\n",
      "Iteration 299000 | Loss: 0.263037\n",
      "Iteration 299025 | Loss: 0.263036\n",
      "Iteration 299050 | Loss: 0.263036\n",
      "Iteration 299075 | Loss: 0.263036\n",
      "Iteration 299100 | Loss: 0.263036\n",
      "Iteration 299125 | Loss: 0.263036\n",
      "Iteration 299150 | Loss: 0.263036\n",
      "Iteration 299175 | Loss: 0.263035\n",
      "Iteration 299200 | Loss: 0.263035\n",
      "Iteration 299225 | Loss: 0.263035\n",
      "Iteration 299250 | Loss: 0.263035\n",
      "Iteration 299275 | Loss: 0.263035\n",
      "Iteration 299300 | Loss: 0.263034\n",
      "Iteration 299325 | Loss: 0.263034\n",
      "Iteration 299350 | Loss: 0.263034\n",
      "Iteration 299375 | Loss: 0.263034\n",
      "Iteration 299400 | Loss: 0.263034\n",
      "Iteration 299425 | Loss: 0.263034\n",
      "Iteration 299450 | Loss: 0.263033\n",
      "Iteration 299475 | Loss: 0.263033\n",
      "Iteration 299500 | Loss: 0.263033\n",
      "Iteration 299525 | Loss: 0.263033\n",
      "Iteration 299550 | Loss: 0.263033\n",
      "Iteration 299575 | Loss: 0.263032\n",
      "Iteration 299600 | Loss: 0.263032\n",
      "Iteration 299625 | Loss: 0.263032\n",
      "Iteration 299650 | Loss: 0.263032\n",
      "Iteration 299675 | Loss: 0.263032\n",
      "Iteration 299700 | Loss: 0.263032\n",
      "Iteration 299725 | Loss: 0.263031\n",
      "Iteration 299750 | Loss: 0.263031\n",
      "Iteration 299775 | Loss: 0.263031\n",
      "Iteration 299800 | Loss: 0.263031\n",
      "Iteration 299825 | Loss: 0.263031\n",
      "Iteration 299850 | Loss: 0.263030\n",
      "Iteration 299875 | Loss: 0.263030\n",
      "Iteration 299900 | Loss: 0.263030\n",
      "Iteration 299925 | Loss: 0.263030\n",
      "Iteration 299950 | Loss: 0.263030\n",
      "Iteration 299975 | Loss: 0.263030\n",
      "Iteration 300000 | Loss: 0.263029\n",
      "Iteration 300025 | Loss: 0.263029\n",
      "Iteration 300050 | Loss: 0.263029\n",
      "Iteration 300075 | Loss: 0.263029\n",
      "Iteration 300100 | Loss: 0.263029\n",
      "Iteration 300125 | Loss: 0.263029\n",
      "Iteration 300150 | Loss: 0.263028\n",
      "Iteration 300175 | Loss: 0.263028\n",
      "Iteration 300200 | Loss: 0.263028\n",
      "Iteration 300225 | Loss: 0.263028\n",
      "Iteration 300250 | Loss: 0.263028\n",
      "Iteration 300275 | Loss: 0.263027\n",
      "Iteration 300300 | Loss: 0.263027\n",
      "Iteration 300325 | Loss: 0.263027\n",
      "Iteration 300350 | Loss: 0.263027\n",
      "Iteration 300375 | Loss: 0.263027\n",
      "Iteration 300400 | Loss: 0.263027\n",
      "Iteration 300425 | Loss: 0.263026\n",
      "Iteration 300450 | Loss: 0.263026\n",
      "Iteration 300475 | Loss: 0.263026\n",
      "Iteration 300500 | Loss: 0.263026\n",
      "Iteration 300525 | Loss: 0.263026\n",
      "Iteration 300550 | Loss: 0.263026\n",
      "Iteration 300575 | Loss: 0.263025\n",
      "Iteration 300600 | Loss: 0.263025\n",
      "Iteration 300625 | Loss: 0.263025\n",
      "Iteration 300650 | Loss: 0.263025\n",
      "Iteration 300675 | Loss: 0.263025\n",
      "Iteration 300700 | Loss: 0.263024\n",
      "Iteration 300725 | Loss: 0.263024\n",
      "Iteration 300750 | Loss: 0.263024\n",
      "Iteration 300775 | Loss: 0.263024\n",
      "Iteration 300800 | Loss: 0.263024\n",
      "Iteration 300825 | Loss: 0.263024\n",
      "Iteration 300850 | Loss: 0.263023\n",
      "Iteration 300875 | Loss: 0.263023\n",
      "Iteration 300900 | Loss: 0.263023\n",
      "Iteration 300925 | Loss: 0.263023\n",
      "Iteration 300950 | Loss: 0.263023\n",
      "Iteration 300975 | Loss: 0.263023\n",
      "Iteration 301000 | Loss: 0.263022\n",
      "Iteration 301025 | Loss: 0.263022\n",
      "Iteration 301050 | Loss: 0.263022\n",
      "Iteration 301075 | Loss: 0.263022\n",
      "Iteration 301100 | Loss: 0.263022\n",
      "Iteration 301125 | Loss: 0.263022\n",
      "Iteration 301150 | Loss: 0.263021\n",
      "Iteration 301175 | Loss: 0.263021\n",
      "Iteration 301200 | Loss: 0.263021\n",
      "Iteration 301225 | Loss: 0.263021\n",
      "Iteration 301250 | Loss: 0.263021\n",
      "Iteration 301275 | Loss: 0.263021\n",
      "Iteration 301300 | Loss: 0.263020\n",
      "Iteration 301325 | Loss: 0.263020\n",
      "Iteration 301350 | Loss: 0.263020\n",
      "Iteration 301375 | Loss: 0.263020\n",
      "Iteration 301400 | Loss: 0.263020\n",
      "Iteration 301425 | Loss: 0.263020\n",
      "Iteration 301450 | Loss: 0.263019\n",
      "Iteration 301475 | Loss: 0.263019\n",
      "Iteration 301500 | Loss: 0.263019\n",
      "Iteration 301525 | Loss: 0.263019\n",
      "Iteration 301550 | Loss: 0.263019\n",
      "Iteration 301575 | Loss: 0.263019\n",
      "Iteration 301600 | Loss: 0.263018\n",
      "Iteration 301625 | Loss: 0.263018\n",
      "Iteration 301650 | Loss: 0.263018\n",
      "Iteration 301675 | Loss: 0.263018\n",
      "Iteration 301700 | Loss: 0.263018\n",
      "Iteration 301725 | Loss: 0.263018\n",
      "Iteration 301750 | Loss: 0.263017\n",
      "Iteration 301775 | Loss: 0.263017\n",
      "Iteration 301800 | Loss: 0.263017\n",
      "Iteration 301825 | Loss: 0.263017\n",
      "Iteration 301850 | Loss: 0.263017\n",
      "Iteration 301875 | Loss: 0.263017\n",
      "Iteration 301900 | Loss: 0.263016\n",
      "Iteration 301925 | Loss: 0.263016\n",
      "Iteration 301950 | Loss: 0.263016\n",
      "Iteration 301975 | Loss: 0.263016\n",
      "Iteration 302000 | Loss: 0.263016\n",
      "Iteration 302025 | Loss: 0.263016\n",
      "Iteration 302050 | Loss: 0.263015\n",
      "Iteration 302075 | Loss: 0.263015\n",
      "Iteration 302100 | Loss: 0.263015\n",
      "Iteration 302125 | Loss: 0.263015\n",
      "Iteration 302150 | Loss: 0.263015\n",
      "Iteration 302175 | Loss: 0.263015\n",
      "Iteration 302200 | Loss: 0.263014\n",
      "Iteration 302225 | Loss: 0.263014\n",
      "Iteration 302250 | Loss: 0.263014\n",
      "Iteration 302275 | Loss: 0.263014\n",
      "Iteration 302300 | Loss: 0.263014\n",
      "Iteration 302325 | Loss: 0.263014\n",
      "Iteration 302350 | Loss: 0.263013\n",
      "Iteration 302375 | Loss: 0.263013\n",
      "Iteration 302400 | Loss: 0.263013\n",
      "Iteration 302425 | Loss: 0.263013\n",
      "Iteration 302450 | Loss: 0.263013\n",
      "Iteration 302475 | Loss: 0.263013\n",
      "Iteration 302500 | Loss: 0.263012\n",
      "Iteration 302525 | Loss: 0.263012\n",
      "Iteration 302550 | Loss: 0.263012\n",
      "Iteration 302575 | Loss: 0.263012\n",
      "Iteration 302600 | Loss: 0.263012\n",
      "Iteration 302625 | Loss: 0.263012\n",
      "Iteration 302650 | Loss: 0.263011\n",
      "Iteration 302675 | Loss: 0.263011\n",
      "Iteration 302700 | Loss: 0.263011\n",
      "Iteration 302725 | Loss: 0.263011\n",
      "Iteration 302750 | Loss: 0.263011\n",
      "Iteration 302775 | Loss: 0.263011\n",
      "Iteration 302800 | Loss: 0.263011\n",
      "Iteration 302825 | Loss: 0.263010\n",
      "Iteration 302850 | Loss: 0.263010\n",
      "Iteration 302875 | Loss: 0.263010\n",
      "Iteration 302900 | Loss: 0.263010\n",
      "Iteration 302925 | Loss: 0.263010\n",
      "Iteration 302950 | Loss: 0.263010\n",
      "Iteration 302975 | Loss: 0.263009\n",
      "Iteration 303000 | Loss: 0.263009\n",
      "Iteration 303025 | Loss: 0.263009\n",
      "Iteration 303050 | Loss: 0.263009\n",
      "Iteration 303075 | Loss: 0.263009\n",
      "Iteration 303100 | Loss: 0.263009\n",
      "Iteration 303125 | Loss: 0.263008\n",
      "Iteration 303150 | Loss: 0.263008\n",
      "Iteration 303175 | Loss: 0.263008\n",
      "Iteration 303200 | Loss: 0.263008\n",
      "Iteration 303225 | Loss: 0.263008\n",
      "Iteration 303250 | Loss: 0.263008\n",
      "Iteration 303275 | Loss: 0.263008\n",
      "Iteration 303300 | Loss: 0.263007\n",
      "Iteration 303325 | Loss: 0.263007\n",
      "Iteration 303350 | Loss: 0.263007\n",
      "Iteration 303375 | Loss: 0.263007\n",
      "Iteration 303400 | Loss: 0.263007\n",
      "Iteration 303425 | Loss: 0.263007\n",
      "Iteration 303450 | Loss: 0.263006\n",
      "Iteration 303475 | Loss: 0.263006\n",
      "Iteration 303500 | Loss: 0.263006\n",
      "Iteration 303525 | Loss: 0.263006\n",
      "Iteration 303550 | Loss: 0.263006\n",
      "Iteration 303575 | Loss: 0.263006\n",
      "Iteration 303600 | Loss: 0.263006\n",
      "Iteration 303625 | Loss: 0.263005\n",
      "Iteration 303650 | Loss: 0.263005\n",
      "Iteration 303675 | Loss: 0.263005\n",
      "Iteration 303700 | Loss: 0.263005\n",
      "Iteration 303725 | Loss: 0.263005\n",
      "Iteration 303750 | Loss: 0.263005\n",
      "Iteration 303775 | Loss: 0.263004\n",
      "Iteration 303800 | Loss: 0.263004\n",
      "Iteration 303825 | Loss: 0.263004\n",
      "Iteration 303850 | Loss: 0.263004\n",
      "Iteration 303875 | Loss: 0.263004\n",
      "Iteration 303900 | Loss: 0.263004\n",
      "Iteration 303925 | Loss: 0.263004\n",
      "Iteration 303950 | Loss: 0.263003\n",
      "Iteration 303975 | Loss: 0.263003\n",
      "Iteration 304000 | Loss: 0.263003\n",
      "Iteration 304025 | Loss: 0.263003\n",
      "Iteration 304050 | Loss: 0.263003\n",
      "Iteration 304075 | Loss: 0.263003\n",
      "Iteration 304100 | Loss: 0.263002\n",
      "Iteration 304125 | Loss: 0.263002\n",
      "Iteration 304150 | Loss: 0.263002\n",
      "Iteration 304175 | Loss: 0.263002\n",
      "Iteration 304200 | Loss: 0.263002\n",
      "Iteration 304225 | Loss: 0.263002\n",
      "Iteration 304250 | Loss: 0.263002\n",
      "Iteration 304275 | Loss: 0.263001\n",
      "Iteration 304300 | Loss: 0.263001\n",
      "Iteration 304325 | Loss: 0.263001\n",
      "Iteration 304350 | Loss: 0.263001\n",
      "Iteration 304375 | Loss: 0.263001\n",
      "Iteration 304400 | Loss: 0.263001\n",
      "Iteration 304425 | Loss: 0.263001\n",
      "Iteration 304450 | Loss: 0.263000\n",
      "Iteration 304475 | Loss: 0.263000\n",
      "Iteration 304500 | Loss: 0.263000\n",
      "Iteration 304525 | Loss: 0.263000\n",
      "Iteration 304550 | Loss: 0.263000\n",
      "Iteration 304575 | Loss: 0.263000\n",
      "Iteration 304600 | Loss: 0.263000\n",
      "Iteration 304625 | Loss: 0.262999\n",
      "Iteration 304650 | Loss: 0.262999\n",
      "Iteration 304675 | Loss: 0.262999\n",
      "Iteration 304700 | Loss: 0.262999\n",
      "Iteration 304725 | Loss: 0.262999\n",
      "Iteration 304750 | Loss: 0.262999\n",
      "Iteration 304775 | Loss: 0.262998\n",
      "Iteration 304800 | Loss: 0.262998\n",
      "Iteration 304825 | Loss: 0.262998\n",
      "Iteration 304850 | Loss: 0.262998\n",
      "Iteration 304875 | Loss: 0.262998\n",
      "Iteration 304900 | Loss: 0.262998\n",
      "Iteration 304925 | Loss: 0.262998\n",
      "Iteration 304950 | Loss: 0.262997\n",
      "Iteration 304975 | Loss: 0.262997\n",
      "Iteration 305000 | Loss: 0.262997\n",
      "Iteration 305025 | Loss: 0.262997\n",
      "Iteration 305050 | Loss: 0.262997\n",
      "Iteration 305075 | Loss: 0.262997\n",
      "Iteration 305100 | Loss: 0.262997\n",
      "Iteration 305125 | Loss: 0.262996\n",
      "Iteration 305150 | Loss: 0.262996\n",
      "Iteration 305175 | Loss: 0.262996\n",
      "Iteration 305200 | Loss: 0.262996\n",
      "Iteration 305225 | Loss: 0.262996\n",
      "Iteration 305250 | Loss: 0.262996\n",
      "Iteration 305275 | Loss: 0.262996\n",
      "Iteration 305300 | Loss: 0.262995\n",
      "Iteration 305325 | Loss: 0.262995\n",
      "Iteration 305350 | Loss: 0.262995\n",
      "Iteration 305375 | Loss: 0.262995\n",
      "Iteration 305400 | Loss: 0.262995\n",
      "Iteration 305425 | Loss: 0.262995\n",
      "Iteration 305450 | Loss: 0.262995\n",
      "Iteration 305475 | Loss: 0.262994\n",
      "Iteration 305500 | Loss: 0.262994\n",
      "Iteration 305525 | Loss: 0.262994\n",
      "Iteration 305550 | Loss: 0.262994\n",
      "Iteration 305575 | Loss: 0.262994\n",
      "Iteration 305600 | Loss: 0.262994\n",
      "Iteration 305625 | Loss: 0.262994\n",
      "Iteration 305650 | Loss: 0.262993\n",
      "Iteration 305675 | Loss: 0.262993\n",
      "Iteration 305700 | Loss: 0.262993\n",
      "Iteration 305725 | Loss: 0.262993\n",
      "Iteration 305750 | Loss: 0.262993\n",
      "Iteration 305775 | Loss: 0.262993\n",
      "Iteration 305800 | Loss: 0.262993\n",
      "Iteration 305825 | Loss: 0.262992\n",
      "Iteration 305850 | Loss: 0.262992\n",
      "Iteration 305875 | Loss: 0.262992\n",
      "Iteration 305900 | Loss: 0.262992\n",
      "Iteration 305925 | Loss: 0.262992\n",
      "Iteration 305950 | Loss: 0.262992\n",
      "Iteration 305975 | Loss: 0.262992\n",
      "Iteration 306000 | Loss: 0.262991\n",
      "Iteration 306025 | Loss: 0.262991\n",
      "Iteration 306050 | Loss: 0.262991\n",
      "Iteration 306075 | Loss: 0.262991\n",
      "Iteration 306100 | Loss: 0.262991\n",
      "Iteration 306125 | Loss: 0.262991\n",
      "Iteration 306150 | Loss: 0.262991\n",
      "Iteration 306175 | Loss: 0.262990\n",
      "Iteration 306200 | Loss: 0.262990\n",
      "Iteration 306225 | Loss: 0.262990\n",
      "Iteration 306250 | Loss: 0.262990\n",
      "Iteration 306275 | Loss: 0.262990\n",
      "Iteration 306300 | Loss: 0.262990\n",
      "Iteration 306325 | Loss: 0.262990\n",
      "Iteration 306350 | Loss: 0.262990\n",
      "Iteration 306375 | Loss: 0.262989\n",
      "Iteration 306400 | Loss: 0.262989\n",
      "Iteration 306425 | Loss: 0.262989\n",
      "Iteration 306450 | Loss: 0.262989\n",
      "Iteration 306475 | Loss: 0.262989\n",
      "Iteration 306500 | Loss: 0.262989\n",
      "Iteration 306525 | Loss: 0.262989\n",
      "Iteration 306550 | Loss: 0.262988\n",
      "Iteration 306575 | Loss: 0.262988\n",
      "Iteration 306600 | Loss: 0.262988\n",
      "Iteration 306625 | Loss: 0.262988\n",
      "Iteration 306650 | Loss: 0.262988\n",
      "Iteration 306675 | Loss: 0.262988\n",
      "Iteration 306700 | Loss: 0.262988\n",
      "Iteration 306725 | Loss: 0.262987\n",
      "Iteration 306750 | Loss: 0.262987\n",
      "Iteration 306775 | Loss: 0.262987\n",
      "Iteration 306800 | Loss: 0.262987\n",
      "Iteration 306825 | Loss: 0.262987\n",
      "Iteration 306850 | Loss: 0.262987\n",
      "Iteration 306875 | Loss: 0.262987\n",
      "Iteration 306900 | Loss: 0.262987\n",
      "Iteration 306925 | Loss: 0.262986\n",
      "Iteration 306950 | Loss: 0.262986\n",
      "Iteration 306975 | Loss: 0.262986\n",
      "Iteration 307000 | Loss: 0.262986\n",
      "Iteration 307025 | Loss: 0.262986\n",
      "Iteration 307050 | Loss: 0.262986\n",
      "Iteration 307075 | Loss: 0.262986\n",
      "Iteration 307100 | Loss: 0.262985\n",
      "Iteration 307125 | Loss: 0.262985\n",
      "Iteration 307150 | Loss: 0.262985\n",
      "Iteration 307175 | Loss: 0.262985\n",
      "Iteration 307200 | Loss: 0.262985\n",
      "Iteration 307225 | Loss: 0.262985\n",
      "Iteration 307250 | Loss: 0.262985\n",
      "Iteration 307275 | Loss: 0.262985\n",
      "Iteration 307300 | Loss: 0.262984\n",
      "Iteration 307325 | Loss: 0.262984\n",
      "Iteration 307350 | Loss: 0.262984\n",
      "Iteration 307375 | Loss: 0.262984\n",
      "Iteration 307400 | Loss: 0.262984\n",
      "Iteration 307425 | Loss: 0.262984\n",
      "Iteration 307450 | Loss: 0.262984\n",
      "Iteration 307475 | Loss: 0.262983\n",
      "Iteration 307500 | Loss: 0.262983\n",
      "Iteration 307525 | Loss: 0.262983\n",
      "Iteration 307550 | Loss: 0.262983\n",
      "Iteration 307575 | Loss: 0.262983\n",
      "Iteration 307600 | Loss: 0.262983\n",
      "Iteration 307625 | Loss: 0.262983\n",
      "Iteration 307650 | Loss: 0.262983\n",
      "Iteration 307675 | Loss: 0.262982\n",
      "Iteration 307700 | Loss: 0.262982\n",
      "Iteration 307725 | Loss: 0.262982\n",
      "Iteration 307750 | Loss: 0.262982\n",
      "Iteration 307775 | Loss: 0.262982\n",
      "Iteration 307800 | Loss: 0.262982\n",
      "Iteration 307825 | Loss: 0.262982\n",
      "Iteration 307850 | Loss: 0.262982\n",
      "Iteration 307875 | Loss: 0.262981\n",
      "Iteration 307900 | Loss: 0.262981\n",
      "Iteration 307925 | Loss: 0.262981\n",
      "Iteration 307950 | Loss: 0.262981\n",
      "Iteration 307975 | Loss: 0.262981\n",
      "Iteration 308000 | Loss: 0.262981\n",
      "Iteration 308025 | Loss: 0.262981\n",
      "Iteration 308050 | Loss: 0.262981\n",
      "Iteration 308075 | Loss: 0.262980\n",
      "Iteration 308100 | Loss: 0.262980\n",
      "Iteration 308125 | Loss: 0.262980\n",
      "Iteration 308150 | Loss: 0.262980\n",
      "Iteration 308175 | Loss: 0.262980\n",
      "Iteration 308200 | Loss: 0.262980\n",
      "Iteration 308225 | Loss: 0.262980\n",
      "Iteration 308250 | Loss: 0.262979\n",
      "Iteration 308275 | Loss: 0.262979\n",
      "Iteration 308300 | Loss: 0.262979\n",
      "Iteration 308325 | Loss: 0.262979\n",
      "Iteration 308350 | Loss: 0.262979\n",
      "Iteration 308375 | Loss: 0.262979\n",
      "Iteration 308400 | Loss: 0.262979\n",
      "Iteration 308425 | Loss: 0.262979\n",
      "Iteration 308450 | Loss: 0.262978\n",
      "Iteration 308475 | Loss: 0.262978\n",
      "Iteration 308500 | Loss: 0.262978\n",
      "Iteration 308525 | Loss: 0.262978\n",
      "Iteration 308550 | Loss: 0.262978\n",
      "Iteration 308575 | Loss: 0.262978\n",
      "Iteration 308600 | Loss: 0.262978\n",
      "Iteration 308625 | Loss: 0.262978\n",
      "Iteration 308650 | Loss: 0.262977\n",
      "Iteration 308675 | Loss: 0.262977\n",
      "Iteration 308700 | Loss: 0.262977\n",
      "Iteration 308725 | Loss: 0.262977\n",
      "Iteration 308750 | Loss: 0.262977\n",
      "Iteration 308775 | Loss: 0.262977\n",
      "Iteration 308800 | Loss: 0.262977\n",
      "Iteration 308825 | Loss: 0.262977\n",
      "Iteration 308850 | Loss: 0.262976\n",
      "Iteration 308875 | Loss: 0.262976\n",
      "Iteration 308900 | Loss: 0.262976\n",
      "Iteration 308925 | Loss: 0.262976\n",
      "Iteration 308950 | Loss: 0.262976\n",
      "Iteration 308975 | Loss: 0.262976\n",
      "Iteration 309000 | Loss: 0.262976\n",
      "Iteration 309025 | Loss: 0.262976\n",
      "Iteration 309050 | Loss: 0.262975\n",
      "Iteration 309075 | Loss: 0.262975\n",
      "Iteration 309100 | Loss: 0.262975\n",
      "Iteration 309125 | Loss: 0.262975\n",
      "Iteration 309150 | Loss: 0.262975\n",
      "Iteration 309175 | Loss: 0.262975\n",
      "Iteration 309200 | Loss: 0.262975\n",
      "Iteration 309225 | Loss: 0.262975\n",
      "Iteration 309250 | Loss: 0.262975\n",
      "Iteration 309275 | Loss: 0.262974\n",
      "Iteration 309300 | Loss: 0.262974\n",
      "Iteration 309325 | Loss: 0.262974\n",
      "Iteration 309350 | Loss: 0.262974\n",
      "Iteration 309375 | Loss: 0.262974\n",
      "Iteration 309400 | Loss: 0.262974\n",
      "Iteration 309425 | Loss: 0.262974\n",
      "Iteration 309450 | Loss: 0.262974\n",
      "Iteration 309475 | Loss: 0.262973\n",
      "Iteration 309500 | Loss: 0.262973\n",
      "Iteration 309525 | Loss: 0.262973\n",
      "Iteration 309550 | Loss: 0.262973\n",
      "Iteration 309575 | Loss: 0.262973\n",
      "Iteration 309600 | Loss: 0.262973\n",
      "Iteration 309625 | Loss: 0.262973\n",
      "Iteration 309650 | Loss: 0.262973\n",
      "Iteration 309675 | Loss: 0.262972\n",
      "Iteration 309700 | Loss: 0.262972\n",
      "Iteration 309725 | Loss: 0.262972\n",
      "Iteration 309750 | Loss: 0.262972\n",
      "Iteration 309775 | Loss: 0.262972\n",
      "Iteration 309800 | Loss: 0.262972\n",
      "Iteration 309825 | Loss: 0.262972\n",
      "Iteration 309850 | Loss: 0.262972\n",
      "Iteration 309875 | Loss: 0.262972\n",
      "Iteration 309900 | Loss: 0.262971\n",
      "Iteration 309925 | Loss: 0.262971\n",
      "Iteration 309950 | Loss: 0.262971\n",
      "Iteration 309975 | Loss: 0.262971\n",
      "Iteration 310000 | Loss: 0.262971\n",
      "Iteration 310025 | Loss: 0.262971\n",
      "Iteration 310050 | Loss: 0.262971\n",
      "Iteration 310075 | Loss: 0.262971\n",
      "Iteration 310100 | Loss: 0.262970\n",
      "Iteration 310125 | Loss: 0.262970\n",
      "Iteration 310150 | Loss: 0.262970\n",
      "Iteration 310175 | Loss: 0.262970\n",
      "Iteration 310200 | Loss: 0.262970\n",
      "Iteration 310225 | Loss: 0.262970\n",
      "Iteration 310250 | Loss: 0.262970\n",
      "Iteration 310275 | Loss: 0.262970\n",
      "Iteration 310300 | Loss: 0.262970\n",
      "Iteration 310325 | Loss: 0.262969\n",
      "Iteration 310350 | Loss: 0.262969\n",
      "Iteration 310375 | Loss: 0.262969\n",
      "Iteration 310400 | Loss: 0.262969\n",
      "Iteration 310425 | Loss: 0.262969\n",
      "Iteration 310450 | Loss: 0.262969\n",
      "Iteration 310475 | Loss: 0.262969\n",
      "Iteration 310500 | Loss: 0.262969\n",
      "Iteration 310525 | Loss: 0.262968\n",
      "Iteration 310550 | Loss: 0.262968\n",
      "Iteration 310575 | Loss: 0.262968\n",
      "Iteration 310600 | Loss: 0.262968\n",
      "Iteration 310625 | Loss: 0.262968\n",
      "Iteration 310650 | Loss: 0.262968\n",
      "Iteration 310675 | Loss: 0.262968\n",
      "Iteration 310700 | Loss: 0.262968\n",
      "Iteration 310725 | Loss: 0.262968\n",
      "Iteration 310750 | Loss: 0.262967\n",
      "Iteration 310775 | Loss: 0.262967\n",
      "Iteration 310800 | Loss: 0.262967\n",
      "Iteration 310825 | Loss: 0.262967\n",
      "Iteration 310850 | Loss: 0.262967\n",
      "Iteration 310875 | Loss: 0.262967\n",
      "Iteration 310900 | Loss: 0.262967\n",
      "Iteration 310925 | Loss: 0.262967\n",
      "Iteration 310950 | Loss: 0.262967\n",
      "Iteration 310975 | Loss: 0.262966\n",
      "Iteration 311000 | Loss: 0.262966\n",
      "Iteration 311025 | Loss: 0.262966\n",
      "Iteration 311050 | Loss: 0.262966\n",
      "Iteration 311075 | Loss: 0.262966\n",
      "Iteration 311100 | Loss: 0.262966\n",
      "Iteration 311125 | Loss: 0.262966\n",
      "Iteration 311150 | Loss: 0.262966\n",
      "Iteration 311175 | Loss: 0.262965\n",
      "Iteration 311200 | Loss: 0.262965\n",
      "Iteration 311225 | Loss: 0.262965\n",
      "Iteration 311250 | Loss: 0.262965\n",
      "Iteration 311275 | Loss: 0.262965\n",
      "Iteration 311300 | Loss: 0.262965\n",
      "Iteration 311325 | Loss: 0.262965\n",
      "Iteration 311350 | Loss: 0.262965\n",
      "Iteration 311375 | Loss: 0.262965\n",
      "Iteration 311400 | Loss: 0.262964\n",
      "Iteration 311425 | Loss: 0.262964\n",
      "Iteration 311450 | Loss: 0.262964\n",
      "Iteration 311475 | Loss: 0.262964\n",
      "Iteration 311500 | Loss: 0.262964\n",
      "Iteration 311525 | Loss: 0.262964\n",
      "Iteration 311550 | Loss: 0.262964\n",
      "Iteration 311575 | Loss: 0.262964\n",
      "Iteration 311600 | Loss: 0.262964\n",
      "Iteration 311625 | Loss: 0.262963\n",
      "Iteration 311650 | Loss: 0.262963\n",
      "Iteration 311675 | Loss: 0.262963\n",
      "Iteration 311700 | Loss: 0.262963\n",
      "Iteration 311725 | Loss: 0.262963\n",
      "Iteration 311750 | Loss: 0.262963\n",
      "Iteration 311775 | Loss: 0.262963\n",
      "Iteration 311800 | Loss: 0.262963\n",
      "Iteration 311825 | Loss: 0.262963\n",
      "Iteration 311850 | Loss: 0.262962\n",
      "Iteration 311875 | Loss: 0.262962\n",
      "Iteration 311900 | Loss: 0.262962\n",
      "Iteration 311925 | Loss: 0.262962\n",
      "Iteration 311950 | Loss: 0.262962\n",
      "Iteration 311975 | Loss: 0.262962\n",
      "Iteration 312000 | Loss: 0.262962\n",
      "Iteration 312025 | Loss: 0.262962\n",
      "Iteration 312050 | Loss: 0.262962\n",
      "Iteration 312075 | Loss: 0.262962\n",
      "Iteration 312100 | Loss: 0.262961\n",
      "Iteration 312125 | Loss: 0.262961\n",
      "Iteration 312150 | Loss: 0.262961\n",
      "Iteration 312175 | Loss: 0.262961\n",
      "Iteration 312200 | Loss: 0.262961\n",
      "Iteration 312225 | Loss: 0.262961\n",
      "Iteration 312250 | Loss: 0.262961\n",
      "Iteration 312275 | Loss: 0.262961\n",
      "Iteration 312300 | Loss: 0.262961\n",
      "Iteration 312325 | Loss: 0.262960\n",
      "Iteration 312350 | Loss: 0.262960\n",
      "Iteration 312375 | Loss: 0.262960\n",
      "Iteration 312400 | Loss: 0.262960\n",
      "Iteration 312425 | Loss: 0.262960\n",
      "Iteration 312450 | Loss: 0.262960\n",
      "Iteration 312475 | Loss: 0.262960\n",
      "Iteration 312500 | Loss: 0.262960\n",
      "Iteration 312525 | Loss: 0.262960\n",
      "Iteration 312550 | Loss: 0.262959\n",
      "Iteration 312575 | Loss: 0.262959\n",
      "Iteration 312600 | Loss: 0.262959\n",
      "Iteration 312625 | Loss: 0.262959\n",
      "Iteration 312650 | Loss: 0.262959\n",
      "Iteration 312675 | Loss: 0.262959\n",
      "Iteration 312700 | Loss: 0.262959\n",
      "Iteration 312725 | Loss: 0.262959\n",
      "Iteration 312750 | Loss: 0.262959\n",
      "Iteration 312775 | Loss: 0.262959\n",
      "Iteration 312800 | Loss: 0.262958\n",
      "Iteration 312825 | Loss: 0.262958\n",
      "Iteration 312850 | Loss: 0.262958\n",
      "Iteration 312875 | Loss: 0.262958\n",
      "Iteration 312900 | Loss: 0.262958\n",
      "Iteration 312925 | Loss: 0.262958\n",
      "Iteration 312950 | Loss: 0.262958\n",
      "Iteration 312975 | Loss: 0.262958\n",
      "Iteration 313000 | Loss: 0.262958\n",
      "Iteration 313025 | Loss: 0.262957\n",
      "Iteration 313050 | Loss: 0.262957\n",
      "Iteration 313075 | Loss: 0.262957\n",
      "Iteration 313100 | Loss: 0.262957\n",
      "Iteration 313125 | Loss: 0.262957\n",
      "Iteration 313150 | Loss: 0.262957\n",
      "Iteration 313175 | Loss: 0.262957\n",
      "Iteration 313200 | Loss: 0.262957\n",
      "Iteration 313225 | Loss: 0.262957\n",
      "Iteration 313250 | Loss: 0.262957\n",
      "Iteration 313275 | Loss: 0.262956\n",
      "Iteration 313300 | Loss: 0.262956\n",
      "Iteration 313325 | Loss: 0.262956\n",
      "Iteration 313350 | Loss: 0.262956\n",
      "Iteration 313375 | Loss: 0.262956\n",
      "Iteration 313400 | Loss: 0.262956\n",
      "Iteration 313425 | Loss: 0.262956\n",
      "Iteration 313450 | Loss: 0.262956\n",
      "Iteration 313475 | Loss: 0.262956\n",
      "Iteration 313500 | Loss: 0.262955\n",
      "Iteration 313525 | Loss: 0.262955\n",
      "Iteration 313550 | Loss: 0.262955\n",
      "Iteration 313575 | Loss: 0.262955\n",
      "Iteration 313600 | Loss: 0.262955\n",
      "Iteration 313625 | Loss: 0.262955\n",
      "Iteration 313650 | Loss: 0.262955\n",
      "Iteration 313675 | Loss: 0.262955\n",
      "Iteration 313700 | Loss: 0.262955\n",
      "Iteration 313725 | Loss: 0.262955\n",
      "Iteration 313750 | Loss: 0.262954\n",
      "Iteration 313775 | Loss: 0.262954\n",
      "Iteration 313800 | Loss: 0.262954\n",
      "Iteration 313825 | Loss: 0.262954\n",
      "Iteration 313850 | Loss: 0.262954\n",
      "Iteration 313875 | Loss: 0.262954\n",
      "Iteration 313900 | Loss: 0.262954\n",
      "Iteration 313925 | Loss: 0.262954\n",
      "Iteration 313950 | Loss: 0.262954\n",
      "Iteration 313975 | Loss: 0.262954\n",
      "Iteration 314000 | Loss: 0.262953\n",
      "Iteration 314025 | Loss: 0.262953\n",
      "Iteration 314050 | Loss: 0.262953\n",
      "Iteration 314075 | Loss: 0.262953\n",
      "Iteration 314100 | Loss: 0.262953\n",
      "Iteration 314125 | Loss: 0.262953\n",
      "Iteration 314150 | Loss: 0.262953\n",
      "Iteration 314175 | Loss: 0.262953\n",
      "Iteration 314200 | Loss: 0.262953\n",
      "Iteration 314225 | Loss: 0.262953\n",
      "Iteration 314250 | Loss: 0.262952\n",
      "Iteration 314275 | Loss: 0.262952\n",
      "Iteration 314300 | Loss: 0.262952\n",
      "Iteration 314325 | Loss: 0.262952\n",
      "Iteration 314350 | Loss: 0.262952\n",
      "Iteration 314375 | Loss: 0.262952\n",
      "Iteration 314400 | Loss: 0.262952\n",
      "Iteration 314425 | Loss: 0.262952\n",
      "Iteration 314450 | Loss: 0.262952\n",
      "Iteration 314475 | Loss: 0.262952\n",
      "Iteration 314500 | Loss: 0.262951\n",
      "Iteration 314525 | Loss: 0.262951\n",
      "Iteration 314550 | Loss: 0.262951\n",
      "Iteration 314575 | Loss: 0.262951\n",
      "Iteration 314600 | Loss: 0.262951\n",
      "Iteration 314625 | Loss: 0.262951\n",
      "Iteration 314650 | Loss: 0.262951\n",
      "Iteration 314675 | Loss: 0.262951\n",
      "Iteration 314700 | Loss: 0.262951\n",
      "Iteration 314725 | Loss: 0.262951\n",
      "Iteration 314750 | Loss: 0.262951\n",
      "Iteration 314775 | Loss: 0.262950\n",
      "Iteration 314800 | Loss: 0.262950\n",
      "Iteration 314825 | Loss: 0.262950\n",
      "Iteration 314850 | Loss: 0.262950\n",
      "Iteration 314875 | Loss: 0.262950\n",
      "Iteration 314900 | Loss: 0.262950\n",
      "Iteration 314925 | Loss: 0.262950\n",
      "Iteration 314950 | Loss: 0.262950\n",
      "Iteration 314975 | Loss: 0.262950\n",
      "Iteration 315000 | Loss: 0.262950\n",
      "Iteration 315025 | Loss: 0.262949\n",
      "Iteration 315050 | Loss: 0.262949\n",
      "Iteration 315075 | Loss: 0.262949\n",
      "Iteration 315100 | Loss: 0.262949\n",
      "Iteration 315125 | Loss: 0.262949\n",
      "Iteration 315150 | Loss: 0.262949\n",
      "Iteration 315175 | Loss: 0.262949\n",
      "Iteration 315200 | Loss: 0.262949\n",
      "Iteration 315225 | Loss: 0.262949\n",
      "Iteration 315250 | Loss: 0.262949\n",
      "Iteration 315275 | Loss: 0.262948\n",
      "Iteration 315300 | Loss: 0.262948\n",
      "Iteration 315325 | Loss: 0.262948\n",
      "Iteration 315350 | Loss: 0.262948\n",
      "Iteration 315375 | Loss: 0.262948\n",
      "Iteration 315400 | Loss: 0.262948\n",
      "Iteration 315425 | Loss: 0.262948\n",
      "Iteration 315450 | Loss: 0.262948\n",
      "Iteration 315475 | Loss: 0.262948\n",
      "Iteration 315500 | Loss: 0.262948\n",
      "Iteration 315525 | Loss: 0.262948\n",
      "Iteration 315550 | Loss: 0.262947\n",
      "Iteration 315575 | Loss: 0.262947\n",
      "Iteration 315600 | Loss: 0.262947\n",
      "Iteration 315625 | Loss: 0.262947\n",
      "Iteration 315650 | Loss: 0.262947\n",
      "Iteration 315675 | Loss: 0.262947\n",
      "Iteration 315700 | Loss: 0.262947\n",
      "Iteration 315725 | Loss: 0.262947\n",
      "Iteration 315750 | Loss: 0.262947\n",
      "Iteration 315775 | Loss: 0.262947\n",
      "Iteration 315800 | Loss: 0.262947\n",
      "Iteration 315825 | Loss: 0.262946\n",
      "Iteration 315850 | Loss: 0.262946\n",
      "Iteration 315875 | Loss: 0.262946\n",
      "Iteration 315900 | Loss: 0.262946\n",
      "Iteration 315925 | Loss: 0.262946\n",
      "Iteration 315950 | Loss: 0.262946\n",
      "Iteration 315975 | Loss: 0.262946\n",
      "Iteration 316000 | Loss: 0.262946\n",
      "Iteration 316025 | Loss: 0.262946\n",
      "Iteration 316050 | Loss: 0.262946\n",
      "Iteration 316075 | Loss: 0.262945\n",
      "Iteration 316100 | Loss: 0.262945\n",
      "Iteration 316125 | Loss: 0.262945\n",
      "Iteration 316150 | Loss: 0.262945\n",
      "Iteration 316175 | Loss: 0.262945\n",
      "Iteration 316200 | Loss: 0.262945\n",
      "Iteration 316225 | Loss: 0.262945\n",
      "Iteration 316250 | Loss: 0.262945\n",
      "Iteration 316275 | Loss: 0.262945\n",
      "Iteration 316300 | Loss: 0.262945\n",
      "Iteration 316325 | Loss: 0.262945\n",
      "Iteration 316350 | Loss: 0.262944\n",
      "Iteration 316375 | Loss: 0.262944\n",
      "Iteration 316400 | Loss: 0.262944\n",
      "Iteration 316425 | Loss: 0.262944\n",
      "Iteration 316450 | Loss: 0.262944\n",
      "Iteration 316475 | Loss: 0.262944\n",
      "Iteration 316500 | Loss: 0.262944\n",
      "Iteration 316525 | Loss: 0.262944\n",
      "Iteration 316550 | Loss: 0.262944\n",
      "Iteration 316575 | Loss: 0.262944\n",
      "Iteration 316600 | Loss: 0.262944\n",
      "Iteration 316625 | Loss: 0.262943\n",
      "Iteration 316650 | Loss: 0.262943\n",
      "Iteration 316675 | Loss: 0.262943\n",
      "Iteration 316700 | Loss: 0.262943\n",
      "Iteration 316725 | Loss: 0.262943\n",
      "Iteration 316750 | Loss: 0.262943\n",
      "Iteration 316775 | Loss: 0.262943\n",
      "Iteration 316800 | Loss: 0.262943\n",
      "Iteration 316825 | Loss: 0.262943\n",
      "Iteration 316850 | Loss: 0.262943\n",
      "Iteration 316875 | Loss: 0.262943\n",
      "Iteration 316900 | Loss: 0.262942\n",
      "Iteration 316925 | Loss: 0.262942\n",
      "Iteration 316950 | Loss: 0.262942\n",
      "Iteration 316975 | Loss: 0.262942\n",
      "Iteration 317000 | Loss: 0.262942\n",
      "Iteration 317025 | Loss: 0.262942\n",
      "Iteration 317050 | Loss: 0.262942\n",
      "Iteration 317075 | Loss: 0.262942\n",
      "Iteration 317100 | Loss: 0.262942\n",
      "Iteration 317125 | Loss: 0.262942\n",
      "Iteration 317150 | Loss: 0.262942\n",
      "Iteration 317175 | Loss: 0.262942\n",
      "Iteration 317200 | Loss: 0.262941\n",
      "Iteration 317225 | Loss: 0.262941\n",
      "Iteration 317250 | Loss: 0.262941\n",
      "Iteration 317275 | Loss: 0.262941\n",
      "Iteration 317300 | Loss: 0.262941\n",
      "Iteration 317325 | Loss: 0.262941\n",
      "Iteration 317350 | Loss: 0.262941\n",
      "Iteration 317375 | Loss: 0.262941\n",
      "Iteration 317400 | Loss: 0.262941\n",
      "Iteration 317425 | Loss: 0.262941\n",
      "Iteration 317450 | Loss: 0.262941\n",
      "Iteration 317475 | Loss: 0.262940\n",
      "Iteration 317500 | Loss: 0.262940\n",
      "Iteration 317525 | Loss: 0.262940\n",
      "Iteration 317550 | Loss: 0.262940\n",
      "Iteration 317575 | Loss: 0.262940\n",
      "Iteration 317600 | Loss: 0.262940\n",
      "Iteration 317625 | Loss: 0.262940\n",
      "Iteration 317650 | Loss: 0.262940\n",
      "Iteration 317675 | Loss: 0.262940\n",
      "Iteration 317700 | Loss: 0.262940\n",
      "Iteration 317725 | Loss: 0.262940\n",
      "Iteration 317750 | Loss: 0.262940\n",
      "Iteration 317775 | Loss: 0.262939\n",
      "Iteration 317800 | Loss: 0.262939\n",
      "Iteration 317825 | Loss: 0.262939\n",
      "Iteration 317850 | Loss: 0.262939\n",
      "Iteration 317875 | Loss: 0.262939\n",
      "Iteration 317900 | Loss: 0.262939\n",
      "Iteration 317925 | Loss: 0.262939\n",
      "Iteration 317950 | Loss: 0.262939\n",
      "Iteration 317975 | Loss: 0.262939\n",
      "Iteration 318000 | Loss: 0.262939\n",
      "Iteration 318025 | Loss: 0.262939\n",
      "Iteration 318050 | Loss: 0.262938\n",
      "Iteration 318075 | Loss: 0.262938\n",
      "Iteration 318100 | Loss: 0.262938\n",
      "Iteration 318125 | Loss: 0.262938\n",
      "Iteration 318150 | Loss: 0.262938\n",
      "Iteration 318175 | Loss: 0.262938\n",
      "Iteration 318200 | Loss: 0.262938\n",
      "Iteration 318225 | Loss: 0.262938\n",
      "Iteration 318250 | Loss: 0.262938\n",
      "Iteration 318275 | Loss: 0.262938\n",
      "Iteration 318300 | Loss: 0.262938\n",
      "Iteration 318325 | Loss: 0.262938\n",
      "Iteration 318350 | Loss: 0.262937\n",
      "Iteration 318375 | Loss: 0.262937\n",
      "Iteration 318400 | Loss: 0.262937\n",
      "Iteration 318425 | Loss: 0.262937\n",
      "Iteration 318450 | Loss: 0.262937\n",
      "Iteration 318475 | Loss: 0.262937\n",
      "Iteration 318500 | Loss: 0.262937\n",
      "Iteration 318525 | Loss: 0.262937\n",
      "Iteration 318550 | Loss: 0.262937\n",
      "Iteration 318575 | Loss: 0.262937\n",
      "Iteration 318600 | Loss: 0.262937\n",
      "Iteration 318625 | Loss: 0.262937\n",
      "Iteration 318650 | Loss: 0.262936\n",
      "Iteration 318675 | Loss: 0.262936\n",
      "Iteration 318700 | Loss: 0.262936\n",
      "Iteration 318725 | Loss: 0.262936\n",
      "Iteration 318750 | Loss: 0.262936\n",
      "Iteration 318775 | Loss: 0.262936\n",
      "Iteration 318800 | Loss: 0.262936\n",
      "Iteration 318825 | Loss: 0.262936\n",
      "Iteration 318850 | Loss: 0.262936\n",
      "Iteration 318875 | Loss: 0.262936\n",
      "Iteration 318900 | Loss: 0.262936\n",
      "Iteration 318925 | Loss: 0.262936\n",
      "Iteration 318950 | Loss: 0.262935\n",
      "Iteration 318975 | Loss: 0.262935\n",
      "Iteration 319000 | Loss: 0.262935\n",
      "Iteration 319025 | Loss: 0.262935\n",
      "Iteration 319050 | Loss: 0.262935\n",
      "Iteration 319075 | Loss: 0.262935\n",
      "Iteration 319100 | Loss: 0.262935\n",
      "Iteration 319125 | Loss: 0.262935\n",
      "Iteration 319150 | Loss: 0.262935\n",
      "Iteration 319175 | Loss: 0.262935\n",
      "Iteration 319200 | Loss: 0.262935\n",
      "Iteration 319225 | Loss: 0.262935\n",
      "Iteration 319250 | Loss: 0.262935\n",
      "Iteration 319275 | Loss: 0.262934\n",
      "Iteration 319300 | Loss: 0.262934\n",
      "Iteration 319325 | Loss: 0.262934\n",
      "Iteration 319350 | Loss: 0.262934\n",
      "Iteration 319375 | Loss: 0.262934\n",
      "Iteration 319400 | Loss: 0.262934\n",
      "Iteration 319425 | Loss: 0.262934\n",
      "Iteration 319450 | Loss: 0.262934\n",
      "Iteration 319475 | Loss: 0.262934\n",
      "Iteration 319500 | Loss: 0.262934\n",
      "Iteration 319525 | Loss: 0.262934\n",
      "Iteration 319550 | Loss: 0.262934\n",
      "Iteration 319575 | Loss: 0.262933\n",
      "Iteration 319600 | Loss: 0.262933\n",
      "Iteration 319625 | Loss: 0.262933\n",
      "Iteration 319650 | Loss: 0.262933\n",
      "Iteration 319675 | Loss: 0.262933\n",
      "Iteration 319700 | Loss: 0.262933\n",
      "Iteration 319725 | Loss: 0.262933\n",
      "Iteration 319750 | Loss: 0.262933\n",
      "Iteration 319775 | Loss: 0.262933\n",
      "Iteration 319800 | Loss: 0.262933\n",
      "Iteration 319825 | Loss: 0.262933\n",
      "Iteration 319850 | Loss: 0.262933\n",
      "Iteration 319875 | Loss: 0.262933\n",
      "Iteration 319900 | Loss: 0.262932\n",
      "Iteration 319925 | Loss: 0.262932\n",
      "Iteration 319950 | Loss: 0.262932\n",
      "Iteration 319975 | Loss: 0.262932\n",
      "Iteration 320000 | Loss: 0.262932\n",
      "Iteration 320025 | Loss: 0.262932\n",
      "Iteration 320050 | Loss: 0.262932\n",
      "Iteration 320075 | Loss: 0.262932\n",
      "Iteration 320100 | Loss: 0.262932\n",
      "Iteration 320125 | Loss: 0.262932\n",
      "Iteration 320150 | Loss: 0.262932\n",
      "Iteration 320175 | Loss: 0.262932\n",
      "Iteration 320200 | Loss: 0.262932\n",
      "Iteration 320225 | Loss: 0.262931\n",
      "Iteration 320250 | Loss: 0.262931\n",
      "Iteration 320275 | Loss: 0.262931\n",
      "Iteration 320300 | Loss: 0.262931\n",
      "Iteration 320325 | Loss: 0.262931\n",
      "Iteration 320350 | Loss: 0.262931\n",
      "Iteration 320375 | Loss: 0.262931\n",
      "Iteration 320400 | Loss: 0.262931\n",
      "Iteration 320425 | Loss: 0.262931\n",
      "Iteration 320450 | Loss: 0.262931\n",
      "Iteration 320475 | Loss: 0.262931\n",
      "Iteration 320500 | Loss: 0.262931\n",
      "Iteration 320525 | Loss: 0.262930\n",
      "Iteration 320550 | Loss: 0.262930\n",
      "Iteration 320575 | Loss: 0.262930\n",
      "Iteration 320600 | Loss: 0.262930\n",
      "Iteration 320625 | Loss: 0.262930\n",
      "Iteration 320650 | Loss: 0.262930\n",
      "Iteration 320675 | Loss: 0.262930\n",
      "Iteration 320700 | Loss: 0.262930\n",
      "Iteration 320725 | Loss: 0.262930\n",
      "Iteration 320750 | Loss: 0.262930\n",
      "Iteration 320775 | Loss: 0.262930\n",
      "Iteration 320800 | Loss: 0.262930\n",
      "Iteration 320825 | Loss: 0.262930\n",
      "Iteration 320850 | Loss: 0.262930\n",
      "Iteration 320875 | Loss: 0.262929\n",
      "Iteration 320900 | Loss: 0.262929\n",
      "Iteration 320925 | Loss: 0.262929\n",
      "Iteration 320950 | Loss: 0.262929\n",
      "Iteration 320975 | Loss: 0.262929\n",
      "Iteration 321000 | Loss: 0.262929\n",
      "Iteration 321025 | Loss: 0.262929\n",
      "Iteration 321050 | Loss: 0.262929\n",
      "Iteration 321075 | Loss: 0.262929\n",
      "Iteration 321100 | Loss: 0.262929\n",
      "Iteration 321125 | Loss: 0.262929\n",
      "Iteration 321150 | Loss: 0.262929\n",
      "Iteration 321175 | Loss: 0.262929\n",
      "Iteration 321200 | Loss: 0.262928\n",
      "Iteration 321225 | Loss: 0.262928\n",
      "Iteration 321250 | Loss: 0.262928\n",
      "Iteration 321275 | Loss: 0.262928\n",
      "Iteration 321300 | Loss: 0.262928\n",
      "Iteration 321325 | Loss: 0.262928\n",
      "Iteration 321350 | Loss: 0.262928\n",
      "Iteration 321375 | Loss: 0.262928\n",
      "Iteration 321400 | Loss: 0.262928\n",
      "Iteration 321425 | Loss: 0.262928\n",
      "Iteration 321450 | Loss: 0.262928\n",
      "Iteration 321475 | Loss: 0.262928\n",
      "Iteration 321500 | Loss: 0.262928\n",
      "Iteration 321525 | Loss: 0.262927\n",
      "Iteration 321550 | Loss: 0.262927\n",
      "Iteration 321575 | Loss: 0.262927\n",
      "Iteration 321600 | Loss: 0.262927\n",
      "Iteration 321625 | Loss: 0.262927\n",
      "Iteration 321650 | Loss: 0.262927\n",
      "Iteration 321675 | Loss: 0.262927\n",
      "Iteration 321700 | Loss: 0.262927\n",
      "Iteration 321725 | Loss: 0.262927\n",
      "Iteration 321750 | Loss: 0.262927\n",
      "Iteration 321775 | Loss: 0.262927\n",
      "Iteration 321800 | Loss: 0.262927\n",
      "Iteration 321825 | Loss: 0.262927\n",
      "Iteration 321850 | Loss: 0.262927\n",
      "Iteration 321875 | Loss: 0.262926\n",
      "Iteration 321900 | Loss: 0.262926\n",
      "Iteration 321925 | Loss: 0.262926\n",
      "Iteration 321950 | Loss: 0.262926\n",
      "Iteration 321975 | Loss: 0.262926\n",
      "Iteration 322000 | Loss: 0.262926\n",
      "Iteration 322025 | Loss: 0.262926\n",
      "Iteration 322050 | Loss: 0.262926\n",
      "Iteration 322075 | Loss: 0.262926\n",
      "Iteration 322100 | Loss: 0.262926\n",
      "Iteration 322125 | Loss: 0.262926\n",
      "Iteration 322150 | Loss: 0.262926\n",
      "Iteration 322175 | Loss: 0.262926\n",
      "Iteration 322200 | Loss: 0.262926\n",
      "Iteration 322225 | Loss: 0.262925\n",
      "Iteration 322250 | Loss: 0.262925\n",
      "Iteration 322275 | Loss: 0.262925\n",
      "Iteration 322300 | Loss: 0.262925\n",
      "Iteration 322325 | Loss: 0.262925\n",
      "Iteration 322350 | Loss: 0.262925\n",
      "Iteration 322375 | Loss: 0.262925\n",
      "Iteration 322400 | Loss: 0.262925\n",
      "Iteration 322425 | Loss: 0.262925\n",
      "Iteration 322450 | Loss: 0.262925\n",
      "Iteration 322475 | Loss: 0.262925\n",
      "Iteration 322500 | Loss: 0.262925\n",
      "Iteration 322525 | Loss: 0.262925\n",
      "Iteration 322550 | Loss: 0.262925\n",
      "Iteration 322575 | Loss: 0.262924\n",
      "Iteration 322600 | Loss: 0.262924\n",
      "Iteration 322625 | Loss: 0.262924\n",
      "Iteration 322650 | Loss: 0.262924\n",
      "Iteration 322675 | Loss: 0.262924\n",
      "Iteration 322700 | Loss: 0.262924\n",
      "Iteration 322725 | Loss: 0.262924\n",
      "Iteration 322750 | Loss: 0.262924\n",
      "Iteration 322775 | Loss: 0.262924\n",
      "Iteration 322800 | Loss: 0.262924\n",
      "Iteration 322825 | Loss: 0.262924\n",
      "Iteration 322850 | Loss: 0.262924\n",
      "Iteration 322875 | Loss: 0.262924\n",
      "Iteration 322900 | Loss: 0.262924\n",
      "Iteration 322925 | Loss: 0.262923\n",
      "Iteration 322950 | Loss: 0.262923\n",
      "Iteration 322975 | Loss: 0.262923\n",
      "Iteration 323000 | Loss: 0.262923\n",
      "Iteration 323025 | Loss: 0.262923\n",
      "Iteration 323050 | Loss: 0.262923\n",
      "Iteration 323075 | Loss: 0.262923\n",
      "Iteration 323100 | Loss: 0.262923\n",
      "Iteration 323125 | Loss: 0.262923\n",
      "Iteration 323150 | Loss: 0.262923\n",
      "Iteration 323175 | Loss: 0.262923\n",
      "Iteration 323200 | Loss: 0.262923\n",
      "Iteration 323225 | Loss: 0.262923\n",
      "Iteration 323250 | Loss: 0.262923\n",
      "Iteration 323275 | Loss: 0.262923\n",
      "Iteration 323300 | Loss: 0.262922\n",
      "Iteration 323325 | Loss: 0.262922\n",
      "Iteration 323350 | Loss: 0.262922\n",
      "Iteration 323375 | Loss: 0.262922\n",
      "Iteration 323400 | Loss: 0.262922\n",
      "Iteration 323425 | Loss: 0.262922\n",
      "Iteration 323450 | Loss: 0.262922\n",
      "Iteration 323475 | Loss: 0.262922\n",
      "Iteration 323500 | Loss: 0.262922\n",
      "Iteration 323525 | Loss: 0.262922\n",
      "Iteration 323550 | Loss: 0.262922\n",
      "Iteration 323575 | Loss: 0.262922\n",
      "Iteration 323600 | Loss: 0.262922\n",
      "Iteration 323625 | Loss: 0.262922\n",
      "Iteration 323650 | Loss: 0.262921\n",
      "Iteration 323675 | Loss: 0.262921\n",
      "Iteration 323700 | Loss: 0.262921\n",
      "Iteration 323725 | Loss: 0.262921\n",
      "Iteration 323750 | Loss: 0.262921\n",
      "Iteration 323775 | Loss: 0.262921\n",
      "Iteration 323800 | Loss: 0.262921\n",
      "Iteration 323825 | Loss: 0.262921\n",
      "Iteration 323850 | Loss: 0.262921\n",
      "Iteration 323875 | Loss: 0.262921\n",
      "Iteration 323900 | Loss: 0.262921\n",
      "Iteration 323925 | Loss: 0.262921\n",
      "Iteration 323950 | Loss: 0.262921\n",
      "Iteration 323975 | Loss: 0.262921\n",
      "Iteration 324000 | Loss: 0.262921\n",
      "Iteration 324025 | Loss: 0.262920\n",
      "Iteration 324050 | Loss: 0.262920\n",
      "Iteration 324075 | Loss: 0.262920\n",
      "Iteration 324100 | Loss: 0.262920\n",
      "Iteration 324125 | Loss: 0.262920\n",
      "Iteration 324150 | Loss: 0.262920\n",
      "Iteration 324175 | Loss: 0.262920\n",
      "Iteration 324200 | Loss: 0.262920\n",
      "Iteration 324225 | Loss: 0.262920\n",
      "Iteration 324250 | Loss: 0.262920\n",
      "Iteration 324275 | Loss: 0.262920\n",
      "Iteration 324300 | Loss: 0.262920\n",
      "Iteration 324325 | Loss: 0.262920\n",
      "Iteration 324350 | Loss: 0.262920\n",
      "Iteration 324375 | Loss: 0.262920\n",
      "Iteration 324400 | Loss: 0.262920\n",
      "Iteration 324425 | Loss: 0.262919\n",
      "Iteration 324450 | Loss: 0.262919\n",
      "Iteration 324475 | Loss: 0.262919\n",
      "Iteration 324500 | Loss: 0.262919\n",
      "Iteration 324525 | Loss: 0.262919\n",
      "Iteration 324550 | Loss: 0.262919\n",
      "Iteration 324575 | Loss: 0.262919\n",
      "Iteration 324600 | Loss: 0.262919\n",
      "Iteration 324625 | Loss: 0.262919\n",
      "Iteration 324650 | Loss: 0.262919\n",
      "Iteration 324675 | Loss: 0.262919\n",
      "Iteration 324700 | Loss: 0.262919\n",
      "Iteration 324725 | Loss: 0.262919\n",
      "Iteration 324750 | Loss: 0.262919\n",
      "Iteration 324775 | Loss: 0.262919\n",
      "Iteration 324800 | Loss: 0.262918\n",
      "Iteration 324825 | Loss: 0.262918\n",
      "Iteration 324850 | Loss: 0.262918\n",
      "Iteration 324875 | Loss: 0.262918\n",
      "Iteration 324900 | Loss: 0.262918\n",
      "Iteration 324925 | Loss: 0.262918\n",
      "Iteration 324950 | Loss: 0.262918\n",
      "Iteration 324975 | Loss: 0.262918\n",
      "Iteration 325000 | Loss: 0.262918\n",
      "Iteration 325025 | Loss: 0.262918\n",
      "Iteration 325050 | Loss: 0.262918\n",
      "Iteration 325075 | Loss: 0.262918\n",
      "Iteration 325100 | Loss: 0.262918\n",
      "Iteration 325125 | Loss: 0.262918\n",
      "Iteration 325150 | Loss: 0.262918\n",
      "Iteration 325175 | Loss: 0.262918\n",
      "Iteration 325200 | Loss: 0.262917\n",
      "Iteration 325225 | Loss: 0.262917\n",
      "Iteration 325250 | Loss: 0.262917\n",
      "Iteration 325275 | Loss: 0.262917\n",
      "Iteration 325300 | Loss: 0.262917\n",
      "Iteration 325325 | Loss: 0.262917\n",
      "Iteration 325350 | Loss: 0.262917\n",
      "Iteration 325375 | Loss: 0.262917\n",
      "Iteration 325400 | Loss: 0.262917\n",
      "Iteration 325425 | Loss: 0.262917\n",
      "Iteration 325450 | Loss: 0.262917\n",
      "Iteration 325475 | Loss: 0.262917\n",
      "Iteration 325500 | Loss: 0.262917\n",
      "Iteration 325525 | Loss: 0.262917\n",
      "Iteration 325550 | Loss: 0.262917\n",
      "Iteration 325575 | Loss: 0.262916\n",
      "Iteration 325600 | Loss: 0.262916\n",
      "Iteration 325625 | Loss: 0.262916\n",
      "Iteration 325650 | Loss: 0.262916\n",
      "Iteration 325675 | Loss: 0.262916\n",
      "Iteration 325700 | Loss: 0.262916\n",
      "Iteration 325725 | Loss: 0.262916\n",
      "Iteration 325750 | Loss: 0.262916\n",
      "Iteration 325775 | Loss: 0.262916\n",
      "Iteration 325800 | Loss: 0.262916\n",
      "Iteration 325825 | Loss: 0.262916\n",
      "Iteration 325850 | Loss: 0.262916\n",
      "Iteration 325875 | Loss: 0.262916\n",
      "Iteration 325900 | Loss: 0.262916\n",
      "Iteration 325925 | Loss: 0.262916\n",
      "Iteration 325950 | Loss: 0.262916\n",
      "Iteration 325975 | Loss: 0.262916\n",
      "Iteration 326000 | Loss: 0.262915\n",
      "Iteration 326025 | Loss: 0.262915\n",
      "Iteration 326050 | Loss: 0.262915\n",
      "Iteration 326075 | Loss: 0.262915\n",
      "Iteration 326100 | Loss: 0.262915\n",
      "Iteration 326125 | Loss: 0.262915\n",
      "Iteration 326150 | Loss: 0.262915\n",
      "Iteration 326175 | Loss: 0.262915\n",
      "Iteration 326200 | Loss: 0.262915\n",
      "Iteration 326225 | Loss: 0.262915\n",
      "Iteration 326250 | Loss: 0.262915\n",
      "Iteration 326275 | Loss: 0.262915\n",
      "Iteration 326300 | Loss: 0.262915\n",
      "Iteration 326325 | Loss: 0.262915\n",
      "Iteration 326350 | Loss: 0.262915\n",
      "Iteration 326375 | Loss: 0.262915\n",
      "Iteration 326400 | Loss: 0.262914\n",
      "Iteration 326425 | Loss: 0.262914\n",
      "Iteration 326450 | Loss: 0.262914\n",
      "Iteration 326475 | Loss: 0.262914\n",
      "Iteration 326500 | Loss: 0.262914\n",
      "Iteration 326525 | Loss: 0.262914\n",
      "Iteration 326550 | Loss: 0.262914\n",
      "Iteration 326575 | Loss: 0.262914\n",
      "Iteration 326600 | Loss: 0.262914\n",
      "Iteration 326625 | Loss: 0.262914\n",
      "Iteration 326650 | Loss: 0.262914\n",
      "Iteration 326675 | Loss: 0.262914\n",
      "Iteration 326700 | Loss: 0.262914\n",
      "Iteration 326725 | Loss: 0.262914\n",
      "Iteration 326750 | Loss: 0.262914\n",
      "Iteration 326775 | Loss: 0.262914\n",
      "Iteration 326800 | Loss: 0.262914\n",
      "Iteration 326825 | Loss: 0.262913\n",
      "Iteration 326850 | Loss: 0.262913\n",
      "Iteration 326875 | Loss: 0.262913\n",
      "Iteration 326900 | Loss: 0.262913\n",
      "Iteration 326925 | Loss: 0.262913\n",
      "Iteration 326950 | Loss: 0.262913\n",
      "Iteration 326975 | Loss: 0.262913\n",
      "Iteration 327000 | Loss: 0.262913\n",
      "Iteration 327025 | Loss: 0.262913\n",
      "Iteration 327050 | Loss: 0.262913\n",
      "Iteration 327075 | Loss: 0.262913\n",
      "Iteration 327100 | Loss: 0.262913\n",
      "Iteration 327125 | Loss: 0.262913\n",
      "Iteration 327150 | Loss: 0.262913\n",
      "Iteration 327175 | Loss: 0.262913\n",
      "Iteration 327200 | Loss: 0.262913\n",
      "Iteration 327225 | Loss: 0.262913\n",
      "Iteration 327250 | Loss: 0.262912\n",
      "Iteration 327275 | Loss: 0.262912\n",
      "Iteration 327300 | Loss: 0.262912\n",
      "Iteration 327325 | Loss: 0.262912\n",
      "Iteration 327350 | Loss: 0.262912\n",
      "Iteration 327375 | Loss: 0.262912\n",
      "Iteration 327400 | Loss: 0.262912\n",
      "Iteration 327425 | Loss: 0.262912\n",
      "Iteration 327450 | Loss: 0.262912\n",
      "Iteration 327475 | Loss: 0.262912\n",
      "Iteration 327500 | Loss: 0.262912\n",
      "Iteration 327525 | Loss: 0.262912\n",
      "Iteration 327550 | Loss: 0.262912\n",
      "Iteration 327575 | Loss: 0.262912\n",
      "Iteration 327600 | Loss: 0.262912\n",
      "Iteration 327625 | Loss: 0.262912\n",
      "Iteration 327650 | Loss: 0.262912\n",
      "Iteration 327675 | Loss: 0.262911\n",
      "Iteration 327700 | Loss: 0.262911\n",
      "Iteration 327725 | Loss: 0.262911\n",
      "Iteration 327750 | Loss: 0.262911\n",
      "Iteration 327775 | Loss: 0.262911\n",
      "Iteration 327800 | Loss: 0.262911\n",
      "Iteration 327825 | Loss: 0.262911\n",
      "Iteration 327850 | Loss: 0.262911\n",
      "Iteration 327875 | Loss: 0.262911\n",
      "Iteration 327900 | Loss: 0.262911\n",
      "Iteration 327925 | Loss: 0.262911\n",
      "Iteration 327950 | Loss: 0.262911\n",
      "Iteration 327975 | Loss: 0.262911\n",
      "Iteration 328000 | Loss: 0.262911\n",
      "Iteration 328025 | Loss: 0.262911\n",
      "Iteration 328050 | Loss: 0.262911\n",
      "Iteration 328075 | Loss: 0.262911\n",
      "Iteration 328100 | Loss: 0.262911\n",
      "Iteration 328125 | Loss: 0.262910\n",
      "Iteration 328150 | Loss: 0.262910\n",
      "Iteration 328175 | Loss: 0.262910\n",
      "Iteration 328200 | Loss: 0.262910\n",
      "Iteration 328225 | Loss: 0.262910\n",
      "Iteration 328250 | Loss: 0.262910\n",
      "Iteration 328275 | Loss: 0.262910\n",
      "Iteration 328300 | Loss: 0.262910\n",
      "Iteration 328325 | Loss: 0.262910\n",
      "Iteration 328350 | Loss: 0.262910\n",
      "Iteration 328375 | Loss: 0.262910\n",
      "Iteration 328400 | Loss: 0.262910\n",
      "Iteration 328425 | Loss: 0.262910\n",
      "Iteration 328450 | Loss: 0.262910\n",
      "Iteration 328475 | Loss: 0.262910\n",
      "Iteration 328500 | Loss: 0.262910\n",
      "Iteration 328525 | Loss: 0.262910\n",
      "Iteration 328550 | Loss: 0.262910\n",
      "Iteration 328575 | Loss: 0.262909\n",
      "Iteration 328600 | Loss: 0.262909\n",
      "Iteration 328625 | Loss: 0.262909\n",
      "Iteration 328650 | Loss: 0.262909\n",
      "Iteration 328675 | Loss: 0.262909\n",
      "Iteration 328700 | Loss: 0.262909\n",
      "Iteration 328725 | Loss: 0.262909\n",
      "Iteration 328750 | Loss: 0.262909\n",
      "Iteration 328775 | Loss: 0.262909\n",
      "Iteration 328800 | Loss: 0.262909\n",
      "Iteration 328825 | Loss: 0.262909\n",
      "Iteration 328850 | Loss: 0.262909\n",
      "Iteration 328875 | Loss: 0.262909\n",
      "Iteration 328900 | Loss: 0.262909\n",
      "Iteration 328925 | Loss: 0.262909\n",
      "Iteration 328950 | Loss: 0.262909\n",
      "Iteration 328975 | Loss: 0.262909\n",
      "Iteration 329000 | Loss: 0.262909\n",
      "Iteration 329025 | Loss: 0.262908\n",
      "Iteration 329050 | Loss: 0.262908\n",
      "Iteration 329075 | Loss: 0.262908\n",
      "Iteration 329100 | Loss: 0.262908\n",
      "Iteration 329125 | Loss: 0.262908\n",
      "Iteration 329150 | Loss: 0.262908\n",
      "Iteration 329175 | Loss: 0.262908\n",
      "Iteration 329200 | Loss: 0.262908\n",
      "Iteration 329225 | Loss: 0.262908\n",
      "Iteration 329250 | Loss: 0.262908\n",
      "Iteration 329275 | Loss: 0.262908\n",
      "Iteration 329300 | Loss: 0.262908\n",
      "Iteration 329325 | Loss: 0.262908\n",
      "Iteration 329350 | Loss: 0.262908\n",
      "Iteration 329375 | Loss: 0.262908\n",
      "Iteration 329400 | Loss: 0.262908\n",
      "Iteration 329425 | Loss: 0.262908\n",
      "Iteration 329450 | Loss: 0.262908\n",
      "Iteration 329475 | Loss: 0.262908\n",
      "Iteration 329500 | Loss: 0.262907\n",
      "Iteration 329525 | Loss: 0.262907\n",
      "Iteration 329550 | Loss: 0.262907\n",
      "Iteration 329575 | Loss: 0.262907\n",
      "Iteration 329600 | Loss: 0.262907\n",
      "Iteration 329625 | Loss: 0.262907\n",
      "Iteration 329650 | Loss: 0.262907\n",
      "Iteration 329675 | Loss: 0.262907\n",
      "Iteration 329700 | Loss: 0.262907\n",
      "Iteration 329725 | Loss: 0.262907\n",
      "Iteration 329750 | Loss: 0.262907\n",
      "Iteration 329775 | Loss: 0.262907\n",
      "Iteration 329800 | Loss: 0.262907\n",
      "Iteration 329825 | Loss: 0.262907\n",
      "Iteration 329850 | Loss: 0.262907\n",
      "Iteration 329875 | Loss: 0.262907\n",
      "Iteration 329900 | Loss: 0.262907\n",
      "Iteration 329925 | Loss: 0.262907\n",
      "Iteration 329950 | Loss: 0.262907\n",
      "Iteration 329975 | Loss: 0.262906\n",
      "Iteration 330000 | Loss: 0.262906\n",
      "Iteration 330025 | Loss: 0.262906\n",
      "Iteration 330050 | Loss: 0.262906\n",
      "Iteration 330075 | Loss: 0.262906\n",
      "Iteration 330100 | Loss: 0.262906\n",
      "Iteration 330125 | Loss: 0.262906\n",
      "Iteration 330150 | Loss: 0.262906\n",
      "Iteration 330175 | Loss: 0.262906\n",
      "Iteration 330200 | Loss: 0.262906\n",
      "Iteration 330225 | Loss: 0.262906\n",
      "Iteration 330250 | Loss: 0.262906\n",
      "Iteration 330275 | Loss: 0.262906\n",
      "Iteration 330300 | Loss: 0.262906\n",
      "Iteration 330325 | Loss: 0.262906\n",
      "Iteration 330350 | Loss: 0.262906\n",
      "Iteration 330375 | Loss: 0.262906\n",
      "Iteration 330400 | Loss: 0.262906\n",
      "Iteration 330425 | Loss: 0.262906\n",
      "Iteration 330450 | Loss: 0.262905\n",
      "Iteration 330475 | Loss: 0.262905\n",
      "Iteration 330500 | Loss: 0.262905\n",
      "Iteration 330525 | Loss: 0.262905\n",
      "Iteration 330550 | Loss: 0.262905\n",
      "Iteration 330575 | Loss: 0.262905\n",
      "Iteration 330600 | Loss: 0.262905\n",
      "Iteration 330625 | Loss: 0.262905\n",
      "Iteration 330650 | Loss: 0.262905\n",
      "Iteration 330675 | Loss: 0.262905\n",
      "Iteration 330700 | Loss: 0.262905\n",
      "Iteration 330725 | Loss: 0.262905\n",
      "Iteration 330750 | Loss: 0.262905\n",
      "Iteration 330775 | Loss: 0.262905\n",
      "Iteration 330800 | Loss: 0.262905\n",
      "Iteration 330825 | Loss: 0.262905\n",
      "Iteration 330850 | Loss: 0.262905\n",
      "Iteration 330875 | Loss: 0.262905\n",
      "Iteration 330900 | Loss: 0.262905\n",
      "Iteration 330925 | Loss: 0.262905\n",
      "Iteration 330950 | Loss: 0.262904\n",
      "Iteration 330975 | Loss: 0.262904\n",
      "Iteration 331000 | Loss: 0.262904\n",
      "Iteration 331025 | Loss: 0.262904\n",
      "Iteration 331050 | Loss: 0.262904\n",
      "Iteration 331075 | Loss: 0.262904\n",
      "Iteration 331100 | Loss: 0.262904\n",
      "Iteration 331125 | Loss: 0.262904\n",
      "Iteration 331150 | Loss: 0.262904\n",
      "Iteration 331175 | Loss: 0.262904\n",
      "Iteration 331200 | Loss: 0.262904\n",
      "Iteration 331225 | Loss: 0.262904\n",
      "Iteration 331250 | Loss: 0.262904\n",
      "Iteration 331275 | Loss: 0.262904\n",
      "Iteration 331300 | Loss: 0.262904\n",
      "Iteration 331325 | Loss: 0.262904\n",
      "Iteration 331350 | Loss: 0.262904\n",
      "Iteration 331375 | Loss: 0.262904\n",
      "Iteration 331400 | Loss: 0.262904\n",
      "Iteration 331425 | Loss: 0.262904\n",
      "Iteration 331450 | Loss: 0.262903\n",
      "Iteration 331475 | Loss: 0.262903\n",
      "Iteration 331500 | Loss: 0.262903\n",
      "Iteration 331525 | Loss: 0.262903\n",
      "Iteration 331550 | Loss: 0.262903\n",
      "Iteration 331575 | Loss: 0.262903\n",
      "Iteration 331600 | Loss: 0.262903\n",
      "Iteration 331625 | Loss: 0.262903\n",
      "Iteration 331650 | Loss: 0.262903\n",
      "Iteration 331675 | Loss: 0.262903\n",
      "Iteration 331700 | Loss: 0.262903\n",
      "Iteration 331725 | Loss: 0.262903\n",
      "Iteration 331750 | Loss: 0.262903\n",
      "Iteration 331775 | Loss: 0.262903\n",
      "Iteration 331800 | Loss: 0.262903\n",
      "Iteration 331825 | Loss: 0.262903\n",
      "Iteration 331850 | Loss: 0.262903\n",
      "Iteration 331875 | Loss: 0.262903\n",
      "Iteration 331900 | Loss: 0.262903\n",
      "Iteration 331925 | Loss: 0.262903\n",
      "Iteration 331950 | Loss: 0.262902\n",
      "Iteration 331975 | Loss: 0.262902\n",
      "Iteration 332000 | Loss: 0.262902\n",
      "Iteration 332025 | Loss: 0.262902\n",
      "Iteration 332050 | Loss: 0.262902\n",
      "Iteration 332075 | Loss: 0.262902\n",
      "Iteration 332100 | Loss: 0.262902\n",
      "Iteration 332125 | Loss: 0.262902\n",
      "Iteration 332150 | Loss: 0.262902\n",
      "Iteration 332175 | Loss: 0.262902\n",
      "Iteration 332200 | Loss: 0.262902\n",
      "Iteration 332225 | Loss: 0.262902\n",
      "Iteration 332250 | Loss: 0.262902\n",
      "Iteration 332275 | Loss: 0.262902\n",
      "Iteration 332300 | Loss: 0.262902\n",
      "Iteration 332325 | Loss: 0.262902\n",
      "Iteration 332350 | Loss: 0.262902\n",
      "Iteration 332375 | Loss: 0.262902\n",
      "Iteration 332400 | Loss: 0.262902\n",
      "Iteration 332425 | Loss: 0.262902\n",
      "Iteration 332450 | Loss: 0.262902\n",
      "Iteration 332475 | Loss: 0.262901\n",
      "Iteration 332500 | Loss: 0.262901\n",
      "Iteration 332525 | Loss: 0.262901\n",
      "Iteration 332550 | Loss: 0.262901\n",
      "Iteration 332575 | Loss: 0.262901\n",
      "Iteration 332600 | Loss: 0.262901\n",
      "Iteration 332625 | Loss: 0.262901\n",
      "Iteration 332650 | Loss: 0.262901\n",
      "Iteration 332675 | Loss: 0.262901\n",
      "Iteration 332700 | Loss: 0.262901\n",
      "Iteration 332725 | Loss: 0.262901\n",
      "Iteration 332750 | Loss: 0.262901\n",
      "Iteration 332775 | Loss: 0.262901\n",
      "Iteration 332800 | Loss: 0.262901\n",
      "Iteration 332825 | Loss: 0.262901\n",
      "Iteration 332850 | Loss: 0.262901\n",
      "Iteration 332875 | Loss: 0.262901\n",
      "Iteration 332900 | Loss: 0.262901\n",
      "Iteration 332925 | Loss: 0.262901\n",
      "Iteration 332950 | Loss: 0.262901\n",
      "Iteration 332975 | Loss: 0.262901\n",
      "Iteration 333000 | Loss: 0.262901\n",
      "Iteration 333025 | Loss: 0.262900\n",
      "Iteration 333050 | Loss: 0.262900\n",
      "Iteration 333075 | Loss: 0.262900\n",
      "Iteration 333100 | Loss: 0.262900\n",
      "Iteration 333125 | Loss: 0.262900\n",
      "Iteration 333150 | Loss: 0.262900\n",
      "Iteration 333175 | Loss: 0.262900\n",
      "Iteration 333200 | Loss: 0.262900\n",
      "Iteration 333225 | Loss: 0.262900\n",
      "Iteration 333250 | Loss: 0.262900\n",
      "Iteration 333275 | Loss: 0.262900\n",
      "Iteration 333300 | Loss: 0.262900\n",
      "Iteration 333325 | Loss: 0.262900\n",
      "Iteration 333350 | Loss: 0.262900\n",
      "Iteration 333375 | Loss: 0.262900\n",
      "Iteration 333400 | Loss: 0.262900\n",
      "Iteration 333425 | Loss: 0.262900\n",
      "Iteration 333450 | Loss: 0.262900\n",
      "Iteration 333475 | Loss: 0.262900\n",
      "Iteration 333500 | Loss: 0.262900\n",
      "Iteration 333525 | Loss: 0.262900\n",
      "Iteration 333550 | Loss: 0.262900\n",
      "Iteration 333575 | Loss: 0.262899\n",
      "Iteration 333600 | Loss: 0.262899\n",
      "Iteration 333625 | Loss: 0.262899\n",
      "Iteration 333650 | Loss: 0.262899\n",
      "Iteration 333675 | Loss: 0.262899\n",
      "Iteration 333700 | Loss: 0.262899\n",
      "Iteration 333725 | Loss: 0.262899\n",
      "Iteration 333750 | Loss: 0.262899\n",
      "Iteration 333775 | Loss: 0.262899\n",
      "Iteration 333800 | Loss: 0.262899\n",
      "Iteration 333825 | Loss: 0.262899\n",
      "Iteration 333850 | Loss: 0.262899\n",
      "Iteration 333875 | Loss: 0.262899\n",
      "Iteration 333900 | Loss: 0.262899\n",
      "Iteration 333925 | Loss: 0.262899\n",
      "Iteration 333950 | Loss: 0.262899\n",
      "Iteration 333975 | Loss: 0.262899\n",
      "Iteration 334000 | Loss: 0.262899\n",
      "Iteration 334025 | Loss: 0.262899\n",
      "Iteration 334050 | Loss: 0.262899\n",
      "Iteration 334075 | Loss: 0.262899\n",
      "Iteration 334100 | Loss: 0.262899\n",
      "Iteration 334125 | Loss: 0.262898\n",
      "Iteration 334150 | Loss: 0.262898\n",
      "Iteration 334175 | Loss: 0.262898\n",
      "Iteration 334200 | Loss: 0.262898\n",
      "Iteration 334225 | Loss: 0.262898\n",
      "Iteration 334250 | Loss: 0.262898\n",
      "Iteration 334275 | Loss: 0.262898\n",
      "Iteration 334300 | Loss: 0.262898\n",
      "Iteration 334325 | Loss: 0.262898\n",
      "Iteration 334350 | Loss: 0.262898\n",
      "Iteration 334375 | Loss: 0.262898\n",
      "Iteration 334400 | Loss: 0.262898\n",
      "Iteration 334425 | Loss: 0.262898\n",
      "Iteration 334450 | Loss: 0.262898\n",
      "Iteration 334475 | Loss: 0.262898\n",
      "Iteration 334500 | Loss: 0.262898\n",
      "Iteration 334525 | Loss: 0.262898\n",
      "Iteration 334550 | Loss: 0.262898\n",
      "Iteration 334575 | Loss: 0.262898\n",
      "Iteration 334600 | Loss: 0.262898\n",
      "Iteration 334625 | Loss: 0.262898\n",
      "Iteration 334650 | Loss: 0.262898\n",
      "Iteration 334675 | Loss: 0.262898\n",
      "Iteration 334700 | Loss: 0.262897\n",
      "Iteration 334725 | Loss: 0.262897\n",
      "Iteration 334750 | Loss: 0.262897\n",
      "Iteration 334775 | Loss: 0.262897\n",
      "Iteration 334800 | Loss: 0.262897\n",
      "Iteration 334825 | Loss: 0.262897\n",
      "Iteration 334850 | Loss: 0.262897\n",
      "Iteration 334875 | Loss: 0.262897\n",
      "Iteration 334900 | Loss: 0.262897\n",
      "Iteration 334925 | Loss: 0.262897\n",
      "Iteration 334950 | Loss: 0.262897\n",
      "Iteration 334975 | Loss: 0.262897\n",
      "Iteration 335000 | Loss: 0.262897\n",
      "Iteration 335025 | Loss: 0.262897\n",
      "Iteration 335050 | Loss: 0.262897\n",
      "Iteration 335075 | Loss: 0.262897\n",
      "Iteration 335100 | Loss: 0.262897\n",
      "Iteration 335125 | Loss: 0.262897\n",
      "Iteration 335150 | Loss: 0.262897\n",
      "Iteration 335175 | Loss: 0.262897\n",
      "Iteration 335200 | Loss: 0.262897\n",
      "Iteration 335225 | Loss: 0.262897\n",
      "Iteration 335250 | Loss: 0.262897\n",
      "Iteration 335275 | Loss: 0.262897\n",
      "Iteration 335300 | Loss: 0.262896\n",
      "Iteration 335325 | Loss: 0.262896\n",
      "Iteration 335350 | Loss: 0.262896\n",
      "Iteration 335375 | Loss: 0.262896\n",
      "Iteration 335400 | Loss: 0.262896\n",
      "Iteration 335425 | Loss: 0.262896\n",
      "Iteration 335450 | Loss: 0.262896\n",
      "Iteration 335475 | Loss: 0.262896\n",
      "Iteration 335500 | Loss: 0.262896\n",
      "Iteration 335525 | Loss: 0.262896\n",
      "Iteration 335550 | Loss: 0.262896\n",
      "Iteration 335575 | Loss: 0.262896\n",
      "Iteration 335600 | Loss: 0.262896\n",
      "Iteration 335625 | Loss: 0.262896\n",
      "Iteration 335650 | Loss: 0.262896\n",
      "Iteration 335675 | Loss: 0.262896\n",
      "Iteration 335700 | Loss: 0.262896\n",
      "Iteration 335725 | Loss: 0.262896\n",
      "Iteration 335750 | Loss: 0.262896\n",
      "Iteration 335775 | Loss: 0.262896\n",
      "Iteration 335800 | Loss: 0.262896\n",
      "Iteration 335825 | Loss: 0.262896\n",
      "Iteration 335850 | Loss: 0.262896\n",
      "Iteration 335875 | Loss: 0.262896\n",
      "Iteration 335900 | Loss: 0.262895\n",
      "Iteration 335925 | Loss: 0.262895\n",
      "Iteration 335950 | Loss: 0.262895\n",
      "Iteration 335975 | Loss: 0.262895\n",
      "Iteration 336000 | Loss: 0.262895\n",
      "Iteration 336025 | Loss: 0.262895\n",
      "Iteration 336050 | Loss: 0.262895\n",
      "Iteration 336075 | Loss: 0.262895\n",
      "Iteration 336100 | Loss: 0.262895\n",
      "Iteration 336125 | Loss: 0.262895\n",
      "Iteration 336150 | Loss: 0.262895\n",
      "Iteration 336175 | Loss: 0.262895\n",
      "Iteration 336200 | Loss: 0.262895\n",
      "Iteration 336225 | Loss: 0.262895\n",
      "Iteration 336250 | Loss: 0.262895\n",
      "Iteration 336275 | Loss: 0.262895\n",
      "Iteration 336300 | Loss: 0.262895\n",
      "Iteration 336325 | Loss: 0.262895\n",
      "Iteration 336350 | Loss: 0.262895\n",
      "Iteration 336375 | Loss: 0.262895\n",
      "Iteration 336400 | Loss: 0.262895\n",
      "Iteration 336425 | Loss: 0.262895\n",
      "Iteration 336450 | Loss: 0.262895\n",
      "Iteration 336475 | Loss: 0.262895\n",
      "Iteration 336500 | Loss: 0.262895\n",
      "Iteration 336525 | Loss: 0.262894\n",
      "Iteration 336550 | Loss: 0.262894\n",
      "Iteration 336575 | Loss: 0.262894\n",
      "Iteration 336600 | Loss: 0.262894\n",
      "Iteration 336625 | Loss: 0.262894\n",
      "Iteration 336650 | Loss: 0.262894\n",
      "Iteration 336675 | Loss: 0.262894\n",
      "Iteration 336700 | Loss: 0.262894\n",
      "Iteration 336725 | Loss: 0.262894\n",
      "Iteration 336750 | Loss: 0.262894\n",
      "Iteration 336775 | Loss: 0.262894\n",
      "Iteration 336800 | Loss: 0.262894\n",
      "Iteration 336825 | Loss: 0.262894\n",
      "Iteration 336850 | Loss: 0.262894\n",
      "Iteration 336875 | Loss: 0.262894\n",
      "Iteration 336900 | Loss: 0.262894\n",
      "Iteration 336925 | Loss: 0.262894\n",
      "Iteration 336950 | Loss: 0.262894\n",
      "Iteration 336975 | Loss: 0.262894\n",
      "Iteration 337000 | Loss: 0.262894\n",
      "Iteration 337025 | Loss: 0.262894\n",
      "Iteration 337050 | Loss: 0.262894\n",
      "Iteration 337075 | Loss: 0.262894\n",
      "Iteration 337100 | Loss: 0.262894\n",
      "Iteration 337125 | Loss: 0.262894\n",
      "Iteration 337150 | Loss: 0.262893\n",
      "Iteration 337175 | Loss: 0.262893\n",
      "Iteration 337200 | Loss: 0.262893\n",
      "Iteration 337225 | Loss: 0.262893\n",
      "Iteration 337250 | Loss: 0.262893\n",
      "Iteration 337275 | Loss: 0.262893\n",
      "Iteration 337300 | Loss: 0.262893\n",
      "Iteration 337325 | Loss: 0.262893\n",
      "Iteration 337350 | Loss: 0.262893\n",
      "Iteration 337375 | Loss: 0.262893\n",
      "Iteration 337400 | Loss: 0.262893\n",
      "Iteration 337425 | Loss: 0.262893\n",
      "Iteration 337450 | Loss: 0.262893\n",
      "Iteration 337475 | Loss: 0.262893\n",
      "Iteration 337500 | Loss: 0.262893\n",
      "Iteration 337525 | Loss: 0.262893\n",
      "Iteration 337550 | Loss: 0.262893\n",
      "Iteration 337575 | Loss: 0.262893\n",
      "Iteration 337600 | Loss: 0.262893\n",
      "Iteration 337625 | Loss: 0.262893\n",
      "Iteration 337650 | Loss: 0.262893\n",
      "Iteration 337675 | Loss: 0.262893\n",
      "Iteration 337700 | Loss: 0.262893\n",
      "Iteration 337725 | Loss: 0.262893\n",
      "Iteration 337750 | Loss: 0.262893\n",
      "Iteration 337775 | Loss: 0.262893\n",
      "Iteration 337800 | Loss: 0.262892\n",
      "Iteration 337825 | Loss: 0.262892\n",
      "Iteration 337850 | Loss: 0.262892\n",
      "Iteration 337875 | Loss: 0.262892\n",
      "Iteration 337900 | Loss: 0.262892\n",
      "Iteration 337925 | Loss: 0.262892\n",
      "Iteration 337950 | Loss: 0.262892\n",
      "Iteration 337975 | Loss: 0.262892\n",
      "Iteration 338000 | Loss: 0.262892\n",
      "Iteration 338025 | Loss: 0.262892\n",
      "Iteration 338050 | Loss: 0.262892\n",
      "Iteration 338075 | Loss: 0.262892\n",
      "Iteration 338100 | Loss: 0.262892\n",
      "Iteration 338125 | Loss: 0.262892\n",
      "Iteration 338150 | Loss: 0.262892\n",
      "Iteration 338175 | Loss: 0.262892\n",
      "Iteration 338200 | Loss: 0.262892\n",
      "Iteration 338225 | Loss: 0.262892\n",
      "Iteration 338250 | Loss: 0.262892\n",
      "Iteration 338275 | Loss: 0.262892\n",
      "Iteration 338300 | Loss: 0.262892\n",
      "Iteration 338325 | Loss: 0.262892\n",
      "Iteration 338350 | Loss: 0.262892\n",
      "Iteration 338375 | Loss: 0.262892\n",
      "Iteration 338400 | Loss: 0.262892\n",
      "Iteration 338425 | Loss: 0.262892\n",
      "Iteration 338450 | Loss: 0.262892\n",
      "Iteration 338475 | Loss: 0.262891\n",
      "Iteration 338500 | Loss: 0.262891\n",
      "Iteration 338525 | Loss: 0.262891\n",
      "Iteration 338550 | Loss: 0.262891\n",
      "Iteration 338575 | Loss: 0.262891\n",
      "Iteration 338600 | Loss: 0.262891\n",
      "Iteration 338625 | Loss: 0.262891\n",
      "Iteration 338650 | Loss: 0.262891\n",
      "Iteration 338675 | Loss: 0.262891\n",
      "Iteration 338700 | Loss: 0.262891\n",
      "Iteration 338725 | Loss: 0.262891\n",
      "Iteration 338750 | Loss: 0.262891\n",
      "Iteration 338775 | Loss: 0.262891\n",
      "Iteration 338800 | Loss: 0.262891\n",
      "Iteration 338825 | Loss: 0.262891\n",
      "Iteration 338850 | Loss: 0.262891\n",
      "Iteration 338875 | Loss: 0.262891\n",
      "Iteration 338900 | Loss: 0.262891\n",
      "Iteration 338925 | Loss: 0.262891\n",
      "Iteration 338950 | Loss: 0.262891\n",
      "Iteration 338975 | Loss: 0.262891\n",
      "Iteration 339000 | Loss: 0.262891\n",
      "Iteration 339025 | Loss: 0.262891\n",
      "Iteration 339050 | Loss: 0.262891\n",
      "Iteration 339075 | Loss: 0.262891\n",
      "Iteration 339100 | Loss: 0.262891\n",
      "Iteration 339125 | Loss: 0.262891\n",
      "Iteration 339150 | Loss: 0.262891\n",
      "Iteration 339175 | Loss: 0.262890\n",
      "Iteration 339200 | Loss: 0.262890\n",
      "Iteration 339225 | Loss: 0.262890\n",
      "Iteration 339250 | Loss: 0.262890\n",
      "Iteration 339275 | Loss: 0.262890\n",
      "Iteration 339300 | Loss: 0.262890\n",
      "Iteration 339325 | Loss: 0.262890\n",
      "Iteration 339350 | Loss: 0.262890\n",
      "Iteration 339375 | Loss: 0.262890\n",
      "Iteration 339400 | Loss: 0.262890\n",
      "Iteration 339425 | Loss: 0.262890\n",
      "Iteration 339450 | Loss: 0.262890\n",
      "Iteration 339475 | Loss: 0.262890\n",
      "Iteration 339500 | Loss: 0.262890\n",
      "Iteration 339525 | Loss: 0.262890\n",
      "Iteration 339550 | Loss: 0.262890\n",
      "Iteration 339575 | Loss: 0.262890\n",
      "Iteration 339600 | Loss: 0.262890\n",
      "Iteration 339625 | Loss: 0.262890\n",
      "Iteration 339650 | Loss: 0.262890\n",
      "Iteration 339675 | Loss: 0.262890\n",
      "Iteration 339700 | Loss: 0.262890\n",
      "Iteration 339725 | Loss: 0.262890\n",
      "Iteration 339750 | Loss: 0.262890\n",
      "Iteration 339775 | Loss: 0.262890\n",
      "Iteration 339800 | Loss: 0.262890\n",
      "Iteration 339825 | Loss: 0.262890\n",
      "Iteration 339850 | Loss: 0.262890\n",
      "Iteration 339875 | Loss: 0.262889\n",
      "Iteration 339900 | Loss: 0.262889\n",
      "Iteration 339925 | Loss: 0.262889\n",
      "Iteration 339950 | Loss: 0.262889\n",
      "Iteration 339975 | Loss: 0.262889\n",
      "Iteration 340000 | Loss: 0.262889\n",
      "Iteration 340025 | Loss: 0.262889\n",
      "Iteration 340050 | Loss: 0.262889\n",
      "Iteration 340075 | Loss: 0.262889\n",
      "Iteration 340100 | Loss: 0.262889\n",
      "Iteration 340125 | Loss: 0.262889\n",
      "Iteration 340150 | Loss: 0.262889\n",
      "Iteration 340175 | Loss: 0.262889\n",
      "Iteration 340200 | Loss: 0.262889\n",
      "Iteration 340225 | Loss: 0.262889\n",
      "Iteration 340250 | Loss: 0.262889\n",
      "Iteration 340275 | Loss: 0.262889\n",
      "Iteration 340300 | Loss: 0.262889\n",
      "Iteration 340325 | Loss: 0.262889\n",
      "Iteration 340350 | Loss: 0.262889\n",
      "Iteration 340375 | Loss: 0.262889\n",
      "Iteration 340400 | Loss: 0.262889\n",
      "Iteration 340425 | Loss: 0.262889\n",
      "Iteration 340450 | Loss: 0.262889\n",
      "Iteration 340475 | Loss: 0.262889\n",
      "Iteration 340500 | Loss: 0.262889\n",
      "Iteration 340525 | Loss: 0.262889\n",
      "Iteration 340550 | Loss: 0.262889\n",
      "Iteration 340575 | Loss: 0.262889\n",
      "Iteration 340600 | Loss: 0.262888\n",
      "Iteration 340625 | Loss: 0.262888\n",
      "Iteration 340650 | Loss: 0.262888\n",
      "Iteration 340675 | Loss: 0.262888\n",
      "Iteration 340700 | Loss: 0.262888\n",
      "Iteration 340725 | Loss: 0.262888\n",
      "Iteration 340750 | Loss: 0.262888\n",
      "Iteration 340775 | Loss: 0.262888\n",
      "Iteration 340800 | Loss: 0.262888\n",
      "Iteration 340825 | Loss: 0.262888\n",
      "Iteration 340850 | Loss: 0.262888\n",
      "Iteration 340875 | Loss: 0.262888\n",
      "Iteration 340900 | Loss: 0.262888\n",
      "Iteration 340925 | Loss: 0.262888\n",
      "Iteration 340950 | Loss: 0.262888\n",
      "Iteration 340975 | Loss: 0.262888\n",
      "Iteration 341000 | Loss: 0.262888\n",
      "Iteration 341025 | Loss: 0.262888\n",
      "Iteration 341050 | Loss: 0.262888\n",
      "Iteration 341075 | Loss: 0.262888\n",
      "Iteration 341100 | Loss: 0.262888\n",
      "Iteration 341125 | Loss: 0.262888\n",
      "Iteration 341150 | Loss: 0.262888\n",
      "Iteration 341175 | Loss: 0.262888\n",
      "Iteration 341200 | Loss: 0.262888\n",
      "Iteration 341225 | Loss: 0.262888\n",
      "Iteration 341250 | Loss: 0.262888\n",
      "Iteration 341275 | Loss: 0.262888\n",
      "Iteration 341300 | Loss: 0.262888\n",
      "Iteration 341325 | Loss: 0.262888\n",
      "Iteration 341350 | Loss: 0.262887\n",
      "Iteration 341375 | Loss: 0.262887\n",
      "Iteration 341400 | Loss: 0.262887\n",
      "Iteration 341425 | Loss: 0.262887\n",
      "Iteration 341450 | Loss: 0.262887\n",
      "Iteration 341475 | Loss: 0.262887\n",
      "Iteration 341500 | Loss: 0.262887\n",
      "Iteration 341525 | Loss: 0.262887\n",
      "Iteration 341550 | Loss: 0.262887\n",
      "Iteration 341575 | Loss: 0.262887\n",
      "Iteration 341600 | Loss: 0.262887\n",
      "Iteration 341625 | Loss: 0.262887\n",
      "Iteration 341650 | Loss: 0.262887\n",
      "Iteration 341675 | Loss: 0.262887\n",
      "Iteration 341700 | Loss: 0.262887\n",
      "Iteration 341725 | Loss: 0.262887\n",
      "Iteration 341750 | Loss: 0.262887\n",
      "Iteration 341775 | Loss: 0.262887\n",
      "Iteration 341800 | Loss: 0.262887\n",
      "Iteration 341825 | Loss: 0.262887\n",
      "Iteration 341850 | Loss: 0.262887\n",
      "Iteration 341875 | Loss: 0.262887\n",
      "Iteration 341900 | Loss: 0.262887\n",
      "Iteration 341925 | Loss: 0.262887\n",
      "Iteration 341950 | Loss: 0.262887\n",
      "Iteration 341975 | Loss: 0.262887\n",
      "Iteration 342000 | Loss: 0.262887\n",
      "Iteration 342025 | Loss: 0.262887\n",
      "Iteration 342050 | Loss: 0.262887\n",
      "Iteration 342075 | Loss: 0.262887\n",
      "Iteration 342100 | Loss: 0.262887\n",
      "Iteration 342125 | Loss: 0.262887\n",
      "Iteration 342150 | Loss: 0.262886\n",
      "Iteration 342175 | Loss: 0.262886\n",
      "Iteration 342200 | Loss: 0.262886\n",
      "Iteration 342225 | Loss: 0.262886\n",
      "Iteration 342250 | Loss: 0.262886\n",
      "Iteration 342275 | Loss: 0.262886\n",
      "Iteration 342300 | Loss: 0.262886\n",
      "Iteration 342325 | Loss: 0.262886\n",
      "Iteration 342350 | Loss: 0.262886\n",
      "Iteration 342375 | Loss: 0.262886\n",
      "Iteration 342400 | Loss: 0.262886\n",
      "Iteration 342425 | Loss: 0.262886\n",
      "Iteration 342450 | Loss: 0.262886\n",
      "Iteration 342475 | Loss: 0.262886\n",
      "Iteration 342500 | Loss: 0.262886\n",
      "Iteration 342525 | Loss: 0.262886\n",
      "Iteration 342550 | Loss: 0.262886\n",
      "Iteration 342575 | Loss: 0.262886\n",
      "Iteration 342600 | Loss: 0.262886\n",
      "Iteration 342625 | Loss: 0.262886\n",
      "Iteration 342650 | Loss: 0.262886\n",
      "Iteration 342675 | Loss: 0.262886\n",
      "Iteration 342700 | Loss: 0.262886\n",
      "Iteration 342725 | Loss: 0.262886\n",
      "Iteration 342750 | Loss: 0.262886\n",
      "Iteration 342775 | Loss: 0.262886\n",
      "Iteration 342800 | Loss: 0.262886\n",
      "Iteration 342825 | Loss: 0.262886\n",
      "Iteration 342850 | Loss: 0.262886\n",
      "Iteration 342875 | Loss: 0.262886\n",
      "Iteration 342900 | Loss: 0.262886\n",
      "Iteration 342925 | Loss: 0.262886\n",
      "Iteration 342950 | Loss: 0.262885\n",
      "Iteration 342975 | Loss: 0.262885\n",
      "Iteration 343000 | Loss: 0.262885\n",
      "Iteration 343025 | Loss: 0.262885\n",
      "Iteration 343050 | Loss: 0.262885\n",
      "Iteration 343075 | Loss: 0.262885\n",
      "Iteration 343100 | Loss: 0.262885\n",
      "Iteration 343125 | Loss: 0.262885\n",
      "Iteration 343150 | Loss: 0.262885\n",
      "Iteration 343175 | Loss: 0.262885\n",
      "Iteration 343200 | Loss: 0.262885\n",
      "Iteration 343225 | Loss: 0.262885\n",
      "Iteration 343250 | Loss: 0.262885\n",
      "Iteration 343275 | Loss: 0.262885\n",
      "Iteration 343300 | Loss: 0.262885\n",
      "Iteration 343325 | Loss: 0.262885\n",
      "Iteration 343350 | Loss: 0.262885\n",
      "Iteration 343375 | Loss: 0.262885\n",
      "Iteration 343400 | Loss: 0.262885\n",
      "Iteration 343425 | Loss: 0.262885\n",
      "Iteration 343450 | Loss: 0.262885\n",
      "Iteration 343475 | Loss: 0.262885\n",
      "Iteration 343500 | Loss: 0.262885\n",
      "Iteration 343525 | Loss: 0.262885\n",
      "Iteration 343550 | Loss: 0.262885\n",
      "Iteration 343575 | Loss: 0.262885\n",
      "Iteration 343600 | Loss: 0.262885\n",
      "Iteration 343625 | Loss: 0.262885\n",
      "Iteration 343650 | Loss: 0.262885\n",
      "Iteration 343675 | Loss: 0.262885\n",
      "Iteration 343700 | Loss: 0.262885\n",
      "Iteration 343725 | Loss: 0.262885\n",
      "Iteration 343750 | Loss: 0.262885\n",
      "Iteration 343775 | Loss: 0.262884\n",
      "Iteration 343800 | Loss: 0.262884\n",
      "Iteration 343825 | Loss: 0.262884\n",
      "Iteration 343850 | Loss: 0.262884\n",
      "Iteration 343875 | Loss: 0.262884\n",
      "Iteration 343900 | Loss: 0.262884\n",
      "Iteration 343925 | Loss: 0.262884\n",
      "Iteration 343950 | Loss: 0.262884\n",
      "Iteration 343975 | Loss: 0.262884\n",
      "Iteration 344000 | Loss: 0.262884\n",
      "Iteration 344025 | Loss: 0.262884\n",
      "Iteration 344050 | Loss: 0.262884\n",
      "Iteration 344075 | Loss: 0.262884\n",
      "Iteration 344100 | Loss: 0.262884\n",
      "Iteration 344125 | Loss: 0.262884\n",
      "Iteration 344150 | Loss: 0.262884\n",
      "Iteration 344175 | Loss: 0.262884\n",
      "Iteration 344200 | Loss: 0.262884\n",
      "Iteration 344225 | Loss: 0.262884\n",
      "Iteration 344250 | Loss: 0.262884\n",
      "Iteration 344275 | Loss: 0.262884\n",
      "Iteration 344300 | Loss: 0.262884\n",
      "Iteration 344325 | Loss: 0.262884\n",
      "Iteration 344350 | Loss: 0.262884\n",
      "Iteration 344375 | Loss: 0.262884\n",
      "Iteration 344400 | Loss: 0.262884\n",
      "Iteration 344425 | Loss: 0.262884\n",
      "Iteration 344450 | Loss: 0.262884\n",
      "Iteration 344475 | Loss: 0.262884\n",
      "Iteration 344500 | Loss: 0.262884\n",
      "Iteration 344525 | Loss: 0.262884\n",
      "Iteration 344550 | Loss: 0.262884\n",
      "Iteration 344575 | Loss: 0.262884\n",
      "Iteration 344600 | Loss: 0.262884\n",
      "Iteration 344625 | Loss: 0.262883\n",
      "Iteration 344650 | Loss: 0.262883\n",
      "Iteration 344675 | Loss: 0.262883\n",
      "Iteration 344700 | Loss: 0.262883\n",
      "Iteration 344725 | Loss: 0.262883\n",
      "Iteration 344750 | Loss: 0.262883\n",
      "Iteration 344775 | Loss: 0.262883\n",
      "Iteration 344800 | Loss: 0.262883\n",
      "Iteration 344825 | Loss: 0.262883\n",
      "Iteration 344850 | Loss: 0.262883\n",
      "Iteration 344875 | Loss: 0.262883\n",
      "Iteration 344900 | Loss: 0.262883\n",
      "Iteration 344925 | Loss: 0.262883\n",
      "Iteration 344950 | Loss: 0.262883\n",
      "Iteration 344975 | Loss: 0.262883\n",
      "Iteration 345000 | Loss: 0.262883\n",
      "Iteration 345025 | Loss: 0.262883\n",
      "Iteration 345050 | Loss: 0.262883\n",
      "Iteration 345075 | Loss: 0.262883\n",
      "Iteration 345100 | Loss: 0.262883\n",
      "Iteration 345125 | Loss: 0.262883\n",
      "Iteration 345150 | Loss: 0.262883\n",
      "Iteration 345175 | Loss: 0.262883\n",
      "Iteration 345200 | Loss: 0.262883\n",
      "Iteration 345225 | Loss: 0.262883\n",
      "Iteration 345250 | Loss: 0.262883\n",
      "Iteration 345275 | Loss: 0.262883\n",
      "Iteration 345300 | Loss: 0.262883\n",
      "Iteration 345325 | Loss: 0.262883\n",
      "Iteration 345350 | Loss: 0.262883\n",
      "Iteration 345375 | Loss: 0.262883\n",
      "Iteration 345400 | Loss: 0.262883\n",
      "Iteration 345425 | Loss: 0.262883\n",
      "Iteration 345450 | Loss: 0.262883\n",
      "Iteration 345475 | Loss: 0.262883\n",
      "Iteration 345500 | Loss: 0.262883\n",
      "Iteration 345525 | Loss: 0.262882\n",
      "Iteration 345550 | Loss: 0.262882\n",
      "Iteration 345575 | Loss: 0.262882\n",
      "Iteration 345600 | Loss: 0.262882\n",
      "Iteration 345625 | Loss: 0.262882\n",
      "Iteration 345650 | Loss: 0.262882\n",
      "Iteration 345675 | Loss: 0.262882\n",
      "Iteration 345700 | Loss: 0.262882\n",
      "Iteration 345725 | Loss: 0.262882\n",
      "Iteration 345750 | Loss: 0.262882\n",
      "Iteration 345775 | Loss: 0.262882\n",
      "Iteration 345800 | Loss: 0.262882\n",
      "Iteration 345825 | Loss: 0.262882\n",
      "Iteration 345850 | Loss: 0.262882\n",
      "Iteration 345875 | Loss: 0.262882\n",
      "Iteration 345900 | Loss: 0.262882\n",
      "Iteration 345925 | Loss: 0.262882\n",
      "Iteration 345950 | Loss: 0.262882\n",
      "Iteration 345975 | Loss: 0.262882\n",
      "Iteration 346000 | Loss: 0.262882\n",
      "Iteration 346025 | Loss: 0.262882\n",
      "Iteration 346050 | Loss: 0.262882\n",
      "Iteration 346075 | Loss: 0.262882\n",
      "Iteration 346100 | Loss: 0.262882\n",
      "Iteration 346125 | Loss: 0.262882\n",
      "Iteration 346150 | Loss: 0.262882\n",
      "Iteration 346175 | Loss: 0.262882\n",
      "Iteration 346200 | Loss: 0.262882\n",
      "Iteration 346225 | Loss: 0.262882\n",
      "Iteration 346250 | Loss: 0.262882\n",
      "Iteration 346275 | Loss: 0.262882\n",
      "Iteration 346300 | Loss: 0.262882\n",
      "Iteration 346325 | Loss: 0.262882\n",
      "Iteration 346350 | Loss: 0.262882\n",
      "Iteration 346375 | Loss: 0.262882\n",
      "Iteration 346400 | Loss: 0.262882\n",
      "Iteration 346425 | Loss: 0.262882\n",
      "Iteration 346450 | Loss: 0.262881\n",
      "Iteration 346475 | Loss: 0.262881\n",
      "Iteration 346500 | Loss: 0.262881\n",
      "Iteration 346525 | Loss: 0.262881\n",
      "Iteration 346550 | Loss: 0.262881\n",
      "Iteration 346575 | Loss: 0.262881\n",
      "Iteration 346600 | Loss: 0.262881\n",
      "Iteration 346625 | Loss: 0.262881\n",
      "Iteration 346650 | Loss: 0.262881\n",
      "Iteration 346675 | Loss: 0.262881\n",
      "Iteration 346700 | Loss: 0.262881\n",
      "Iteration 346725 | Loss: 0.262881\n",
      "Iteration 346750 | Loss: 0.262881\n",
      "Iteration 346775 | Loss: 0.262881\n",
      "Iteration 346800 | Loss: 0.262881\n",
      "Iteration 346825 | Loss: 0.262881\n",
      "Iteration 346850 | Loss: 0.262881\n",
      "Iteration 346875 | Loss: 0.262881\n",
      "Iteration 346900 | Loss: 0.262881\n",
      "Iteration 346925 | Loss: 0.262881\n",
      "Iteration 346950 | Loss: 0.262881\n",
      "Iteration 346975 | Loss: 0.262881\n",
      "Iteration 347000 | Loss: 0.262881\n",
      "Iteration 347025 | Loss: 0.262881\n",
      "Iteration 347050 | Loss: 0.262881\n",
      "Iteration 347075 | Loss: 0.262881\n",
      "Iteration 347100 | Loss: 0.262881\n",
      "Iteration 347125 | Loss: 0.262881\n",
      "Iteration 347150 | Loss: 0.262881\n",
      "Iteration 347175 | Loss: 0.262881\n",
      "Iteration 347200 | Loss: 0.262881\n",
      "Iteration 347225 | Loss: 0.262881\n",
      "Iteration 347250 | Loss: 0.262881\n",
      "Iteration 347275 | Loss: 0.262881\n",
      "Iteration 347300 | Loss: 0.262881\n",
      "Iteration 347325 | Loss: 0.262881\n",
      "Iteration 347350 | Loss: 0.262881\n",
      "Iteration 347375 | Loss: 0.262881\n",
      "Iteration 347400 | Loss: 0.262880\n",
      "Iteration 347425 | Loss: 0.262880\n",
      "Iteration 347450 | Loss: 0.262880\n",
      "Iteration 347475 | Loss: 0.262880\n",
      "Iteration 347500 | Loss: 0.262880\n",
      "Iteration 347525 | Loss: 0.262880\n",
      "Iteration 347550 | Loss: 0.262880\n",
      "Iteration 347575 | Loss: 0.262880\n",
      "Iteration 347600 | Loss: 0.262880\n",
      "Iteration 347625 | Loss: 0.262880\n",
      "Iteration 347650 | Loss: 0.262880\n",
      "Iteration 347675 | Loss: 0.262880\n",
      "Iteration 347700 | Loss: 0.262880\n",
      "Iteration 347725 | Loss: 0.262880\n",
      "Iteration 347750 | Loss: 0.262880\n",
      "Iteration 347775 | Loss: 0.262880\n",
      "Iteration 347800 | Loss: 0.262880\n",
      "Iteration 347825 | Loss: 0.262880\n",
      "Iteration 347850 | Loss: 0.262880\n",
      "Iteration 347875 | Loss: 0.262880\n",
      "Iteration 347900 | Loss: 0.262880\n",
      "Iteration 347925 | Loss: 0.262880\n",
      "Iteration 347950 | Loss: 0.262880\n",
      "Iteration 347975 | Loss: 0.262880\n",
      "Iteration 348000 | Loss: 0.262880\n",
      "Iteration 348025 | Loss: 0.262880\n",
      "Iteration 348050 | Loss: 0.262880\n",
      "Iteration 348075 | Loss: 0.262880\n",
      "Iteration 348100 | Loss: 0.262880\n",
      "Iteration 348125 | Loss: 0.262880\n",
      "Iteration 348150 | Loss: 0.262880\n",
      "Iteration 348175 | Loss: 0.262880\n",
      "Iteration 348200 | Loss: 0.262880\n",
      "Iteration 348225 | Loss: 0.262880\n",
      "Iteration 348250 | Loss: 0.262880\n",
      "Iteration 348275 | Loss: 0.262880\n",
      "Iteration 348300 | Loss: 0.262880\n",
      "Iteration 348325 | Loss: 0.262880\n",
      "Iteration 348350 | Loss: 0.262880\n",
      "Iteration 348375 | Loss: 0.262880\n",
      "Iteration 348400 | Loss: 0.262879\n",
      "Iteration 348425 | Loss: 0.262879\n",
      "Iteration 348450 | Loss: 0.262879\n",
      "Iteration 348475 | Loss: 0.262879\n",
      "Iteration 348500 | Loss: 0.262879\n",
      "Iteration 348525 | Loss: 0.262879\n",
      "Iteration 348550 | Loss: 0.262879\n",
      "Iteration 348575 | Loss: 0.262879\n",
      "Iteration 348600 | Loss: 0.262879\n",
      "Iteration 348625 | Loss: 0.262879\n",
      "Iteration 348650 | Loss: 0.262879\n",
      "Iteration 348675 | Loss: 0.262879\n",
      "Iteration 348700 | Loss: 0.262879\n",
      "Iteration 348725 | Loss: 0.262879\n",
      "Iteration 348750 | Loss: 0.262879\n",
      "Iteration 348775 | Loss: 0.262879\n",
      "Iteration 348800 | Loss: 0.262879\n",
      "Iteration 348825 | Loss: 0.262879\n",
      "Iteration 348850 | Loss: 0.262879\n",
      "Iteration 348875 | Loss: 0.262879\n",
      "Iteration 348900 | Loss: 0.262879\n",
      "Iteration 348925 | Loss: 0.262879\n",
      "Iteration 348950 | Loss: 0.262879\n",
      "Iteration 348975 | Loss: 0.262879\n",
      "Iteration 349000 | Loss: 0.262879\n",
      "Iteration 349025 | Loss: 0.262879\n",
      "Iteration 349050 | Loss: 0.262879\n",
      "Iteration 349075 | Loss: 0.262879\n",
      "Iteration 349100 | Loss: 0.262879\n",
      "Iteration 349125 | Loss: 0.262879\n",
      "Iteration 349150 | Loss: 0.262879\n",
      "Iteration 349175 | Loss: 0.262879\n",
      "Iteration 349200 | Loss: 0.262879\n",
      "Iteration 349225 | Loss: 0.262879\n",
      "Iteration 349250 | Loss: 0.262879\n",
      "Iteration 349275 | Loss: 0.262879\n",
      "Iteration 349300 | Loss: 0.262879\n",
      "Iteration 349325 | Loss: 0.262879\n",
      "Iteration 349350 | Loss: 0.262879\n",
      "Iteration 349375 | Loss: 0.262879\n",
      "Iteration 349400 | Loss: 0.262879\n",
      "Iteration 349425 | Loss: 0.262879\n",
      "Iteration 349450 | Loss: 0.262878\n",
      "Iteration 349475 | Loss: 0.262878\n",
      "Iteration 349500 | Loss: 0.262878\n",
      "Iteration 349525 | Loss: 0.262878\n",
      "Iteration 349550 | Loss: 0.262878\n",
      "Iteration 349575 | Loss: 0.262878\n",
      "Iteration 349600 | Loss: 0.262878\n",
      "Iteration 349625 | Loss: 0.262878\n",
      "Iteration 349650 | Loss: 0.262878\n",
      "Iteration 349675 | Loss: 0.262878\n",
      "Iteration 349700 | Loss: 0.262878\n",
      "Iteration 349725 | Loss: 0.262878\n",
      "Iteration 349750 | Loss: 0.262878\n",
      "Iteration 349775 | Loss: 0.262878\n",
      "Iteration 349800 | Loss: 0.262878\n",
      "Iteration 349825 | Loss: 0.262878\n",
      "Iteration 349850 | Loss: 0.262878\n",
      "Iteration 349875 | Loss: 0.262878\n",
      "Iteration 349900 | Loss: 0.262878\n",
      "Iteration 349925 | Loss: 0.262878\n",
      "Iteration 349950 | Loss: 0.262878\n",
      "Iteration 349975 | Loss: 0.262878\n",
      "Iteration 350000 | Loss: 0.262878\n",
      "Iteration 350025 | Loss: 0.262878\n",
      "Iteration 350050 | Loss: 0.262878\n",
      "Iteration 350075 | Loss: 0.262878\n",
      "Iteration 350100 | Loss: 0.262878\n",
      "Iteration 350125 | Loss: 0.262878\n",
      "Iteration 350150 | Loss: 0.262878\n",
      "Iteration 350175 | Loss: 0.262878\n",
      "Iteration 350200 | Loss: 0.262878\n",
      "Iteration 350225 | Loss: 0.262878\n",
      "Iteration 350250 | Loss: 0.262878\n",
      "Iteration 350275 | Loss: 0.262878\n",
      "Iteration 350300 | Loss: 0.262878\n",
      "Iteration 350325 | Loss: 0.262878\n",
      "Iteration 350350 | Loss: 0.262878\n",
      "Iteration 350375 | Loss: 0.262878\n",
      "Iteration 350400 | Loss: 0.262878\n",
      "Iteration 350425 | Loss: 0.262878\n",
      "Iteration 350450 | Loss: 0.262878\n",
      "Iteration 350475 | Loss: 0.262878\n",
      "Iteration 350500 | Loss: 0.262878\n",
      "Iteration 350525 | Loss: 0.262877\n",
      "Iteration 350550 | Loss: 0.262877\n",
      "Iteration 350575 | Loss: 0.262877\n",
      "Iteration 350600 | Loss: 0.262877\n",
      "Iteration 350625 | Loss: 0.262877\n",
      "Iteration 350650 | Loss: 0.262877\n",
      "Iteration 350675 | Loss: 0.262877\n",
      "Iteration 350700 | Loss: 0.262877\n",
      "Iteration 350725 | Loss: 0.262877\n",
      "Iteration 350750 | Loss: 0.262877\n",
      "Iteration 350775 | Loss: 0.262877\n",
      "Iteration 350800 | Loss: 0.262877\n",
      "Iteration 350825 | Loss: 0.262877\n",
      "Iteration 350850 | Loss: 0.262877\n",
      "Iteration 350875 | Loss: 0.262877\n",
      "Iteration 350900 | Loss: 0.262877\n",
      "Iteration 350925 | Loss: 0.262877\n",
      "Iteration 350950 | Loss: 0.262877\n",
      "Iteration 350975 | Loss: 0.262877\n",
      "Iteration 351000 | Loss: 0.262877\n",
      "Iteration 351025 | Loss: 0.262877\n",
      "Iteration 351050 | Loss: 0.262877\n",
      "Iteration 351075 | Loss: 0.262877\n",
      "Iteration 351100 | Loss: 0.262877\n",
      "Iteration 351125 | Loss: 0.262877\n",
      "Iteration 351150 | Loss: 0.262877\n",
      "Iteration 351175 | Loss: 0.262877\n",
      "Iteration 351200 | Loss: 0.262877\n",
      "Iteration 351225 | Loss: 0.262877\n",
      "Iteration 351250 | Loss: 0.262877\n",
      "Iteration 351275 | Loss: 0.262877\n",
      "Iteration 351300 | Loss: 0.262877\n",
      "Iteration 351325 | Loss: 0.262877\n",
      "Iteration 351350 | Loss: 0.262877\n",
      "Iteration 351375 | Loss: 0.262877\n",
      "Iteration 351400 | Loss: 0.262877\n",
      "Iteration 351425 | Loss: 0.262877\n",
      "Iteration 351450 | Loss: 0.262877\n",
      "Iteration 351475 | Loss: 0.262877\n",
      "Iteration 351500 | Loss: 0.262877\n",
      "Iteration 351525 | Loss: 0.262877\n",
      "Iteration 351550 | Loss: 0.262877\n",
      "Iteration 351575 | Loss: 0.262877\n",
      "Iteration 351600 | Loss: 0.262877\n",
      "Iteration 351625 | Loss: 0.262877\n",
      "Iteration 351650 | Loss: 0.262877\n",
      "Iteration 351675 | Loss: 0.262876\n",
      "Iteration 351700 | Loss: 0.262876\n",
      "Iteration 351725 | Loss: 0.262876\n",
      "Iteration 351750 | Loss: 0.262876\n",
      "Iteration 351775 | Loss: 0.262876\n",
      "Iteration 351800 | Loss: 0.262876\n",
      "Iteration 351825 | Loss: 0.262876\n",
      "Iteration 351850 | Loss: 0.262876\n",
      "Iteration 351875 | Loss: 0.262876\n",
      "Iteration 351900 | Loss: 0.262876\n",
      "Iteration 351925 | Loss: 0.262876\n",
      "Iteration 351950 | Loss: 0.262876\n",
      "Iteration 351975 | Loss: 0.262876\n",
      "Iteration 352000 | Loss: 0.262876\n",
      "Iteration 352025 | Loss: 0.262876\n",
      "Iteration 352050 | Loss: 0.262876\n",
      "Iteration 352075 | Loss: 0.262876\n",
      "Iteration 352100 | Loss: 0.262876\n",
      "Iteration 352125 | Loss: 0.262876\n",
      "Iteration 352150 | Loss: 0.262876\n",
      "Iteration 352175 | Loss: 0.262876\n",
      "Iteration 352200 | Loss: 0.262876\n",
      "Iteration 352225 | Loss: 0.262876\n",
      "Iteration 352250 | Loss: 0.262876\n",
      "Iteration 352275 | Loss: 0.262876\n",
      "Iteration 352300 | Loss: 0.262876\n",
      "Iteration 352325 | Loss: 0.262876\n",
      "Iteration 352350 | Loss: 0.262876\n",
      "Iteration 352375 | Loss: 0.262876\n",
      "Iteration 352400 | Loss: 0.262876\n",
      "Iteration 352425 | Loss: 0.262876\n",
      "Iteration 352450 | Loss: 0.262876\n",
      "Iteration 352475 | Loss: 0.262876\n",
      "Iteration 352500 | Loss: 0.262876\n",
      "Iteration 352525 | Loss: 0.262876\n",
      "Iteration 352550 | Loss: 0.262876\n",
      "Iteration 352575 | Loss: 0.262876\n",
      "Iteration 352600 | Loss: 0.262876\n",
      "Iteration 352625 | Loss: 0.262876\n",
      "Iteration 352650 | Loss: 0.262876\n",
      "Iteration 352675 | Loss: 0.262876\n",
      "Iteration 352700 | Loss: 0.262876\n",
      "Iteration 352725 | Loss: 0.262876\n",
      "Iteration 352750 | Loss: 0.262876\n",
      "Iteration 352775 | Loss: 0.262876\n",
      "Iteration 352800 | Loss: 0.262876\n",
      "Iteration 352825 | Loss: 0.262876\n",
      "Iteration 352850 | Loss: 0.262875\n",
      "Iteration 352875 | Loss: 0.262875\n",
      "Iteration 352900 | Loss: 0.262875\n",
      "Iteration 352925 | Loss: 0.262875\n",
      "Iteration 352950 | Loss: 0.262875\n",
      "Iteration 352975 | Loss: 0.262875\n",
      "Iteration 353000 | Loss: 0.262875\n",
      "Iteration 353025 | Loss: 0.262875\n",
      "Iteration 353050 | Loss: 0.262875\n",
      "Iteration 353075 | Loss: 0.262875\n",
      "Iteration 353100 | Loss: 0.262875\n",
      "Iteration 353125 | Loss: 0.262875\n",
      "Iteration 353150 | Loss: 0.262875\n",
      "Iteration 353175 | Loss: 0.262875\n",
      "Iteration 353200 | Loss: 0.262875\n",
      "Iteration 353225 | Loss: 0.262875\n",
      "Iteration 353250 | Loss: 0.262875\n",
      "Iteration 353275 | Loss: 0.262875\n",
      "Iteration 353300 | Loss: 0.262875\n",
      "Iteration 353325 | Loss: 0.262875\n",
      "Iteration 353350 | Loss: 0.262875\n",
      "Iteration 353375 | Loss: 0.262875\n",
      "Iteration 353400 | Loss: 0.262875\n",
      "Iteration 353425 | Loss: 0.262875\n",
      "Iteration 353450 | Loss: 0.262875\n",
      "Iteration 353475 | Loss: 0.262875\n",
      "Iteration 353500 | Loss: 0.262875\n",
      "Iteration 353525 | Loss: 0.262875\n",
      "Iteration 353550 | Loss: 0.262875\n",
      "Iteration 353575 | Loss: 0.262875\n",
      "Iteration 353600 | Loss: 0.262875\n",
      "Iteration 353625 | Loss: 0.262875\n",
      "Iteration 353650 | Loss: 0.262875\n",
      "Iteration 353675 | Loss: 0.262875\n",
      "Iteration 353700 | Loss: 0.262875\n",
      "Iteration 353725 | Loss: 0.262875\n",
      "Iteration 353750 | Loss: 0.262875\n",
      "Iteration 353775 | Loss: 0.262875\n",
      "Iteration 353800 | Loss: 0.262875\n",
      "Iteration 353825 | Loss: 0.262875\n",
      "Iteration 353850 | Loss: 0.262875\n",
      "Iteration 353875 | Loss: 0.262875\n",
      "Iteration 353900 | Loss: 0.262875\n",
      "Iteration 353925 | Loss: 0.262875\n",
      "Iteration 353950 | Loss: 0.262875\n",
      "Iteration 353975 | Loss: 0.262875\n",
      "Iteration 354000 | Loss: 0.262875\n",
      "Iteration 354025 | Loss: 0.262875\n",
      "Iteration 354050 | Loss: 0.262875\n",
      "Iteration 354075 | Loss: 0.262875\n",
      "Iteration 354100 | Loss: 0.262875\n",
      "Iteration 354125 | Loss: 0.262874\n",
      "Iteration 354150 | Loss: 0.262874\n",
      "Iteration 354175 | Loss: 0.262874\n",
      "Iteration 354200 | Loss: 0.262874\n",
      "Iteration 354225 | Loss: 0.262874\n",
      "Iteration 354250 | Loss: 0.262874\n",
      "Iteration 354275 | Loss: 0.262874\n",
      "Iteration 354300 | Loss: 0.262874\n",
      "Iteration 354325 | Loss: 0.262874\n",
      "Iteration 354350 | Loss: 0.262874\n",
      "Iteration 354375 | Loss: 0.262874\n",
      "Iteration 354400 | Loss: 0.262874\n",
      "Iteration 354425 | Loss: 0.262874\n",
      "Iteration 354450 | Loss: 0.262874\n",
      "Iteration 354475 | Loss: 0.262874\n",
      "Iteration 354500 | Loss: 0.262874\n",
      "Iteration 354525 | Loss: 0.262874\n",
      "Iteration 354550 | Loss: 0.262874\n",
      "Iteration 354575 | Loss: 0.262874\n",
      "Iteration 354600 | Loss: 0.262874\n",
      "Iteration 354625 | Loss: 0.262874\n",
      "Iteration 354650 | Loss: 0.262874\n",
      "Iteration 354675 | Loss: 0.262874\n",
      "Iteration 354700 | Loss: 0.262874\n",
      "Iteration 354725 | Loss: 0.262874\n",
      "Iteration 354750 | Loss: 0.262874\n",
      "Iteration 354775 | Loss: 0.262874\n",
      "Iteration 354800 | Loss: 0.262874\n",
      "Iteration 354825 | Loss: 0.262874\n",
      "Iteration 354850 | Loss: 0.262874\n",
      "Iteration 354875 | Loss: 0.262874\n",
      "Iteration 354900 | Loss: 0.262874\n",
      "Iteration 354925 | Loss: 0.262874\n",
      "Iteration 354950 | Loss: 0.262874\n",
      "Iteration 354975 | Loss: 0.262874\n",
      "Iteration 355000 | Loss: 0.262874\n",
      "Iteration 355025 | Loss: 0.262874\n",
      "Iteration 355050 | Loss: 0.262874\n",
      "Iteration 355075 | Loss: 0.262874\n",
      "Iteration 355100 | Loss: 0.262874\n",
      "Iteration 355125 | Loss: 0.262874\n",
      "Iteration 355150 | Loss: 0.262874\n",
      "Iteration 355175 | Loss: 0.262874\n",
      "Iteration 355200 | Loss: 0.262874\n",
      "Iteration 355225 | Loss: 0.262874\n",
      "Iteration 355250 | Loss: 0.262874\n",
      "Iteration 355275 | Loss: 0.262874\n",
      "Iteration 355300 | Loss: 0.262874\n",
      "Iteration 355325 | Loss: 0.262874\n",
      "Iteration 355350 | Loss: 0.262874\n",
      "Iteration 355375 | Loss: 0.262874\n",
      "Iteration 355400 | Loss: 0.262874\n",
      "Iteration 355425 | Loss: 0.262873\n",
      "Iteration 355450 | Loss: 0.262873\n",
      "Iteration 355475 | Loss: 0.262873\n",
      "Iteration 355500 | Loss: 0.262873\n",
      "Iteration 355525 | Loss: 0.262873\n",
      "Iteration 355550 | Loss: 0.262873\n",
      "Iteration 355575 | Loss: 0.262873\n",
      "Iteration 355600 | Loss: 0.262873\n",
      "Iteration 355625 | Loss: 0.262873\n",
      "Iteration 355650 | Loss: 0.262873\n",
      "Iteration 355675 | Loss: 0.262873\n",
      "Iteration 355700 | Loss: 0.262873\n",
      "Iteration 355725 | Loss: 0.262873\n",
      "Iteration 355750 | Loss: 0.262873\n",
      "Iteration 355775 | Loss: 0.262873\n",
      "Iteration 355800 | Loss: 0.262873\n",
      "Iteration 355825 | Loss: 0.262873\n",
      "Iteration 355850 | Loss: 0.262873\n",
      "Iteration 355875 | Loss: 0.262873\n",
      "Iteration 355900 | Loss: 0.262873\n",
      "Iteration 355925 | Loss: 0.262873\n",
      "Iteration 355950 | Loss: 0.262873\n",
      "Iteration 355975 | Loss: 0.262873\n",
      "Iteration 356000 | Loss: 0.262873\n",
      "Iteration 356025 | Loss: 0.262873\n",
      "Iteration 356050 | Loss: 0.262873\n",
      "Iteration 356075 | Loss: 0.262873\n",
      "Iteration 356100 | Loss: 0.262873\n",
      "Iteration 356125 | Loss: 0.262873\n",
      "Iteration 356150 | Loss: 0.262873\n",
      "Iteration 356175 | Loss: 0.262873\n",
      "Iteration 356200 | Loss: 0.262873\n",
      "Iteration 356225 | Loss: 0.262873\n",
      "Iteration 356250 | Loss: 0.262873\n",
      "Iteration 356275 | Loss: 0.262873\n",
      "Iteration 356300 | Loss: 0.262873\n",
      "Iteration 356325 | Loss: 0.262873\n",
      "Iteration 356350 | Loss: 0.262873\n",
      "Iteration 356375 | Loss: 0.262873\n",
      "Iteration 356400 | Loss: 0.262873\n",
      "Iteration 356425 | Loss: 0.262873\n",
      "Iteration 356450 | Loss: 0.262873\n",
      "Iteration 356475 | Loss: 0.262873\n",
      "Iteration 356500 | Loss: 0.262873\n",
      "Iteration 356525 | Loss: 0.262873\n",
      "Iteration 356550 | Loss: 0.262873\n",
      "Iteration 356575 | Loss: 0.262873\n",
      "Iteration 356600 | Loss: 0.262873\n",
      "Iteration 356625 | Loss: 0.262873\n",
      "Iteration 356650 | Loss: 0.262873\n",
      "Iteration 356675 | Loss: 0.262873\n",
      "Iteration 356700 | Loss: 0.262873\n",
      "Iteration 356725 | Loss: 0.262873\n",
      "Iteration 356750 | Loss: 0.262873\n",
      "Iteration 356775 | Loss: 0.262873\n",
      "Iteration 356800 | Loss: 0.262873\n",
      "Iteration 356825 | Loss: 0.262872\n",
      "Iteration 356850 | Loss: 0.262872\n",
      "Iteration 356875 | Loss: 0.262872\n",
      "Iteration 356900 | Loss: 0.262872\n",
      "Iteration 356925 | Loss: 0.262872\n",
      "Iteration 356950 | Loss: 0.262872\n",
      "Iteration 356975 | Loss: 0.262872\n",
      "Iteration 357000 | Loss: 0.262872\n",
      "Iteration 357025 | Loss: 0.262872\n",
      "Iteration 357050 | Loss: 0.262872\n",
      "Iteration 357075 | Loss: 0.262872\n",
      "Iteration 357100 | Loss: 0.262872\n",
      "Iteration 357125 | Loss: 0.262872\n",
      "Iteration 357150 | Loss: 0.262872\n",
      "Iteration 357175 | Loss: 0.262872\n",
      "Iteration 357200 | Loss: 0.262872\n",
      "Iteration 357225 | Loss: 0.262872\n",
      "Iteration 357250 | Loss: 0.262872\n",
      "Iteration 357275 | Loss: 0.262872\n",
      "Iteration 357300 | Loss: 0.262872\n",
      "Iteration 357325 | Loss: 0.262872\n",
      "Iteration 357350 | Loss: 0.262872\n",
      "Iteration 357375 | Loss: 0.262872\n",
      "Iteration 357400 | Loss: 0.262872\n",
      "Iteration 357425 | Loss: 0.262872\n",
      "Iteration 357450 | Loss: 0.262872\n",
      "Iteration 357475 | Loss: 0.262872\n",
      "Iteration 357500 | Loss: 0.262872\n",
      "Iteration 357525 | Loss: 0.262872\n",
      "Iteration 357550 | Loss: 0.262872\n",
      "Iteration 357575 | Loss: 0.262872\n",
      "Iteration 357600 | Loss: 0.262872\n",
      "Iteration 357625 | Loss: 0.262872\n",
      "Iteration 357650 | Loss: 0.262872\n",
      "Iteration 357675 | Loss: 0.262872\n",
      "Iteration 357700 | Loss: 0.262872\n",
      "Iteration 357725 | Loss: 0.262872\n",
      "Iteration 357750 | Loss: 0.262872\n",
      "Iteration 357775 | Loss: 0.262872\n",
      "Iteration 357800 | Loss: 0.262872\n",
      "Iteration 357825 | Loss: 0.262872\n",
      "Iteration 357850 | Loss: 0.262872\n",
      "Iteration 357875 | Loss: 0.262872\n",
      "Iteration 357900 | Loss: 0.262872\n",
      "Iteration 357925 | Loss: 0.262872\n",
      "Iteration 357950 | Loss: 0.262872\n",
      "Iteration 357975 | Loss: 0.262872\n",
      "Iteration 358000 | Loss: 0.262872\n",
      "Iteration 358025 | Loss: 0.262872\n",
      "Iteration 358050 | Loss: 0.262872\n",
      "Iteration 358075 | Loss: 0.262872\n",
      "Iteration 358100 | Loss: 0.262872\n",
      "Iteration 358125 | Loss: 0.262872\n",
      "Iteration 358150 | Loss: 0.262872\n",
      "Iteration 358175 | Loss: 0.262872\n",
      "Iteration 358200 | Loss: 0.262872\n",
      "Iteration 358225 | Loss: 0.262872\n",
      "Iteration 358250 | Loss: 0.262872\n",
      "Iteration 358275 | Loss: 0.262872\n",
      "Iteration 358300 | Loss: 0.262872\n",
      "Iteration 358325 | Loss: 0.262871\n",
      "Iteration 358350 | Loss: 0.262871\n",
      "Iteration 358375 | Loss: 0.262871\n",
      "Iteration 358400 | Loss: 0.262871\n",
      "Iteration 358425 | Loss: 0.262871\n",
      "Iteration 358450 | Loss: 0.262871\n",
      "Iteration 358475 | Loss: 0.262871\n",
      "Iteration 358500 | Loss: 0.262871\n",
      "Iteration 358525 | Loss: 0.262871\n",
      "Iteration 358550 | Loss: 0.262871\n",
      "Iteration 358575 | Loss: 0.262871\n",
      "Iteration 358600 | Loss: 0.262871\n",
      "Iteration 358625 | Loss: 0.262871\n",
      "Iteration 358650 | Loss: 0.262871\n",
      "Iteration 358675 | Loss: 0.262871\n",
      "Iteration 358700 | Loss: 0.262871\n",
      "Iteration 358725 | Loss: 0.262871\n",
      "Iteration 358750 | Loss: 0.262871\n",
      "Iteration 358775 | Loss: 0.262871\n",
      "Iteration 358800 | Loss: 0.262871\n",
      "Iteration 358825 | Loss: 0.262871\n",
      "Iteration 358850 | Loss: 0.262871\n",
      "Iteration 358875 | Loss: 0.262871\n",
      "Iteration 358900 | Loss: 0.262871\n",
      "Iteration 358925 | Loss: 0.262871\n",
      "Iteration 358950 | Loss: 0.262871\n",
      "Iteration 358975 | Loss: 0.262871\n",
      "Iteration 359000 | Loss: 0.262871\n",
      "Iteration 359025 | Loss: 0.262871\n",
      "Iteration 359050 | Loss: 0.262871\n",
      "Iteration 359075 | Loss: 0.262871\n",
      "Iteration 359100 | Loss: 0.262871\n",
      "Iteration 359125 | Loss: 0.262871\n",
      "Iteration 359150 | Loss: 0.262871\n",
      "Iteration 359175 | Loss: 0.262871\n",
      "Iteration 359200 | Loss: 0.262871\n",
      "Iteration 359225 | Loss: 0.262871\n",
      "Iteration 359250 | Loss: 0.262871\n",
      "Iteration 359275 | Loss: 0.262871\n",
      "Iteration 359300 | Loss: 0.262871\n",
      "Iteration 359325 | Loss: 0.262871\n",
      "Iteration 359350 | Loss: 0.262871\n",
      "Iteration 359375 | Loss: 0.262871\n",
      "Iteration 359400 | Loss: 0.262871\n",
      "Iteration 359425 | Loss: 0.262871\n",
      "Iteration 359450 | Loss: 0.262871\n",
      "Iteration 359475 | Loss: 0.262871\n",
      "Iteration 359500 | Loss: 0.262871\n",
      "Iteration 359525 | Loss: 0.262871\n",
      "Iteration 359550 | Loss: 0.262871\n",
      "Iteration 359575 | Loss: 0.262871\n",
      "Iteration 359600 | Loss: 0.262871\n",
      "Iteration 359625 | Loss: 0.262871\n",
      "Iteration 359650 | Loss: 0.262871\n",
      "Iteration 359675 | Loss: 0.262871\n",
      "Iteration 359700 | Loss: 0.262871\n",
      "Iteration 359725 | Loss: 0.262871\n",
      "Iteration 359750 | Loss: 0.262871\n",
      "Iteration 359775 | Loss: 0.262871\n",
      "Iteration 359800 | Loss: 0.262871\n",
      "Iteration 359825 | Loss: 0.262871\n",
      "Iteration 359850 | Loss: 0.262871\n",
      "Iteration 359875 | Loss: 0.262871\n",
      "Iteration 359900 | Loss: 0.262870\n",
      "Iteration 359925 | Loss: 0.262870\n",
      "Iteration 359950 | Loss: 0.262870\n",
      "Iteration 359975 | Loss: 0.262870\n",
      "Iteration 360000 | Loss: 0.262870\n",
      "Iteration 360025 | Loss: 0.262870\n",
      "Iteration 360050 | Loss: 0.262870\n",
      "Iteration 360075 | Loss: 0.262870\n",
      "Iteration 360100 | Loss: 0.262870\n",
      "Iteration 360125 | Loss: 0.262870\n",
      "Iteration 360150 | Loss: 0.262870\n",
      "Iteration 360175 | Loss: 0.262870\n",
      "Iteration 360200 | Loss: 0.262870\n",
      "Iteration 360225 | Loss: 0.262870\n",
      "Iteration 360250 | Loss: 0.262870\n",
      "Iteration 360275 | Loss: 0.262870\n",
      "Iteration 360300 | Loss: 0.262870\n",
      "Iteration 360325 | Loss: 0.262870\n",
      "Iteration 360350 | Loss: 0.262870\n",
      "Iteration 360375 | Loss: 0.262870\n",
      "Iteration 360400 | Loss: 0.262870\n",
      "Iteration 360425 | Loss: 0.262870\n",
      "Iteration 360450 | Loss: 0.262870\n",
      "Iteration 360475 | Loss: 0.262870\n",
      "Iteration 360500 | Loss: 0.262870\n",
      "Iteration 360525 | Loss: 0.262870\n",
      "Iteration 360550 | Loss: 0.262870\n",
      "Iteration 360575 | Loss: 0.262870\n",
      "Iteration 360600 | Loss: 0.262870\n",
      "Iteration 360625 | Loss: 0.262870\n",
      "Iteration 360650 | Loss: 0.262870\n",
      "Iteration 360675 | Loss: 0.262870\n",
      "Iteration 360700 | Loss: 0.262870\n",
      "Iteration 360725 | Loss: 0.262870\n",
      "Iteration 360750 | Loss: 0.262870\n",
      "Iteration 360775 | Loss: 0.262870\n",
      "Iteration 360800 | Loss: 0.262870\n",
      "Iteration 360825 | Loss: 0.262870\n",
      "Iteration 360850 | Loss: 0.262870\n",
      "Iteration 360875 | Loss: 0.262870\n",
      "Iteration 360900 | Loss: 0.262870\n",
      "Iteration 360925 | Loss: 0.262870\n",
      "Iteration 360950 | Loss: 0.262870\n",
      "Iteration 360975 | Loss: 0.262870\n",
      "Iteration 361000 | Loss: 0.262870\n",
      "Iteration 361025 | Loss: 0.262870\n",
      "Iteration 361050 | Loss: 0.262870\n",
      "Iteration 361075 | Loss: 0.262870\n",
      "Iteration 361100 | Loss: 0.262870\n",
      "Iteration 361125 | Loss: 0.262870\n",
      "Iteration 361150 | Loss: 0.262870\n",
      "Iteration 361175 | Loss: 0.262870\n",
      "Iteration 361200 | Loss: 0.262870\n",
      "Iteration 361225 | Loss: 0.262870\n",
      "Iteration 361250 | Loss: 0.262870\n",
      "Iteration 361275 | Loss: 0.262870\n",
      "Iteration 361300 | Loss: 0.262870\n",
      "Iteration 361325 | Loss: 0.262870\n",
      "Iteration 361350 | Loss: 0.262870\n",
      "Iteration 361375 | Loss: 0.262870\n",
      "Iteration 361400 | Loss: 0.262870\n",
      "Iteration 361425 | Loss: 0.262870\n",
      "Iteration 361450 | Loss: 0.262870\n",
      "Iteration 361475 | Loss: 0.262870\n",
      "Iteration 361500 | Loss: 0.262870\n",
      "Iteration 361525 | Loss: 0.262870\n",
      "Iteration 361550 | Loss: 0.262870\n",
      "Iteration 361575 | Loss: 0.262869\n",
      "Iteration 361600 | Loss: 0.262869\n",
      "Iteration 361625 | Loss: 0.262869\n",
      "Iteration 361650 | Loss: 0.262869\n",
      "Iteration 361675 | Loss: 0.262869\n",
      "Iteration 361700 | Loss: 0.262869\n",
      "Iteration 361725 | Loss: 0.262869\n",
      "Iteration 361750 | Loss: 0.262869\n",
      "Iteration 361775 | Loss: 0.262869\n",
      "Iteration 361800 | Loss: 0.262869\n",
      "Iteration 361825 | Loss: 0.262869\n",
      "Iteration 361850 | Loss: 0.262869\n",
      "Iteration 361875 | Loss: 0.262869\n",
      "Iteration 361900 | Loss: 0.262869\n",
      "Iteration 361925 | Loss: 0.262869\n",
      "Iteration 361950 | Loss: 0.262869\n",
      "Iteration 361975 | Loss: 0.262869\n",
      "Iteration 362000 | Loss: 0.262869\n",
      "Iteration 362025 | Loss: 0.262869\n",
      "Iteration 362050 | Loss: 0.262869\n",
      "Iteration 362075 | Loss: 0.262869\n",
      "Iteration 362100 | Loss: 0.262869\n",
      "Iteration 362125 | Loss: 0.262869\n",
      "Iteration 362150 | Loss: 0.262869\n",
      "Iteration 362175 | Loss: 0.262869\n",
      "Iteration 362200 | Loss: 0.262869\n",
      "Iteration 362225 | Loss: 0.262869\n",
      "Iteration 362250 | Loss: 0.262869\n",
      "Iteration 362275 | Loss: 0.262869\n",
      "Iteration 362300 | Loss: 0.262869\n",
      "Iteration 362325 | Loss: 0.262869\n",
      "Iteration 362350 | Loss: 0.262869\n",
      "Iteration 362375 | Loss: 0.262869\n",
      "Iteration 362400 | Loss: 0.262869\n",
      "Iteration 362425 | Loss: 0.262869\n",
      "Iteration 362450 | Loss: 0.262869\n",
      "Iteration 362475 | Loss: 0.262869\n",
      "Iteration 362500 | Loss: 0.262869\n",
      "Iteration 362525 | Loss: 0.262869\n",
      "Iteration 362550 | Loss: 0.262869\n",
      "Iteration 362575 | Loss: 0.262869\n",
      "Iteration 362600 | Loss: 0.262869\n",
      "Iteration 362625 | Loss: 0.262869\n",
      "Iteration 362650 | Loss: 0.262869\n",
      "Iteration 362675 | Loss: 0.262869\n",
      "Iteration 362700 | Loss: 0.262869\n",
      "Iteration 362725 | Loss: 0.262869\n",
      "Iteration 362750 | Loss: 0.262869\n",
      "Iteration 362775 | Loss: 0.262869\n",
      "Iteration 362800 | Loss: 0.262869\n",
      "Iteration 362825 | Loss: 0.262869\n",
      "Iteration 362850 | Loss: 0.262869\n",
      "Iteration 362875 | Loss: 0.262869\n",
      "Iteration 362900 | Loss: 0.262869\n",
      "Iteration 362925 | Loss: 0.262869\n",
      "Iteration 362950 | Loss: 0.262869\n",
      "Iteration 362975 | Loss: 0.262869\n",
      "Iteration 363000 | Loss: 0.262869\n",
      "Iteration 363025 | Loss: 0.262869\n",
      "Iteration 363050 | Loss: 0.262869\n",
      "Iteration 363075 | Loss: 0.262869\n",
      "Iteration 363100 | Loss: 0.262869\n",
      "Iteration 363125 | Loss: 0.262869\n",
      "Iteration 363150 | Loss: 0.262869\n",
      "Iteration 363175 | Loss: 0.262869\n",
      "Iteration 363200 | Loss: 0.262869\n",
      "Iteration 363225 | Loss: 0.262869\n",
      "Iteration 363250 | Loss: 0.262869\n",
      "Iteration 363275 | Loss: 0.262869\n",
      "Iteration 363300 | Loss: 0.262869\n",
      "Iteration 363325 | Loss: 0.262869\n",
      "Iteration 363350 | Loss: 0.262869\n",
      "Iteration 363375 | Loss: 0.262869\n",
      "Iteration 363400 | Loss: 0.262868\n",
      "Iteration 363425 | Loss: 0.262868\n",
      "Iteration 363450 | Loss: 0.262868\n",
      "Iteration 363475 | Loss: 0.262868\n",
      "Iteration 363500 | Loss: 0.262868\n",
      "Iteration 363525 | Loss: 0.262868\n",
      "Iteration 363550 | Loss: 0.262868\n",
      "Iteration 363575 | Loss: 0.262868\n",
      "Iteration 363600 | Loss: 0.262868\n",
      "Iteration 363625 | Loss: 0.262868\n",
      "Iteration 363650 | Loss: 0.262868\n",
      "Iteration 363675 | Loss: 0.262868\n",
      "Iteration 363700 | Loss: 0.262868\n",
      "Iteration 363725 | Loss: 0.262868\n",
      "Iteration 363750 | Loss: 0.262868\n",
      "Iteration 363775 | Loss: 0.262868\n",
      "Iteration 363800 | Loss: 0.262868\n",
      "Iteration 363825 | Loss: 0.262868\n",
      "Iteration 363850 | Loss: 0.262868\n",
      "Iteration 363875 | Loss: 0.262868\n",
      "Iteration 363900 | Loss: 0.262868\n",
      "Iteration 363925 | Loss: 0.262868\n",
      "Iteration 363950 | Loss: 0.262868\n",
      "Iteration 363975 | Loss: 0.262868\n",
      "Iteration 364000 | Loss: 0.262868\n",
      "Iteration 364025 | Loss: 0.262868\n",
      "Iteration 364050 | Loss: 0.262868\n",
      "Iteration 364075 | Loss: 0.262868\n",
      "Iteration 364100 | Loss: 0.262868\n",
      "Iteration 364125 | Loss: 0.262868\n",
      "Iteration 364150 | Loss: 0.262868\n",
      "Iteration 364175 | Loss: 0.262868\n",
      "Iteration 364200 | Loss: 0.262868\n",
      "Iteration 364225 | Loss: 0.262868\n",
      "Iteration 364250 | Loss: 0.262868\n",
      "Iteration 364275 | Loss: 0.262868\n",
      "Iteration 364300 | Loss: 0.262868\n",
      "Iteration 364325 | Loss: 0.262868\n",
      "Iteration 364350 | Loss: 0.262868\n",
      "Iteration 364375 | Loss: 0.262868\n",
      "Iteration 364400 | Loss: 0.262868\n",
      "Iteration 364425 | Loss: 0.262868\n",
      "Iteration 364450 | Loss: 0.262868\n",
      "Iteration 364475 | Loss: 0.262868\n",
      "Iteration 364500 | Loss: 0.262868\n",
      "Iteration 364525 | Loss: 0.262868\n",
      "Iteration 364550 | Loss: 0.262868\n",
      "Iteration 364575 | Loss: 0.262868\n",
      "Iteration 364600 | Loss: 0.262868\n",
      "Iteration 364625 | Loss: 0.262868\n",
      "Iteration 364650 | Loss: 0.262868\n",
      "Iteration 364675 | Loss: 0.262868\n",
      "Iteration 364700 | Loss: 0.262868\n",
      "Iteration 364725 | Loss: 0.262868\n",
      "Iteration 364750 | Loss: 0.262868\n",
      "Iteration 364775 | Loss: 0.262868\n",
      "Iteration 364800 | Loss: 0.262868\n",
      "Iteration 364825 | Loss: 0.262868\n",
      "Iteration 364850 | Loss: 0.262868\n",
      "Iteration 364875 | Loss: 0.262868\n",
      "Iteration 364900 | Loss: 0.262868\n",
      "Iteration 364925 | Loss: 0.262868\n",
      "Iteration 364950 | Loss: 0.262868\n",
      "Iteration 364975 | Loss: 0.262868\n",
      "Iteration 365000 | Loss: 0.262868\n",
      "Iteration 365025 | Loss: 0.262868\n",
      "Iteration 365050 | Loss: 0.262868\n",
      "Iteration 365075 | Loss: 0.262868\n",
      "Iteration 365100 | Loss: 0.262868\n",
      "Iteration 365125 | Loss: 0.262868\n",
      "Iteration 365150 | Loss: 0.262868\n",
      "Iteration 365175 | Loss: 0.262868\n",
      "Iteration 365200 | Loss: 0.262868\n",
      "Iteration 365225 | Loss: 0.262868\n",
      "Iteration 365250 | Loss: 0.262868\n",
      "Iteration 365275 | Loss: 0.262868\n",
      "Iteration 365300 | Loss: 0.262868\n",
      "Iteration 365325 | Loss: 0.262868\n",
      "Iteration 365350 | Loss: 0.262867\n",
      "Iteration 365375 | Loss: 0.262867\n",
      "Iteration 365400 | Loss: 0.262867\n",
      "Iteration 365425 | Loss: 0.262867\n",
      "Iteration 365450 | Loss: 0.262867\n",
      "Iteration 365475 | Loss: 0.262867\n",
      "Iteration 365500 | Loss: 0.262867\n",
      "Iteration 365525 | Loss: 0.262867\n",
      "Iteration 365550 | Loss: 0.262867\n",
      "Iteration 365575 | Loss: 0.262867\n",
      "Iteration 365600 | Loss: 0.262867\n",
      "Iteration 365625 | Loss: 0.262867\n",
      "Iteration 365650 | Loss: 0.262867\n",
      "Iteration 365675 | Loss: 0.262867\n",
      "Iteration 365700 | Loss: 0.262867\n",
      "Iteration 365725 | Loss: 0.262867\n",
      "Iteration 365750 | Loss: 0.262867\n",
      "Iteration 365775 | Loss: 0.262867\n",
      "Iteration 365800 | Loss: 0.262867\n",
      "Iteration 365825 | Loss: 0.262867\n",
      "Iteration 365850 | Loss: 0.262867\n",
      "Iteration 365875 | Loss: 0.262867\n",
      "Iteration 365900 | Loss: 0.262867\n",
      "Iteration 365925 | Loss: 0.262867\n",
      "Iteration 365950 | Loss: 0.262867\n",
      "Iteration 365975 | Loss: 0.262867\n",
      "Iteration 366000 | Loss: 0.262867\n",
      "Iteration 366025 | Loss: 0.262867\n",
      "Iteration 366050 | Loss: 0.262867\n",
      "Iteration 366075 | Loss: 0.262867\n",
      "Iteration 366100 | Loss: 0.262867\n",
      "Iteration 366125 | Loss: 0.262867\n",
      "Iteration 366150 | Loss: 0.262867\n",
      "Iteration 366175 | Loss: 0.262867\n",
      "Iteration 366200 | Loss: 0.262867\n",
      "Iteration 366225 | Loss: 0.262867\n",
      "Iteration 366250 | Loss: 0.262867\n",
      "Iteration 366275 | Loss: 0.262867\n",
      "Iteration 366300 | Loss: 0.262867\n",
      "Iteration 366325 | Loss: 0.262867\n",
      "Iteration 366350 | Loss: 0.262867\n",
      "Iteration 366375 | Loss: 0.262867\n",
      "Iteration 366400 | Loss: 0.262867\n",
      "Iteration 366425 | Loss: 0.262867\n",
      "Iteration 366450 | Loss: 0.262867\n",
      "Iteration 366475 | Loss: 0.262867\n",
      "Iteration 366500 | Loss: 0.262867\n",
      "Iteration 366525 | Loss: 0.262867\n",
      "Iteration 366550 | Loss: 0.262867\n",
      "Iteration 366575 | Loss: 0.262867\n",
      "Iteration 366600 | Loss: 0.262867\n",
      "Iteration 366625 | Loss: 0.262867\n",
      "Iteration 366650 | Loss: 0.262867\n",
      "Iteration 366675 | Loss: 0.262867\n",
      "Iteration 366700 | Loss: 0.262867\n",
      "Iteration 366725 | Loss: 0.262867\n",
      "Iteration 366750 | Loss: 0.262867\n",
      "Iteration 366775 | Loss: 0.262867\n",
      "Iteration 366800 | Loss: 0.262867\n",
      "Iteration 366825 | Loss: 0.262867\n",
      "Iteration 366850 | Loss: 0.262867\n",
      "Iteration 366875 | Loss: 0.262867\n",
      "Iteration 366900 | Loss: 0.262867\n",
      "Iteration 366925 | Loss: 0.262867\n",
      "Iteration 366950 | Loss: 0.262867\n",
      "Iteration 366975 | Loss: 0.262867\n",
      "Iteration 367000 | Loss: 0.262867\n",
      "Iteration 367025 | Loss: 0.262867\n",
      "Iteration 367050 | Loss: 0.262867\n",
      "Iteration 367075 | Loss: 0.262867\n",
      "Iteration 367100 | Loss: 0.262867\n",
      "Iteration 367125 | Loss: 0.262867\n",
      "Iteration 367150 | Loss: 0.262867\n",
      "Iteration 367175 | Loss: 0.262867\n",
      "Iteration 367200 | Loss: 0.262867\n",
      "Iteration 367225 | Loss: 0.262867\n",
      "Iteration 367250 | Loss: 0.262867\n",
      "Iteration 367275 | Loss: 0.262867\n",
      "Iteration 367300 | Loss: 0.262867\n",
      "Iteration 367325 | Loss: 0.262867\n",
      "Iteration 367350 | Loss: 0.262867\n",
      "Iteration 367375 | Loss: 0.262867\n",
      "Iteration 367400 | Loss: 0.262867\n",
      "Iteration 367425 | Loss: 0.262867\n",
      "Iteration 367450 | Loss: 0.262867\n",
      "Iteration 367475 | Loss: 0.262866\n",
      "Iteration 367500 | Loss: 0.262866\n",
      "Iteration 367525 | Loss: 0.262866\n",
      "Iteration 367550 | Loss: 0.262866\n",
      "Iteration 367575 | Loss: 0.262866\n",
      "Iteration 367600 | Loss: 0.262866\n",
      "Iteration 367625 | Loss: 0.262866\n",
      "Iteration 367650 | Loss: 0.262866\n",
      "Iteration 367675 | Loss: 0.262866\n",
      "Iteration 367700 | Loss: 0.262866\n",
      "Iteration 367725 | Loss: 0.262866\n",
      "Iteration 367750 | Loss: 0.262866\n",
      "Iteration 367775 | Loss: 0.262866\n",
      "Iteration 367800 | Loss: 0.262866\n",
      "Iteration 367825 | Loss: 0.262866\n",
      "Iteration 367850 | Loss: 0.262866\n",
      "Iteration 367875 | Loss: 0.262866\n",
      "Iteration 367900 | Loss: 0.262866\n",
      "Iteration 367925 | Loss: 0.262866\n",
      "Iteration 367950 | Loss: 0.262866\n",
      "Iteration 367975 | Loss: 0.262866\n",
      "Iteration 368000 | Loss: 0.262866\n",
      "Iteration 368025 | Loss: 0.262866\n",
      "Iteration 368050 | Loss: 0.262866\n",
      "Iteration 368075 | Loss: 0.262866\n",
      "Iteration 368100 | Loss: 0.262866\n",
      "Iteration 368125 | Loss: 0.262866\n",
      "Iteration 368150 | Loss: 0.262866\n",
      "Iteration 368175 | Loss: 0.262866\n",
      "Iteration 368200 | Loss: 0.262866\n",
      "Iteration 368225 | Loss: 0.262866\n",
      "Iteration 368250 | Loss: 0.262866\n",
      "Iteration 368275 | Loss: 0.262866\n",
      "Iteration 368300 | Loss: 0.262866\n",
      "Iteration 368325 | Loss: 0.262866\n",
      "Iteration 368350 | Loss: 0.262866\n",
      "Iteration 368375 | Loss: 0.262866\n",
      "Iteration 368400 | Loss: 0.262866\n",
      "Iteration 368425 | Loss: 0.262866\n",
      "Iteration 368450 | Loss: 0.262866\n",
      "Iteration 368475 | Loss: 0.262866\n",
      "Iteration 368500 | Loss: 0.262866\n",
      "Iteration 368525 | Loss: 0.262866\n",
      "Iteration 368550 | Loss: 0.262866\n",
      "Iteration 368575 | Loss: 0.262866\n",
      "Iteration 368600 | Loss: 0.262866\n",
      "Iteration 368625 | Loss: 0.262866\n",
      "Iteration 368650 | Loss: 0.262866\n",
      "Iteration 368675 | Loss: 0.262866\n",
      "Iteration 368700 | Loss: 0.262866\n",
      "Iteration 368725 | Loss: 0.262866\n",
      "Iteration 368750 | Loss: 0.262866\n",
      "Iteration 368775 | Loss: 0.262866\n",
      "Iteration 368800 | Loss: 0.262866\n",
      "Iteration 368825 | Loss: 0.262866\n",
      "Iteration 368850 | Loss: 0.262866\n",
      "Iteration 368875 | Loss: 0.262866\n",
      "Iteration 368900 | Loss: 0.262866\n",
      "Iteration 368925 | Loss: 0.262866\n",
      "Iteration 368950 | Loss: 0.262866\n",
      "Iteration 368975 | Loss: 0.262866\n",
      "Iteration 369000 | Loss: 0.262866\n",
      "Iteration 369025 | Loss: 0.262866\n",
      "Iteration 369050 | Loss: 0.262866\n",
      "Iteration 369075 | Loss: 0.262866\n",
      "Iteration 369100 | Loss: 0.262866\n",
      "Iteration 369125 | Loss: 0.262866\n",
      "Iteration 369150 | Loss: 0.262866\n",
      "Iteration 369175 | Loss: 0.262866\n",
      "Iteration 369200 | Loss: 0.262866\n",
      "Iteration 369225 | Loss: 0.262866\n",
      "Iteration 369250 | Loss: 0.262866\n",
      "Iteration 369275 | Loss: 0.262866\n",
      "Iteration 369300 | Loss: 0.262866\n",
      "Iteration 369325 | Loss: 0.262866\n",
      "Iteration 369350 | Loss: 0.262866\n",
      "Iteration 369375 | Loss: 0.262866\n",
      "Iteration 369400 | Loss: 0.262866\n",
      "Iteration 369425 | Loss: 0.262866\n",
      "Iteration 369450 | Loss: 0.262866\n",
      "Iteration 369475 | Loss: 0.262866\n",
      "Iteration 369500 | Loss: 0.262866\n",
      "Iteration 369525 | Loss: 0.262866\n",
      "Iteration 369550 | Loss: 0.262866\n",
      "Iteration 369575 | Loss: 0.262866\n",
      "Iteration 369600 | Loss: 0.262866\n",
      "Iteration 369625 | Loss: 0.262866\n",
      "Iteration 369650 | Loss: 0.262866\n",
      "Iteration 369675 | Loss: 0.262866\n",
      "Iteration 369700 | Loss: 0.262866\n",
      "Iteration 369725 | Loss: 0.262866\n",
      "Iteration 369750 | Loss: 0.262866\n",
      "Iteration 369775 | Loss: 0.262866\n",
      "Iteration 369800 | Loss: 0.262865\n",
      "Iteration 369825 | Loss: 0.262865\n",
      "Iteration 369850 | Loss: 0.262865\n",
      "Iteration 369875 | Loss: 0.262865\n",
      "Iteration 369900 | Loss: 0.262865\n",
      "Iteration 369925 | Loss: 0.262865\n",
      "Iteration 369950 | Loss: 0.262865\n",
      "Iteration 369975 | Loss: 0.262865\n",
      "Iteration 370000 | Loss: 0.262865\n",
      "Iteration 370025 | Loss: 0.262865\n",
      "Iteration 370050 | Loss: 0.262865\n",
      "Iteration 370075 | Loss: 0.262865\n",
      "Iteration 370100 | Loss: 0.262865\n",
      "Iteration 370125 | Loss: 0.262865\n",
      "Iteration 370150 | Loss: 0.262865\n",
      "Iteration 370175 | Loss: 0.262865\n",
      "Iteration 370200 | Loss: 0.262865\n",
      "Iteration 370225 | Loss: 0.262865\n",
      "Iteration 370250 | Loss: 0.262865\n",
      "Iteration 370275 | Loss: 0.262865\n",
      "Iteration 370300 | Loss: 0.262865\n",
      "Iteration 370325 | Loss: 0.262865\n",
      "Iteration 370350 | Loss: 0.262865\n",
      "Iteration 370375 | Loss: 0.262865\n",
      "Iteration 370400 | Loss: 0.262865\n",
      "Iteration 370425 | Loss: 0.262865\n",
      "Iteration 370450 | Loss: 0.262865\n",
      "Iteration 370475 | Loss: 0.262865\n",
      "Iteration 370500 | Loss: 0.262865\n",
      "Iteration 370525 | Loss: 0.262865\n",
      "Iteration 370550 | Loss: 0.262865\n",
      "Iteration 370575 | Loss: 0.262865\n",
      "Iteration 370600 | Loss: 0.262865\n",
      "Iteration 370625 | Loss: 0.262865\n",
      "Iteration 370650 | Loss: 0.262865\n",
      "Iteration 370675 | Loss: 0.262865\n",
      "Iteration 370700 | Loss: 0.262865\n",
      "Iteration 370725 | Loss: 0.262865\n",
      "Iteration 370750 | Loss: 0.262865\n",
      "Iteration 370775 | Loss: 0.262865\n",
      "Iteration 370800 | Loss: 0.262865\n",
      "Iteration 370825 | Loss: 0.262865\n",
      "Iteration 370850 | Loss: 0.262865\n",
      "Iteration 370875 | Loss: 0.262865\n",
      "Iteration 370900 | Loss: 0.262865\n",
      "Iteration 370925 | Loss: 0.262865\n",
      "Iteration 370950 | Loss: 0.262865\n",
      "Iteration 370975 | Loss: 0.262865\n",
      "Iteration 371000 | Loss: 0.262865\n",
      "Iteration 371025 | Loss: 0.262865\n",
      "Iteration 371050 | Loss: 0.262865\n",
      "Iteration 371075 | Loss: 0.262865\n",
      "Iteration 371100 | Loss: 0.262865\n",
      "Iteration 371125 | Loss: 0.262865\n",
      "Iteration 371150 | Loss: 0.262865\n",
      "Iteration 371175 | Loss: 0.262865\n",
      "Iteration 371200 | Loss: 0.262865\n",
      "Iteration 371225 | Loss: 0.262865\n",
      "Iteration 371250 | Loss: 0.262865\n",
      "Iteration 371275 | Loss: 0.262865\n",
      "Iteration 371300 | Loss: 0.262865\n",
      "Iteration 371325 | Loss: 0.262865\n",
      "Iteration 371350 | Loss: 0.262865\n",
      "Iteration 371375 | Loss: 0.262865\n",
      "Iteration 371400 | Loss: 0.262865\n",
      "Iteration 371425 | Loss: 0.262865\n",
      "Iteration 371450 | Loss: 0.262865\n",
      "Iteration 371475 | Loss: 0.262865\n",
      "Iteration 371500 | Loss: 0.262865\n",
      "Iteration 371525 | Loss: 0.262865\n",
      "Iteration 371550 | Loss: 0.262865\n",
      "Iteration 371575 | Loss: 0.262865\n",
      "Iteration 371600 | Loss: 0.262865\n",
      "Iteration 371625 | Loss: 0.262865\n",
      "Iteration 371650 | Loss: 0.262865\n",
      "Iteration 371675 | Loss: 0.262865\n",
      "Iteration 371700 | Loss: 0.262865\n",
      "Iteration 371725 | Loss: 0.262865\n",
      "Iteration 371750 | Loss: 0.262865\n",
      "Iteration 371775 | Loss: 0.262865\n",
      "Iteration 371800 | Loss: 0.262865\n",
      "Iteration 371825 | Loss: 0.262865\n",
      "Iteration 371850 | Loss: 0.262865\n",
      "Iteration 371875 | Loss: 0.262865\n",
      "Iteration 371900 | Loss: 0.262865\n",
      "Iteration 371925 | Loss: 0.262865\n",
      "Iteration 371950 | Loss: 0.262865\n",
      "Iteration 371975 | Loss: 0.262865\n",
      "Iteration 372000 | Loss: 0.262865\n",
      "Iteration 372025 | Loss: 0.262865\n",
      "Iteration 372050 | Loss: 0.262865\n",
      "Iteration 372075 | Loss: 0.262865\n",
      "Iteration 372100 | Loss: 0.262865\n",
      "Iteration 372125 | Loss: 0.262865\n",
      "Iteration 372150 | Loss: 0.262865\n",
      "Iteration 372175 | Loss: 0.262865\n",
      "Iteration 372200 | Loss: 0.262865\n",
      "Iteration 372225 | Loss: 0.262865\n",
      "Iteration 372250 | Loss: 0.262865\n",
      "Iteration 372275 | Loss: 0.262865\n",
      "Iteration 372300 | Loss: 0.262865\n",
      "Iteration 372325 | Loss: 0.262865\n",
      "Iteration 372350 | Loss: 0.262865\n",
      "Iteration 372375 | Loss: 0.262864\n",
      "Iteration 372400 | Loss: 0.262864\n",
      "Iteration 372425 | Loss: 0.262864\n",
      "Iteration 372450 | Loss: 0.262864\n",
      "Iteration 372475 | Loss: 0.262864\n",
      "Iteration 372500 | Loss: 0.262864\n",
      "Iteration 372525 | Loss: 0.262864\n",
      "Iteration 372550 | Loss: 0.262864\n",
      "Iteration 372575 | Loss: 0.262864\n",
      "Iteration 372600 | Loss: 0.262864\n",
      "Iteration 372625 | Loss: 0.262864\n",
      "Iteration 372650 | Loss: 0.262864\n",
      "Iteration 372675 | Loss: 0.262864\n",
      "Iteration 372700 | Loss: 0.262864\n",
      "Iteration 372725 | Loss: 0.262864\n",
      "Iteration 372750 | Loss: 0.262864\n",
      "Iteration 372775 | Loss: 0.262864\n",
      "Iteration 372800 | Loss: 0.262864\n",
      "Iteration 372825 | Loss: 0.262864\n",
      "Iteration 372850 | Loss: 0.262864\n",
      "Iteration 372875 | Loss: 0.262864\n",
      "Iteration 372900 | Loss: 0.262864\n",
      "Iteration 372925 | Loss: 0.262864\n",
      "Iteration 372950 | Loss: 0.262864\n",
      "Iteration 372975 | Loss: 0.262864\n",
      "Iteration 373000 | Loss: 0.262864\n",
      "Iteration 373025 | Loss: 0.262864\n",
      "Iteration 373050 | Loss: 0.262864\n",
      "Iteration 373075 | Loss: 0.262864\n",
      "Iteration 373100 | Loss: 0.262864\n",
      "Iteration 373125 | Loss: 0.262864\n",
      "Iteration 373150 | Loss: 0.262864\n",
      "Iteration 373175 | Loss: 0.262864\n",
      "Iteration 373200 | Loss: 0.262864\n",
      "Iteration 373225 | Loss: 0.262864\n",
      "Iteration 373250 | Loss: 0.262864\n",
      "Iteration 373275 | Loss: 0.262864\n",
      "Iteration 373300 | Loss: 0.262864\n",
      "Iteration 373325 | Loss: 0.262864\n",
      "Iteration 373350 | Loss: 0.262864\n",
      "Iteration 373375 | Loss: 0.262864\n",
      "Iteration 373400 | Loss: 0.262864\n",
      "Iteration 373425 | Loss: 0.262864\n",
      "Iteration 373450 | Loss: 0.262864\n",
      "Iteration 373475 | Loss: 0.262864\n",
      "Iteration 373500 | Loss: 0.262864\n",
      "Iteration 373525 | Loss: 0.262864\n",
      "Iteration 373550 | Loss: 0.262864\n",
      "Iteration 373575 | Loss: 0.262864\n",
      "Iteration 373600 | Loss: 0.262864\n",
      "Iteration 373625 | Loss: 0.262864\n",
      "Iteration 373650 | Loss: 0.262864\n",
      "Iteration 373675 | Loss: 0.262864\n",
      "Iteration 373700 | Loss: 0.262864\n",
      "Iteration 373725 | Loss: 0.262864\n",
      "Iteration 373750 | Loss: 0.262864\n",
      "Iteration 373775 | Loss: 0.262864\n",
      "Iteration 373800 | Loss: 0.262864\n",
      "Iteration 373825 | Loss: 0.262864\n",
      "Iteration 373850 | Loss: 0.262864\n",
      "Iteration 373875 | Loss: 0.262864\n",
      "Iteration 373900 | Loss: 0.262864\n",
      "Iteration 373925 | Loss: 0.262864\n",
      "Iteration 373950 | Loss: 0.262864\n",
      "Iteration 373975 | Loss: 0.262864\n",
      "Iteration 374000 | Loss: 0.262864\n",
      "Iteration 374025 | Loss: 0.262864\n",
      "Iteration 374050 | Loss: 0.262864\n",
      "Iteration 374075 | Loss: 0.262864\n",
      "Iteration 374100 | Loss: 0.262864\n",
      "Iteration 374125 | Loss: 0.262864\n",
      "Iteration 374150 | Loss: 0.262864\n",
      "Iteration 374175 | Loss: 0.262864\n",
      "Iteration 374200 | Loss: 0.262864\n",
      "Iteration 374225 | Loss: 0.262864\n",
      "Iteration 374250 | Loss: 0.262864\n",
      "Iteration 374275 | Loss: 0.262864\n",
      "Iteration 374300 | Loss: 0.262864\n",
      "Iteration 374325 | Loss: 0.262864\n",
      "Iteration 374350 | Loss: 0.262864\n",
      "Iteration 374375 | Loss: 0.262864\n",
      "Iteration 374400 | Loss: 0.262864\n",
      "Iteration 374425 | Loss: 0.262864\n",
      "Iteration 374450 | Loss: 0.262864\n",
      "Iteration 374475 | Loss: 0.262864\n",
      "Iteration 374500 | Loss: 0.262864\n",
      "Iteration 374525 | Loss: 0.262864\n",
      "Iteration 374550 | Loss: 0.262864\n",
      "Iteration 374575 | Loss: 0.262864\n",
      "Iteration 374600 | Loss: 0.262864\n",
      "Iteration 374625 | Loss: 0.262864\n",
      "Iteration 374650 | Loss: 0.262864\n",
      "Iteration 374675 | Loss: 0.262864\n",
      "Iteration 374700 | Loss: 0.262864\n",
      "Iteration 374725 | Loss: 0.262864\n",
      "Iteration 374750 | Loss: 0.262864\n",
      "Iteration 374775 | Loss: 0.262864\n",
      "Iteration 374800 | Loss: 0.262864\n",
      "Iteration 374825 | Loss: 0.262864\n",
      "Iteration 374850 | Loss: 0.262864\n",
      "Iteration 374875 | Loss: 0.262864\n",
      "Iteration 374900 | Loss: 0.262864\n",
      "Iteration 374925 | Loss: 0.262864\n",
      "Iteration 374950 | Loss: 0.262864\n",
      "Iteration 374975 | Loss: 0.262864\n",
      "Iteration 375000 | Loss: 0.262864\n",
      "Iteration 375025 | Loss: 0.262864\n",
      "Iteration 375050 | Loss: 0.262864\n",
      "Iteration 375075 | Loss: 0.262864\n",
      "Iteration 375100 | Loss: 0.262864\n",
      "Iteration 375125 | Loss: 0.262864\n",
      "Iteration 375150 | Loss: 0.262864\n",
      "Iteration 375175 | Loss: 0.262864\n",
      "Iteration 375200 | Loss: 0.262864\n",
      "Iteration 375225 | Loss: 0.262863\n",
      "Iteration 375250 | Loss: 0.262863\n",
      "Iteration 375275 | Loss: 0.262863\n",
      "Iteration 375300 | Loss: 0.262863\n",
      "Iteration 375325 | Loss: 0.262863\n",
      "Iteration 375350 | Loss: 0.262863\n",
      "Iteration 375375 | Loss: 0.262863\n",
      "Iteration 375400 | Loss: 0.262863\n",
      "Iteration 375425 | Loss: 0.262863\n",
      "Iteration 375450 | Loss: 0.262863\n",
      "Iteration 375475 | Loss: 0.262863\n",
      "Iteration 375500 | Loss: 0.262863\n",
      "Iteration 375525 | Loss: 0.262863\n",
      "Iteration 375550 | Loss: 0.262863\n",
      "Iteration 375575 | Loss: 0.262863\n",
      "Iteration 375600 | Loss: 0.262863\n",
      "Iteration 375625 | Loss: 0.262863\n",
      "Iteration 375650 | Loss: 0.262863\n",
      "Iteration 375675 | Loss: 0.262863\n",
      "Iteration 375700 | Loss: 0.262863\n",
      "Iteration 375725 | Loss: 0.262863\n",
      "Iteration 375750 | Loss: 0.262863\n",
      "Iteration 375775 | Loss: 0.262863\n",
      "Iteration 375800 | Loss: 0.262863\n",
      "Iteration 375825 | Loss: 0.262863\n",
      "Iteration 375850 | Loss: 0.262863\n",
      "Iteration 375875 | Loss: 0.262863\n",
      "Iteration 375900 | Loss: 0.262863\n",
      "Iteration 375925 | Loss: 0.262863\n",
      "Iteration 375950 | Loss: 0.262863\n",
      "Iteration 375975 | Loss: 0.262863\n",
      "Iteration 376000 | Loss: 0.262863\n",
      "Iteration 376025 | Loss: 0.262863\n",
      "Iteration 376050 | Loss: 0.262863\n",
      "Iteration 376075 | Loss: 0.262863\n",
      "Iteration 376100 | Loss: 0.262863\n",
      "Iteration 376125 | Loss: 0.262863\n",
      "Iteration 376150 | Loss: 0.262863\n",
      "Iteration 376175 | Loss: 0.262863\n",
      "Iteration 376200 | Loss: 0.262863\n",
      "Iteration 376225 | Loss: 0.262863\n",
      "Iteration 376250 | Loss: 0.262863\n",
      "Iteration 376275 | Loss: 0.262863\n",
      "Iteration 376300 | Loss: 0.262863\n",
      "Iteration 376325 | Loss: 0.262863\n",
      "Iteration 376350 | Loss: 0.262863\n",
      "Iteration 376375 | Loss: 0.262863\n",
      "Iteration 376400 | Loss: 0.262863\n",
      "Iteration 376425 | Loss: 0.262863\n",
      "Iteration 376450 | Loss: 0.262863\n",
      "Iteration 376475 | Loss: 0.262863\n",
      "Iteration 376500 | Loss: 0.262863\n",
      "Iteration 376525 | Loss: 0.262863\n",
      "Iteration 376550 | Loss: 0.262863\n",
      "Iteration 376575 | Loss: 0.262863\n",
      "Iteration 376600 | Loss: 0.262863\n",
      "Iteration 376625 | Loss: 0.262863\n",
      "Iteration 376650 | Loss: 0.262863\n",
      "Iteration 376675 | Loss: 0.262863\n",
      "Iteration 376700 | Loss: 0.262863\n",
      "Iteration 376725 | Loss: 0.262863\n",
      "Iteration 376750 | Loss: 0.262863\n",
      "Iteration 376775 | Loss: 0.262863\n",
      "Iteration 376800 | Loss: 0.262863\n",
      "Iteration 376825 | Loss: 0.262863\n",
      "Iteration 376850 | Loss: 0.262863\n",
      "Iteration 376875 | Loss: 0.262863\n",
      "Iteration 376900 | Loss: 0.262863\n",
      "Iteration 376925 | Loss: 0.262863\n",
      "Iteration 376950 | Loss: 0.262863\n",
      "Iteration 376975 | Loss: 0.262863\n",
      "Iteration 377000 | Loss: 0.262863\n",
      "Iteration 377025 | Loss: 0.262863\n",
      "Iteration 377050 | Loss: 0.262863\n",
      "Iteration 377075 | Loss: 0.262863\n",
      "Iteration 377100 | Loss: 0.262863\n",
      "Iteration 377125 | Loss: 0.262863\n",
      "Iteration 377150 | Loss: 0.262863\n",
      "Iteration 377175 | Loss: 0.262863\n",
      "Iteration 377200 | Loss: 0.262863\n",
      "Iteration 377225 | Loss: 0.262863\n",
      "Iteration 377250 | Loss: 0.262863\n",
      "Iteration 377275 | Loss: 0.262863\n",
      "Iteration 377300 | Loss: 0.262863\n",
      "Iteration 377325 | Loss: 0.262863\n",
      "Iteration 377350 | Loss: 0.262863\n",
      "Iteration 377375 | Loss: 0.262863\n",
      "Iteration 377400 | Loss: 0.262863\n",
      "Iteration 377425 | Loss: 0.262863\n",
      "Iteration 377450 | Loss: 0.262863\n",
      "Iteration 377475 | Loss: 0.262863\n",
      "Iteration 377500 | Loss: 0.262863\n",
      "Iteration 377525 | Loss: 0.262863\n",
      "Iteration 377550 | Loss: 0.262863\n",
      "Iteration 377575 | Loss: 0.262863\n",
      "Iteration 377600 | Loss: 0.262863\n",
      "Iteration 377625 | Loss: 0.262863\n",
      "Iteration 377650 | Loss: 0.262863\n",
      "Iteration 377675 | Loss: 0.262863\n",
      "Iteration 377700 | Loss: 0.262863\n",
      "Iteration 377725 | Loss: 0.262863\n",
      "Iteration 377750 | Loss: 0.262863\n",
      "Iteration 377775 | Loss: 0.262863\n",
      "Iteration 377800 | Loss: 0.262863\n",
      "Iteration 377825 | Loss: 0.262863\n",
      "Iteration 377850 | Loss: 0.262863\n",
      "Iteration 377875 | Loss: 0.262863\n",
      "Iteration 377900 | Loss: 0.262863\n",
      "Iteration 377925 | Loss: 0.262863\n",
      "Iteration 377950 | Loss: 0.262863\n",
      "Iteration 377975 | Loss: 0.262863\n",
      "Iteration 378000 | Loss: 0.262863\n",
      "Iteration 378025 | Loss: 0.262863\n",
      "Iteration 378050 | Loss: 0.262863\n",
      "Iteration 378075 | Loss: 0.262863\n",
      "Iteration 378100 | Loss: 0.262863\n",
      "Iteration 378125 | Loss: 0.262863\n",
      "Iteration 378150 | Loss: 0.262863\n",
      "Iteration 378175 | Loss: 0.262863\n",
      "Iteration 378200 | Loss: 0.262863\n",
      "Iteration 378225 | Loss: 0.262863\n",
      "Iteration 378250 | Loss: 0.262863\n",
      "Iteration 378275 | Loss: 0.262863\n",
      "Iteration 378300 | Loss: 0.262863\n",
      "Iteration 378325 | Loss: 0.262863\n",
      "Iteration 378350 | Loss: 0.262863\n",
      "Iteration 378375 | Loss: 0.262863\n",
      "Iteration 378400 | Loss: 0.262863\n",
      "Iteration 378425 | Loss: 0.262863\n",
      "Iteration 378450 | Loss: 0.262863\n",
      "Iteration 378475 | Loss: 0.262862\n",
      "Iteration 378500 | Loss: 0.262862\n",
      "Iteration 378525 | Loss: 0.262862\n",
      "Iteration 378550 | Loss: 0.262862\n",
      "Iteration 378575 | Loss: 0.262862\n",
      "Iteration 378600 | Loss: 0.262862\n",
      "Iteration 378625 | Loss: 0.262862\n",
      "Iteration 378650 | Loss: 0.262862\n",
      "Iteration 378675 | Loss: 0.262862\n",
      "Iteration 378700 | Loss: 0.262862\n",
      "Iteration 378725 | Loss: 0.262862\n",
      "Iteration 378750 | Loss: 0.262862\n",
      "Iteration 378775 | Loss: 0.262862\n",
      "Iteration 378800 | Loss: 0.262862\n",
      "Iteration 378825 | Loss: 0.262862\n",
      "Iteration 378850 | Loss: 0.262862\n",
      "Iteration 378875 | Loss: 0.262862\n",
      "Iteration 378900 | Loss: 0.262862\n",
      "Iteration 378925 | Loss: 0.262862\n",
      "Iteration 378950 | Loss: 0.262862\n",
      "Iteration 378975 | Loss: 0.262862\n",
      "Iteration 379000 | Loss: 0.262862\n",
      "Iteration 379025 | Loss: 0.262862\n",
      "Iteration 379050 | Loss: 0.262862\n",
      "Iteration 379075 | Loss: 0.262862\n",
      "Iteration 379100 | Loss: 0.262862\n",
      "Iteration 379125 | Loss: 0.262862\n",
      "Iteration 379150 | Loss: 0.262862\n",
      "Iteration 379175 | Loss: 0.262862\n",
      "Iteration 379200 | Loss: 0.262862\n",
      "Iteration 379225 | Loss: 0.262862\n",
      "Iteration 379250 | Loss: 0.262862\n",
      "Iteration 379275 | Loss: 0.262862\n",
      "Iteration 379300 | Loss: 0.262862\n",
      "Iteration 379325 | Loss: 0.262862\n",
      "Iteration 379350 | Loss: 0.262862\n",
      "Iteration 379375 | Loss: 0.262862\n",
      "Iteration 379400 | Loss: 0.262862\n",
      "Iteration 379425 | Loss: 0.262862\n",
      "Iteration 379450 | Loss: 0.262862\n",
      "Iteration 379475 | Loss: 0.262862\n",
      "Iteration 379500 | Loss: 0.262862\n",
      "Iteration 379525 | Loss: 0.262862\n",
      "Iteration 379550 | Loss: 0.262862\n",
      "Iteration 379575 | Loss: 0.262862\n",
      "Iteration 379600 | Loss: 0.262862\n",
      "Iteration 379625 | Loss: 0.262862\n",
      "Iteration 379650 | Loss: 0.262862\n",
      "Iteration 379675 | Loss: 0.262862\n",
      "Iteration 379700 | Loss: 0.262862\n",
      "Iteration 379725 | Loss: 0.262862\n",
      "Iteration 379750 | Loss: 0.262862\n",
      "Iteration 379775 | Loss: 0.262862\n",
      "Iteration 379800 | Loss: 0.262862\n",
      "Iteration 379825 | Loss: 0.262862\n",
      "Iteration 379850 | Loss: 0.262862\n",
      "Iteration 379875 | Loss: 0.262862\n",
      "Iteration 379900 | Loss: 0.262862\n",
      "Iteration 379925 | Loss: 0.262862\n",
      "Iteration 379950 | Loss: 0.262862\n",
      "Iteration 379975 | Loss: 0.262862\n",
      "Iteration 380000 | Loss: 0.262862\n",
      "Iteration 380025 | Loss: 0.262862\n",
      "Iteration 380050 | Loss: 0.262862\n",
      "Iteration 380075 | Loss: 0.262862\n",
      "Iteration 380100 | Loss: 0.262862\n",
      "Iteration 380125 | Loss: 0.262862\n",
      "Iteration 380150 | Loss: 0.262862\n",
      "Iteration 380175 | Loss: 0.262862\n",
      "Iteration 380200 | Loss: 0.262862\n",
      "Iteration 380225 | Loss: 0.262862\n",
      "Iteration 380250 | Loss: 0.262862\n",
      "Iteration 380275 | Loss: 0.262862\n",
      "Iteration 380300 | Loss: 0.262862\n",
      "Iteration 380325 | Loss: 0.262862\n",
      "Iteration 380350 | Loss: 0.262862\n",
      "Iteration 380375 | Loss: 0.262862\n",
      "Iteration 380400 | Loss: 0.262862\n",
      "Iteration 380425 | Loss: 0.262862\n",
      "Iteration 380450 | Loss: 0.262862\n",
      "Iteration 380475 | Loss: 0.262862\n",
      "Iteration 380500 | Loss: 0.262862\n",
      "Iteration 380525 | Loss: 0.262862\n",
      "Iteration 380550 | Loss: 0.262862\n",
      "Iteration 380575 | Loss: 0.262862\n",
      "Iteration 380600 | Loss: 0.262862\n",
      "Iteration 380625 | Loss: 0.262862\n",
      "Iteration 380650 | Loss: 0.262862\n",
      "Iteration 380675 | Loss: 0.262862\n",
      "Iteration 380700 | Loss: 0.262862\n",
      "Iteration 380725 | Loss: 0.262862\n",
      "Iteration 380750 | Loss: 0.262862\n",
      "Iteration 380775 | Loss: 0.262862\n",
      "Iteration 380800 | Loss: 0.262862\n",
      "Iteration 380825 | Loss: 0.262862\n",
      "Iteration 380850 | Loss: 0.262862\n",
      "Iteration 380875 | Loss: 0.262862\n",
      "Iteration 380900 | Loss: 0.262862\n",
      "Iteration 380925 | Loss: 0.262862\n",
      "Iteration 380950 | Loss: 0.262862\n",
      "Iteration 380975 | Loss: 0.262862\n",
      "Iteration 381000 | Loss: 0.262862\n",
      "Iteration 381025 | Loss: 0.262862\n",
      "Iteration 381050 | Loss: 0.262862\n",
      "Iteration 381075 | Loss: 0.262862\n",
      "Iteration 381100 | Loss: 0.262862\n",
      "Iteration 381125 | Loss: 0.262862\n",
      "Iteration 381150 | Loss: 0.262862\n",
      "Iteration 381175 | Loss: 0.262862\n",
      "Iteration 381200 | Loss: 0.262862\n",
      "Iteration 381225 | Loss: 0.262862\n",
      "Iteration 381250 | Loss: 0.262862\n",
      "Iteration 381275 | Loss: 0.262862\n",
      "Iteration 381300 | Loss: 0.262862\n",
      "Iteration 381325 | Loss: 0.262862\n",
      "Iteration 381350 | Loss: 0.262862\n",
      "Iteration 381375 | Loss: 0.262862\n",
      "Iteration 381400 | Loss: 0.262862\n",
      "Iteration 381425 | Loss: 0.262862\n",
      "Iteration 381450 | Loss: 0.262862\n",
      "Iteration 381475 | Loss: 0.262862\n",
      "Iteration 381500 | Loss: 0.262862\n",
      "Iteration 381525 | Loss: 0.262862\n",
      "Iteration 381550 | Loss: 0.262862\n",
      "Iteration 381575 | Loss: 0.262862\n",
      "Iteration 381600 | Loss: 0.262862\n",
      "Iteration 381625 | Loss: 0.262862\n",
      "Iteration 381650 | Loss: 0.262862\n",
      "Iteration 381675 | Loss: 0.262862\n",
      "Iteration 381700 | Loss: 0.262862\n",
      "Iteration 381725 | Loss: 0.262862\n",
      "Iteration 381750 | Loss: 0.262862\n",
      "Iteration 381775 | Loss: 0.262862\n",
      "Iteration 381800 | Loss: 0.262862\n",
      "Iteration 381825 | Loss: 0.262862\n",
      "Iteration 381850 | Loss: 0.262862\n",
      "Iteration 381875 | Loss: 0.262862\n",
      "Iteration 381900 | Loss: 0.262862\n",
      "Iteration 381925 | Loss: 0.262862\n",
      "Iteration 381950 | Loss: 0.262862\n",
      "Iteration 381975 | Loss: 0.262862\n",
      "Iteration 382000 | Loss: 0.262862\n",
      "Iteration 382025 | Loss: 0.262862\n",
      "Iteration 382050 | Loss: 0.262862\n",
      "Iteration 382075 | Loss: 0.262862\n",
      "Iteration 382100 | Loss: 0.262862\n",
      "Iteration 382125 | Loss: 0.262862\n",
      "Iteration 382150 | Loss: 0.262862\n",
      "Iteration 382175 | Loss: 0.262862\n",
      "Iteration 382200 | Loss: 0.262862\n",
      "Iteration 382225 | Loss: 0.262861\n",
      "Iteration 382250 | Loss: 0.262861\n",
      "Iteration 382275 | Loss: 0.262861\n",
      "Iteration 382300 | Loss: 0.262861\n",
      "Iteration 382325 | Loss: 0.262861\n",
      "Iteration 382350 | Loss: 0.262861\n",
      "Iteration 382375 | Loss: 0.262861\n",
      "Iteration 382400 | Loss: 0.262861\n",
      "Iteration 382425 | Loss: 0.262861\n",
      "Iteration 382450 | Loss: 0.262861\n",
      "Iteration 382475 | Loss: 0.262861\n",
      "Iteration 382500 | Loss: 0.262861\n",
      "Iteration 382525 | Loss: 0.262861\n",
      "Iteration 382550 | Loss: 0.262861\n",
      "Iteration 382575 | Loss: 0.262861\n",
      "Iteration 382600 | Loss: 0.262861\n",
      "Iteration 382625 | Loss: 0.262861\n",
      "Iteration 382650 | Loss: 0.262861\n",
      "Iteration 382675 | Loss: 0.262861\n",
      "Iteration 382700 | Loss: 0.262861\n",
      "Iteration 382725 | Loss: 0.262861\n",
      "Iteration 382750 | Loss: 0.262861\n",
      "Iteration 382775 | Loss: 0.262861\n",
      "Iteration 382800 | Loss: 0.262861\n",
      "Iteration 382825 | Loss: 0.262861\n",
      "Iteration 382850 | Loss: 0.262861\n",
      "Iteration 382875 | Loss: 0.262861\n",
      "Iteration 382900 | Loss: 0.262861\n",
      "Iteration 382925 | Loss: 0.262861\n",
      "Iteration 382950 | Loss: 0.262861\n",
      "Iteration 382975 | Loss: 0.262861\n",
      "Iteration 383000 | Loss: 0.262861\n",
      "Iteration 383025 | Loss: 0.262861\n",
      "Iteration 383050 | Loss: 0.262861\n",
      "Iteration 383075 | Loss: 0.262861\n",
      "Iteration 383100 | Loss: 0.262861\n",
      "Iteration 383125 | Loss: 0.262861\n",
      "Iteration 383150 | Loss: 0.262861\n",
      "Iteration 383175 | Loss: 0.262861\n",
      "Iteration 383200 | Loss: 0.262861\n",
      "Iteration 383225 | Loss: 0.262861\n",
      "Iteration 383250 | Loss: 0.262861\n",
      "Iteration 383275 | Loss: 0.262861\n",
      "Iteration 383300 | Loss: 0.262861\n",
      "Iteration 383325 | Loss: 0.262861\n",
      "Iteration 383350 | Loss: 0.262861\n",
      "Iteration 383375 | Loss: 0.262861\n",
      "Iteration 383400 | Loss: 0.262861\n",
      "Iteration 383425 | Loss: 0.262861\n",
      "Iteration 383450 | Loss: 0.262861\n",
      "Iteration 383475 | Loss: 0.262861\n",
      "Iteration 383500 | Loss: 0.262861\n",
      "Iteration 383525 | Loss: 0.262861\n",
      "Iteration 383550 | Loss: 0.262861\n",
      "Iteration 383575 | Loss: 0.262861\n",
      "Iteration 383600 | Loss: 0.262861\n",
      "Iteration 383625 | Loss: 0.262861\n",
      "Iteration 383650 | Loss: 0.262861\n",
      "Iteration 383675 | Loss: 0.262861\n",
      "Iteration 383700 | Loss: 0.262861\n",
      "Iteration 383725 | Loss: 0.262861\n",
      "Iteration 383750 | Loss: 0.262861\n",
      "Iteration 383775 | Loss: 0.262861\n",
      "Iteration 383800 | Loss: 0.262861\n",
      "Iteration 383825 | Loss: 0.262861\n",
      "Iteration 383850 | Loss: 0.262861\n",
      "Iteration 383875 | Loss: 0.262861\n",
      "Iteration 383900 | Loss: 0.262861\n",
      "Iteration 383925 | Loss: 0.262861\n",
      "Iteration 383950 | Loss: 0.262861\n",
      "Iteration 383975 | Loss: 0.262861\n",
      "Iteration 384000 | Loss: 0.262861\n",
      "Iteration 384025 | Loss: 0.262861\n",
      "Iteration 384050 | Loss: 0.262861\n",
      "Iteration 384075 | Loss: 0.262861\n",
      "Iteration 384100 | Loss: 0.262861\n",
      "Iteration 384125 | Loss: 0.262861\n",
      "Iteration 384150 | Loss: 0.262861\n",
      "Iteration 384175 | Loss: 0.262861\n",
      "Iteration 384200 | Loss: 0.262861\n",
      "Iteration 384225 | Loss: 0.262861\n",
      "Iteration 384250 | Loss: 0.262861\n",
      "Iteration 384275 | Loss: 0.262861\n",
      "Iteration 384300 | Loss: 0.262861\n",
      "Iteration 384325 | Loss: 0.262861\n",
      "Iteration 384350 | Loss: 0.262861\n",
      "Iteration 384375 | Loss: 0.262861\n",
      "Iteration 384400 | Loss: 0.262861\n",
      "Iteration 384425 | Loss: 0.262861\n",
      "Iteration 384450 | Loss: 0.262861\n",
      "Iteration 384475 | Loss: 0.262861\n",
      "Iteration 384500 | Loss: 0.262861\n",
      "Iteration 384525 | Loss: 0.262861\n",
      "Iteration 384550 | Loss: 0.262861\n",
      "Iteration 384575 | Loss: 0.262861\n",
      "Iteration 384600 | Loss: 0.262861\n",
      "Iteration 384625 | Loss: 0.262861\n",
      "Iteration 384650 | Loss: 0.262861\n",
      "Iteration 384675 | Loss: 0.262861\n",
      "Iteration 384700 | Loss: 0.262861\n",
      "Iteration 384725 | Loss: 0.262861\n",
      "Iteration 384750 | Loss: 0.262861\n",
      "Iteration 384775 | Loss: 0.262861\n",
      "Iteration 384800 | Loss: 0.262861\n",
      "Iteration 384825 | Loss: 0.262861\n",
      "Iteration 384850 | Loss: 0.262861\n",
      "Iteration 384875 | Loss: 0.262861\n",
      "Iteration 384900 | Loss: 0.262861\n",
      "Iteration 384925 | Loss: 0.262861\n",
      "Iteration 384950 | Loss: 0.262861\n",
      "Iteration 384975 | Loss: 0.262861\n",
      "Iteration 385000 | Loss: 0.262861\n",
      "Iteration 385025 | Loss: 0.262861\n",
      "Iteration 385050 | Loss: 0.262861\n",
      "Iteration 385075 | Loss: 0.262861\n",
      "Iteration 385100 | Loss: 0.262861\n",
      "Iteration 385125 | Loss: 0.262861\n",
      "Iteration 385150 | Loss: 0.262861\n",
      "Iteration 385175 | Loss: 0.262861\n",
      "Iteration 385200 | Loss: 0.262861\n",
      "Iteration 385225 | Loss: 0.262861\n",
      "Iteration 385250 | Loss: 0.262861\n",
      "Iteration 385275 | Loss: 0.262861\n",
      "Iteration 385300 | Loss: 0.262861\n",
      "Iteration 385325 | Loss: 0.262861\n",
      "Iteration 385350 | Loss: 0.262861\n",
      "Iteration 385375 | Loss: 0.262861\n",
      "Iteration 385400 | Loss: 0.262861\n",
      "Iteration 385425 | Loss: 0.262861\n",
      "Iteration 385450 | Loss: 0.262861\n",
      "Iteration 385475 | Loss: 0.262861\n",
      "Iteration 385500 | Loss: 0.262861\n",
      "Iteration 385525 | Loss: 0.262861\n",
      "Iteration 385550 | Loss: 0.262861\n",
      "Iteration 385575 | Loss: 0.262861\n",
      "Iteration 385600 | Loss: 0.262861\n",
      "Iteration 385625 | Loss: 0.262861\n",
      "Iteration 385650 | Loss: 0.262861\n",
      "Iteration 385675 | Loss: 0.262861\n",
      "Iteration 385700 | Loss: 0.262861\n",
      "Iteration 385725 | Loss: 0.262861\n",
      "Iteration 385750 | Loss: 0.262861\n",
      "Iteration 385775 | Loss: 0.262861\n",
      "Iteration 385800 | Loss: 0.262861\n",
      "Iteration 385825 | Loss: 0.262861\n",
      "Iteration 385850 | Loss: 0.262861\n",
      "Iteration 385875 | Loss: 0.262861\n",
      "Iteration 385900 | Loss: 0.262861\n",
      "Iteration 385925 | Loss: 0.262861\n",
      "Iteration 385950 | Loss: 0.262861\n",
      "Iteration 385975 | Loss: 0.262861\n",
      "Iteration 386000 | Loss: 0.262861\n",
      "Iteration 386025 | Loss: 0.262861\n",
      "Iteration 386050 | Loss: 0.262861\n",
      "Iteration 386075 | Loss: 0.262861\n",
      "Iteration 386100 | Loss: 0.262861\n",
      "Iteration 386125 | Loss: 0.262861\n",
      "Iteration 386150 | Loss: 0.262861\n",
      "Iteration 386175 | Loss: 0.262861\n",
      "Iteration 386200 | Loss: 0.262861\n",
      "Iteration 386225 | Loss: 0.262861\n",
      "Iteration 386250 | Loss: 0.262861\n",
      "Iteration 386275 | Loss: 0.262861\n",
      "Iteration 386300 | Loss: 0.262861\n",
      "Iteration 386325 | Loss: 0.262861\n",
      "Iteration 386350 | Loss: 0.262861\n",
      "Iteration 386375 | Loss: 0.262861\n",
      "Iteration 386400 | Loss: 0.262861\n",
      "Iteration 386425 | Loss: 0.262861\n",
      "Iteration 386450 | Loss: 0.262861\n",
      "Iteration 386475 | Loss: 0.262861\n",
      "Iteration 386500 | Loss: 0.262861\n",
      "Iteration 386525 | Loss: 0.262861\n",
      "Iteration 386550 | Loss: 0.262861\n",
      "Iteration 386575 | Loss: 0.262861\n",
      "Iteration 386600 | Loss: 0.262861\n",
      "Iteration 386625 | Loss: 0.262861\n",
      "Iteration 386650 | Loss: 0.262860\n",
      "Iteration 386675 | Loss: 0.262860\n",
      "Iteration 386700 | Loss: 0.262860\n",
      "Iteration 386725 | Loss: 0.262860\n",
      "Iteration 386750 | Loss: 0.262860\n",
      "Iteration 386775 | Loss: 0.262860\n",
      "Iteration 386800 | Loss: 0.262860\n",
      "Iteration 386825 | Loss: 0.262860\n",
      "Iteration 386850 | Loss: 0.262860\n",
      "Iteration 386875 | Loss: 0.262860\n",
      "Iteration 386900 | Loss: 0.262860\n",
      "Iteration 386925 | Loss: 0.262860\n",
      "Iteration 386950 | Loss: 0.262860\n",
      "Iteration 386975 | Loss: 0.262860\n",
      "Iteration 387000 | Loss: 0.262860\n",
      "Iteration 387025 | Loss: 0.262860\n",
      "Iteration 387050 | Loss: 0.262860\n",
      "Iteration 387075 | Loss: 0.262860\n",
      "Iteration 387100 | Loss: 0.262860\n",
      "Iteration 387125 | Loss: 0.262860\n",
      "Iteration 387150 | Loss: 0.262860\n",
      "Iteration 387175 | Loss: 0.262860\n",
      "Iteration 387200 | Loss: 0.262860\n",
      "Iteration 387225 | Loss: 0.262860\n",
      "Iteration 387250 | Loss: 0.262860\n",
      "Iteration 387275 | Loss: 0.262860\n",
      "Iteration 387300 | Loss: 0.262860\n",
      "Iteration 387325 | Loss: 0.262860\n",
      "Iteration 387350 | Loss: 0.262860\n",
      "Iteration 387375 | Loss: 0.262860\n",
      "Iteration 387400 | Loss: 0.262860\n",
      "Iteration 387425 | Loss: 0.262860\n",
      "Iteration 387450 | Loss: 0.262860\n",
      "Iteration 387475 | Loss: 0.262860\n",
      "Iteration 387500 | Loss: 0.262860\n",
      "Iteration 387525 | Loss: 0.262860\n",
      "Iteration 387550 | Loss: 0.262860\n",
      "Iteration 387575 | Loss: 0.262860\n",
      "Iteration 387600 | Loss: 0.262860\n",
      "Iteration 387625 | Loss: 0.262860\n",
      "Iteration 387650 | Loss: 0.262860\n",
      "Iteration 387675 | Loss: 0.262860\n",
      "Iteration 387700 | Loss: 0.262860\n",
      "Iteration 387725 | Loss: 0.262860\n",
      "Iteration 387750 | Loss: 0.262860\n",
      "Iteration 387775 | Loss: 0.262860\n",
      "Iteration 387800 | Loss: 0.262860\n",
      "Iteration 387825 | Loss: 0.262860\n",
      "Iteration 387850 | Loss: 0.262860\n",
      "Iteration 387875 | Loss: 0.262860\n",
      "Iteration 387900 | Loss: 0.262860\n",
      "Iteration 387925 | Loss: 0.262860\n",
      "Iteration 387950 | Loss: 0.262860\n",
      "Iteration 387975 | Loss: 0.262860\n",
      "Iteration 388000 | Loss: 0.262860\n",
      "Iteration 388025 | Loss: 0.262860\n",
      "Iteration 388050 | Loss: 0.262860\n",
      "Iteration 388075 | Loss: 0.262860\n",
      "Iteration 388100 | Loss: 0.262860\n",
      "Iteration 388125 | Loss: 0.262860\n",
      "Iteration 388150 | Loss: 0.262860\n",
      "Iteration 388175 | Loss: 0.262860\n",
      "Iteration 388200 | Loss: 0.262860\n",
      "Iteration 388225 | Loss: 0.262860\n",
      "Iteration 388250 | Loss: 0.262860\n",
      "Iteration 388275 | Loss: 0.262860\n",
      "Iteration 388300 | Loss: 0.262860\n",
      "Iteration 388325 | Loss: 0.262860\n",
      "Iteration 388350 | Loss: 0.262860\n",
      "Iteration 388375 | Loss: 0.262860\n",
      "Iteration 388400 | Loss: 0.262860\n",
      "Iteration 388425 | Loss: 0.262860\n",
      "Iteration 388450 | Loss: 0.262860\n",
      "Iteration 388475 | Loss: 0.262860\n",
      "Iteration 388500 | Loss: 0.262860\n",
      "Iteration 388525 | Loss: 0.262860\n",
      "Iteration 388550 | Loss: 0.262860\n",
      "Iteration 388575 | Loss: 0.262860\n",
      "Iteration 388600 | Loss: 0.262860\n",
      "Iteration 388625 | Loss: 0.262860\n",
      "Iteration 388650 | Loss: 0.262860\n",
      "Iteration 388675 | Loss: 0.262860\n",
      "Iteration 388700 | Loss: 0.262860\n",
      "Iteration 388725 | Loss: 0.262860\n",
      "Iteration 388750 | Loss: 0.262860\n",
      "Iteration 388775 | Loss: 0.262860\n",
      "Iteration 388800 | Loss: 0.262860\n",
      "Iteration 388825 | Loss: 0.262860\n",
      "Iteration 388850 | Loss: 0.262860\n",
      "Iteration 388875 | Loss: 0.262860\n",
      "Iteration 388900 | Loss: 0.262860\n",
      "Iteration 388925 | Loss: 0.262860\n",
      "Iteration 388950 | Loss: 0.262860\n",
      "Iteration 388975 | Loss: 0.262860\n",
      "Iteration 389000 | Loss: 0.262860\n",
      "Iteration 389025 | Loss: 0.262860\n",
      "Iteration 389050 | Loss: 0.262860\n",
      "Iteration 389075 | Loss: 0.262860\n",
      "Iteration 389100 | Loss: 0.262860\n",
      "Iteration 389125 | Loss: 0.262860\n",
      "Iteration 389150 | Loss: 0.262860\n",
      "Iteration 389175 | Loss: 0.262860\n",
      "Iteration 389200 | Loss: 0.262860\n",
      "Iteration 389225 | Loss: 0.262860\n",
      "Iteration 389250 | Loss: 0.262860\n",
      "Iteration 389275 | Loss: 0.262860\n",
      "Iteration 389300 | Loss: 0.262860\n",
      "Iteration 389325 | Loss: 0.262860\n",
      "Iteration 389350 | Loss: 0.262860\n",
      "Iteration 389375 | Loss: 0.262860\n",
      "Iteration 389400 | Loss: 0.262860\n",
      "Iteration 389425 | Loss: 0.262860\n",
      "Iteration 389450 | Loss: 0.262860\n",
      "Iteration 389475 | Loss: 0.262860\n",
      "Iteration 389500 | Loss: 0.262860\n",
      "Iteration 389525 | Loss: 0.262860\n",
      "Iteration 389550 | Loss: 0.262860\n",
      "Iteration 389575 | Loss: 0.262860\n",
      "Iteration 389600 | Loss: 0.262860\n",
      "Iteration 389625 | Loss: 0.262860\n",
      "Iteration 389650 | Loss: 0.262860\n",
      "Iteration 389675 | Loss: 0.262860\n",
      "Iteration 389700 | Loss: 0.262860\n",
      "Iteration 389725 | Loss: 0.262860\n",
      "Iteration 389750 | Loss: 0.262860\n",
      "Iteration 389775 | Loss: 0.262860\n",
      "Iteration 389800 | Loss: 0.262860\n",
      "Iteration 389825 | Loss: 0.262860\n",
      "Iteration 389850 | Loss: 0.262860\n",
      "Iteration 389875 | Loss: 0.262860\n",
      "Iteration 389900 | Loss: 0.262860\n",
      "Iteration 389925 | Loss: 0.262860\n",
      "Iteration 389950 | Loss: 0.262860\n",
      "Iteration 389975 | Loss: 0.262860\n",
      "Iteration 390000 | Loss: 0.262860\n",
      "Iteration 390025 | Loss: 0.262860\n",
      "Iteration 390050 | Loss: 0.262860\n",
      "Iteration 390075 | Loss: 0.262860\n",
      "Iteration 390100 | Loss: 0.262860\n",
      "Iteration 390125 | Loss: 0.262860\n",
      "Iteration 390150 | Loss: 0.262860\n",
      "Iteration 390175 | Loss: 0.262860\n",
      "Iteration 390200 | Loss: 0.262860\n",
      "Iteration 390225 | Loss: 0.262860\n",
      "Iteration 390250 | Loss: 0.262860\n",
      "Iteration 390275 | Loss: 0.262860\n",
      "Iteration 390300 | Loss: 0.262860\n",
      "Iteration 390325 | Loss: 0.262860\n",
      "Iteration 390350 | Loss: 0.262860\n",
      "Iteration 390375 | Loss: 0.262860\n",
      "Iteration 390400 | Loss: 0.262860\n",
      "Iteration 390425 | Loss: 0.262860\n",
      "Iteration 390450 | Loss: 0.262860\n",
      "Iteration 390475 | Loss: 0.262860\n",
      "Iteration 390500 | Loss: 0.262860\n",
      "Iteration 390525 | Loss: 0.262860\n",
      "Iteration 390550 | Loss: 0.262860\n",
      "Iteration 390575 | Loss: 0.262860\n",
      "Iteration 390600 | Loss: 0.262860\n",
      "Iteration 390625 | Loss: 0.262860\n",
      "Iteration 390650 | Loss: 0.262860\n",
      "Iteration 390675 | Loss: 0.262860\n",
      "Iteration 390700 | Loss: 0.262860\n",
      "Iteration 390725 | Loss: 0.262860\n",
      "Iteration 390750 | Loss: 0.262860\n",
      "Iteration 390775 | Loss: 0.262860\n",
      "Iteration 390800 | Loss: 0.262860\n",
      "Iteration 390825 | Loss: 0.262860\n",
      "Iteration 390850 | Loss: 0.262860\n",
      "Iteration 390875 | Loss: 0.262860\n",
      "Iteration 390900 | Loss: 0.262860\n",
      "Iteration 390925 | Loss: 0.262860\n",
      "Iteration 390950 | Loss: 0.262860\n",
      "Iteration 390975 | Loss: 0.262860\n",
      "Iteration 391000 | Loss: 0.262860\n",
      "Iteration 391025 | Loss: 0.262860\n",
      "Iteration 391050 | Loss: 0.262860\n",
      "Iteration 391075 | Loss: 0.262860\n",
      "Iteration 391100 | Loss: 0.262860\n",
      "Iteration 391125 | Loss: 0.262860\n",
      "Iteration 391150 | Loss: 0.262860\n",
      "Iteration 391175 | Loss: 0.262860\n",
      "Iteration 391200 | Loss: 0.262860\n",
      "Iteration 391225 | Loss: 0.262860\n",
      "Iteration 391250 | Loss: 0.262860\n",
      "Iteration 391275 | Loss: 0.262860\n",
      "Iteration 391300 | Loss: 0.262860\n",
      "Iteration 391325 | Loss: 0.262860\n",
      "Iteration 391350 | Loss: 0.262860\n",
      "Iteration 391375 | Loss: 0.262860\n",
      "Iteration 391400 | Loss: 0.262860\n",
      "Iteration 391425 | Loss: 0.262860\n",
      "Iteration 391450 | Loss: 0.262860\n",
      "Iteration 391475 | Loss: 0.262860\n",
      "Iteration 391500 | Loss: 0.262860\n",
      "Iteration 391525 | Loss: 0.262860\n",
      "Iteration 391550 | Loss: 0.262860\n",
      "Iteration 391575 | Loss: 0.262860\n",
      "Iteration 391600 | Loss: 0.262860\n",
      "Iteration 391625 | Loss: 0.262860\n",
      "Iteration 391650 | Loss: 0.262860\n",
      "Iteration 391675 | Loss: 0.262860\n",
      "Iteration 391700 | Loss: 0.262860\n",
      "Iteration 391725 | Loss: 0.262860\n",
      "Iteration 391750 | Loss: 0.262860\n",
      "Iteration 391775 | Loss: 0.262860\n",
      "Iteration 391800 | Loss: 0.262860\n",
      "Iteration 391825 | Loss: 0.262860\n",
      "Iteration 391850 | Loss: 0.262860\n",
      "Iteration 391875 | Loss: 0.262860\n",
      "Iteration 391900 | Loss: 0.262860\n",
      "Iteration 391925 | Loss: 0.262860\n",
      "Iteration 391950 | Loss: 0.262860\n",
      "Iteration 391975 | Loss: 0.262860\n",
      "Iteration 392000 | Loss: 0.262860\n",
      "Iteration 392025 | Loss: 0.262860\n",
      "Iteration 392050 | Loss: 0.262859\n",
      "Iteration 392075 | Loss: 0.262859\n",
      "Iteration 392100 | Loss: 0.262859\n",
      "Iteration 392125 | Loss: 0.262859\n",
      "Iteration 392150 | Loss: 0.262859\n",
      "Iteration 392175 | Loss: 0.262859\n",
      "Iteration 392200 | Loss: 0.262859\n",
      "Iteration 392225 | Loss: 0.262859\n",
      "Iteration 392250 | Loss: 0.262859\n",
      "Iteration 392275 | Loss: 0.262859\n",
      "Iteration 392300 | Loss: 0.262859\n",
      "Iteration 392325 | Loss: 0.262859\n",
      "Iteration 392350 | Loss: 0.262859\n",
      "Iteration 392375 | Loss: 0.262859\n",
      "Iteration 392400 | Loss: 0.262859\n",
      "Iteration 392425 | Loss: 0.262859\n",
      "Iteration 392450 | Loss: 0.262859\n",
      "Iteration 392475 | Loss: 0.262859\n",
      "Iteration 392500 | Loss: 0.262859\n",
      "Iteration 392525 | Loss: 0.262859\n",
      "Iteration 392550 | Loss: 0.262859\n",
      "Iteration 392575 | Loss: 0.262859\n",
      "Iteration 392600 | Loss: 0.262859\n",
      "Iteration 392625 | Loss: 0.262859\n",
      "Iteration 392650 | Loss: 0.262859\n",
      "Iteration 392675 | Loss: 0.262859\n",
      "Iteration 392700 | Loss: 0.262859\n",
      "Iteration 392725 | Loss: 0.262859\n",
      "Iteration 392750 | Loss: 0.262859\n",
      "Iteration 392775 | Loss: 0.262859\n",
      "Iteration 392800 | Loss: 0.262859\n",
      "Iteration 392825 | Loss: 0.262859\n",
      "Iteration 392850 | Loss: 0.262859\n",
      "Iteration 392875 | Loss: 0.262859\n",
      "Iteration 392900 | Loss: 0.262859\n",
      "Iteration 392925 | Loss: 0.262859\n",
      "Iteration 392950 | Loss: 0.262859\n",
      "Iteration 392975 | Loss: 0.262859\n",
      "Iteration 393000 | Loss: 0.262859\n",
      "Iteration 393025 | Loss: 0.262859\n",
      "Iteration 393050 | Loss: 0.262859\n",
      "Iteration 393075 | Loss: 0.262859\n",
      "Iteration 393100 | Loss: 0.262859\n",
      "Iteration 393125 | Loss: 0.262859\n",
      "Iteration 393150 | Loss: 0.262859\n",
      "Iteration 393175 | Loss: 0.262859\n",
      "Iteration 393200 | Loss: 0.262859\n",
      "Iteration 393225 | Loss: 0.262859\n",
      "Iteration 393250 | Loss: 0.262859\n",
      "Iteration 393275 | Loss: 0.262859\n",
      "Iteration 393300 | Loss: 0.262859\n",
      "Iteration 393325 | Loss: 0.262859\n",
      "Iteration 393350 | Loss: 0.262859\n",
      "Iteration 393375 | Loss: 0.262859\n",
      "Iteration 393400 | Loss: 0.262859\n",
      "Iteration 393425 | Loss: 0.262859\n",
      "Iteration 393450 | Loss: 0.262859\n",
      "Iteration 393475 | Loss: 0.262859\n",
      "Iteration 393500 | Loss: 0.262859\n",
      "Iteration 393525 | Loss: 0.262859\n",
      "Iteration 393550 | Loss: 0.262859\n",
      "Iteration 393575 | Loss: 0.262859\n",
      "Iteration 393600 | Loss: 0.262859\n",
      "Iteration 393625 | Loss: 0.262859\n",
      "Iteration 393650 | Loss: 0.262859\n",
      "Iteration 393675 | Loss: 0.262859\n",
      "Iteration 393700 | Loss: 0.262859\n",
      "Iteration 393725 | Loss: 0.262859\n",
      "Iteration 393750 | Loss: 0.262859\n",
      "Iteration 393775 | Loss: 0.262859\n",
      "Iteration 393800 | Loss: 0.262859\n",
      "Iteration 393825 | Loss: 0.262859\n",
      "Iteration 393850 | Loss: 0.262859\n",
      "Iteration 393875 | Loss: 0.262859\n",
      "Iteration 393900 | Loss: 0.262859\n",
      "Iteration 393925 | Loss: 0.262859\n",
      "Iteration 393950 | Loss: 0.262859\n",
      "Iteration 393975 | Loss: 0.262859\n",
      "Iteration 394000 | Loss: 0.262859\n",
      "Iteration 394025 | Loss: 0.262859\n",
      "Iteration 394050 | Loss: 0.262859\n",
      "Iteration 394075 | Loss: 0.262859\n",
      "Iteration 394100 | Loss: 0.262859\n",
      "Iteration 394125 | Loss: 0.262859\n",
      "Iteration 394150 | Loss: 0.262859\n",
      "Iteration 394175 | Loss: 0.262859\n",
      "Iteration 394200 | Loss: 0.262859\n",
      "Iteration 394225 | Loss: 0.262859\n",
      "Iteration 394250 | Loss: 0.262859\n",
      "Iteration 394275 | Loss: 0.262859\n",
      "Iteration 394300 | Loss: 0.262859\n",
      "Iteration 394325 | Loss: 0.262859\n",
      "Iteration 394350 | Loss: 0.262859\n",
      "Iteration 394375 | Loss: 0.262859\n",
      "Iteration 394400 | Loss: 0.262859\n",
      "Iteration 394425 | Loss: 0.262859\n",
      "Iteration 394450 | Loss: 0.262859\n",
      "Iteration 394475 | Loss: 0.262859\n",
      "Iteration 394500 | Loss: 0.262859\n",
      "Iteration 394525 | Loss: 0.262859\n",
      "Iteration 394550 | Loss: 0.262859\n",
      "Iteration 394575 | Loss: 0.262859\n",
      "Iteration 394600 | Loss: 0.262859\n",
      "Iteration 394625 | Loss: 0.262859\n",
      "Iteration 394650 | Loss: 0.262859\n",
      "Iteration 394675 | Loss: 0.262859\n",
      "Iteration 394700 | Loss: 0.262859\n",
      "Iteration 394725 | Loss: 0.262859\n",
      "Iteration 394750 | Loss: 0.262859\n",
      "Iteration 394775 | Loss: 0.262859\n",
      "Iteration 394800 | Loss: 0.262859\n",
      "Iteration 394825 | Loss: 0.262859\n",
      "Iteration 394850 | Loss: 0.262859\n",
      "Iteration 394875 | Loss: 0.262859\n",
      "Iteration 394900 | Loss: 0.262859\n",
      "Iteration 394925 | Loss: 0.262859\n",
      "Iteration 394950 | Loss: 0.262859\n",
      "Iteration 394975 | Loss: 0.262859\n",
      "Iteration 395000 | Loss: 0.262859\n",
      "Iteration 395025 | Loss: 0.262859\n",
      "Iteration 395050 | Loss: 0.262859\n",
      "Iteration 395075 | Loss: 0.262859\n",
      "Iteration 395100 | Loss: 0.262859\n",
      "Iteration 395125 | Loss: 0.262859\n",
      "Iteration 395150 | Loss: 0.262859\n",
      "Iteration 395175 | Loss: 0.262859\n",
      "Iteration 395200 | Loss: 0.262859\n",
      "Iteration 395225 | Loss: 0.262859\n",
      "Iteration 395250 | Loss: 0.262859\n",
      "Iteration 395275 | Loss: 0.262859\n",
      "Iteration 395300 | Loss: 0.262859\n",
      "Iteration 395325 | Loss: 0.262859\n",
      "Iteration 395350 | Loss: 0.262859\n",
      "Iteration 395375 | Loss: 0.262859\n",
      "Iteration 395400 | Loss: 0.262859\n",
      "Iteration 395425 | Loss: 0.262859\n",
      "Iteration 395450 | Loss: 0.262859\n",
      "Iteration 395475 | Loss: 0.262859\n",
      "Iteration 395500 | Loss: 0.262859\n",
      "Iteration 395525 | Loss: 0.262859\n",
      "Iteration 395550 | Loss: 0.262859\n",
      "Iteration 395575 | Loss: 0.262859\n",
      "Iteration 395600 | Loss: 0.262859\n",
      "Iteration 395625 | Loss: 0.262859\n",
      "Iteration 395650 | Loss: 0.262859\n",
      "Iteration 395675 | Loss: 0.262859\n",
      "Iteration 395700 | Loss: 0.262859\n",
      "Iteration 395725 | Loss: 0.262859\n",
      "Iteration 395750 | Loss: 0.262859\n",
      "Iteration 395775 | Loss: 0.262859\n",
      "Iteration 395800 | Loss: 0.262859\n",
      "Iteration 395825 | Loss: 0.262859\n",
      "Iteration 395850 | Loss: 0.262859\n",
      "Iteration 395875 | Loss: 0.262859\n",
      "Iteration 395900 | Loss: 0.262859\n",
      "Iteration 395925 | Loss: 0.262859\n",
      "Iteration 395950 | Loss: 0.262859\n",
      "Iteration 395975 | Loss: 0.262859\n",
      "Iteration 396000 | Loss: 0.262859\n",
      "Iteration 396025 | Loss: 0.262859\n",
      "Iteration 396050 | Loss: 0.262859\n",
      "Iteration 396075 | Loss: 0.262859\n",
      "Iteration 396100 | Loss: 0.262859\n",
      "Iteration 396125 | Loss: 0.262859\n",
      "Iteration 396150 | Loss: 0.262859\n",
      "Iteration 396175 | Loss: 0.262859\n",
      "Iteration 396200 | Loss: 0.262859\n",
      "Iteration 396225 | Loss: 0.262859\n",
      "Iteration 396250 | Loss: 0.262859\n",
      "Iteration 396275 | Loss: 0.262859\n",
      "Iteration 396300 | Loss: 0.262859\n",
      "Iteration 396325 | Loss: 0.262859\n",
      "Iteration 396350 | Loss: 0.262859\n",
      "Iteration 396375 | Loss: 0.262859\n",
      "Iteration 396400 | Loss: 0.262859\n",
      "Iteration 396425 | Loss: 0.262859\n",
      "Iteration 396450 | Loss: 0.262859\n",
      "Iteration 396475 | Loss: 0.262859\n",
      "Iteration 396500 | Loss: 0.262859\n",
      "Iteration 396525 | Loss: 0.262859\n",
      "Iteration 396550 | Loss: 0.262859\n",
      "Iteration 396575 | Loss: 0.262859\n",
      "Iteration 396600 | Loss: 0.262859\n",
      "Iteration 396625 | Loss: 0.262859\n",
      "Iteration 396650 | Loss: 0.262859\n",
      "Iteration 396675 | Loss: 0.262859\n",
      "Iteration 396700 | Loss: 0.262859\n",
      "Iteration 396725 | Loss: 0.262859\n",
      "Iteration 396750 | Loss: 0.262859\n",
      "Iteration 396775 | Loss: 0.262859\n",
      "Iteration 396800 | Loss: 0.262859\n",
      "Iteration 396825 | Loss: 0.262859\n",
      "Iteration 396850 | Loss: 0.262859\n",
      "Iteration 396875 | Loss: 0.262859\n",
      "Iteration 396900 | Loss: 0.262859\n",
      "Iteration 396925 | Loss: 0.262859\n",
      "Iteration 396950 | Loss: 0.262859\n",
      "Iteration 396975 | Loss: 0.262859\n",
      "Iteration 397000 | Loss: 0.262859\n",
      "Iteration 397025 | Loss: 0.262859\n",
      "Iteration 397050 | Loss: 0.262859\n",
      "Iteration 397075 | Loss: 0.262859\n",
      "Iteration 397100 | Loss: 0.262859\n",
      "Iteration 397125 | Loss: 0.262859\n",
      "Iteration 397150 | Loss: 0.262859\n",
      "Iteration 397175 | Loss: 0.262859\n",
      "Iteration 397200 | Loss: 0.262859\n",
      "Iteration 397225 | Loss: 0.262859\n",
      "Iteration 397250 | Loss: 0.262859\n",
      "Iteration 397275 | Loss: 0.262859\n",
      "Iteration 397300 | Loss: 0.262859\n",
      "Iteration 397325 | Loss: 0.262859\n",
      "Iteration 397350 | Loss: 0.262859\n",
      "Iteration 397375 | Loss: 0.262859\n",
      "Iteration 397400 | Loss: 0.262859\n",
      "Iteration 397425 | Loss: 0.262859\n",
      "Iteration 397450 | Loss: 0.262859\n",
      "Iteration 397475 | Loss: 0.262859\n",
      "Iteration 397500 | Loss: 0.262859\n",
      "Iteration 397525 | Loss: 0.262859\n",
      "Iteration 397550 | Loss: 0.262859\n",
      "Iteration 397575 | Loss: 0.262859\n",
      "Iteration 397600 | Loss: 0.262859\n",
      "Iteration 397625 | Loss: 0.262859\n",
      "Iteration 397650 | Loss: 0.262859\n",
      "Iteration 397675 | Loss: 0.262859\n",
      "Iteration 397700 | Loss: 0.262859\n",
      "Iteration 397725 | Loss: 0.262859\n",
      "Iteration 397750 | Loss: 0.262859\n",
      "Iteration 397775 | Loss: 0.262859\n",
      "Iteration 397800 | Loss: 0.262859\n",
      "Iteration 397825 | Loss: 0.262859\n",
      "Iteration 397850 | Loss: 0.262859\n",
      "Iteration 397875 | Loss: 0.262859\n",
      "Iteration 397900 | Loss: 0.262859\n",
      "Iteration 397925 | Loss: 0.262859\n",
      "Iteration 397950 | Loss: 0.262859\n",
      "Iteration 397975 | Loss: 0.262859\n",
      "Iteration 398000 | Loss: 0.262859\n",
      "Iteration 398025 | Loss: 0.262859\n",
      "Iteration 398050 | Loss: 0.262859\n",
      "Iteration 398075 | Loss: 0.262859\n",
      "Iteration 398100 | Loss: 0.262859\n",
      "Iteration 398125 | Loss: 0.262859\n",
      "Iteration 398150 | Loss: 0.262859\n",
      "Iteration 398175 | Loss: 0.262859\n",
      "Iteration 398200 | Loss: 0.262859\n",
      "Iteration 398225 | Loss: 0.262859\n",
      "Iteration 398250 | Loss: 0.262859\n",
      "Iteration 398275 | Loss: 0.262859\n",
      "Iteration 398300 | Loss: 0.262859\n",
      "Iteration 398325 | Loss: 0.262859\n",
      "Iteration 398350 | Loss: 0.262859\n",
      "Iteration 398375 | Loss: 0.262859\n",
      "Iteration 398400 | Loss: 0.262859\n",
      "Iteration 398425 | Loss: 0.262859\n",
      "Iteration 398450 | Loss: 0.262859\n",
      "Iteration 398475 | Loss: 0.262859\n",
      "Iteration 398500 | Loss: 0.262859\n",
      "Iteration 398525 | Loss: 0.262859\n",
      "Iteration 398550 | Loss: 0.262859\n",
      "Iteration 398575 | Loss: 0.262859\n",
      "Iteration 398600 | Loss: 0.262859\n",
      "Iteration 398625 | Loss: 0.262859\n",
      "Iteration 398650 | Loss: 0.262859\n",
      "Iteration 398675 | Loss: 0.262859\n",
      "Iteration 398700 | Loss: 0.262859\n",
      "Iteration 398725 | Loss: 0.262859\n",
      "Iteration 398750 | Loss: 0.262859\n",
      "Iteration 398775 | Loss: 0.262859\n",
      "Iteration 398800 | Loss: 0.262859\n",
      "Iteration 398825 | Loss: 0.262859\n",
      "Iteration 398850 | Loss: 0.262859\n",
      "Iteration 398875 | Loss: 0.262859\n",
      "Iteration 398900 | Loss: 0.262859\n",
      "Iteration 398925 | Loss: 0.262859\n",
      "Iteration 398950 | Loss: 0.262858\n",
      "Iteration 398975 | Loss: 0.262858\n",
      "Iteration 399000 | Loss: 0.262858\n",
      "Iteration 399025 | Loss: 0.262858\n",
      "Iteration 399050 | Loss: 0.262858\n",
      "Iteration 399075 | Loss: 0.262858\n",
      "Iteration 399100 | Loss: 0.262858\n",
      "Iteration 399125 | Loss: 0.262858\n",
      "Iteration 399150 | Loss: 0.262858\n",
      "Iteration 399175 | Loss: 0.262858\n",
      "Iteration 399200 | Loss: 0.262858\n",
      "Iteration 399225 | Loss: 0.262858\n",
      "Iteration 399250 | Loss: 0.262858\n",
      "Iteration 399275 | Loss: 0.262858\n",
      "Iteration 399300 | Loss: 0.262858\n",
      "Iteration 399325 | Loss: 0.262858\n",
      "Iteration 399350 | Loss: 0.262858\n",
      "Iteration 399375 | Loss: 0.262858\n",
      "Iteration 399400 | Loss: 0.262858\n",
      "Iteration 399425 | Loss: 0.262858\n",
      "Iteration 399450 | Loss: 0.262858\n",
      "Iteration 399475 | Loss: 0.262858\n",
      "Iteration 399500 | Loss: 0.262858\n",
      "Iteration 399525 | Loss: 0.262858\n",
      "Iteration 399550 | Loss: 0.262858\n",
      "Iteration 399575 | Loss: 0.262858\n",
      "Iteration 399600 | Loss: 0.262858\n",
      "Iteration 399625 | Loss: 0.262858\n",
      "Iteration 399650 | Loss: 0.262858\n",
      "Iteration 399675 | Loss: 0.262858\n",
      "Iteration 399700 | Loss: 0.262858\n",
      "Iteration 399725 | Loss: 0.262858\n",
      "Iteration 399750 | Loss: 0.262858\n",
      "Iteration 399775 | Loss: 0.262858\n",
      "Iteration 399800 | Loss: 0.262858\n",
      "Iteration 399825 | Loss: 0.262858\n",
      "Iteration 399850 | Loss: 0.262858\n",
      "Iteration 399875 | Loss: 0.262858\n",
      "Iteration 399900 | Loss: 0.262858\n",
      "Iteration 399925 | Loss: 0.262858\n",
      "Iteration 399950 | Loss: 0.262858\n",
      "Iteration 399975 | Loss: 0.262858\n",
      "Iteration 400000 | Loss: 0.262858\n",
      "Iteration 400025 | Loss: 0.262858\n",
      "Iteration 400050 | Loss: 0.262858\n",
      "Iteration 400075 | Loss: 0.262858\n",
      "Iteration 400100 | Loss: 0.262858\n",
      "Iteration 400125 | Loss: 0.262858\n",
      "Iteration 400150 | Loss: 0.262858\n",
      "Iteration 400175 | Loss: 0.262858\n",
      "Iteration 400200 | Loss: 0.262858\n",
      "Iteration 400225 | Loss: 0.262858\n",
      "Iteration 400250 | Loss: 0.262858\n",
      "Iteration 400275 | Loss: 0.262858\n",
      "Iteration 400300 | Loss: 0.262858\n",
      "Iteration 400325 | Loss: 0.262858\n",
      "Iteration 400350 | Loss: 0.262858\n",
      "Iteration 400375 | Loss: 0.262858\n",
      "Iteration 400400 | Loss: 0.262858\n",
      "Iteration 400425 | Loss: 0.262858\n",
      "Iteration 400450 | Loss: 0.262858\n",
      "Iteration 400475 | Loss: 0.262858\n",
      "Iteration 400500 | Loss: 0.262858\n",
      "Iteration 400525 | Loss: 0.262858\n",
      "Iteration 400550 | Loss: 0.262858\n",
      "Iteration 400575 | Loss: 0.262858\n",
      "Iteration 400600 | Loss: 0.262858\n",
      "Iteration 400625 | Loss: 0.262858\n",
      "Iteration 400650 | Loss: 0.262858\n",
      "Iteration 400675 | Loss: 0.262858\n",
      "Iteration 400700 | Loss: 0.262858\n",
      "Iteration 400725 | Loss: 0.262858\n",
      "Iteration 400750 | Loss: 0.262858\n",
      "Iteration 400775 | Loss: 0.262858\n",
      "Iteration 400800 | Loss: 0.262858\n",
      "Iteration 400825 | Loss: 0.262858\n",
      "Iteration 400850 | Loss: 0.262858\n",
      "Iteration 400875 | Loss: 0.262858\n",
      "Iteration 400900 | Loss: 0.262858\n",
      "Iteration 400925 | Loss: 0.262858\n",
      "Iteration 400950 | Loss: 0.262858\n",
      "Iteration 400975 | Loss: 0.262858\n",
      "Iteration 401000 | Loss: 0.262858\n",
      "Iteration 401025 | Loss: 0.262858\n",
      "Iteration 401050 | Loss: 0.262858\n",
      "Iteration 401075 | Loss: 0.262858\n",
      "Iteration 401100 | Loss: 0.262858\n",
      "Iteration 401125 | Loss: 0.262858\n",
      "Iteration 401150 | Loss: 0.262858\n",
      "Iteration 401175 | Loss: 0.262858\n",
      "Iteration 401200 | Loss: 0.262858\n",
      "Iteration 401225 | Loss: 0.262858\n",
      "Iteration 401250 | Loss: 0.262858\n",
      "Iteration 401275 | Loss: 0.262858\n",
      "Iteration 401300 | Loss: 0.262858\n",
      "Iteration 401325 | Loss: 0.262858\n",
      "Iteration 401350 | Loss: 0.262858\n",
      "Iteration 401375 | Loss: 0.262858\n",
      "Iteration 401400 | Loss: 0.262858\n",
      "Iteration 401425 | Loss: 0.262858\n",
      "Iteration 401450 | Loss: 0.262858\n",
      "Iteration 401475 | Loss: 0.262858\n",
      "Iteration 401500 | Loss: 0.262858\n",
      "Iteration 401525 | Loss: 0.262858\n",
      "Iteration 401550 | Loss: 0.262858\n",
      "Iteration 401575 | Loss: 0.262858\n",
      "Iteration 401600 | Loss: 0.262858\n",
      "Iteration 401625 | Loss: 0.262858\n",
      "Iteration 401650 | Loss: 0.262858\n",
      "Iteration 401675 | Loss: 0.262858\n",
      "Iteration 401700 | Loss: 0.262858\n",
      "Iteration 401725 | Loss: 0.262858\n",
      "Iteration 401750 | Loss: 0.262858\n",
      "Iteration 401775 | Loss: 0.262858\n",
      "Iteration 401800 | Loss: 0.262858\n",
      "Iteration 401825 | Loss: 0.262858\n",
      "Iteration 401850 | Loss: 0.262858\n",
      "Iteration 401875 | Loss: 0.262858\n",
      "Iteration 401900 | Loss: 0.262858\n",
      "Iteration 401925 | Loss: 0.262858\n",
      "Iteration 401950 | Loss: 0.262858\n",
      "Iteration 401975 | Loss: 0.262858\n",
      "Iteration 402000 | Loss: 0.262858\n",
      "Iteration 402025 | Loss: 0.262858\n",
      "Iteration 402050 | Loss: 0.262858\n",
      "Iteration 402075 | Loss: 0.262858\n",
      "Iteration 402100 | Loss: 0.262858\n",
      "Iteration 402125 | Loss: 0.262858\n",
      "Iteration 402150 | Loss: 0.262858\n",
      "Iteration 402175 | Loss: 0.262858\n",
      "Iteration 402200 | Loss: 0.262858\n",
      "Iteration 402225 | Loss: 0.262858\n",
      "Iteration 402250 | Loss: 0.262858\n",
      "Iteration 402275 | Loss: 0.262858\n",
      "Iteration 402300 | Loss: 0.262858\n",
      "Iteration 402325 | Loss: 0.262858\n",
      "Iteration 402350 | Loss: 0.262858\n",
      "Iteration 402375 | Loss: 0.262858\n",
      "Iteration 402400 | Loss: 0.262858\n",
      "Iteration 402425 | Loss: 0.262858\n",
      "Iteration 402450 | Loss: 0.262858\n",
      "Iteration 402475 | Loss: 0.262858\n",
      "Iteration 402500 | Loss: 0.262858\n",
      "Iteration 402525 | Loss: 0.262858\n",
      "Iteration 402550 | Loss: 0.262858\n",
      "Iteration 402575 | Loss: 0.262858\n",
      "Iteration 402600 | Loss: 0.262858\n",
      "Iteration 402625 | Loss: 0.262858\n",
      "Iteration 402650 | Loss: 0.262858\n",
      "Iteration 402675 | Loss: 0.262858\n",
      "Iteration 402700 | Loss: 0.262858\n",
      "Iteration 402725 | Loss: 0.262858\n",
      "Iteration 402750 | Loss: 0.262858\n",
      "Iteration 402775 | Loss: 0.262858\n",
      "Iteration 402800 | Loss: 0.262858\n",
      "Iteration 402825 | Loss: 0.262858\n",
      "Iteration 402850 | Loss: 0.262858\n",
      "Iteration 402875 | Loss: 0.262858\n",
      "Iteration 402900 | Loss: 0.262858\n",
      "Iteration 402925 | Loss: 0.262858\n",
      "Iteration 402950 | Loss: 0.262858\n",
      "Iteration 402975 | Loss: 0.262858\n",
      "Iteration 403000 | Loss: 0.262858\n",
      "Iteration 403025 | Loss: 0.262858\n",
      "Iteration 403050 | Loss: 0.262858\n",
      "Iteration 403075 | Loss: 0.262858\n",
      "Iteration 403100 | Loss: 0.262858\n",
      "Iteration 403125 | Loss: 0.262858\n",
      "Iteration 403150 | Loss: 0.262858\n",
      "Iteration 403175 | Loss: 0.262858\n",
      "Iteration 403200 | Loss: 0.262858\n",
      "Iteration 403225 | Loss: 0.262858\n",
      "Iteration 403250 | Loss: 0.262858\n",
      "Iteration 403275 | Loss: 0.262858\n",
      "Iteration 403300 | Loss: 0.262858\n",
      "Iteration 403325 | Loss: 0.262858\n",
      "Iteration 403350 | Loss: 0.262858\n",
      "Iteration 403375 | Loss: 0.262858\n",
      "Iteration 403400 | Loss: 0.262858\n",
      "Iteration 403425 | Loss: 0.262858\n",
      "Iteration 403450 | Loss: 0.262858\n",
      "Iteration 403475 | Loss: 0.262858\n",
      "Iteration 403500 | Loss: 0.262858\n",
      "Iteration 403525 | Loss: 0.262858\n",
      "Iteration 403550 | Loss: 0.262858\n",
      "Iteration 403575 | Loss: 0.262858\n",
      "Iteration 403600 | Loss: 0.262858\n",
      "Iteration 403625 | Loss: 0.262858\n",
      "Iteration 403650 | Loss: 0.262858\n",
      "Iteration 403675 | Loss: 0.262858\n",
      "Iteration 403700 | Loss: 0.262858\n",
      "Iteration 403725 | Loss: 0.262858\n",
      "Iteration 403750 | Loss: 0.262858\n",
      "Iteration 403775 | Loss: 0.262858\n",
      "Iteration 403800 | Loss: 0.262858\n",
      "Iteration 403825 | Loss: 0.262858\n",
      "Iteration 403850 | Loss: 0.262858\n",
      "Iteration 403875 | Loss: 0.262858\n",
      "Iteration 403900 | Loss: 0.262858\n",
      "Iteration 403925 | Loss: 0.262858\n",
      "Iteration 403950 | Loss: 0.262858\n",
      "Iteration 403975 | Loss: 0.262858\n",
      "Iteration 404000 | Loss: 0.262858\n",
      "Iteration 404025 | Loss: 0.262858\n",
      "Iteration 404050 | Loss: 0.262858\n",
      "Iteration 404075 | Loss: 0.262858\n",
      "Iteration 404100 | Loss: 0.262858\n",
      "Iteration 404125 | Loss: 0.262858\n",
      "Iteration 404150 | Loss: 0.262858\n",
      "Iteration 404175 | Loss: 0.262858\n",
      "Iteration 404200 | Loss: 0.262858\n",
      "Iteration 404225 | Loss: 0.262858\n",
      "Iteration 404250 | Loss: 0.262858\n",
      "Iteration 404275 | Loss: 0.262858\n",
      "Iteration 404300 | Loss: 0.262858\n",
      "Iteration 404325 | Loss: 0.262858\n",
      "Iteration 404350 | Loss: 0.262858\n",
      "Iteration 404375 | Loss: 0.262858\n",
      "Iteration 404400 | Loss: 0.262858\n",
      "Iteration 404425 | Loss: 0.262858\n",
      "Iteration 404450 | Loss: 0.262858\n",
      "Iteration 404475 | Loss: 0.262858\n",
      "Iteration 404500 | Loss: 0.262858\n",
      "Iteration 404525 | Loss: 0.262858\n",
      "Iteration 404550 | Loss: 0.262858\n",
      "Iteration 404575 | Loss: 0.262858\n",
      "Iteration 404600 | Loss: 0.262858\n",
      "Iteration 404625 | Loss: 0.262858\n",
      "Iteration 404650 | Loss: 0.262858\n",
      "Iteration 404675 | Loss: 0.262858\n",
      "Iteration 404700 | Loss: 0.262858\n",
      "Iteration 404725 | Loss: 0.262858\n",
      "Iteration 404750 | Loss: 0.262858\n",
      "Iteration 404775 | Loss: 0.262858\n",
      "Iteration 404800 | Loss: 0.262858\n",
      "Iteration 404825 | Loss: 0.262858\n",
      "Iteration 404850 | Loss: 0.262858\n",
      "Iteration 404875 | Loss: 0.262858\n",
      "Iteration 404900 | Loss: 0.262858\n",
      "Iteration 404925 | Loss: 0.262858\n",
      "Iteration 404950 | Loss: 0.262858\n",
      "Iteration 404975 | Loss: 0.262858\n",
      "Iteration 405000 | Loss: 0.262858\n",
      "Iteration 405025 | Loss: 0.262858\n",
      "Iteration 405050 | Loss: 0.262858\n",
      "Iteration 405075 | Loss: 0.262858\n",
      "Iteration 405100 | Loss: 0.262858\n",
      "Iteration 405125 | Loss: 0.262858\n",
      "Iteration 405150 | Loss: 0.262858\n",
      "Iteration 405175 | Loss: 0.262858\n",
      "Iteration 405200 | Loss: 0.262858\n",
      "Iteration 405225 | Loss: 0.262858\n",
      "Iteration 405250 | Loss: 0.262858\n",
      "Iteration 405275 | Loss: 0.262858\n",
      "Iteration 405300 | Loss: 0.262858\n",
      "Iteration 405325 | Loss: 0.262858\n",
      "Iteration 405350 | Loss: 0.262858\n",
      "Iteration 405375 | Loss: 0.262858\n",
      "Iteration 405400 | Loss: 0.262858\n",
      "Iteration 405425 | Loss: 0.262858\n",
      "Iteration 405450 | Loss: 0.262858\n",
      "Iteration 405475 | Loss: 0.262858\n",
      "Iteration 405500 | Loss: 0.262858\n",
      "Iteration 405525 | Loss: 0.262858\n",
      "Iteration 405550 | Loss: 0.262858\n",
      "Iteration 405575 | Loss: 0.262858\n",
      "Iteration 405600 | Loss: 0.262858\n",
      "Iteration 405625 | Loss: 0.262858\n",
      "Iteration 405650 | Loss: 0.262858\n",
      "Iteration 405675 | Loss: 0.262858\n",
      "Iteration 405700 | Loss: 0.262858\n",
      "Iteration 405725 | Loss: 0.262858\n",
      "Iteration 405750 | Loss: 0.262858\n",
      "Iteration 405775 | Loss: 0.262858\n",
      "Iteration 405800 | Loss: 0.262858\n",
      "Iteration 405825 | Loss: 0.262858\n",
      "Iteration 405850 | Loss: 0.262858\n",
      "Iteration 405875 | Loss: 0.262858\n",
      "Iteration 405900 | Loss: 0.262858\n",
      "Iteration 405925 | Loss: 0.262858\n",
      "Iteration 405950 | Loss: 0.262858\n",
      "Iteration 405975 | Loss: 0.262858\n",
      "Iteration 406000 | Loss: 0.262858\n",
      "Iteration 406025 | Loss: 0.262858\n",
      "Iteration 406050 | Loss: 0.262858\n",
      "Iteration 406075 | Loss: 0.262858\n",
      "Iteration 406100 | Loss: 0.262858\n",
      "Iteration 406125 | Loss: 0.262858\n",
      "Iteration 406150 | Loss: 0.262858\n",
      "Iteration 406175 | Loss: 0.262858\n",
      "Iteration 406200 | Loss: 0.262858\n",
      "Iteration 406225 | Loss: 0.262858\n",
      "Iteration 406250 | Loss: 0.262858\n",
      "Iteration 406275 | Loss: 0.262858\n",
      "Iteration 406300 | Loss: 0.262858\n",
      "Iteration 406325 | Loss: 0.262858\n",
      "Iteration 406350 | Loss: 0.262858\n",
      "Iteration 406375 | Loss: 0.262858\n",
      "Iteration 406400 | Loss: 0.262858\n",
      "Iteration 406425 | Loss: 0.262858\n",
      "Iteration 406450 | Loss: 0.262858\n",
      "Iteration 406475 | Loss: 0.262858\n",
      "Iteration 406500 | Loss: 0.262858\n",
      "Iteration 406525 | Loss: 0.262858\n",
      "Iteration 406550 | Loss: 0.262858\n",
      "Iteration 406575 | Loss: 0.262858\n",
      "Iteration 406600 | Loss: 0.262858\n",
      "Iteration 406625 | Loss: 0.262858\n",
      "Iteration 406650 | Loss: 0.262858\n",
      "Iteration 406675 | Loss: 0.262858\n",
      "Iteration 406700 | Loss: 0.262858\n",
      "Iteration 406725 | Loss: 0.262858\n",
      "Iteration 406750 | Loss: 0.262858\n",
      "Iteration 406775 | Loss: 0.262858\n",
      "Iteration 406800 | Loss: 0.262858\n",
      "Iteration 406825 | Loss: 0.262858\n",
      "Iteration 406850 | Loss: 0.262858\n",
      "Iteration 406875 | Loss: 0.262858\n",
      "Iteration 406900 | Loss: 0.262858\n",
      "Iteration 406925 | Loss: 0.262858\n",
      "Iteration 406950 | Loss: 0.262858\n",
      "Iteration 406975 | Loss: 0.262858\n",
      "Iteration 407000 | Loss: 0.262858\n",
      "Iteration 407025 | Loss: 0.262858\n",
      "Iteration 407050 | Loss: 0.262858\n",
      "Iteration 407075 | Loss: 0.262858\n",
      "Iteration 407100 | Loss: 0.262858\n",
      "Iteration 407125 | Loss: 0.262858\n",
      "Iteration 407150 | Loss: 0.262858\n",
      "Iteration 407175 | Loss: 0.262858\n",
      "Iteration 407200 | Loss: 0.262858\n",
      "Iteration 407225 | Loss: 0.262858\n",
      "Iteration 407250 | Loss: 0.262858\n",
      "Iteration 407275 | Loss: 0.262858\n",
      "Iteration 407300 | Loss: 0.262858\n",
      "Iteration 407325 | Loss: 0.262858\n",
      "Iteration 407350 | Loss: 0.262858\n",
      "Iteration 407375 | Loss: 0.262858\n",
      "Iteration 407400 | Loss: 0.262858\n",
      "Iteration 407425 | Loss: 0.262858\n",
      "Iteration 407450 | Loss: 0.262858\n",
      "Iteration 407475 | Loss: 0.262858\n",
      "Iteration 407500 | Loss: 0.262858\n",
      "Iteration 407525 | Loss: 0.262858\n",
      "Iteration 407550 | Loss: 0.262858\n",
      "Iteration 407575 | Loss: 0.262858\n",
      "Iteration 407600 | Loss: 0.262858\n",
      "Iteration 407625 | Loss: 0.262858\n",
      "Iteration 407650 | Loss: 0.262858\n",
      "Iteration 407675 | Loss: 0.262858\n",
      "Iteration 407700 | Loss: 0.262858\n",
      "Iteration 407725 | Loss: 0.262858\n",
      "Iteration 407750 | Loss: 0.262858\n",
      "Iteration 407775 | Loss: 0.262858\n",
      "Iteration 407800 | Loss: 0.262858\n",
      "Iteration 407825 | Loss: 0.262858\n",
      "Iteration 407850 | Loss: 0.262858\n",
      "Iteration 407875 | Loss: 0.262858\n",
      "Iteration 407900 | Loss: 0.262858\n",
      "Iteration 407925 | Loss: 0.262858\n",
      "Iteration 407950 | Loss: 0.262858\n",
      "Iteration 407975 | Loss: 0.262858\n",
      "Iteration 408000 | Loss: 0.262858\n",
      "Iteration 408025 | Loss: 0.262858\n",
      "Iteration 408050 | Loss: 0.262858\n",
      "Iteration 408075 | Loss: 0.262858\n",
      "Iteration 408100 | Loss: 0.262858\n",
      "Iteration 408125 | Loss: 0.262858\n",
      "Iteration 408150 | Loss: 0.262858\n",
      "Iteration 408175 | Loss: 0.262858\n",
      "Iteration 408200 | Loss: 0.262858\n",
      "Iteration 408225 | Loss: 0.262858\n",
      "Iteration 408250 | Loss: 0.262858\n",
      "Iteration 408275 | Loss: 0.262858\n",
      "Iteration 408300 | Loss: 0.262858\n",
      "Iteration 408325 | Loss: 0.262858\n",
      "Iteration 408350 | Loss: 0.262858\n",
      "Iteration 408375 | Loss: 0.262858\n",
      "Iteration 408400 | Loss: 0.262858\n",
      "Iteration 408425 | Loss: 0.262858\n",
      "Iteration 408450 | Loss: 0.262858\n",
      "Iteration 408475 | Loss: 0.262858\n",
      "Iteration 408500 | Loss: 0.262858\n",
      "Iteration 408525 | Loss: 0.262858\n",
      "Iteration 408550 | Loss: 0.262858\n",
      "Iteration 408575 | Loss: 0.262857\n",
      "Iteration 408600 | Loss: 0.262857\n",
      "Iteration 408625 | Loss: 0.262857\n",
      "Iteration 408650 | Loss: 0.262857\n",
      "Iteration 408675 | Loss: 0.262857\n",
      "Iteration 408700 | Loss: 0.262857\n",
      "Iteration 408725 | Loss: 0.262857\n",
      "Iteration 408750 | Loss: 0.262857\n",
      "Iteration 408775 | Loss: 0.262857\n",
      "Iteration 408800 | Loss: 0.262857\n",
      "Iteration 408825 | Loss: 0.262857\n",
      "Iteration 408850 | Loss: 0.262857\n",
      "Iteration 408875 | Loss: 0.262857\n",
      "Iteration 408900 | Loss: 0.262857\n",
      "Iteration 408925 | Loss: 0.262857\n",
      "Iteration 408950 | Loss: 0.262857\n",
      "Iteration 408975 | Loss: 0.262857\n",
      "Iteration 409000 | Loss: 0.262857\n",
      "Iteration 409025 | Loss: 0.262857\n",
      "Iteration 409050 | Loss: 0.262857\n",
      "Iteration 409075 | Loss: 0.262857\n",
      "Iteration 409100 | Loss: 0.262857\n",
      "Iteration 409125 | Loss: 0.262857\n",
      "Iteration 409150 | Loss: 0.262857\n",
      "Iteration 409175 | Loss: 0.262857\n",
      "Iteration 409200 | Loss: 0.262857\n",
      "Iteration 409225 | Loss: 0.262857\n",
      "Iteration 409250 | Loss: 0.262857\n",
      "Iteration 409275 | Loss: 0.262857\n",
      "Iteration 409300 | Loss: 0.262857\n",
      "Iteration 409325 | Loss: 0.262857\n",
      "Iteration 409350 | Loss: 0.262857\n",
      "Iteration 409375 | Loss: 0.262857\n",
      "Iteration 409400 | Loss: 0.262857\n",
      "Iteration 409425 | Loss: 0.262857\n",
      "Iteration 409450 | Loss: 0.262857\n",
      "Iteration 409475 | Loss: 0.262857\n",
      "Iteration 409500 | Loss: 0.262857\n",
      "Iteration 409525 | Loss: 0.262857\n",
      "Iteration 409550 | Loss: 0.262857\n",
      "Iteration 409575 | Loss: 0.262857\n",
      "Iteration 409600 | Loss: 0.262857\n",
      "Iteration 409625 | Loss: 0.262857\n",
      "Iteration 409650 | Loss: 0.262857\n",
      "Iteration 409675 | Loss: 0.262857\n",
      "Iteration 409700 | Loss: 0.262857\n",
      "Iteration 409725 | Loss: 0.262857\n",
      "Iteration 409750 | Loss: 0.262857\n",
      "Iteration 409775 | Loss: 0.262857\n",
      "Iteration 409800 | Loss: 0.262857\n",
      "Iteration 409825 | Loss: 0.262857\n",
      "Iteration 409850 | Loss: 0.262857\n",
      "Iteration 409875 | Loss: 0.262857\n",
      "Iteration 409900 | Loss: 0.262857\n",
      "Iteration 409925 | Loss: 0.262857\n",
      "Iteration 409950 | Loss: 0.262857\n",
      "Iteration 409975 | Loss: 0.262857\n",
      "Iteration 410000 | Loss: 0.262857\n",
      "Iteration 410025 | Loss: 0.262857\n",
      "Iteration 410050 | Loss: 0.262857\n",
      "Iteration 410075 | Loss: 0.262857\n",
      "Iteration 410100 | Loss: 0.262857\n",
      "Iteration 410125 | Loss: 0.262857\n",
      "Iteration 410150 | Loss: 0.262857\n",
      "Iteration 410175 | Loss: 0.262857\n",
      "Iteration 410200 | Loss: 0.262857\n",
      "Iteration 410225 | Loss: 0.262857\n",
      "Iteration 410250 | Loss: 0.262857\n",
      "Iteration 410275 | Loss: 0.262857\n",
      "Iteration 410300 | Loss: 0.262857\n",
      "Iteration 410325 | Loss: 0.262857\n",
      "Iteration 410350 | Loss: 0.262857\n",
      "Iteration 410375 | Loss: 0.262857\n",
      "Iteration 410400 | Loss: 0.262857\n",
      "Iteration 410425 | Loss: 0.262857\n",
      "Iteration 410450 | Loss: 0.262857\n",
      "Iteration 410475 | Loss: 0.262857\n",
      "Iteration 410500 | Loss: 0.262857\n",
      "Iteration 410525 | Loss: 0.262857\n",
      "Iteration 410550 | Loss: 0.262857\n",
      "Iteration 410575 | Loss: 0.262857\n",
      "Iteration 410600 | Loss: 0.262857\n",
      "Iteration 410625 | Loss: 0.262857\n",
      "Iteration 410650 | Loss: 0.262857\n",
      "Iteration 410675 | Loss: 0.262857\n",
      "Iteration 410700 | Loss: 0.262857\n",
      "Iteration 410725 | Loss: 0.262857\n",
      "Iteration 410750 | Loss: 0.262857\n",
      "Iteration 410775 | Loss: 0.262857\n",
      "Iteration 410800 | Loss: 0.262857\n",
      "Iteration 410825 | Loss: 0.262857\n",
      "Iteration 410850 | Loss: 0.262857\n",
      "Iteration 410875 | Loss: 0.262857\n",
      "Iteration 410900 | Loss: 0.262857\n",
      "Iteration 410925 | Loss: 0.262857\n",
      "Iteration 410950 | Loss: 0.262857\n",
      "Iteration 410975 | Loss: 0.262857\n",
      "Iteration 411000 | Loss: 0.262857\n",
      "Iteration 411025 | Loss: 0.262857\n",
      "Iteration 411050 | Loss: 0.262857\n",
      "Iteration 411075 | Loss: 0.262857\n",
      "Iteration 411100 | Loss: 0.262857\n",
      "Iteration 411125 | Loss: 0.262857\n",
      "Iteration 411150 | Loss: 0.262857\n",
      "Iteration 411175 | Loss: 0.262857\n",
      "Iteration 411200 | Loss: 0.262857\n",
      "Iteration 411225 | Loss: 0.262857\n",
      "Iteration 411250 | Loss: 0.262857\n",
      "Iteration 411275 | Loss: 0.262857\n",
      "Iteration 411300 | Loss: 0.262857\n",
      "Iteration 411325 | Loss: 0.262857\n",
      "Iteration 411350 | Loss: 0.262857\n",
      "Iteration 411375 | Loss: 0.262857\n",
      "Iteration 411400 | Loss: 0.262857\n",
      "Iteration 411425 | Loss: 0.262857\n",
      "Iteration 411450 | Loss: 0.262857\n",
      "Iteration 411475 | Loss: 0.262857\n",
      "Iteration 411500 | Loss: 0.262857\n",
      "Iteration 411525 | Loss: 0.262857\n",
      "Iteration 411550 | Loss: 0.262857\n",
      "Iteration 411575 | Loss: 0.262857\n",
      "Iteration 411600 | Loss: 0.262857\n",
      "Iteration 411625 | Loss: 0.262857\n",
      "Iteration 411650 | Loss: 0.262857\n",
      "Iteration 411675 | Loss: 0.262857\n",
      "Iteration 411700 | Loss: 0.262857\n",
      "Iteration 411725 | Loss: 0.262857\n",
      "Iteration 411750 | Loss: 0.262857\n",
      "Iteration 411775 | Loss: 0.262857\n",
      "Iteration 411800 | Loss: 0.262857\n",
      "Iteration 411825 | Loss: 0.262857\n",
      "Iteration 411850 | Loss: 0.262857\n",
      "Iteration 411875 | Loss: 0.262857\n",
      "Iteration 411900 | Loss: 0.262857\n",
      "Iteration 411925 | Loss: 0.262857\n",
      "Iteration 411950 | Loss: 0.262857\n",
      "Iteration 411975 | Loss: 0.262857\n",
      "Iteration 412000 | Loss: 0.262857\n",
      "Iteration 412025 | Loss: 0.262857\n",
      "Iteration 412050 | Loss: 0.262857\n",
      "Iteration 412075 | Loss: 0.262857\n",
      "Iteration 412100 | Loss: 0.262857\n",
      "Iteration 412125 | Loss: 0.262857\n",
      "Iteration 412150 | Loss: 0.262857\n",
      "Iteration 412175 | Loss: 0.262857\n",
      "Iteration 412200 | Loss: 0.262857\n",
      "Iteration 412225 | Loss: 0.262857\n",
      "Iteration 412250 | Loss: 0.262857\n",
      "Iteration 412275 | Loss: 0.262857\n",
      "Iteration 412300 | Loss: 0.262857\n",
      "Iteration 412325 | Loss: 0.262857\n",
      "Iteration 412350 | Loss: 0.262857\n",
      "Iteration 412375 | Loss: 0.262857\n",
      "Iteration 412400 | Loss: 0.262857\n",
      "Iteration 412425 | Loss: 0.262857\n",
      "Iteration 412450 | Loss: 0.262857\n",
      "Iteration 412475 | Loss: 0.262857\n",
      "Iteration 412500 | Loss: 0.262857\n",
      "Iteration 412525 | Loss: 0.262857\n",
      "Iteration 412550 | Loss: 0.262857\n",
      "Iteration 412575 | Loss: 0.262857\n",
      "Iteration 412600 | Loss: 0.262857\n",
      "Iteration 412625 | Loss: 0.262857\n",
      "Iteration 412650 | Loss: 0.262857\n",
      "Iteration 412675 | Loss: 0.262857\n",
      "Iteration 412700 | Loss: 0.262857\n",
      "Iteration 412725 | Loss: 0.262857\n",
      "Iteration 412750 | Loss: 0.262857\n",
      "Iteration 412775 | Loss: 0.262857\n",
      "Iteration 412800 | Loss: 0.262857\n",
      "Iteration 412825 | Loss: 0.262857\n",
      "Iteration 412850 | Loss: 0.262857\n",
      "Iteration 412875 | Loss: 0.262857\n",
      "Iteration 412900 | Loss: 0.262857\n",
      "Iteration 412925 | Loss: 0.262857\n",
      "Iteration 412950 | Loss: 0.262857\n",
      "Iteration 412975 | Loss: 0.262857\n",
      "Iteration 413000 | Loss: 0.262857\n",
      "Iteration 413025 | Loss: 0.262857\n",
      "Iteration 413050 | Loss: 0.262857\n",
      "Iteration 413075 | Loss: 0.262857\n",
      "Iteration 413100 | Loss: 0.262857\n",
      "Iteration 413125 | Loss: 0.262857\n",
      "Iteration 413150 | Loss: 0.262857\n",
      "Iteration 413175 | Loss: 0.262857\n",
      "Iteration 413200 | Loss: 0.262857\n",
      "Iteration 413225 | Loss: 0.262857\n",
      "Iteration 413250 | Loss: 0.262857\n",
      "Iteration 413275 | Loss: 0.262857\n",
      "Iteration 413300 | Loss: 0.262857\n",
      "Iteration 413325 | Loss: 0.262857\n",
      "Iteration 413350 | Loss: 0.262857\n",
      "Iteration 413375 | Loss: 0.262857\n",
      "Iteration 413400 | Loss: 0.262857\n",
      "Iteration 413425 | Loss: 0.262857\n",
      "Iteration 413450 | Loss: 0.262857\n",
      "Iteration 413475 | Loss: 0.262857\n",
      "Iteration 413500 | Loss: 0.262857\n",
      "Iteration 413525 | Loss: 0.262857\n",
      "Iteration 413550 | Loss: 0.262857\n",
      "Iteration 413575 | Loss: 0.262857\n",
      "Iteration 413600 | Loss: 0.262857\n",
      "Iteration 413625 | Loss: 0.262857\n",
      "Iteration 413650 | Loss: 0.262857\n",
      "Iteration 413675 | Loss: 0.262857\n",
      "Iteration 413700 | Loss: 0.262857\n",
      "Iteration 413725 | Loss: 0.262857\n",
      "Iteration 413750 | Loss: 0.262857\n",
      "Iteration 413775 | Loss: 0.262857\n",
      "Iteration 413800 | Loss: 0.262857\n",
      "Iteration 413825 | Loss: 0.262857\n",
      "Iteration 413850 | Loss: 0.262857\n",
      "Iteration 413875 | Loss: 0.262857\n",
      "Iteration 413900 | Loss: 0.262857\n",
      "Iteration 413925 | Loss: 0.262857\n",
      "Iteration 413950 | Loss: 0.262857\n",
      "Iteration 413975 | Loss: 0.262857\n",
      "Iteration 414000 | Loss: 0.262857\n",
      "Iteration 414025 | Loss: 0.262857\n",
      "Iteration 414050 | Loss: 0.262857\n",
      "Iteration 414075 | Loss: 0.262857\n",
      "Iteration 414100 | Loss: 0.262857\n",
      "Iteration 414125 | Loss: 0.262857\n",
      "Iteration 414150 | Loss: 0.262857\n",
      "Iteration 414175 | Loss: 0.262857\n",
      "Iteration 414200 | Loss: 0.262857\n",
      "Iteration 414225 | Loss: 0.262857\n",
      "Iteration 414250 | Loss: 0.262857\n",
      "Iteration 414275 | Loss: 0.262857\n",
      "Iteration 414300 | Loss: 0.262857\n",
      "Iteration 414325 | Loss: 0.262857\n",
      "Iteration 414350 | Loss: 0.262857\n",
      "Iteration 414375 | Loss: 0.262857\n",
      "Iteration 414400 | Loss: 0.262857\n",
      "Iteration 414425 | Loss: 0.262857\n",
      "Iteration 414450 | Loss: 0.262857\n",
      "Iteration 414475 | Loss: 0.262857\n",
      "Iteration 414500 | Loss: 0.262857\n",
      "Iteration 414525 | Loss: 0.262857\n",
      "Iteration 414550 | Loss: 0.262857\n",
      "Iteration 414575 | Loss: 0.262857\n",
      "Iteration 414600 | Loss: 0.262857\n",
      "Iteration 414625 | Loss: 0.262857\n",
      "Iteration 414650 | Loss: 0.262857\n",
      "Iteration 414675 | Loss: 0.262857\n",
      "Iteration 414700 | Loss: 0.262857\n",
      "Iteration 414725 | Loss: 0.262857\n",
      "Iteration 414750 | Loss: 0.262857\n",
      "Iteration 414775 | Loss: 0.262857\n",
      "Iteration 414800 | Loss: 0.262857\n",
      "Iteration 414825 | Loss: 0.262857\n",
      "Iteration 414850 | Loss: 0.262857\n",
      "Iteration 414875 | Loss: 0.262857\n",
      "Iteration 414900 | Loss: 0.262857\n",
      "Iteration 414925 | Loss: 0.262857\n",
      "Iteration 414950 | Loss: 0.262857\n",
      "Iteration 414975 | Loss: 0.262857\n",
      "Iteration 415000 | Loss: 0.262857\n",
      "Iteration 415025 | Loss: 0.262857\n",
      "Iteration 415050 | Loss: 0.262857\n",
      "Iteration 415075 | Loss: 0.262857\n",
      "Iteration 415100 | Loss: 0.262857\n",
      "Iteration 415125 | Loss: 0.262857\n",
      "Iteration 415150 | Loss: 0.262857\n",
      "Iteration 415175 | Loss: 0.262857\n",
      "Iteration 415200 | Loss: 0.262857\n",
      "Iteration 415225 | Loss: 0.262857\n",
      "Iteration 415250 | Loss: 0.262857\n",
      "Iteration 415275 | Loss: 0.262857\n",
      "Iteration 415300 | Loss: 0.262857\n",
      "Iteration 415325 | Loss: 0.262857\n",
      "Iteration 415350 | Loss: 0.262857\n",
      "Iteration 415375 | Loss: 0.262857\n",
      "Iteration 415400 | Loss: 0.262857\n",
      "Iteration 415425 | Loss: 0.262857\n",
      "Iteration 415450 | Loss: 0.262857\n",
      "Iteration 415475 | Loss: 0.262857\n",
      "Iteration 415500 | Loss: 0.262857\n",
      "Iteration 415525 | Loss: 0.262857\n",
      "Iteration 415550 | Loss: 0.262857\n",
      "Iteration 415575 | Loss: 0.262857\n",
      "Iteration 415600 | Loss: 0.262857\n",
      "Iteration 415625 | Loss: 0.262857\n",
      "Iteration 415650 | Loss: 0.262857\n",
      "Iteration 415675 | Loss: 0.262857\n",
      "Iteration 415700 | Loss: 0.262857\n",
      "Iteration 415725 | Loss: 0.262857\n",
      "Iteration 415750 | Loss: 0.262857\n",
      "Iteration 415775 | Loss: 0.262857\n",
      "Iteration 415800 | Loss: 0.262857\n",
      "Iteration 415825 | Loss: 0.262857\n",
      "Iteration 415850 | Loss: 0.262857\n",
      "Iteration 415875 | Loss: 0.262857\n",
      "Iteration 415900 | Loss: 0.262857\n",
      "Iteration 415925 | Loss: 0.262857\n",
      "Iteration 415950 | Loss: 0.262857\n",
      "Iteration 415975 | Loss: 0.262857\n",
      "Iteration 416000 | Loss: 0.262857\n",
      "Iteration 416025 | Loss: 0.262857\n",
      "Iteration 416050 | Loss: 0.262857\n",
      "Iteration 416075 | Loss: 0.262857\n",
      "Iteration 416100 | Loss: 0.262857\n",
      "Iteration 416125 | Loss: 0.262857\n",
      "Iteration 416150 | Loss: 0.262857\n",
      "Iteration 416175 | Loss: 0.262857\n",
      "Iteration 416200 | Loss: 0.262857\n",
      "Iteration 416225 | Loss: 0.262857\n",
      "Iteration 416250 | Loss: 0.262857\n",
      "Iteration 416275 | Loss: 0.262857\n",
      "Iteration 416300 | Loss: 0.262857\n",
      "Iteration 416325 | Loss: 0.262857\n",
      "Iteration 416350 | Loss: 0.262857\n",
      "Iteration 416375 | Loss: 0.262857\n",
      "Iteration 416400 | Loss: 0.262857\n",
      "Iteration 416425 | Loss: 0.262857\n",
      "Iteration 416450 | Loss: 0.262857\n",
      "Iteration 416475 | Loss: 0.262857\n",
      "Iteration 416500 | Loss: 0.262857\n",
      "Iteration 416525 | Loss: 0.262857\n",
      "Iteration 416550 | Loss: 0.262857\n",
      "Iteration 416575 | Loss: 0.262857\n",
      "Iteration 416600 | Loss: 0.262857\n",
      "Iteration 416625 | Loss: 0.262857\n",
      "Iteration 416650 | Loss: 0.262857\n",
      "Iteration 416675 | Loss: 0.262857\n",
      "Iteration 416700 | Loss: 0.262857\n",
      "Iteration 416725 | Loss: 0.262857\n",
      "Iteration 416750 | Loss: 0.262857\n",
      "Iteration 416775 | Loss: 0.262857\n",
      "Iteration 416800 | Loss: 0.262857\n",
      "Iteration 416825 | Loss: 0.262857\n",
      "Iteration 416850 | Loss: 0.262857\n",
      "Iteration 416875 | Loss: 0.262857\n",
      "Iteration 416900 | Loss: 0.262857\n",
      "Iteration 416925 | Loss: 0.262857\n",
      "Iteration 416950 | Loss: 0.262857\n",
      "Iteration 416975 | Loss: 0.262857\n",
      "Iteration 417000 | Loss: 0.262857\n",
      "Iteration 417025 | Loss: 0.262857\n",
      "Iteration 417050 | Loss: 0.262857\n",
      "Iteration 417075 | Loss: 0.262857\n",
      "Iteration 417100 | Loss: 0.262857\n",
      "Iteration 417125 | Loss: 0.262857\n",
      "Iteration 417150 | Loss: 0.262857\n",
      "Iteration 417175 | Loss: 0.262857\n",
      "Iteration 417200 | Loss: 0.262857\n",
      "Iteration 417225 | Loss: 0.262857\n",
      "Iteration 417250 | Loss: 0.262857\n",
      "Iteration 417275 | Loss: 0.262857\n",
      "Iteration 417300 | Loss: 0.262857\n",
      "Iteration 417325 | Loss: 0.262857\n",
      "Iteration 417350 | Loss: 0.262857\n",
      "Iteration 417375 | Loss: 0.262857\n",
      "Iteration 417400 | Loss: 0.262857\n",
      "Iteration 417425 | Loss: 0.262857\n",
      "Iteration 417450 | Loss: 0.262857\n",
      "Iteration 417475 | Loss: 0.262857\n",
      "Iteration 417500 | Loss: 0.262857\n",
      "Iteration 417525 | Loss: 0.262857\n",
      "Iteration 417550 | Loss: 0.262857\n",
      "Iteration 417575 | Loss: 0.262857\n",
      "Iteration 417600 | Loss: 0.262857\n",
      "Iteration 417625 | Loss: 0.262857\n",
      "Iteration 417650 | Loss: 0.262857\n",
      "Iteration 417675 | Loss: 0.262857\n",
      "Iteration 417700 | Loss: 0.262857\n",
      "Iteration 417725 | Loss: 0.262857\n",
      "Iteration 417750 | Loss: 0.262857\n",
      "Iteration 417775 | Loss: 0.262857\n",
      "Iteration 417800 | Loss: 0.262857\n",
      "Iteration 417825 | Loss: 0.262857\n",
      "Iteration 417850 | Loss: 0.262857\n",
      "Iteration 417875 | Loss: 0.262857\n",
      "Iteration 417900 | Loss: 0.262857\n",
      "Iteration 417925 | Loss: 0.262857\n",
      "Iteration 417950 | Loss: 0.262857\n",
      "Iteration 417975 | Loss: 0.262857\n",
      "Iteration 418000 | Loss: 0.262857\n",
      "Iteration 418025 | Loss: 0.262857\n",
      "Iteration 418050 | Loss: 0.262857\n",
      "Iteration 418075 | Loss: 0.262857\n",
      "Iteration 418100 | Loss: 0.262857\n",
      "Iteration 418125 | Loss: 0.262857\n",
      "Iteration 418150 | Loss: 0.262857\n",
      "Iteration 418175 | Loss: 0.262857\n",
      "Iteration 418200 | Loss: 0.262857\n",
      "Iteration 418225 | Loss: 0.262857\n",
      "Iteration 418250 | Loss: 0.262857\n",
      "Iteration 418275 | Loss: 0.262857\n",
      "Iteration 418300 | Loss: 0.262857\n",
      "Iteration 418325 | Loss: 0.262857\n",
      "Iteration 418350 | Loss: 0.262857\n",
      "Iteration 418375 | Loss: 0.262857\n",
      "Iteration 418400 | Loss: 0.262857\n",
      "Iteration 418425 | Loss: 0.262857\n",
      "Iteration 418450 | Loss: 0.262857\n",
      "Iteration 418475 | Loss: 0.262857\n",
      "Iteration 418500 | Loss: 0.262857\n",
      "Iteration 418525 | Loss: 0.262857\n",
      "Iteration 418550 | Loss: 0.262857\n",
      "Iteration 418575 | Loss: 0.262857\n",
      "Iteration 418600 | Loss: 0.262857\n",
      "Iteration 418625 | Loss: 0.262857\n",
      "Iteration 418650 | Loss: 0.262857\n",
      "Iteration 418675 | Loss: 0.262857\n",
      "Iteration 418700 | Loss: 0.262857\n",
      "Iteration 418725 | Loss: 0.262857\n",
      "Iteration 418750 | Loss: 0.262857\n",
      "Iteration 418775 | Loss: 0.262857\n",
      "Iteration 418800 | Loss: 0.262857\n",
      "Iteration 418825 | Loss: 0.262857\n",
      "Iteration 418850 | Loss: 0.262857\n",
      "Iteration 418875 | Loss: 0.262857\n",
      "Iteration 418900 | Loss: 0.262857\n",
      "Iteration 418925 | Loss: 0.262857\n",
      "Iteration 418950 | Loss: 0.262857\n",
      "Iteration 418975 | Loss: 0.262857\n",
      "Iteration 419000 | Loss: 0.262857\n",
      "Iteration 419025 | Loss: 0.262857\n",
      "Iteration 419050 | Loss: 0.262857\n",
      "Iteration 419075 | Loss: 0.262857\n",
      "Iteration 419100 | Loss: 0.262857\n",
      "Iteration 419125 | Loss: 0.262857\n",
      "Iteration 419150 | Loss: 0.262857\n",
      "Iteration 419175 | Loss: 0.262857\n",
      "Iteration 419200 | Loss: 0.262857\n",
      "Iteration 419225 | Loss: 0.262857\n",
      "Iteration 419250 | Loss: 0.262857\n",
      "Iteration 419275 | Loss: 0.262857\n",
      "Iteration 419300 | Loss: 0.262857\n",
      "Iteration 419325 | Loss: 0.262857\n",
      "Iteration 419350 | Loss: 0.262857\n",
      "Iteration 419375 | Loss: 0.262857\n",
      "Iteration 419400 | Loss: 0.262857\n",
      "Iteration 419425 | Loss: 0.262857\n",
      "Iteration 419450 | Loss: 0.262857\n",
      "Iteration 419475 | Loss: 0.262857\n",
      "Iteration 419500 | Loss: 0.262857\n",
      "Iteration 419525 | Loss: 0.262857\n",
      "Iteration 419550 | Loss: 0.262857\n",
      "Iteration 419575 | Loss: 0.262857\n",
      "Iteration 419600 | Loss: 0.262857\n",
      "Iteration 419625 | Loss: 0.262857\n",
      "Iteration 419650 | Loss: 0.262857\n",
      "Iteration 419675 | Loss: 0.262857\n",
      "Iteration 419700 | Loss: 0.262857\n",
      "Iteration 419725 | Loss: 0.262857\n",
      "Iteration 419750 | Loss: 0.262857\n",
      "Iteration 419775 | Loss: 0.262857\n",
      "Iteration 419800 | Loss: 0.262857\n",
      "Iteration 419825 | Loss: 0.262857\n",
      "Iteration 419850 | Loss: 0.262857\n",
      "Iteration 419875 | Loss: 0.262857\n",
      "Iteration 419900 | Loss: 0.262857\n",
      "Iteration 419925 | Loss: 0.262857\n",
      "Iteration 419950 | Loss: 0.262857\n",
      "Iteration 419975 | Loss: 0.262857\n",
      "Iteration 420000 | Loss: 0.262857\n",
      "Iteration 420025 | Loss: 0.262857\n",
      "Iteration 420050 | Loss: 0.262857\n",
      "Iteration 420075 | Loss: 0.262857\n",
      "Iteration 420100 | Loss: 0.262857\n",
      "Iteration 420125 | Loss: 0.262857\n",
      "Iteration 420150 | Loss: 0.262857\n",
      "Iteration 420175 | Loss: 0.262857\n",
      "Iteration 420200 | Loss: 0.262857\n",
      "Iteration 420225 | Loss: 0.262857\n",
      "Iteration 420250 | Loss: 0.262857\n",
      "Iteration 420275 | Loss: 0.262857\n",
      "Iteration 420300 | Loss: 0.262857\n",
      "Iteration 420325 | Loss: 0.262857\n",
      "Iteration 420350 | Loss: 0.262857\n",
      "Iteration 420375 | Loss: 0.262857\n",
      "Iteration 420400 | Loss: 0.262857\n",
      "Iteration 420425 | Loss: 0.262857\n",
      "Iteration 420450 | Loss: 0.262857\n",
      "Iteration 420475 | Loss: 0.262857\n",
      "Iteration 420500 | Loss: 0.262857\n",
      "Iteration 420525 | Loss: 0.262857\n",
      "Iteration 420550 | Loss: 0.262857\n",
      "Iteration 420575 | Loss: 0.262857\n",
      "Iteration 420600 | Loss: 0.262857\n",
      "Iteration 420625 | Loss: 0.262857\n",
      "Iteration 420650 | Loss: 0.262857\n",
      "Iteration 420675 | Loss: 0.262857\n",
      "Iteration 420700 | Loss: 0.262857\n",
      "Iteration 420725 | Loss: 0.262857\n",
      "Iteration 420750 | Loss: 0.262857\n",
      "Iteration 420775 | Loss: 0.262857\n",
      "Iteration 420800 | Loss: 0.262857\n",
      "Iteration 420825 | Loss: 0.262857\n",
      "Iteration 420850 | Loss: 0.262857\n",
      "Iteration 420875 | Loss: 0.262857\n",
      "Iteration 420900 | Loss: 0.262857\n",
      "Iteration 420925 | Loss: 0.262857\n",
      "Iteration 420950 | Loss: 0.262857\n",
      "Iteration 420975 | Loss: 0.262857\n",
      "Iteration 421000 | Loss: 0.262857\n",
      "Iteration 421025 | Loss: 0.262857\n",
      "Iteration 421050 | Loss: 0.262857\n",
      "Iteration 421075 | Loss: 0.262857\n",
      "Iteration 421100 | Loss: 0.262857\n",
      "Iteration 421125 | Loss: 0.262857\n",
      "Iteration 421150 | Loss: 0.262857\n",
      "Iteration 421175 | Loss: 0.262857\n",
      "Iteration 421200 | Loss: 0.262857\n",
      "Iteration 421225 | Loss: 0.262857\n",
      "Iteration 421250 | Loss: 0.262857\n",
      "Iteration 421275 | Loss: 0.262857\n",
      "Iteration 421300 | Loss: 0.262857\n",
      "Iteration 421325 | Loss: 0.262857\n",
      "Iteration 421350 | Loss: 0.262857\n",
      "Iteration 421375 | Loss: 0.262857\n",
      "Iteration 421400 | Loss: 0.262857\n",
      "Iteration 421425 | Loss: 0.262857\n",
      "Iteration 421450 | Loss: 0.262857\n",
      "Iteration 421475 | Loss: 0.262857\n",
      "Iteration 421500 | Loss: 0.262857\n",
      "Iteration 421525 | Loss: 0.262857\n",
      "Iteration 421550 | Loss: 0.262857\n",
      "Iteration 421575 | Loss: 0.262857\n",
      "Iteration 421600 | Loss: 0.262857\n",
      "Iteration 421625 | Loss: 0.262857\n",
      "Iteration 421650 | Loss: 0.262857\n",
      "Iteration 421675 | Loss: 0.262857\n",
      "Iteration 421700 | Loss: 0.262857\n",
      "Iteration 421725 | Loss: 0.262857\n",
      "Iteration 421750 | Loss: 0.262857\n",
      "Iteration 421775 | Loss: 0.262857\n",
      "Iteration 421800 | Loss: 0.262857\n",
      "Iteration 421825 | Loss: 0.262857\n",
      "Iteration 421850 | Loss: 0.262857\n",
      "Iteration 421875 | Loss: 0.262857\n",
      "Iteration 421900 | Loss: 0.262857\n",
      "Iteration 421925 | Loss: 0.262857\n",
      "Iteration 421950 | Loss: 0.262857\n",
      "Iteration 421975 | Loss: 0.262857\n",
      "Iteration 422000 | Loss: 0.262857\n",
      "Iteration 422025 | Loss: 0.262857\n",
      "Iteration 422050 | Loss: 0.262857\n",
      "Iteration 422075 | Loss: 0.262857\n",
      "Iteration 422100 | Loss: 0.262857\n",
      "Iteration 422125 | Loss: 0.262857\n",
      "Iteration 422150 | Loss: 0.262857\n",
      "Iteration 422175 | Loss: 0.262857\n",
      "Iteration 422200 | Loss: 0.262857\n",
      "Iteration 422225 | Loss: 0.262857\n",
      "Iteration 422250 | Loss: 0.262857\n",
      "Iteration 422275 | Loss: 0.262857\n",
      "Iteration 422300 | Loss: 0.262857\n",
      "Iteration 422325 | Loss: 0.262857\n",
      "Iteration 422350 | Loss: 0.262857\n",
      "Iteration 422375 | Loss: 0.262857\n",
      "Iteration 422400 | Loss: 0.262857\n",
      "Iteration 422425 | Loss: 0.262857\n",
      "Iteration 422450 | Loss: 0.262857\n",
      "Iteration 422475 | Loss: 0.262857\n",
      "Iteration 422500 | Loss: 0.262857\n",
      "Iteration 422525 | Loss: 0.262857\n",
      "Iteration 422550 | Loss: 0.262857\n",
      "Iteration 422575 | Loss: 0.262857\n",
      "Iteration 422600 | Loss: 0.262857\n",
      "Iteration 422625 | Loss: 0.262857\n",
      "Iteration 422650 | Loss: 0.262857\n",
      "Iteration 422675 | Loss: 0.262857\n",
      "Iteration 422700 | Loss: 0.262857\n",
      "Iteration 422725 | Loss: 0.262857\n",
      "Iteration 422750 | Loss: 0.262857\n",
      "Iteration 422775 | Loss: 0.262857\n",
      "Iteration 422800 | Loss: 0.262857\n",
      "Iteration 422825 | Loss: 0.262857\n",
      "Iteration 422850 | Loss: 0.262857\n",
      "Iteration 422875 | Loss: 0.262857\n",
      "Iteration 422900 | Loss: 0.262857\n",
      "Iteration 422925 | Loss: 0.262857\n",
      "Iteration 422950 | Loss: 0.262857\n",
      "Iteration 422975 | Loss: 0.262857\n",
      "Iteration 423000 | Loss: 0.262857\n",
      "Iteration 423025 | Loss: 0.262857\n",
      "Iteration 423050 | Loss: 0.262857\n",
      "Iteration 423075 | Loss: 0.262857\n",
      "Iteration 423100 | Loss: 0.262857\n",
      "Iteration 423125 | Loss: 0.262857\n",
      "Iteration 423150 | Loss: 0.262857\n",
      "Iteration 423175 | Loss: 0.262857\n",
      "Iteration 423200 | Loss: 0.262857\n",
      "Iteration 423225 | Loss: 0.262857\n",
      "Iteration 423250 | Loss: 0.262857\n",
      "Iteration 423275 | Loss: 0.262857\n",
      "Iteration 423300 | Loss: 0.262857\n",
      "Iteration 423325 | Loss: 0.262857\n",
      "Iteration 423350 | Loss: 0.262857\n",
      "Iteration 423375 | Loss: 0.262857\n",
      "Iteration 423400 | Loss: 0.262857\n",
      "Iteration 423425 | Loss: 0.262857\n",
      "Iteration 423450 | Loss: 0.262857\n",
      "Iteration 423475 | Loss: 0.262857\n",
      "Iteration 423500 | Loss: 0.262857\n",
      "Iteration 423525 | Loss: 0.262857\n",
      "Iteration 423550 | Loss: 0.262857\n",
      "Iteration 423575 | Loss: 0.262857\n",
      "Iteration 423600 | Loss: 0.262857\n",
      "Iteration 423625 | Loss: 0.262857\n",
      "Iteration 423650 | Loss: 0.262857\n",
      "Iteration 423675 | Loss: 0.262857\n",
      "Iteration 423700 | Loss: 0.262857\n",
      "Iteration 423725 | Loss: 0.262857\n",
      "Iteration 423750 | Loss: 0.262857\n",
      "Iteration 423775 | Loss: 0.262857\n",
      "Iteration 423800 | Loss: 0.262857\n",
      "Iteration 423825 | Loss: 0.262857\n",
      "Iteration 423850 | Loss: 0.262857\n",
      "Iteration 423875 | Loss: 0.262857\n",
      "Iteration 423900 | Loss: 0.262857\n",
      "Iteration 423925 | Loss: 0.262857\n",
      "Iteration 423950 | Loss: 0.262857\n",
      "Iteration 423975 | Loss: 0.262857\n",
      "Iteration 424000 | Loss: 0.262857\n",
      "Iteration 424025 | Loss: 0.262857\n",
      "Iteration 424050 | Loss: 0.262857\n",
      "Iteration 424075 | Loss: 0.262857\n",
      "Iteration 424100 | Loss: 0.262857\n",
      "Iteration 424125 | Loss: 0.262857\n",
      "Iteration 424150 | Loss: 0.262857\n",
      "Iteration 424175 | Loss: 0.262857\n",
      "Iteration 424200 | Loss: 0.262857\n",
      "Iteration 424225 | Loss: 0.262857\n",
      "Iteration 424250 | Loss: 0.262857\n",
      "Iteration 424275 | Loss: 0.262857\n",
      "Iteration 424300 | Loss: 0.262857\n",
      "Iteration 424325 | Loss: 0.262857\n",
      "Iteration 424350 | Loss: 0.262857\n",
      "Iteration 424375 | Loss: 0.262857\n",
      "Iteration 424400 | Loss: 0.262857\n",
      "Iteration 424425 | Loss: 0.262857\n",
      "Iteration 424450 | Loss: 0.262857\n",
      "Iteration 424475 | Loss: 0.262857\n",
      "Iteration 424500 | Loss: 0.262857\n",
      "Iteration 424525 | Loss: 0.262857\n",
      "Iteration 424550 | Loss: 0.262856\n",
      "Iteration 424575 | Loss: 0.262856\n",
      "Iteration 424600 | Loss: 0.262856\n",
      "Iteration 424625 | Loss: 0.262856\n",
      "Iteration 424650 | Loss: 0.262856\n",
      "Iteration 424675 | Loss: 0.262856\n",
      "Iteration 424700 | Loss: 0.262856\n",
      "Iteration 424725 | Loss: 0.262856\n",
      "Iteration 424750 | Loss: 0.262856\n",
      "Iteration 424775 | Loss: 0.262856\n",
      "Iteration 424800 | Loss: 0.262856\n",
      "Iteration 424825 | Loss: 0.262856\n",
      "Iteration 424850 | Loss: 0.262856\n",
      "Iteration 424875 | Loss: 0.262856\n",
      "Iteration 424900 | Loss: 0.262856\n",
      "Iteration 424925 | Loss: 0.262856\n",
      "Iteration 424950 | Loss: 0.262856\n",
      "Iteration 424975 | Loss: 0.262856\n",
      "Iteration 425000 | Loss: 0.262856\n",
      "Iteration 425025 | Loss: 0.262856\n",
      "Iteration 425050 | Loss: 0.262856\n",
      "Iteration 425075 | Loss: 0.262856\n",
      "Iteration 425100 | Loss: 0.262856\n",
      "Iteration 425125 | Loss: 0.262856\n",
      "Iteration 425150 | Loss: 0.262856\n",
      "Iteration 425175 | Loss: 0.262856\n",
      "Iteration 425200 | Loss: 0.262856\n",
      "Iteration 425225 | Loss: 0.262856\n",
      "Iteration 425250 | Loss: 0.262856\n",
      "Iteration 425275 | Loss: 0.262856\n",
      "Iteration 425300 | Loss: 0.262856\n",
      "Iteration 425325 | Loss: 0.262856\n",
      "Iteration 425350 | Loss: 0.262856\n",
      "Iteration 425375 | Loss: 0.262856\n",
      "Iteration 425400 | Loss: 0.262856\n",
      "Iteration 425425 | Loss: 0.262856\n",
      "Iteration 425450 | Loss: 0.262856\n",
      "Iteration 425475 | Loss: 0.262856\n",
      "Iteration 425500 | Loss: 0.262856\n",
      "Iteration 425525 | Loss: 0.262856\n",
      "Iteration 425550 | Loss: 0.262856\n",
      "Iteration 425575 | Loss: 0.262856\n",
      "Iteration 425600 | Loss: 0.262856\n",
      "Iteration 425625 | Loss: 0.262856\n",
      "Iteration 425650 | Loss: 0.262856\n",
      "Iteration 425675 | Loss: 0.262856\n",
      "Iteration 425700 | Loss: 0.262856\n",
      "Iteration 425725 | Loss: 0.262856\n",
      "Iteration 425750 | Loss: 0.262856\n",
      "Iteration 425775 | Loss: 0.262856\n",
      "Iteration 425800 | Loss: 0.262856\n",
      "Iteration 425825 | Loss: 0.262856\n",
      "Iteration 425850 | Loss: 0.262856\n",
      "Iteration 425875 | Loss: 0.262856\n",
      "Iteration 425900 | Loss: 0.262856\n",
      "Iteration 425925 | Loss: 0.262856\n",
      "Iteration 425950 | Loss: 0.262856\n",
      "Iteration 425975 | Loss: 0.262856\n",
      "Iteration 426000 | Loss: 0.262856\n",
      "Iteration 426025 | Loss: 0.262856\n",
      "Iteration 426050 | Loss: 0.262856\n",
      "Iteration 426075 | Loss: 0.262856\n",
      "Iteration 426100 | Loss: 0.262856\n",
      "Iteration 426125 | Loss: 0.262856\n",
      "Iteration 426150 | Loss: 0.262856\n",
      "Iteration 426175 | Loss: 0.262856\n",
      "Iteration 426200 | Loss: 0.262856\n",
      "Iteration 426225 | Loss: 0.262856\n",
      "Iteration 426250 | Loss: 0.262856\n",
      "Iteration 426275 | Loss: 0.262856\n",
      "Iteration 426300 | Loss: 0.262856\n",
      "Iteration 426325 | Loss: 0.262856\n",
      "Iteration 426350 | Loss: 0.262856\n",
      "Iteration 426375 | Loss: 0.262856\n",
      "Iteration 426400 | Loss: 0.262856\n",
      "Iteration 426425 | Loss: 0.262856\n",
      "Iteration 426450 | Loss: 0.262856\n",
      "Iteration 426475 | Loss: 0.262856\n",
      "Iteration 426500 | Loss: 0.262856\n",
      "Iteration 426525 | Loss: 0.262856\n",
      "Iteration 426550 | Loss: 0.262856\n",
      "Iteration 426575 | Loss: 0.262856\n",
      "Iteration 426600 | Loss: 0.262856\n",
      "Iteration 426625 | Loss: 0.262856\n",
      "Iteration 426650 | Loss: 0.262856\n",
      "Iteration 426675 | Loss: 0.262856\n",
      "Iteration 426700 | Loss: 0.262856\n",
      "Iteration 426725 | Loss: 0.262856\n",
      "Iteration 426750 | Loss: 0.262856\n",
      "Iteration 426775 | Loss: 0.262856\n",
      "Iteration 426800 | Loss: 0.262856\n",
      "Iteration 426825 | Loss: 0.262856\n",
      "Iteration 426850 | Loss: 0.262856\n",
      "Iteration 426875 | Loss: 0.262856\n",
      "Iteration 426900 | Loss: 0.262856\n",
      "Iteration 426925 | Loss: 0.262856\n",
      "Iteration 426950 | Loss: 0.262856\n",
      "Iteration 426975 | Loss: 0.262856\n",
      "Iteration 427000 | Loss: 0.262856\n",
      "Iteration 427025 | Loss: 0.262856\n",
      "Iteration 427050 | Loss: 0.262856\n",
      "Iteration 427075 | Loss: 0.262856\n",
      "Iteration 427100 | Loss: 0.262856\n",
      "Iteration 427125 | Loss: 0.262856\n",
      "Iteration 427150 | Loss: 0.262856\n",
      "Iteration 427175 | Loss: 0.262856\n",
      "Iteration 427200 | Loss: 0.262856\n",
      "Iteration 427225 | Loss: 0.262856\n",
      "Iteration 427250 | Loss: 0.262856\n",
      "Iteration 427275 | Loss: 0.262856\n",
      "Iteration 427300 | Loss: 0.262856\n",
      "Iteration 427325 | Loss: 0.262856\n",
      "Iteration 427350 | Loss: 0.262856\n",
      "Iteration 427375 | Loss: 0.262856\n",
      "Iteration 427400 | Loss: 0.262856\n",
      "Iteration 427425 | Loss: 0.262856\n",
      "Iteration 427450 | Loss: 0.262856\n",
      "Iteration 427475 | Loss: 0.262856\n",
      "Iteration 427500 | Loss: 0.262856\n",
      "Iteration 427525 | Loss: 0.262856\n",
      "Iteration 427550 | Loss: 0.262856\n",
      "Iteration 427575 | Loss: 0.262856\n",
      "Iteration 427600 | Loss: 0.262856\n",
      "Iteration 427625 | Loss: 0.262856\n",
      "Iteration 427650 | Loss: 0.262856\n",
      "Iteration 427675 | Loss: 0.262856\n",
      "Iteration 427700 | Loss: 0.262856\n",
      "Iteration 427725 | Loss: 0.262856\n",
      "Iteration 427750 | Loss: 0.262856\n",
      "Iteration 427775 | Loss: 0.262856\n",
      "Iteration 427800 | Loss: 0.262856\n",
      "Iteration 427825 | Loss: 0.262856\n",
      "Iteration 427850 | Loss: 0.262856\n",
      "Iteration 427875 | Loss: 0.262856\n",
      "Iteration 427900 | Loss: 0.262856\n",
      "Iteration 427925 | Loss: 0.262856\n",
      "Iteration 427950 | Loss: 0.262856\n",
      "Iteration 427975 | Loss: 0.262856\n",
      "Iteration 428000 | Loss: 0.262856\n",
      "Iteration 428025 | Loss: 0.262856\n",
      "Iteration 428050 | Loss: 0.262856\n",
      "Iteration 428075 | Loss: 0.262856\n",
      "Iteration 428100 | Loss: 0.262856\n",
      "Iteration 428125 | Loss: 0.262856\n",
      "Iteration 428150 | Loss: 0.262856\n",
      "Iteration 428175 | Loss: 0.262856\n",
      "Iteration 428200 | Loss: 0.262856\n",
      "Iteration 428225 | Loss: 0.262856\n",
      "Iteration 428250 | Loss: 0.262856\n",
      "Iteration 428275 | Loss: 0.262856\n",
      "Iteration 428300 | Loss: 0.262856\n",
      "Iteration 428325 | Loss: 0.262856\n",
      "Iteration 428350 | Loss: 0.262856\n",
      "Iteration 428375 | Loss: 0.262856\n",
      "Iteration 428400 | Loss: 0.262856\n",
      "Iteration 428425 | Loss: 0.262856\n",
      "Iteration 428450 | Loss: 0.262856\n",
      "Iteration 428475 | Loss: 0.262856\n",
      "Iteration 428500 | Loss: 0.262856\n",
      "Iteration 428525 | Loss: 0.262856\n",
      "Iteration 428550 | Loss: 0.262856\n",
      "Iteration 428575 | Loss: 0.262856\n",
      "Iteration 428600 | Loss: 0.262856\n",
      "Iteration 428625 | Loss: 0.262856\n",
      "Iteration 428650 | Loss: 0.262856\n",
      "Iteration 428675 | Loss: 0.262856\n",
      "Iteration 428700 | Loss: 0.262856\n",
      "Iteration 428725 | Loss: 0.262856\n",
      "Iteration 428750 | Loss: 0.262856\n",
      "Iteration 428775 | Loss: 0.262856\n",
      "Iteration 428800 | Loss: 0.262856\n",
      "Iteration 428825 | Loss: 0.262856\n",
      "Iteration 428850 | Loss: 0.262856\n",
      "Iteration 428875 | Loss: 0.262856\n",
      "Iteration 428900 | Loss: 0.262856\n",
      "Iteration 428925 | Loss: 0.262856\n",
      "Iteration 428950 | Loss: 0.262856\n",
      "Iteration 428975 | Loss: 0.262856\n",
      "Iteration 429000 | Loss: 0.262856\n",
      "Iteration 429025 | Loss: 0.262856\n",
      "Iteration 429050 | Loss: 0.262856\n",
      "Iteration 429075 | Loss: 0.262856\n",
      "Iteration 429100 | Loss: 0.262856\n",
      "Iteration 429125 | Loss: 0.262856\n",
      "Iteration 429150 | Loss: 0.262856\n",
      "Iteration 429175 | Loss: 0.262856\n",
      "Iteration 429200 | Loss: 0.262856\n",
      "Iteration 429225 | Loss: 0.262856\n",
      "Iteration 429250 | Loss: 0.262856\n",
      "Iteration 429275 | Loss: 0.262856\n",
      "Iteration 429300 | Loss: 0.262856\n",
      "Iteration 429325 | Loss: 0.262856\n",
      "Iteration 429350 | Loss: 0.262856\n",
      "Iteration 429375 | Loss: 0.262856\n",
      "Iteration 429400 | Loss: 0.262856\n",
      "Iteration 429425 | Loss: 0.262856\n",
      "Iteration 429450 | Loss: 0.262856\n",
      "Iteration 429475 | Loss: 0.262856\n",
      "Iteration 429500 | Loss: 0.262856\n",
      "Iteration 429525 | Loss: 0.262856\n",
      "Iteration 429550 | Loss: 0.262856\n",
      "Iteration 429575 | Loss: 0.262856\n",
      "Iteration 429600 | Loss: 0.262856\n",
      "Iteration 429625 | Loss: 0.262856\n",
      "Iteration 429650 | Loss: 0.262856\n",
      "Iteration 429675 | Loss: 0.262856\n",
      "Iteration 429700 | Loss: 0.262856\n",
      "Iteration 429725 | Loss: 0.262856\n",
      "Iteration 429750 | Loss: 0.262856\n",
      "Iteration 429775 | Loss: 0.262856\n",
      "Iteration 429800 | Loss: 0.262856\n",
      "Iteration 429825 | Loss: 0.262856\n",
      "Iteration 429850 | Loss: 0.262856\n",
      "Iteration 429875 | Loss: 0.262856\n",
      "Iteration 429900 | Loss: 0.262856\n",
      "Iteration 429925 | Loss: 0.262856\n",
      "Iteration 429950 | Loss: 0.262856\n",
      "Iteration 429975 | Loss: 0.262856\n",
      "Iteration 430000 | Loss: 0.262856\n",
      "Iteration 430025 | Loss: 0.262856\n",
      "Iteration 430050 | Loss: 0.262856\n",
      "Iteration 430075 | Loss: 0.262856\n",
      "Iteration 430100 | Loss: 0.262856\n",
      "Iteration 430125 | Loss: 0.262856\n",
      "Iteration 430150 | Loss: 0.262856\n",
      "Iteration 430175 | Loss: 0.262856\n",
      "Iteration 430200 | Loss: 0.262856\n",
      "Iteration 430225 | Loss: 0.262856\n",
      "Iteration 430250 | Loss: 0.262856\n",
      "Iteration 430275 | Loss: 0.262856\n",
      "Iteration 430300 | Loss: 0.262856\n",
      "Iteration 430325 | Loss: 0.262856\n",
      "Iteration 430350 | Loss: 0.262856\n",
      "Iteration 430375 | Loss: 0.262856\n",
      "Iteration 430400 | Loss: 0.262856\n",
      "Iteration 430425 | Loss: 0.262856\n",
      "Iteration 430450 | Loss: 0.262856\n",
      "Iteration 430475 | Loss: 0.262856\n",
      "Iteration 430500 | Loss: 0.262856\n",
      "Iteration 430525 | Loss: 0.262856\n",
      "Iteration 430550 | Loss: 0.262856\n",
      "Iteration 430575 | Loss: 0.262856\n",
      "Iteration 430600 | Loss: 0.262856\n",
      "Iteration 430625 | Loss: 0.262856\n",
      "Iteration 430650 | Loss: 0.262856\n",
      "Iteration 430675 | Loss: 0.262856\n",
      "Iteration 430700 | Loss: 0.262856\n",
      "Iteration 430725 | Loss: 0.262856\n",
      "Iteration 430750 | Loss: 0.262856\n",
      "Iteration 430775 | Loss: 0.262856\n",
      "Iteration 430800 | Loss: 0.262856\n",
      "Iteration 430825 | Loss: 0.262856\n",
      "Iteration 430850 | Loss: 0.262856\n",
      "Iteration 430875 | Loss: 0.262856\n",
      "Iteration 430900 | Loss: 0.262856\n",
      "Iteration 430925 | Loss: 0.262856\n",
      "Iteration 430950 | Loss: 0.262856\n",
      "Iteration 430975 | Loss: 0.262856\n",
      "Iteration 431000 | Loss: 0.262856\n",
      "Iteration 431025 | Loss: 0.262856\n",
      "Iteration 431050 | Loss: 0.262856\n",
      "Iteration 431075 | Loss: 0.262856\n",
      "Iteration 431100 | Loss: 0.262856\n",
      "Iteration 431125 | Loss: 0.262856\n",
      "Iteration 431150 | Loss: 0.262856\n",
      "Iteration 431175 | Loss: 0.262856\n",
      "Iteration 431200 | Loss: 0.262856\n",
      "Iteration 431225 | Loss: 0.262856\n",
      "Iteration 431250 | Loss: 0.262856\n",
      "Iteration 431275 | Loss: 0.262856\n",
      "Iteration 431300 | Loss: 0.262856\n",
      "Iteration 431325 | Loss: 0.262856\n",
      "Iteration 431350 | Loss: 0.262856\n",
      "Iteration 431375 | Loss: 0.262856\n",
      "Iteration 431400 | Loss: 0.262856\n",
      "Iteration 431425 | Loss: 0.262856\n",
      "Iteration 431450 | Loss: 0.262856\n",
      "Iteration 431475 | Loss: 0.262856\n",
      "Iteration 431500 | Loss: 0.262856\n",
      "Iteration 431525 | Loss: 0.262856\n",
      "Iteration 431550 | Loss: 0.262856\n",
      "Iteration 431575 | Loss: 0.262856\n",
      "Iteration 431600 | Loss: 0.262856\n",
      "Iteration 431625 | Loss: 0.262856\n",
      "Iteration 431650 | Loss: 0.262856\n",
      "Iteration 431675 | Loss: 0.262856\n",
      "Iteration 431700 | Loss: 0.262856\n",
      "Iteration 431725 | Loss: 0.262856\n",
      "Iteration 431750 | Loss: 0.262856\n",
      "Iteration 431775 | Loss: 0.262856\n",
      "Iteration 431800 | Loss: 0.262856\n",
      "Iteration 431825 | Loss: 0.262856\n",
      "Iteration 431850 | Loss: 0.262856\n",
      "Iteration 431875 | Loss: 0.262856\n",
      "Iteration 431900 | Loss: 0.262856\n",
      "Iteration 431925 | Loss: 0.262856\n",
      "Iteration 431950 | Loss: 0.262856\n",
      "Iteration 431975 | Loss: 0.262856\n",
      "Iteration 432000 | Loss: 0.262856\n",
      "Iteration 432025 | Loss: 0.262856\n",
      "Iteration 432050 | Loss: 0.262856\n",
      "Iteration 432075 | Loss: 0.262856\n",
      "Iteration 432100 | Loss: 0.262856\n",
      "Iteration 432125 | Loss: 0.262856\n",
      "Iteration 432150 | Loss: 0.262856\n",
      "Iteration 432175 | Loss: 0.262856\n",
      "Iteration 432200 | Loss: 0.262856\n",
      "Iteration 432225 | Loss: 0.262856\n",
      "Iteration 432250 | Loss: 0.262856\n",
      "Iteration 432275 | Loss: 0.262856\n",
      "Iteration 432300 | Loss: 0.262856\n",
      "Iteration 432325 | Loss: 0.262856\n",
      "Iteration 432350 | Loss: 0.262856\n",
      "Iteration 432375 | Loss: 0.262856\n",
      "Iteration 432400 | Loss: 0.262856\n",
      "Iteration 432425 | Loss: 0.262856\n",
      "Iteration 432450 | Loss: 0.262856\n",
      "Iteration 432475 | Loss: 0.262856\n",
      "Iteration 432500 | Loss: 0.262856\n",
      "Iteration 432525 | Loss: 0.262856\n",
      "Iteration 432550 | Loss: 0.262856\n",
      "Iteration 432575 | Loss: 0.262856\n",
      "Iteration 432600 | Loss: 0.262856\n",
      "Iteration 432625 | Loss: 0.262856\n",
      "Iteration 432650 | Loss: 0.262856\n",
      "Iteration 432675 | Loss: 0.262856\n",
      "Iteration 432700 | Loss: 0.262856\n",
      "Iteration 432725 | Loss: 0.262856\n",
      "Iteration 432750 | Loss: 0.262856\n",
      "Iteration 432775 | Loss: 0.262856\n",
      "Iteration 432800 | Loss: 0.262856\n",
      "Iteration 432825 | Loss: 0.262856\n",
      "Iteration 432850 | Loss: 0.262856\n",
      "Iteration 432875 | Loss: 0.262856\n",
      "Iteration 432900 | Loss: 0.262856\n",
      "Iteration 432925 | Loss: 0.262856\n",
      "Iteration 432950 | Loss: 0.262856\n",
      "Iteration 432975 | Loss: 0.262856\n",
      "Iteration 433000 | Loss: 0.262856\n",
      "Iteration 433025 | Loss: 0.262856\n",
      "Iteration 433050 | Loss: 0.262856\n",
      "Iteration 433075 | Loss: 0.262856\n",
      "Iteration 433100 | Loss: 0.262856\n",
      "Iteration 433125 | Loss: 0.262856\n",
      "Iteration 433150 | Loss: 0.262856\n",
      "Iteration 433175 | Loss: 0.262856\n",
      "Iteration 433200 | Loss: 0.262856\n",
      "Iteration 433225 | Loss: 0.262856\n",
      "Iteration 433250 | Loss: 0.262856\n",
      "Iteration 433275 | Loss: 0.262856\n",
      "Iteration 433300 | Loss: 0.262856\n",
      "Iteration 433325 | Loss: 0.262856\n",
      "Iteration 433350 | Loss: 0.262856\n",
      "Iteration 433375 | Loss: 0.262856\n",
      "Iteration 433400 | Loss: 0.262856\n",
      "Iteration 433425 | Loss: 0.262856\n",
      "Iteration 433450 | Loss: 0.262856\n",
      "Iteration 433475 | Loss: 0.262856\n",
      "Iteration 433500 | Loss: 0.262856\n",
      "Iteration 433525 | Loss: 0.262856\n",
      "Iteration 433550 | Loss: 0.262856\n",
      "Iteration 433575 | Loss: 0.262856\n",
      "Iteration 433600 | Loss: 0.262856\n",
      "Iteration 433625 | Loss: 0.262856\n",
      "Iteration 433650 | Loss: 0.262856\n",
      "Iteration 433675 | Loss: 0.262856\n",
      "Iteration 433700 | Loss: 0.262856\n",
      "Iteration 433725 | Loss: 0.262856\n",
      "Iteration 433750 | Loss: 0.262856\n",
      "Iteration 433775 | Loss: 0.262856\n",
      "Iteration 433800 | Loss: 0.262856\n",
      "Iteration 433825 | Loss: 0.262856\n",
      "Iteration 433850 | Loss: 0.262856\n",
      "Iteration 433875 | Loss: 0.262856\n",
      "Iteration 433900 | Loss: 0.262856\n",
      "Iteration 433925 | Loss: 0.262856\n",
      "Iteration 433950 | Loss: 0.262856\n",
      "Iteration 433975 | Loss: 0.262856\n",
      "Iteration 434000 | Loss: 0.262856\n",
      "Iteration 434025 | Loss: 0.262856\n",
      "Iteration 434050 | Loss: 0.262856\n",
      "Iteration 434075 | Loss: 0.262856\n",
      "Iteration 434100 | Loss: 0.262856\n",
      "Iteration 434125 | Loss: 0.262856\n",
      "Iteration 434150 | Loss: 0.262856\n",
      "Iteration 434175 | Loss: 0.262856\n",
      "Iteration 434200 | Loss: 0.262856\n",
      "Iteration 434225 | Loss: 0.262856\n",
      "Iteration 434250 | Loss: 0.262856\n",
      "Iteration 434275 | Loss: 0.262856\n",
      "Iteration 434300 | Loss: 0.262856\n",
      "Iteration 434325 | Loss: 0.262856\n",
      "Iteration 434350 | Loss: 0.262856\n",
      "Iteration 434375 | Loss: 0.262856\n",
      "Iteration 434400 | Loss: 0.262856\n",
      "Iteration 434425 | Loss: 0.262856\n",
      "Iteration 434450 | Loss: 0.262856\n",
      "Iteration 434475 | Loss: 0.262856\n",
      "Iteration 434500 | Loss: 0.262856\n",
      "Iteration 434525 | Loss: 0.262856\n",
      "Iteration 434550 | Loss: 0.262856\n",
      "Iteration 434575 | Loss: 0.262856\n",
      "Iteration 434600 | Loss: 0.262856\n",
      "Iteration 434625 | Loss: 0.262856\n",
      "Iteration 434650 | Loss: 0.262856\n",
      "Iteration 434675 | Loss: 0.262856\n",
      "Iteration 434700 | Loss: 0.262856\n",
      "Iteration 434725 | Loss: 0.262856\n",
      "Iteration 434750 | Loss: 0.262856\n",
      "Iteration 434775 | Loss: 0.262856\n",
      "Iteration 434800 | Loss: 0.262856\n",
      "Iteration 434825 | Loss: 0.262856\n",
      "Iteration 434850 | Loss: 0.262856\n",
      "Iteration 434875 | Loss: 0.262856\n",
      "Iteration 434900 | Loss: 0.262856\n",
      "Iteration 434925 | Loss: 0.262856\n",
      "Iteration 434950 | Loss: 0.262856\n",
      "Iteration 434975 | Loss: 0.262856\n",
      "Iteration 435000 | Loss: 0.262856\n",
      "Iteration 435025 | Loss: 0.262856\n",
      "Iteration 435050 | Loss: 0.262856\n",
      "Iteration 435075 | Loss: 0.262856\n",
      "Iteration 435100 | Loss: 0.262856\n",
      "Iteration 435125 | Loss: 0.262856\n",
      "Iteration 435150 | Loss: 0.262856\n",
      "Iteration 435175 | Loss: 0.262856\n",
      "Iteration 435200 | Loss: 0.262856\n",
      "Iteration 435225 | Loss: 0.262856\n",
      "Iteration 435250 | Loss: 0.262856\n",
      "Iteration 435275 | Loss: 0.262856\n",
      "Iteration 435300 | Loss: 0.262856\n",
      "Iteration 435325 | Loss: 0.262856\n",
      "Iteration 435350 | Loss: 0.262856\n",
      "Iteration 435375 | Loss: 0.262856\n",
      "Iteration 435400 | Loss: 0.262856\n",
      "Iteration 435425 | Loss: 0.262856\n",
      "Iteration 435450 | Loss: 0.262856\n",
      "Iteration 435475 | Loss: 0.262856\n",
      "Iteration 435500 | Loss: 0.262856\n",
      "Iteration 435525 | Loss: 0.262856\n",
      "Iteration 435550 | Loss: 0.262856\n",
      "Iteration 435575 | Loss: 0.262856\n",
      "Iteration 435600 | Loss: 0.262856\n",
      "Iteration 435625 | Loss: 0.262856\n",
      "Iteration 435650 | Loss: 0.262856\n",
      "Iteration 435675 | Loss: 0.262856\n",
      "Iteration 435700 | Loss: 0.262856\n",
      "Iteration 435725 | Loss: 0.262856\n",
      "Iteration 435750 | Loss: 0.262856\n",
      "Iteration 435775 | Loss: 0.262856\n",
      "Iteration 435800 | Loss: 0.262856\n",
      "Iteration 435825 | Loss: 0.262856\n",
      "Iteration 435850 | Loss: 0.262856\n",
      "Iteration 435875 | Loss: 0.262856\n",
      "Iteration 435900 | Loss: 0.262856\n",
      "Iteration 435925 | Loss: 0.262856\n",
      "Iteration 435950 | Loss: 0.262856\n",
      "Iteration 435975 | Loss: 0.262856\n",
      "Iteration 436000 | Loss: 0.262856\n",
      "Iteration 436025 | Loss: 0.262856\n",
      "Iteration 436050 | Loss: 0.262856\n",
      "Iteration 436075 | Loss: 0.262856\n",
      "Iteration 436100 | Loss: 0.262856\n",
      "Iteration 436125 | Loss: 0.262856\n",
      "Iteration 436150 | Loss: 0.262856\n",
      "Iteration 436175 | Loss: 0.262856\n",
      "Iteration 436200 | Loss: 0.262856\n",
      "Iteration 436225 | Loss: 0.262856\n",
      "Iteration 436250 | Loss: 0.262856\n",
      "Iteration 436275 | Loss: 0.262856\n",
      "Iteration 436300 | Loss: 0.262856\n",
      "Iteration 436325 | Loss: 0.262856\n",
      "Iteration 436350 | Loss: 0.262856\n",
      "Iteration 436375 | Loss: 0.262856\n",
      "Iteration 436400 | Loss: 0.262856\n",
      "Iteration 436425 | Loss: 0.262856\n",
      "Iteration 436450 | Loss: 0.262856\n",
      "Iteration 436475 | Loss: 0.262856\n",
      "Iteration 436500 | Loss: 0.262856\n",
      "Iteration 436525 | Loss: 0.262856\n",
      "Iteration 436550 | Loss: 0.262856\n",
      "Iteration 436575 | Loss: 0.262856\n",
      "Iteration 436600 | Loss: 0.262856\n",
      "Iteration 436625 | Loss: 0.262856\n",
      "Iteration 436650 | Loss: 0.262856\n",
      "Iteration 436675 | Loss: 0.262856\n",
      "Iteration 436700 | Loss: 0.262856\n",
      "Iteration 436725 | Loss: 0.262856\n",
      "Iteration 436750 | Loss: 0.262856\n",
      "Iteration 436775 | Loss: 0.262856\n",
      "Iteration 436800 | Loss: 0.262856\n",
      "Iteration 436825 | Loss: 0.262856\n",
      "Iteration 436850 | Loss: 0.262856\n",
      "Iteration 436875 | Loss: 0.262856\n",
      "Iteration 436900 | Loss: 0.262856\n",
      "Iteration 436925 | Loss: 0.262856\n",
      "Iteration 436950 | Loss: 0.262856\n",
      "Iteration 436975 | Loss: 0.262856\n",
      "Iteration 437000 | Loss: 0.262856\n",
      "Iteration 437025 | Loss: 0.262856\n",
      "Iteration 437050 | Loss: 0.262856\n",
      "Iteration 437075 | Loss: 0.262856\n",
      "Iteration 437100 | Loss: 0.262856\n",
      "Iteration 437125 | Loss: 0.262856\n",
      "Iteration 437150 | Loss: 0.262856\n",
      "Iteration 437175 | Loss: 0.262856\n",
      "Iteration 437200 | Loss: 0.262856\n",
      "Iteration 437225 | Loss: 0.262856\n",
      "Iteration 437250 | Loss: 0.262856\n",
      "Iteration 437275 | Loss: 0.262856\n",
      "Iteration 437300 | Loss: 0.262856\n",
      "Iteration 437325 | Loss: 0.262856\n",
      "Iteration 437350 | Loss: 0.262856\n",
      "Iteration 437375 | Loss: 0.262856\n",
      "Iteration 437400 | Loss: 0.262856\n",
      "Iteration 437425 | Loss: 0.262856\n",
      "Iteration 437450 | Loss: 0.262856\n",
      "Iteration 437475 | Loss: 0.262856\n",
      "Iteration 437500 | Loss: 0.262856\n",
      "Iteration 437525 | Loss: 0.262856\n",
      "Iteration 437550 | Loss: 0.262856\n",
      "Iteration 437575 | Loss: 0.262856\n",
      "Iteration 437600 | Loss: 0.262856\n",
      "Iteration 437625 | Loss: 0.262856\n",
      "Iteration 437650 | Loss: 0.262856\n",
      "Iteration 437675 | Loss: 0.262856\n",
      "Iteration 437700 | Loss: 0.262856\n",
      "Iteration 437725 | Loss: 0.262856\n",
      "Iteration 437750 | Loss: 0.262856\n",
      "Iteration 437775 | Loss: 0.262856\n",
      "Iteration 437800 | Loss: 0.262856\n",
      "Iteration 437825 | Loss: 0.262856\n",
      "Iteration 437850 | Loss: 0.262856\n",
      "Iteration 437875 | Loss: 0.262856\n",
      "Iteration 437900 | Loss: 0.262856\n",
      "Iteration 437925 | Loss: 0.262856\n",
      "Iteration 437950 | Loss: 0.262856\n",
      "Iteration 437975 | Loss: 0.262856\n",
      "Iteration 438000 | Loss: 0.262856\n",
      "Iteration 438025 | Loss: 0.262856\n",
      "Iteration 438050 | Loss: 0.262856\n",
      "Iteration 438075 | Loss: 0.262856\n",
      "Iteration 438100 | Loss: 0.262856\n",
      "Iteration 438125 | Loss: 0.262856\n",
      "Iteration 438150 | Loss: 0.262856\n",
      "Iteration 438175 | Loss: 0.262856\n",
      "Iteration 438200 | Loss: 0.262856\n",
      "Iteration 438225 | Loss: 0.262856\n",
      "Iteration 438250 | Loss: 0.262856\n",
      "Iteration 438275 | Loss: 0.262856\n",
      "Iteration 438300 | Loss: 0.262856\n",
      "Iteration 438325 | Loss: 0.262856\n",
      "Iteration 438350 | Loss: 0.262856\n",
      "Iteration 438375 | Loss: 0.262856\n",
      "Iteration 438400 | Loss: 0.262856\n",
      "Iteration 438425 | Loss: 0.262856\n",
      "Iteration 438450 | Loss: 0.262856\n",
      "Iteration 438475 | Loss: 0.262856\n",
      "Iteration 438500 | Loss: 0.262856\n",
      "Iteration 438525 | Loss: 0.262856\n",
      "Iteration 438550 | Loss: 0.262856\n",
      "Iteration 438575 | Loss: 0.262856\n",
      "Iteration 438600 | Loss: 0.262856\n",
      "Iteration 438625 | Loss: 0.262856\n",
      "Iteration 438650 | Loss: 0.262856\n",
      "Iteration 438675 | Loss: 0.262856\n",
      "Iteration 438700 | Loss: 0.262856\n",
      "Iteration 438725 | Loss: 0.262856\n",
      "Iteration 438750 | Loss: 0.262856\n",
      "Iteration 438775 | Loss: 0.262856\n",
      "Iteration 438800 | Loss: 0.262856\n",
      "Iteration 438825 | Loss: 0.262856\n",
      "Iteration 438850 | Loss: 0.262856\n",
      "Iteration 438875 | Loss: 0.262856\n",
      "Iteration 438900 | Loss: 0.262856\n",
      "Iteration 438925 | Loss: 0.262856\n",
      "Iteration 438950 | Loss: 0.262856\n",
      "Iteration 438975 | Loss: 0.262856\n",
      "Iteration 439000 | Loss: 0.262856\n",
      "Iteration 439025 | Loss: 0.262856\n",
      "Iteration 439050 | Loss: 0.262856\n",
      "Iteration 439075 | Loss: 0.262856\n",
      "Iteration 439100 | Loss: 0.262856\n",
      "Iteration 439125 | Loss: 0.262856\n",
      "Iteration 439150 | Loss: 0.262856\n",
      "Iteration 439175 | Loss: 0.262856\n",
      "Iteration 439200 | Loss: 0.262856\n",
      "Iteration 439225 | Loss: 0.262856\n",
      "Iteration 439250 | Loss: 0.262856\n",
      "Iteration 439275 | Loss: 0.262856\n",
      "Iteration 439300 | Loss: 0.262856\n",
      "Iteration 439325 | Loss: 0.262856\n",
      "Iteration 439350 | Loss: 0.262856\n",
      "Iteration 439375 | Loss: 0.262856\n",
      "Iteration 439400 | Loss: 0.262856\n",
      "Iteration 439425 | Loss: 0.262856\n",
      "Iteration 439450 | Loss: 0.262856\n",
      "Iteration 439475 | Loss: 0.262856\n",
      "Iteration 439500 | Loss: 0.262856\n",
      "Iteration 439525 | Loss: 0.262856\n",
      "Iteration 439550 | Loss: 0.262856\n",
      "Iteration 439575 | Loss: 0.262856\n",
      "Iteration 439600 | Loss: 0.262856\n",
      "Iteration 439625 | Loss: 0.262856\n",
      "Iteration 439650 | Loss: 0.262856\n",
      "Iteration 439675 | Loss: 0.262856\n",
      "Iteration 439700 | Loss: 0.262856\n",
      "Iteration 439725 | Loss: 0.262856\n",
      "Iteration 439750 | Loss: 0.262856\n",
      "Iteration 439775 | Loss: 0.262856\n",
      "Iteration 439800 | Loss: 0.262856\n",
      "Iteration 439825 | Loss: 0.262856\n",
      "Iteration 439850 | Loss: 0.262856\n",
      "Iteration 439875 | Loss: 0.262856\n",
      "Iteration 439900 | Loss: 0.262856\n",
      "Iteration 439925 | Loss: 0.262856\n",
      "Iteration 439950 | Loss: 0.262856\n",
      "Iteration 439975 | Loss: 0.262856\n",
      "Iteration 440000 | Loss: 0.262856\n",
      "Iteration 440025 | Loss: 0.262856\n",
      "Iteration 440050 | Loss: 0.262856\n",
      "Iteration 440075 | Loss: 0.262856\n",
      "Iteration 440100 | Loss: 0.262856\n",
      "Iteration 440125 | Loss: 0.262856\n",
      "Iteration 440150 | Loss: 0.262856\n",
      "Iteration 440175 | Loss: 0.262856\n",
      "Iteration 440200 | Loss: 0.262856\n",
      "Iteration 440225 | Loss: 0.262856\n",
      "Iteration 440250 | Loss: 0.262856\n",
      "Iteration 440275 | Loss: 0.262856\n",
      "Iteration 440300 | Loss: 0.262856\n",
      "Iteration 440325 | Loss: 0.262856\n",
      "Iteration 440350 | Loss: 0.262856\n",
      "Iteration 440375 | Loss: 0.262856\n",
      "Iteration 440400 | Loss: 0.262856\n",
      "Iteration 440425 | Loss: 0.262856\n",
      "Iteration 440450 | Loss: 0.262856\n",
      "Iteration 440475 | Loss: 0.262856\n",
      "Iteration 440500 | Loss: 0.262856\n",
      "Iteration 440525 | Loss: 0.262856\n",
      "Iteration 440550 | Loss: 0.262856\n",
      "Iteration 440575 | Loss: 0.262856\n",
      "Iteration 440600 | Loss: 0.262856\n",
      "Iteration 440625 | Loss: 0.262856\n",
      "Iteration 440650 | Loss: 0.262856\n",
      "Iteration 440675 | Loss: 0.262856\n",
      "Iteration 440700 | Loss: 0.262856\n",
      "Iteration 440725 | Loss: 0.262856\n",
      "Iteration 440750 | Loss: 0.262856\n",
      "Iteration 440775 | Loss: 0.262856\n",
      "Iteration 440800 | Loss: 0.262856\n",
      "Iteration 440825 | Loss: 0.262856\n",
      "Iteration 440850 | Loss: 0.262856\n",
      "Iteration 440875 | Loss: 0.262856\n",
      "Iteration 440900 | Loss: 0.262856\n",
      "Iteration 440925 | Loss: 0.262856\n",
      "Iteration 440950 | Loss: 0.262856\n",
      "Iteration 440975 | Loss: 0.262856\n",
      "Iteration 441000 | Loss: 0.262856\n",
      "Iteration 441025 | Loss: 0.262856\n",
      "Iteration 441050 | Loss: 0.262856\n",
      "Iteration 441075 | Loss: 0.262856\n",
      "Iteration 441100 | Loss: 0.262856\n",
      "Iteration 441125 | Loss: 0.262856\n",
      "Iteration 441150 | Loss: 0.262856\n",
      "Iteration 441175 | Loss: 0.262856\n",
      "Iteration 441200 | Loss: 0.262856\n",
      "Iteration 441225 | Loss: 0.262856\n",
      "Iteration 441250 | Loss: 0.262856\n",
      "Iteration 441275 | Loss: 0.262856\n",
      "Iteration 441300 | Loss: 0.262856\n",
      "Iteration 441325 | Loss: 0.262856\n",
      "Iteration 441350 | Loss: 0.262856\n",
      "Iteration 441375 | Loss: 0.262856\n",
      "Iteration 441400 | Loss: 0.262856\n",
      "Iteration 441425 | Loss: 0.262856\n",
      "Iteration 441450 | Loss: 0.262856\n",
      "Iteration 441475 | Loss: 0.262856\n",
      "Iteration 441500 | Loss: 0.262856\n",
      "Iteration 441525 | Loss: 0.262856\n",
      "Iteration 441550 | Loss: 0.262856\n",
      "Iteration 441575 | Loss: 0.262856\n",
      "Iteration 441600 | Loss: 0.262856\n",
      "Iteration 441625 | Loss: 0.262856\n",
      "Iteration 441650 | Loss: 0.262856\n",
      "Iteration 441675 | Loss: 0.262856\n",
      "Iteration 441700 | Loss: 0.262856\n",
      "Iteration 441725 | Loss: 0.262856\n",
      "Iteration 441750 | Loss: 0.262856\n",
      "Iteration 441775 | Loss: 0.262856\n",
      "Iteration 441800 | Loss: 0.262856\n",
      "Iteration 441825 | Loss: 0.262856\n",
      "Iteration 441850 | Loss: 0.262856\n",
      "Iteration 441875 | Loss: 0.262856\n",
      "Iteration 441900 | Loss: 0.262856\n",
      "Iteration 441925 | Loss: 0.262856\n",
      "Iteration 441950 | Loss: 0.262856\n",
      "Iteration 441975 | Loss: 0.262856\n",
      "Iteration 442000 | Loss: 0.262856\n",
      "Iteration 442025 | Loss: 0.262856\n",
      "Iteration 442050 | Loss: 0.262856\n",
      "Iteration 442075 | Loss: 0.262856\n",
      "Iteration 442100 | Loss: 0.262856\n",
      "Iteration 442125 | Loss: 0.262856\n",
      "Iteration 442150 | Loss: 0.262856\n",
      "Iteration 442175 | Loss: 0.262856\n",
      "Iteration 442200 | Loss: 0.262856\n",
      "Iteration 442225 | Loss: 0.262856\n",
      "Iteration 442250 | Loss: 0.262856\n",
      "Iteration 442275 | Loss: 0.262856\n",
      "Iteration 442300 | Loss: 0.262856\n",
      "Iteration 442325 | Loss: 0.262856\n",
      "Iteration 442350 | Loss: 0.262856\n",
      "Iteration 442375 | Loss: 0.262856\n",
      "Iteration 442400 | Loss: 0.262856\n",
      "Iteration 442425 | Loss: 0.262856\n",
      "Iteration 442450 | Loss: 0.262856\n",
      "Iteration 442475 | Loss: 0.262856\n",
      "Iteration 442500 | Loss: 0.262856\n",
      "Iteration 442525 | Loss: 0.262856\n",
      "Iteration 442550 | Loss: 0.262856\n",
      "Iteration 442575 | Loss: 0.262856\n",
      "Iteration 442600 | Loss: 0.262856\n",
      "Iteration 442625 | Loss: 0.262856\n",
      "Iteration 442650 | Loss: 0.262856\n",
      "Iteration 442675 | Loss: 0.262856\n",
      "Iteration 442700 | Loss: 0.262856\n",
      "Iteration 442725 | Loss: 0.262856\n",
      "Iteration 442750 | Loss: 0.262856\n",
      "Iteration 442775 | Loss: 0.262856\n",
      "Iteration 442800 | Loss: 0.262856\n",
      "Iteration 442825 | Loss: 0.262856\n",
      "Iteration 442850 | Loss: 0.262856\n",
      "Iteration 442875 | Loss: 0.262856\n",
      "Iteration 442900 | Loss: 0.262856\n",
      "Iteration 442925 | Loss: 0.262856\n",
      "Iteration 442950 | Loss: 0.262856\n",
      "Iteration 442975 | Loss: 0.262856\n",
      "Iteration 443000 | Loss: 0.262856\n",
      "Iteration 443025 | Loss: 0.262856\n",
      "Iteration 443050 | Loss: 0.262856\n",
      "Iteration 443075 | Loss: 0.262856\n",
      "Iteration 443100 | Loss: 0.262856\n",
      "Iteration 443125 | Loss: 0.262856\n",
      "Iteration 443150 | Loss: 0.262856\n",
      "Iteration 443175 | Loss: 0.262856\n",
      "Iteration 443200 | Loss: 0.262856\n",
      "Iteration 443225 | Loss: 0.262856\n",
      "Iteration 443250 | Loss: 0.262856\n",
      "Iteration 443275 | Loss: 0.262856\n",
      "Iteration 443300 | Loss: 0.262856\n",
      "Iteration 443325 | Loss: 0.262856\n",
      "Iteration 443350 | Loss: 0.262856\n",
      "Iteration 443375 | Loss: 0.262856\n",
      "Iteration 443400 | Loss: 0.262856\n",
      "Iteration 443425 | Loss: 0.262856\n",
      "Iteration 443450 | Loss: 0.262856\n",
      "Iteration 443475 | Loss: 0.262856\n",
      "Iteration 443500 | Loss: 0.262856\n",
      "Iteration 443525 | Loss: 0.262856\n",
      "Iteration 443550 | Loss: 0.262856\n",
      "Iteration 443575 | Loss: 0.262856\n",
      "Iteration 443600 | Loss: 0.262856\n",
      "Iteration 443625 | Loss: 0.262856\n",
      "Iteration 443650 | Loss: 0.262856\n",
      "Iteration 443675 | Loss: 0.262856\n",
      "Iteration 443700 | Loss: 0.262856\n",
      "Iteration 443725 | Loss: 0.262856\n",
      "Iteration 443750 | Loss: 0.262856\n",
      "Iteration 443775 | Loss: 0.262856\n",
      "Iteration 443800 | Loss: 0.262856\n",
      "Iteration 443825 | Loss: 0.262856\n",
      "Iteration 443850 | Loss: 0.262856\n",
      "Iteration 443875 | Loss: 0.262856\n",
      "Iteration 443900 | Loss: 0.262856\n",
      "Iteration 443925 | Loss: 0.262856\n",
      "Iteration 443950 | Loss: 0.262856\n",
      "Iteration 443975 | Loss: 0.262856\n",
      "Iteration 444000 | Loss: 0.262856\n",
      "Iteration 444025 | Loss: 0.262856\n",
      "Iteration 444050 | Loss: 0.262856\n",
      "Iteration 444075 | Loss: 0.262856\n",
      "Iteration 444100 | Loss: 0.262856\n",
      "Iteration 444125 | Loss: 0.262856\n",
      "Iteration 444150 | Loss: 0.262856\n",
      "Iteration 444175 | Loss: 0.262856\n",
      "Iteration 444200 | Loss: 0.262856\n",
      "Iteration 444225 | Loss: 0.262856\n",
      "Iteration 444250 | Loss: 0.262856\n",
      "Iteration 444275 | Loss: 0.262856\n",
      "Iteration 444300 | Loss: 0.262856\n",
      "Iteration 444325 | Loss: 0.262856\n",
      "Iteration 444350 | Loss: 0.262856\n",
      "Iteration 444375 | Loss: 0.262856\n",
      "Iteration 444400 | Loss: 0.262856\n",
      "Iteration 444425 | Loss: 0.262856\n",
      "Iteration 444450 | Loss: 0.262856\n",
      "Iteration 444475 | Loss: 0.262856\n",
      "Iteration 444500 | Loss: 0.262856\n",
      "Iteration 444525 | Loss: 0.262856\n",
      "Iteration 444550 | Loss: 0.262856\n",
      "Iteration 444575 | Loss: 0.262856\n",
      "Iteration 444600 | Loss: 0.262856\n",
      "Iteration 444625 | Loss: 0.262856\n",
      "Iteration 444650 | Loss: 0.262856\n",
      "Iteration 444675 | Loss: 0.262856\n",
      "Iteration 444700 | Loss: 0.262856\n",
      "Iteration 444725 | Loss: 0.262856\n",
      "Iteration 444750 | Loss: 0.262856\n",
      "Iteration 444775 | Loss: 0.262856\n",
      "Iteration 444800 | Loss: 0.262856\n",
      "Iteration 444825 | Loss: 0.262856\n",
      "Iteration 444850 | Loss: 0.262856\n",
      "Iteration 444875 | Loss: 0.262856\n",
      "Iteration 444900 | Loss: 0.262856\n",
      "Iteration 444925 | Loss: 0.262856\n",
      "Iteration 444950 | Loss: 0.262856\n",
      "Iteration 444975 | Loss: 0.262856\n",
      "Iteration 445000 | Loss: 0.262856\n",
      "Iteration 445025 | Loss: 0.262856\n",
      "Iteration 445050 | Loss: 0.262856\n",
      "Iteration 445075 | Loss: 0.262856\n",
      "Iteration 445100 | Loss: 0.262856\n",
      "Iteration 445125 | Loss: 0.262856\n",
      "Iteration 445150 | Loss: 0.262856\n",
      "Iteration 445175 | Loss: 0.262856\n",
      "Iteration 445200 | Loss: 0.262856\n",
      "Iteration 445225 | Loss: 0.262856\n",
      "Iteration 445250 | Loss: 0.262856\n",
      "Iteration 445275 | Loss: 0.262856\n",
      "Iteration 445300 | Loss: 0.262856\n",
      "Iteration 445325 | Loss: 0.262856\n",
      "Iteration 445350 | Loss: 0.262856\n",
      "Iteration 445375 | Loss: 0.262856\n",
      "Iteration 445400 | Loss: 0.262856\n",
      "Iteration 445425 | Loss: 0.262856\n",
      "Iteration 445450 | Loss: 0.262856\n",
      "Iteration 445475 | Loss: 0.262856\n",
      "Iteration 445500 | Loss: 0.262856\n",
      "Iteration 445525 | Loss: 0.262856\n",
      "Iteration 445550 | Loss: 0.262856\n",
      "Iteration 445575 | Loss: 0.262856\n",
      "Iteration 445600 | Loss: 0.262856\n",
      "Iteration 445625 | Loss: 0.262856\n",
      "Iteration 445650 | Loss: 0.262856\n",
      "Iteration 445675 | Loss: 0.262856\n",
      "Iteration 445700 | Loss: 0.262856\n",
      "Iteration 445725 | Loss: 0.262856\n",
      "Iteration 445750 | Loss: 0.262856\n",
      "Iteration 445775 | Loss: 0.262856\n",
      "Iteration 445800 | Loss: 0.262856\n",
      "Iteration 445825 | Loss: 0.262856\n",
      "Iteration 445850 | Loss: 0.262856\n",
      "Iteration 445875 | Loss: 0.262856\n",
      "Iteration 445900 | Loss: 0.262856\n",
      "Iteration 445925 | Loss: 0.262856\n",
      "Iteration 445950 | Loss: 0.262856\n",
      "Iteration 445975 | Loss: 0.262856\n",
      "Iteration 446000 | Loss: 0.262856\n",
      "Iteration 446025 | Loss: 0.262856\n",
      "Iteration 446050 | Loss: 0.262856\n",
      "Iteration 446075 | Loss: 0.262856\n",
      "Iteration 446100 | Loss: 0.262856\n",
      "Iteration 446125 | Loss: 0.262856\n",
      "Iteration 446150 | Loss: 0.262856\n",
      "Iteration 446175 | Loss: 0.262856\n",
      "Iteration 446200 | Loss: 0.262856\n",
      "Iteration 446225 | Loss: 0.262856\n",
      "Iteration 446250 | Loss: 0.262856\n",
      "Iteration 446275 | Loss: 0.262856\n",
      "Iteration 446300 | Loss: 0.262856\n",
      "Iteration 446325 | Loss: 0.262856\n",
      "Iteration 446350 | Loss: 0.262856\n",
      "Iteration 446375 | Loss: 0.262856\n",
      "Iteration 446400 | Loss: 0.262856\n",
      "Iteration 446425 | Loss: 0.262856\n",
      "Iteration 446450 | Loss: 0.262856\n",
      "Iteration 446475 | Loss: 0.262856\n",
      "Iteration 446500 | Loss: 0.262856\n",
      "Iteration 446525 | Loss: 0.262856\n",
      "Iteration 446550 | Loss: 0.262856\n",
      "Iteration 446575 | Loss: 0.262856\n",
      "Iteration 446600 | Loss: 0.262856\n",
      "Iteration 446625 | Loss: 0.262856\n",
      "Iteration 446650 | Loss: 0.262856\n",
      "Iteration 446675 | Loss: 0.262856\n",
      "Iteration 446700 | Loss: 0.262856\n",
      "Iteration 446725 | Loss: 0.262856\n",
      "Iteration 446750 | Loss: 0.262856\n",
      "Iteration 446775 | Loss: 0.262856\n",
      "Iteration 446800 | Loss: 0.262856\n",
      "Iteration 446825 | Loss: 0.262856\n",
      "Iteration 446850 | Loss: 0.262856\n",
      "Iteration 446875 | Loss: 0.262856\n",
      "Iteration 446900 | Loss: 0.262856\n",
      "Iteration 446925 | Loss: 0.262856\n",
      "Iteration 446950 | Loss: 0.262856\n",
      "Iteration 446975 | Loss: 0.262856\n",
      "Iteration 447000 | Loss: 0.262856\n",
      "Iteration 447025 | Loss: 0.262856\n",
      "Iteration 447050 | Loss: 0.262856\n",
      "Iteration 447075 | Loss: 0.262856\n",
      "Iteration 447100 | Loss: 0.262856\n",
      "Iteration 447125 | Loss: 0.262856\n",
      "Iteration 447150 | Loss: 0.262856\n",
      "Iteration 447175 | Loss: 0.262856\n",
      "Iteration 447200 | Loss: 0.262856\n",
      "Iteration 447225 | Loss: 0.262856\n",
      "Iteration 447250 | Loss: 0.262856\n",
      "Iteration 447275 | Loss: 0.262856\n",
      "Iteration 447300 | Loss: 0.262856\n",
      "Iteration 447325 | Loss: 0.262856\n",
      "Iteration 447350 | Loss: 0.262856\n",
      "Iteration 447375 | Loss: 0.262856\n",
      "Iteration 447400 | Loss: 0.262856\n",
      "Iteration 447425 | Loss: 0.262856\n",
      "Iteration 447450 | Loss: 0.262856\n",
      "Iteration 447475 | Loss: 0.262856\n",
      "Iteration 447500 | Loss: 0.262856\n",
      "Iteration 447525 | Loss: 0.262856\n",
      "Iteration 447550 | Loss: 0.262856\n",
      "Iteration 447575 | Loss: 0.262856\n",
      "Iteration 447600 | Loss: 0.262856\n",
      "Iteration 447625 | Loss: 0.262856\n",
      "Iteration 447650 | Loss: 0.262856\n",
      "Iteration 447675 | Loss: 0.262856\n",
      "Iteration 447700 | Loss: 0.262856\n",
      "Iteration 447725 | Loss: 0.262856\n",
      "Iteration 447750 | Loss: 0.262856\n",
      "Iteration 447775 | Loss: 0.262856\n",
      "Iteration 447800 | Loss: 0.262856\n",
      "Iteration 447825 | Loss: 0.262856\n",
      "Iteration 447850 | Loss: 0.262856\n",
      "Iteration 447875 | Loss: 0.262856\n",
      "Iteration 447900 | Loss: 0.262856\n",
      "Iteration 447925 | Loss: 0.262856\n",
      "Iteration 447950 | Loss: 0.262856\n",
      "Iteration 447975 | Loss: 0.262856\n",
      "Iteration 448000 | Loss: 0.262856\n",
      "Iteration 448025 | Loss: 0.262856\n",
      "Iteration 448050 | Loss: 0.262856\n",
      "Iteration 448075 | Loss: 0.262856\n",
      "Iteration 448100 | Loss: 0.262856\n",
      "Iteration 448125 | Loss: 0.262856\n",
      "Iteration 448150 | Loss: 0.262856\n",
      "Iteration 448175 | Loss: 0.262856\n",
      "Iteration 448200 | Loss: 0.262856\n",
      "Iteration 448225 | Loss: 0.262856\n",
      "Iteration 448250 | Loss: 0.262856\n",
      "Iteration 448275 | Loss: 0.262856\n",
      "Iteration 448300 | Loss: 0.262856\n",
      "Iteration 448325 | Loss: 0.262856\n",
      "Iteration 448350 | Loss: 0.262856\n",
      "Iteration 448375 | Loss: 0.262856\n",
      "Iteration 448400 | Loss: 0.262856\n",
      "Iteration 448425 | Loss: 0.262856\n",
      "Iteration 448450 | Loss: 0.262856\n",
      "Iteration 448475 | Loss: 0.262856\n",
      "Iteration 448500 | Loss: 0.262856\n",
      "Iteration 448525 | Loss: 0.262856\n",
      "Iteration 448550 | Loss: 0.262856\n",
      "Iteration 448575 | Loss: 0.262856\n",
      "Iteration 448600 | Loss: 0.262856\n",
      "Iteration 448625 | Loss: 0.262856\n",
      "Iteration 448650 | Loss: 0.262856\n",
      "Iteration 448675 | Loss: 0.262856\n",
      "Iteration 448700 | Loss: 0.262856\n",
      "Iteration 448725 | Loss: 0.262856\n",
      "Iteration 448750 | Loss: 0.262856\n",
      "Iteration 448775 | Loss: 0.262856\n",
      "Iteration 448800 | Loss: 0.262856\n",
      "Iteration 448825 | Loss: 0.262856\n",
      "Iteration 448850 | Loss: 0.262856\n",
      "Iteration 448875 | Loss: 0.262856\n",
      "Iteration 448900 | Loss: 0.262856\n",
      "Iteration 448925 | Loss: 0.262856\n",
      "Iteration 448950 | Loss: 0.262856\n",
      "Iteration 448975 | Loss: 0.262856\n",
      "Iteration 449000 | Loss: 0.262856\n",
      "Iteration 449025 | Loss: 0.262856\n",
      "Iteration 449050 | Loss: 0.262856\n",
      "Iteration 449075 | Loss: 0.262856\n",
      "Iteration 449100 | Loss: 0.262856\n",
      "Iteration 449125 | Loss: 0.262856\n",
      "Iteration 449150 | Loss: 0.262856\n",
      "Iteration 449175 | Loss: 0.262856\n",
      "Iteration 449200 | Loss: 0.262856\n",
      "Iteration 449225 | Loss: 0.262856\n",
      "Iteration 449250 | Loss: 0.262856\n",
      "Iteration 449275 | Loss: 0.262856\n",
      "Iteration 449300 | Loss: 0.262856\n",
      "Iteration 449325 | Loss: 0.262856\n",
      "Iteration 449350 | Loss: 0.262856\n",
      "Iteration 449375 | Loss: 0.262856\n",
      "Iteration 449400 | Loss: 0.262856\n",
      "Iteration 449425 | Loss: 0.262856\n",
      "Iteration 449450 | Loss: 0.262856\n",
      "Iteration 449475 | Loss: 0.262856\n",
      "Iteration 449500 | Loss: 0.262856\n",
      "Iteration 449525 | Loss: 0.262856\n",
      "Iteration 449550 | Loss: 0.262856\n",
      "Iteration 449575 | Loss: 0.262856\n",
      "Iteration 449600 | Loss: 0.262856\n",
      "Iteration 449625 | Loss: 0.262856\n",
      "Iteration 449650 | Loss: 0.262856\n",
      "Iteration 449675 | Loss: 0.262856\n",
      "Iteration 449700 | Loss: 0.262856\n",
      "Iteration 449725 | Loss: 0.262856\n",
      "Iteration 449750 | Loss: 0.262856\n",
      "Iteration 449775 | Loss: 0.262856\n",
      "Iteration 449800 | Loss: 0.262856\n",
      "Iteration 449825 | Loss: 0.262856\n",
      "Iteration 449850 | Loss: 0.262856\n",
      "Iteration 449875 | Loss: 0.262856\n",
      "Iteration 449900 | Loss: 0.262856\n",
      "Iteration 449925 | Loss: 0.262856\n",
      "Iteration 449950 | Loss: 0.262856\n",
      "Iteration 449975 | Loss: 0.262856\n",
      "Iteration 450000 | Loss: 0.262856\n",
      "Iteration 450025 | Loss: 0.262856\n",
      "Iteration 450050 | Loss: 0.262856\n",
      "Iteration 450075 | Loss: 0.262856\n",
      "Iteration 450100 | Loss: 0.262856\n",
      "Iteration 450125 | Loss: 0.262856\n",
      "Iteration 450150 | Loss: 0.262856\n",
      "Iteration 450175 | Loss: 0.262856\n",
      "Iteration 450200 | Loss: 0.262856\n",
      "Iteration 450225 | Loss: 0.262856\n",
      "Iteration 450250 | Loss: 0.262856\n",
      "Iteration 450275 | Loss: 0.262856\n",
      "Iteration 450300 | Loss: 0.262856\n",
      "Iteration 450325 | Loss: 0.262856\n",
      "Iteration 450350 | Loss: 0.262856\n",
      "Iteration 450375 | Loss: 0.262856\n",
      "Iteration 450400 | Loss: 0.262856\n",
      "Iteration 450425 | Loss: 0.262856\n",
      "Iteration 450450 | Loss: 0.262856\n",
      "Iteration 450475 | Loss: 0.262856\n",
      "Iteration 450500 | Loss: 0.262856\n",
      "Iteration 450525 | Loss: 0.262856\n",
      "Iteration 450550 | Loss: 0.262856\n",
      "Iteration 450575 | Loss: 0.262856\n",
      "Iteration 450600 | Loss: 0.262856\n",
      "Iteration 450625 | Loss: 0.262856\n",
      "Iteration 450650 | Loss: 0.262856\n",
      "Iteration 450675 | Loss: 0.262856\n",
      "Iteration 450700 | Loss: 0.262856\n",
      "Iteration 450725 | Loss: 0.262856\n",
      "Iteration 450750 | Loss: 0.262856\n",
      "Iteration 450775 | Loss: 0.262856\n",
      "Iteration 450800 | Loss: 0.262856\n",
      "Iteration 450825 | Loss: 0.262856\n",
      "Iteration 450850 | Loss: 0.262856\n",
      "Iteration 450875 | Loss: 0.262856\n",
      "Iteration 450900 | Loss: 0.262856\n",
      "Iteration 450925 | Loss: 0.262856\n",
      "Iteration 450950 | Loss: 0.262856\n",
      "Iteration 450975 | Loss: 0.262856\n",
      "Iteration 451000 | Loss: 0.262856\n",
      "Iteration 451025 | Loss: 0.262856\n",
      "Iteration 451050 | Loss: 0.262856\n",
      "Iteration 451075 | Loss: 0.262856\n",
      "Iteration 451100 | Loss: 0.262856\n",
      "Iteration 451125 | Loss: 0.262856\n",
      "Iteration 451150 | Loss: 0.262856\n",
      "Iteration 451175 | Loss: 0.262856\n",
      "Iteration 451200 | Loss: 0.262856\n",
      "Iteration 451225 | Loss: 0.262856\n",
      "Iteration 451250 | Loss: 0.262856\n",
      "Iteration 451275 | Loss: 0.262856\n",
      "Iteration 451300 | Loss: 0.262856\n",
      "Iteration 451325 | Loss: 0.262856\n",
      "Iteration 451350 | Loss: 0.262856\n",
      "Iteration 451375 | Loss: 0.262856\n",
      "Iteration 451400 | Loss: 0.262856\n",
      "Iteration 451425 | Loss: 0.262856\n",
      "Iteration 451450 | Loss: 0.262856\n",
      "Iteration 451475 | Loss: 0.262856\n",
      "Iteration 451500 | Loss: 0.262856\n",
      "Iteration 451525 | Loss: 0.262856\n",
      "Iteration 451550 | Loss: 0.262856\n",
      "Iteration 451575 | Loss: 0.262856\n",
      "Iteration 451600 | Loss: 0.262856\n",
      "Iteration 451625 | Loss: 0.262856\n",
      "Iteration 451650 | Loss: 0.262856\n",
      "Iteration 451675 | Loss: 0.262856\n",
      "Iteration 451700 | Loss: 0.262856\n",
      "Iteration 451725 | Loss: 0.262856\n",
      "Iteration 451750 | Loss: 0.262856\n",
      "Iteration 451775 | Loss: 0.262856\n",
      "Iteration 451800 | Loss: 0.262856\n",
      "Iteration 451825 | Loss: 0.262856\n",
      "Iteration 451850 | Loss: 0.262856\n",
      "Iteration 451875 | Loss: 0.262856\n",
      "Iteration 451900 | Loss: 0.262856\n",
      "Iteration 451925 | Loss: 0.262856\n",
      "Iteration 451950 | Loss: 0.262856\n",
      "Iteration 451975 | Loss: 0.262856\n",
      "Iteration 452000 | Loss: 0.262856\n",
      "Iteration 452025 | Loss: 0.262856\n",
      "Iteration 452050 | Loss: 0.262856\n",
      "Iteration 452075 | Loss: 0.262856\n",
      "Iteration 452100 | Loss: 0.262856\n",
      "Iteration 452125 | Loss: 0.262856\n",
      "Iteration 452150 | Loss: 0.262856\n",
      "Iteration 452175 | Loss: 0.262856\n",
      "Iteration 452200 | Loss: 0.262856\n",
      "Iteration 452225 | Loss: 0.262856\n",
      "Iteration 452250 | Loss: 0.262856\n",
      "Iteration 452275 | Loss: 0.262856\n",
      "Iteration 452300 | Loss: 0.262856\n",
      "Iteration 452325 | Loss: 0.262856\n",
      "Iteration 452350 | Loss: 0.262856\n",
      "Iteration 452375 | Loss: 0.262856\n",
      "Iteration 452400 | Loss: 0.262856\n",
      "Iteration 452425 | Loss: 0.262856\n",
      "Iteration 452450 | Loss: 0.262856\n",
      "Iteration 452475 | Loss: 0.262856\n",
      "Iteration 452500 | Loss: 0.262856\n",
      "Iteration 452525 | Loss: 0.262856\n",
      "Iteration 452550 | Loss: 0.262856\n",
      "Iteration 452575 | Loss: 0.262856\n",
      "Iteration 452600 | Loss: 0.262856\n",
      "Iteration 452625 | Loss: 0.262856\n",
      "Iteration 452650 | Loss: 0.262856\n",
      "Iteration 452675 | Loss: 0.262856\n",
      "Iteration 452700 | Loss: 0.262856\n",
      "Iteration 452725 | Loss: 0.262856\n",
      "Iteration 452750 | Loss: 0.262856\n",
      "Iteration 452775 | Loss: 0.262856\n",
      "Iteration 452800 | Loss: 0.262856\n",
      "Iteration 452825 | Loss: 0.262856\n",
      "Iteration 452850 | Loss: 0.262856\n",
      "Iteration 452875 | Loss: 0.262856\n",
      "Iteration 452900 | Loss: 0.262856\n",
      "Iteration 452925 | Loss: 0.262856\n",
      "Iteration 452950 | Loss: 0.262856\n",
      "Iteration 452975 | Loss: 0.262856\n",
      "Iteration 453000 | Loss: 0.262856\n",
      "Iteration 453025 | Loss: 0.262856\n",
      "Iteration 453050 | Loss: 0.262856\n",
      "Iteration 453075 | Loss: 0.262856\n",
      "Iteration 453100 | Loss: 0.262856\n",
      "Iteration 453125 | Loss: 0.262856\n",
      "Iteration 453150 | Loss: 0.262856\n",
      "Iteration 453175 | Loss: 0.262856\n",
      "Iteration 453200 | Loss: 0.262856\n",
      "Iteration 453225 | Loss: 0.262856\n",
      "Iteration 453250 | Loss: 0.262856\n",
      "Iteration 453275 | Loss: 0.262856\n",
      "Iteration 453300 | Loss: 0.262856\n",
      "Iteration 453325 | Loss: 0.262856\n",
      "Iteration 453350 | Loss: 0.262856\n",
      "Iteration 453375 | Loss: 0.262856\n",
      "Iteration 453400 | Loss: 0.262856\n",
      "Iteration 453425 | Loss: 0.262856\n",
      "Iteration 453450 | Loss: 0.262856\n",
      "Iteration 453475 | Loss: 0.262856\n",
      "Iteration 453500 | Loss: 0.262856\n",
      "Iteration 453525 | Loss: 0.262856\n",
      "Iteration 453550 | Loss: 0.262856\n",
      "Iteration 453575 | Loss: 0.262856\n",
      "Iteration 453600 | Loss: 0.262856\n",
      "Iteration 453625 | Loss: 0.262856\n",
      "Iteration 453650 | Loss: 0.262856\n",
      "Iteration 453675 | Loss: 0.262856\n",
      "Iteration 453700 | Loss: 0.262856\n",
      "Iteration 453725 | Loss: 0.262856\n",
      "Iteration 453750 | Loss: 0.262856\n",
      "Iteration 453775 | Loss: 0.262856\n",
      "Iteration 453800 | Loss: 0.262856\n",
      "Iteration 453825 | Loss: 0.262856\n",
      "Iteration 453850 | Loss: 0.262856\n",
      "Iteration 453875 | Loss: 0.262856\n",
      "Iteration 453900 | Loss: 0.262856\n",
      "Iteration 453925 | Loss: 0.262856\n",
      "Iteration 453950 | Loss: 0.262856\n",
      "Iteration 453975 | Loss: 0.262856\n",
      "Iteration 454000 | Loss: 0.262856\n",
      "Iteration 454025 | Loss: 0.262856\n",
      "Iteration 454050 | Loss: 0.262856\n",
      "Iteration 454075 | Loss: 0.262856\n",
      "Iteration 454100 | Loss: 0.262856\n",
      "Iteration 454125 | Loss: 0.262856\n",
      "Iteration 454150 | Loss: 0.262856\n",
      "Iteration 454175 | Loss: 0.262856\n",
      "Iteration 454200 | Loss: 0.262856\n",
      "Iteration 454225 | Loss: 0.262856\n",
      "Iteration 454250 | Loss: 0.262856\n",
      "Iteration 454275 | Loss: 0.262856\n",
      "Iteration 454300 | Loss: 0.262856\n",
      "Iteration 454325 | Loss: 0.262856\n",
      "Iteration 454350 | Loss: 0.262856\n",
      "Iteration 454375 | Loss: 0.262856\n",
      "Iteration 454400 | Loss: 0.262856\n",
      "Iteration 454425 | Loss: 0.262856\n",
      "Iteration 454450 | Loss: 0.262856\n",
      "Iteration 454475 | Loss: 0.262856\n",
      "Iteration 454500 | Loss: 0.262856\n",
      "Iteration 454525 | Loss: 0.262856\n",
      "Iteration 454550 | Loss: 0.262856\n",
      "Iteration 454575 | Loss: 0.262856\n",
      "Iteration 454600 | Loss: 0.262856\n",
      "Iteration 454625 | Loss: 0.262856\n",
      "Iteration 454650 | Loss: 0.262856\n",
      "Iteration 454675 | Loss: 0.262856\n",
      "Iteration 454700 | Loss: 0.262856\n",
      "Iteration 454725 | Loss: 0.262856\n",
      "Iteration 454750 | Loss: 0.262856\n",
      "Iteration 454775 | Loss: 0.262856\n",
      "Iteration 454800 | Loss: 0.262856\n",
      "Iteration 454825 | Loss: 0.262856\n",
      "Iteration 454850 | Loss: 0.262856\n",
      "Iteration 454875 | Loss: 0.262856\n",
      "Iteration 454900 | Loss: 0.262856\n",
      "Iteration 454925 | Loss: 0.262856\n",
      "Iteration 454950 | Loss: 0.262856\n",
      "Iteration 454975 | Loss: 0.262856\n",
      "Iteration 455000 | Loss: 0.262856\n",
      "Iteration 455025 | Loss: 0.262856\n",
      "Iteration 455050 | Loss: 0.262856\n",
      "Iteration 455075 | Loss: 0.262856\n",
      "Iteration 455100 | Loss: 0.262856\n",
      "Iteration 455125 | Loss: 0.262856\n",
      "Iteration 455150 | Loss: 0.262856\n",
      "Iteration 455175 | Loss: 0.262856\n",
      "Iteration 455200 | Loss: 0.262856\n",
      "Iteration 455225 | Loss: 0.262856\n",
      "Iteration 455250 | Loss: 0.262856\n",
      "Iteration 455275 | Loss: 0.262856\n",
      "Iteration 455300 | Loss: 0.262856\n",
      "Iteration 455325 | Loss: 0.262856\n",
      "Iteration 455350 | Loss: 0.262856\n",
      "Iteration 455375 | Loss: 0.262856\n",
      "Iteration 455400 | Loss: 0.262856\n",
      "Iteration 455425 | Loss: 0.262856\n",
      "Iteration 455450 | Loss: 0.262856\n",
      "Iteration 455475 | Loss: 0.262856\n",
      "Iteration 455500 | Loss: 0.262856\n",
      "Iteration 455525 | Loss: 0.262856\n",
      "Iteration 455550 | Loss: 0.262856\n",
      "Iteration 455575 | Loss: 0.262856\n",
      "Iteration 455600 | Loss: 0.262856\n",
      "Iteration 455625 | Loss: 0.262856\n",
      "Iteration 455650 | Loss: 0.262856\n",
      "Iteration 455675 | Loss: 0.262856\n",
      "Iteration 455700 | Loss: 0.262856\n",
      "Iteration 455725 | Loss: 0.262856\n",
      "Iteration 455750 | Loss: 0.262856\n",
      "Iteration 455775 | Loss: 0.262856\n",
      "Iteration 455800 | Loss: 0.262856\n",
      "Iteration 455825 | Loss: 0.262856\n",
      "Iteration 455850 | Loss: 0.262856\n",
      "Iteration 455875 | Loss: 0.262856\n",
      "Iteration 455900 | Loss: 0.262856\n",
      "Iteration 455925 | Loss: 0.262856\n",
      "Iteration 455950 | Loss: 0.262856\n",
      "Iteration 455975 | Loss: 0.262856\n",
      "Iteration 456000 | Loss: 0.262856\n",
      "Iteration 456025 | Loss: 0.262856\n",
      "Iteration 456050 | Loss: 0.262856\n",
      "Iteration 456075 | Loss: 0.262856\n",
      "Iteration 456100 | Loss: 0.262856\n",
      "Iteration 456125 | Loss: 0.262856\n",
      "Iteration 456150 | Loss: 0.262856\n",
      "Iteration 456175 | Loss: 0.262856\n",
      "Iteration 456200 | Loss: 0.262856\n",
      "Iteration 456225 | Loss: 0.262856\n",
      "Iteration 456250 | Loss: 0.262856\n",
      "Iteration 456275 | Loss: 0.262856\n",
      "Iteration 456300 | Loss: 0.262856\n",
      "Iteration 456325 | Loss: 0.262856\n",
      "Iteration 456350 | Loss: 0.262856\n",
      "Iteration 456375 | Loss: 0.262856\n",
      "Iteration 456400 | Loss: 0.262856\n",
      "Iteration 456425 | Loss: 0.262856\n",
      "Iteration 456450 | Loss: 0.262856\n",
      "Iteration 456475 | Loss: 0.262856\n",
      "Iteration 456500 | Loss: 0.262856\n",
      "Iteration 456525 | Loss: 0.262856\n",
      "Iteration 456550 | Loss: 0.262856\n",
      "Iteration 456575 | Loss: 0.262856\n",
      "Iteration 456600 | Loss: 0.262856\n",
      "Iteration 456625 | Loss: 0.262856\n",
      "Iteration 456650 | Loss: 0.262856\n",
      "Iteration 456675 | Loss: 0.262856\n",
      "Iteration 456700 | Loss: 0.262856\n",
      "Iteration 456725 | Loss: 0.262856\n",
      "Iteration 456750 | Loss: 0.262856\n",
      "Iteration 456775 | Loss: 0.262856\n",
      "Iteration 456800 | Loss: 0.262856\n",
      "Iteration 456825 | Loss: 0.262856\n",
      "Iteration 456850 | Loss: 0.262856\n",
      "Iteration 456875 | Loss: 0.262856\n",
      "Iteration 456900 | Loss: 0.262856\n",
      "Iteration 456925 | Loss: 0.262856\n",
      "Iteration 456950 | Loss: 0.262856\n",
      "Iteration 456975 | Loss: 0.262856\n",
      "Iteration 457000 | Loss: 0.262856\n",
      "Iteration 457025 | Loss: 0.262856\n",
      "Iteration 457050 | Loss: 0.262856\n",
      "Iteration 457075 | Loss: 0.262856\n",
      "Iteration 457100 | Loss: 0.262856\n",
      "Iteration 457125 | Loss: 0.262856\n",
      "Iteration 457150 | Loss: 0.262856\n",
      "Iteration 457175 | Loss: 0.262856\n",
      "Iteration 457200 | Loss: 0.262856\n",
      "Iteration 457225 | Loss: 0.262856\n",
      "Iteration 457250 | Loss: 0.262856\n",
      "Iteration 457275 | Loss: 0.262856\n",
      "Iteration 457300 | Loss: 0.262856\n",
      "Iteration 457325 | Loss: 0.262856\n",
      "Iteration 457350 | Loss: 0.262856\n",
      "Iteration 457375 | Loss: 0.262856\n",
      "Iteration 457400 | Loss: 0.262856\n",
      "Iteration 457425 | Loss: 0.262856\n",
      "Iteration 457450 | Loss: 0.262856\n",
      "Iteration 457475 | Loss: 0.262856\n",
      "Iteration 457500 | Loss: 0.262856\n",
      "Iteration 457525 | Loss: 0.262856\n",
      "Iteration 457550 | Loss: 0.262856\n",
      "Iteration 457575 | Loss: 0.262856\n",
      "Iteration 457600 | Loss: 0.262856\n",
      "Iteration 457625 | Loss: 0.262856\n",
      "Iteration 457650 | Loss: 0.262856\n",
      "Iteration 457675 | Loss: 0.262856\n",
      "Iteration 457700 | Loss: 0.262856\n",
      "Iteration 457725 | Loss: 0.262856\n",
      "Iteration 457750 | Loss: 0.262856\n",
      "Iteration 457775 | Loss: 0.262856\n",
      "Iteration 457800 | Loss: 0.262856\n",
      "Iteration 457825 | Loss: 0.262856\n",
      "Iteration 457850 | Loss: 0.262856\n",
      "Iteration 457875 | Loss: 0.262856\n",
      "Iteration 457900 | Loss: 0.262856\n",
      "Iteration 457925 | Loss: 0.262856\n",
      "Iteration 457950 | Loss: 0.262856\n",
      "Iteration 457975 | Loss: 0.262856\n",
      "Iteration 458000 | Loss: 0.262856\n",
      "Iteration 458025 | Loss: 0.262856\n",
      "Iteration 458050 | Loss: 0.262856\n",
      "Iteration 458075 | Loss: 0.262856\n",
      "Iteration 458100 | Loss: 0.262856\n",
      "Iteration 458125 | Loss: 0.262856\n",
      "Iteration 458150 | Loss: 0.262856\n",
      "Iteration 458175 | Loss: 0.262856\n",
      "Iteration 458200 | Loss: 0.262856\n",
      "Iteration 458225 | Loss: 0.262856\n",
      "Iteration 458250 | Loss: 0.262856\n",
      "Iteration 458275 | Loss: 0.262856\n",
      "Iteration 458300 | Loss: 0.262856\n",
      "Iteration 458325 | Loss: 0.262856\n",
      "Iteration 458350 | Loss: 0.262856\n",
      "Iteration 458375 | Loss: 0.262856\n",
      "Iteration 458400 | Loss: 0.262856\n",
      "Iteration 458425 | Loss: 0.262856\n",
      "Iteration 458450 | Loss: 0.262856\n",
      "Iteration 458475 | Loss: 0.262856\n",
      "Iteration 458500 | Loss: 0.262856\n",
      "Iteration 458525 | Loss: 0.262856\n",
      "Iteration 458550 | Loss: 0.262856\n",
      "Iteration 458575 | Loss: 0.262856\n",
      "Iteration 458600 | Loss: 0.262856\n",
      "Iteration 458625 | Loss: 0.262856\n",
      "Iteration 458650 | Loss: 0.262856\n",
      "Iteration 458675 | Loss: 0.262856\n",
      "Iteration 458700 | Loss: 0.262856\n",
      "Iteration 458725 | Loss: 0.262856\n",
      "Iteration 458750 | Loss: 0.262856\n",
      "Iteration 458775 | Loss: 0.262856\n",
      "Iteration 458800 | Loss: 0.262856\n",
      "Iteration 458825 | Loss: 0.262856\n",
      "Iteration 458850 | Loss: 0.262856\n",
      "Iteration 458875 | Loss: 0.262856\n",
      "Iteration 458900 | Loss: 0.262856\n",
      "Iteration 458925 | Loss: 0.262856\n",
      "Iteration 458950 | Loss: 0.262856\n",
      "Iteration 458975 | Loss: 0.262856\n",
      "Iteration 459000 | Loss: 0.262856\n",
      "Iteration 459025 | Loss: 0.262856\n",
      "Iteration 459050 | Loss: 0.262856\n",
      "Iteration 459075 | Loss: 0.262856\n",
      "Iteration 459100 | Loss: 0.262856\n",
      "Iteration 459125 | Loss: 0.262856\n",
      "Iteration 459150 | Loss: 0.262856\n",
      "Iteration 459175 | Loss: 0.262856\n",
      "Iteration 459200 | Loss: 0.262856\n",
      "Iteration 459225 | Loss: 0.262856\n",
      "Iteration 459250 | Loss: 0.262856\n",
      "Iteration 459275 | Loss: 0.262856\n",
      "Iteration 459300 | Loss: 0.262856\n",
      "Iteration 459325 | Loss: 0.262856\n",
      "Iteration 459350 | Loss: 0.262856\n",
      "Iteration 459375 | Loss: 0.262856\n",
      "Iteration 459400 | Loss: 0.262856\n",
      "Iteration 459425 | Loss: 0.262856\n",
      "Iteration 459450 | Loss: 0.262856\n",
      "Iteration 459475 | Loss: 0.262856\n",
      "Iteration 459500 | Loss: 0.262856\n",
      "Iteration 459525 | Loss: 0.262856\n",
      "Iteration 459550 | Loss: 0.262856\n",
      "Iteration 459575 | Loss: 0.262856\n",
      "Iteration 459600 | Loss: 0.262856\n",
      "Iteration 459625 | Loss: 0.262856\n",
      "Iteration 459650 | Loss: 0.262856\n",
      "Iteration 459675 | Loss: 0.262856\n",
      "Iteration 459700 | Loss: 0.262856\n",
      "Iteration 459725 | Loss: 0.262856\n",
      "Iteration 459750 | Loss: 0.262856\n",
      "Iteration 459775 | Loss: 0.262856\n",
      "Iteration 459800 | Loss: 0.262856\n",
      "Iteration 459825 | Loss: 0.262856\n",
      "Iteration 459850 | Loss: 0.262856\n",
      "Iteration 459875 | Loss: 0.262856\n",
      "Iteration 459900 | Loss: 0.262856\n",
      "Iteration 459925 | Loss: 0.262856\n",
      "Iteration 459950 | Loss: 0.262856\n",
      "Iteration 459975 | Loss: 0.262856\n",
      "Iteration 460000 | Loss: 0.262856\n",
      "Iteration 460025 | Loss: 0.262856\n",
      "Iteration 460050 | Loss: 0.262856\n",
      "Iteration 460075 | Loss: 0.262856\n",
      "Iteration 460100 | Loss: 0.262856\n",
      "Iteration 460125 | Loss: 0.262856\n",
      "Iteration 460150 | Loss: 0.262856\n",
      "Iteration 460175 | Loss: 0.262856\n",
      "Iteration 460200 | Loss: 0.262856\n",
      "Iteration 460225 | Loss: 0.262856\n",
      "Iteration 460250 | Loss: 0.262856\n",
      "Iteration 460275 | Loss: 0.262856\n",
      "Iteration 460300 | Loss: 0.262856\n",
      "Iteration 460325 | Loss: 0.262856\n",
      "Iteration 460350 | Loss: 0.262856\n",
      "Iteration 460375 | Loss: 0.262856\n",
      "Iteration 460400 | Loss: 0.262856\n",
      "Iteration 460425 | Loss: 0.262856\n",
      "Iteration 460450 | Loss: 0.262856\n",
      "Iteration 460475 | Loss: 0.262856\n",
      "Iteration 460500 | Loss: 0.262856\n",
      "Iteration 460525 | Loss: 0.262856\n",
      "Iteration 460550 | Loss: 0.262856\n",
      "Iteration 460575 | Loss: 0.262856\n",
      "Iteration 460600 | Loss: 0.262856\n",
      "Iteration 460625 | Loss: 0.262856\n",
      "Iteration 460650 | Loss: 0.262856\n",
      "Iteration 460675 | Loss: 0.262856\n",
      "Iteration 460700 | Loss: 0.262856\n",
      "Iteration 460725 | Loss: 0.262856\n",
      "Iteration 460750 | Loss: 0.262856\n",
      "Iteration 460775 | Loss: 0.262856\n",
      "Iteration 460800 | Loss: 0.262856\n",
      "Iteration 460825 | Loss: 0.262856\n",
      "Iteration 460850 | Loss: 0.262856\n",
      "Iteration 460875 | Loss: 0.262856\n",
      "Iteration 460900 | Loss: 0.262856\n",
      "Iteration 460925 | Loss: 0.262856\n",
      "Iteration 460950 | Loss: 0.262856\n",
      "Iteration 460975 | Loss: 0.262856\n",
      "Iteration 461000 | Loss: 0.262856\n",
      "Iteration 461025 | Loss: 0.262856\n",
      "Iteration 461050 | Loss: 0.262856\n",
      "Iteration 461075 | Loss: 0.262856\n",
      "Iteration 461100 | Loss: 0.262856\n",
      "Iteration 461125 | Loss: 0.262856\n",
      "Iteration 461150 | Loss: 0.262856\n",
      "Iteration 461175 | Loss: 0.262856\n",
      "Iteration 461200 | Loss: 0.262856\n",
      "Iteration 461225 | Loss: 0.262856\n",
      "Iteration 461250 | Loss: 0.262856\n",
      "Iteration 461275 | Loss: 0.262856\n",
      "Iteration 461300 | Loss: 0.262856\n",
      "Iteration 461325 | Loss: 0.262856\n",
      "Iteration 461350 | Loss: 0.262856\n",
      "Iteration 461375 | Loss: 0.262856\n",
      "Iteration 461400 | Loss: 0.262856\n",
      "Iteration 461425 | Loss: 0.262856\n",
      "Iteration 461450 | Loss: 0.262856\n",
      "Iteration 461475 | Loss: 0.262856\n",
      "Iteration 461500 | Loss: 0.262856\n",
      "Iteration 461525 | Loss: 0.262856\n",
      "Iteration 461550 | Loss: 0.262856\n",
      "Iteration 461575 | Loss: 0.262856\n",
      "Iteration 461600 | Loss: 0.262856\n",
      "Iteration 461625 | Loss: 0.262856\n",
      "Iteration 461650 | Loss: 0.262856\n",
      "Iteration 461675 | Loss: 0.262856\n",
      "Iteration 461700 | Loss: 0.262856\n",
      "Iteration 461725 | Loss: 0.262856\n",
      "Iteration 461750 | Loss: 0.262856\n",
      "Iteration 461775 | Loss: 0.262856\n",
      "Iteration 461800 | Loss: 0.262856\n",
      "Iteration 461825 | Loss: 0.262856\n",
      "Iteration 461850 | Loss: 0.262856\n",
      "Iteration 461875 | Loss: 0.262856\n",
      "Iteration 461900 | Loss: 0.262856\n",
      "Iteration 461925 | Loss: 0.262856\n",
      "Iteration 461950 | Loss: 0.262856\n",
      "Iteration 461975 | Loss: 0.262856\n",
      "Iteration 462000 | Loss: 0.262856\n",
      "Iteration 462025 | Loss: 0.262856\n",
      "Iteration 462050 | Loss: 0.262856\n",
      "Iteration 462075 | Loss: 0.262856\n",
      "Iteration 462100 | Loss: 0.262856\n",
      "Iteration 462125 | Loss: 0.262856\n",
      "Iteration 462150 | Loss: 0.262856\n",
      "Iteration 462175 | Loss: 0.262856\n",
      "Iteration 462200 | Loss: 0.262856\n",
      "Iteration 462225 | Loss: 0.262856\n",
      "Iteration 462250 | Loss: 0.262856\n",
      "Iteration 462275 | Loss: 0.262856\n",
      "Iteration 462300 | Loss: 0.262856\n",
      "Iteration 462325 | Loss: 0.262856\n",
      "Iteration 462350 | Loss: 0.262856\n",
      "Iteration 462375 | Loss: 0.262856\n",
      "Iteration 462400 | Loss: 0.262856\n",
      "Iteration 462425 | Loss: 0.262856\n",
      "Iteration 462450 | Loss: 0.262856\n",
      "Iteration 462475 | Loss: 0.262856\n",
      "Iteration 462500 | Loss: 0.262856\n",
      "Iteration 462525 | Loss: 0.262856\n",
      "Iteration 462550 | Loss: 0.262856\n",
      "Iteration 462575 | Loss: 0.262856\n",
      "Iteration 462600 | Loss: 0.262856\n",
      "Iteration 462625 | Loss: 0.262856\n",
      "Iteration 462650 | Loss: 0.262856\n",
      "Iteration 462675 | Loss: 0.262856\n",
      "Iteration 462700 | Loss: 0.262856\n",
      "Iteration 462725 | Loss: 0.262856\n",
      "Iteration 462750 | Loss: 0.262856\n",
      "Iteration 462775 | Loss: 0.262856\n",
      "Iteration 462800 | Loss: 0.262856\n",
      "Iteration 462825 | Loss: 0.262856\n",
      "Iteration 462850 | Loss: 0.262856\n",
      "Iteration 462875 | Loss: 0.262856\n",
      "Iteration 462900 | Loss: 0.262856\n",
      "Iteration 462925 | Loss: 0.262856\n",
      "Iteration 462950 | Loss: 0.262856\n",
      "Iteration 462975 | Loss: 0.262856\n",
      "Iteration 463000 | Loss: 0.262856\n",
      "Iteration 463025 | Loss: 0.262856\n",
      "Iteration 463050 | Loss: 0.262856\n",
      "Iteration 463075 | Loss: 0.262856\n",
      "Iteration 463100 | Loss: 0.262856\n",
      "Iteration 463125 | Loss: 0.262856\n",
      "Iteration 463150 | Loss: 0.262856\n",
      "Iteration 463175 | Loss: 0.262856\n",
      "Iteration 463200 | Loss: 0.262856\n",
      "Iteration 463225 | Loss: 0.262856\n",
      "Iteration 463250 | Loss: 0.262856\n",
      "Iteration 463275 | Loss: 0.262856\n",
      "Iteration 463300 | Loss: 0.262856\n",
      "Iteration 463325 | Loss: 0.262856\n",
      "Iteration 463350 | Loss: 0.262856\n",
      "Iteration 463375 | Loss: 0.262856\n",
      "Iteration 463400 | Loss: 0.262856\n",
      "Iteration 463425 | Loss: 0.262856\n",
      "Iteration 463450 | Loss: 0.262856\n",
      "Iteration 463475 | Loss: 0.262856\n",
      "Iteration 463500 | Loss: 0.262856\n",
      "Iteration 463525 | Loss: 0.262856\n",
      "Iteration 463550 | Loss: 0.262856\n",
      "Iteration 463575 | Loss: 0.262856\n",
      "Iteration 463600 | Loss: 0.262856\n",
      "Iteration 463625 | Loss: 0.262856\n",
      "Iteration 463650 | Loss: 0.262856\n",
      "Iteration 463675 | Loss: 0.262856\n",
      "Iteration 463700 | Loss: 0.262856\n",
      "Iteration 463725 | Loss: 0.262856\n",
      "Iteration 463750 | Loss: 0.262856\n",
      "Iteration 463775 | Loss: 0.262856\n",
      "Iteration 463800 | Loss: 0.262856\n",
      "Iteration 463825 | Loss: 0.262856\n",
      "Iteration 463850 | Loss: 0.262856\n",
      "Iteration 463875 | Loss: 0.262856\n",
      "Iteration 463900 | Loss: 0.262856\n",
      "Iteration 463925 | Loss: 0.262856\n",
      "Iteration 463950 | Loss: 0.262856\n",
      "Iteration 463975 | Loss: 0.262856\n",
      "Iteration 464000 | Loss: 0.262856\n",
      "Iteration 464025 | Loss: 0.262856\n",
      "Iteration 464050 | Loss: 0.262856\n",
      "Iteration 464075 | Loss: 0.262856\n",
      "Iteration 464100 | Loss: 0.262856\n",
      "Iteration 464125 | Loss: 0.262856\n",
      "Iteration 464150 | Loss: 0.262856\n",
      "Iteration 464175 | Loss: 0.262856\n",
      "Iteration 464200 | Loss: 0.262856\n",
      "Iteration 464225 | Loss: 0.262856\n",
      "Iteration 464250 | Loss: 0.262856\n",
      "Iteration 464275 | Loss: 0.262856\n",
      "Iteration 464300 | Loss: 0.262856\n",
      "Iteration 464325 | Loss: 0.262856\n",
      "Iteration 464350 | Loss: 0.262856\n",
      "Iteration 464375 | Loss: 0.262856\n",
      "Iteration 464400 | Loss: 0.262856\n",
      "Iteration 464425 | Loss: 0.262856\n",
      "Iteration 464450 | Loss: 0.262856\n",
      "Iteration 464475 | Loss: 0.262856\n",
      "Iteration 464500 | Loss: 0.262856\n",
      "Iteration 464525 | Loss: 0.262856\n",
      "Iteration 464550 | Loss: 0.262856\n",
      "Iteration 464575 | Loss: 0.262856\n",
      "Iteration 464600 | Loss: 0.262856\n",
      "Iteration 464625 | Loss: 0.262856\n",
      "Iteration 464650 | Loss: 0.262856\n",
      "Iteration 464675 | Loss: 0.262856\n",
      "Iteration 464700 | Loss: 0.262856\n",
      "Iteration 464725 | Loss: 0.262856\n",
      "Iteration 464750 | Loss: 0.262856\n",
      "Iteration 464775 | Loss: 0.262856\n",
      "Iteration 464800 | Loss: 0.262856\n",
      "Iteration 464825 | Loss: 0.262856\n",
      "Iteration 464850 | Loss: 0.262856\n",
      "Iteration 464875 | Loss: 0.262856\n",
      "Iteration 464900 | Loss: 0.262856\n",
      "Iteration 464925 | Loss: 0.262856\n",
      "Iteration 464950 | Loss: 0.262856\n",
      "Iteration 464975 | Loss: 0.262856\n",
      "Iteration 465000 | Loss: 0.262856\n",
      "Iteration 465025 | Loss: 0.262856\n",
      "Iteration 465050 | Loss: 0.262856\n",
      "Iteration 465075 | Loss: 0.262856\n",
      "Iteration 465100 | Loss: 0.262856\n",
      "Iteration 465125 | Loss: 0.262856\n",
      "Iteration 465150 | Loss: 0.262856\n",
      "Iteration 465175 | Loss: 0.262856\n",
      "Iteration 465200 | Loss: 0.262856\n",
      "Iteration 465225 | Loss: 0.262856\n",
      "Iteration 465250 | Loss: 0.262856\n",
      "Iteration 465275 | Loss: 0.262856\n",
      "Iteration 465300 | Loss: 0.262856\n",
      "Iteration 465325 | Loss: 0.262856\n",
      "Iteration 465350 | Loss: 0.262856\n",
      "Iteration 465375 | Loss: 0.262856\n",
      "Iteration 465400 | Loss: 0.262856\n",
      "Iteration 465425 | Loss: 0.262856\n",
      "Iteration 465450 | Loss: 0.262856\n",
      "Iteration 465475 | Loss: 0.262856\n",
      "Iteration 465500 | Loss: 0.262856\n",
      "Iteration 465525 | Loss: 0.262856\n",
      "Iteration 465550 | Loss: 0.262856\n",
      "Iteration 465575 | Loss: 0.262856\n",
      "Iteration 465600 | Loss: 0.262856\n",
      "Iteration 465625 | Loss: 0.262856\n",
      "Iteration 465650 | Loss: 0.262856\n",
      "Iteration 465675 | Loss: 0.262856\n",
      "Iteration 465700 | Loss: 0.262856\n",
      "Iteration 465725 | Loss: 0.262856\n",
      "Iteration 465750 | Loss: 0.262856\n",
      "Iteration 465775 | Loss: 0.262856\n",
      "Iteration 465800 | Loss: 0.262856\n",
      "Iteration 465825 | Loss: 0.262856\n",
      "Iteration 465850 | Loss: 0.262856\n",
      "Iteration 465875 | Loss: 0.262856\n",
      "Iteration 465900 | Loss: 0.262856\n",
      "Iteration 465925 | Loss: 0.262856\n",
      "Iteration 465950 | Loss: 0.262856\n",
      "Iteration 465975 | Loss: 0.262856\n",
      "Iteration 466000 | Loss: 0.262856\n",
      "Iteration 466025 | Loss: 0.262856\n",
      "Iteration 466050 | Loss: 0.262856\n",
      "Iteration 466075 | Loss: 0.262856\n",
      "Iteration 466100 | Loss: 0.262856\n",
      "Iteration 466125 | Loss: 0.262856\n",
      "Iteration 466150 | Loss: 0.262856\n",
      "Iteration 466175 | Loss: 0.262856\n",
      "Iteration 466200 | Loss: 0.262856\n",
      "Iteration 466225 | Loss: 0.262856\n",
      "Iteration 466250 | Loss: 0.262856\n",
      "Iteration 466275 | Loss: 0.262856\n",
      "Iteration 466300 | Loss: 0.262856\n",
      "Iteration 466325 | Loss: 0.262856\n",
      "Iteration 466350 | Loss: 0.262856\n",
      "Iteration 466375 | Loss: 0.262856\n",
      "Iteration 466400 | Loss: 0.262856\n",
      "Iteration 466425 | Loss: 0.262856\n",
      "Iteration 466450 | Loss: 0.262856\n",
      "Iteration 466475 | Loss: 0.262856\n",
      "Iteration 466500 | Loss: 0.262856\n",
      "Iteration 466525 | Loss: 0.262856\n",
      "Iteration 466550 | Loss: 0.262856\n",
      "Iteration 466575 | Loss: 0.262856\n",
      "Iteration 466600 | Loss: 0.262856\n",
      "Iteration 466625 | Loss: 0.262856\n",
      "Iteration 466650 | Loss: 0.262856\n",
      "Iteration 466675 | Loss: 0.262856\n",
      "Iteration 466700 | Loss: 0.262856\n",
      "Iteration 466725 | Loss: 0.262856\n",
      "Iteration 466750 | Loss: 0.262856\n",
      "Iteration 466775 | Loss: 0.262856\n",
      "Iteration 466800 | Loss: 0.262856\n",
      "Iteration 466825 | Loss: 0.262856\n",
      "Iteration 466850 | Loss: 0.262856\n",
      "Iteration 466875 | Loss: 0.262856\n",
      "Iteration 466900 | Loss: 0.262856\n",
      "Iteration 466925 | Loss: 0.262856\n",
      "Iteration 466950 | Loss: 0.262856\n",
      "Iteration 466975 | Loss: 0.262856\n",
      "Iteration 467000 | Loss: 0.262856\n",
      "Iteration 467025 | Loss: 0.262856\n",
      "Iteration 467050 | Loss: 0.262856\n",
      "Iteration 467075 | Loss: 0.262856\n",
      "Iteration 467100 | Loss: 0.262856\n",
      "Iteration 467125 | Loss: 0.262856\n",
      "Iteration 467150 | Loss: 0.262856\n",
      "Iteration 467175 | Loss: 0.262856\n",
      "Iteration 467200 | Loss: 0.262856\n",
      "Iteration 467225 | Loss: 0.262856\n",
      "Iteration 467250 | Loss: 0.262856\n",
      "Iteration 467275 | Loss: 0.262856\n",
      "Iteration 467300 | Loss: 0.262856\n",
      "Iteration 467325 | Loss: 0.262856\n",
      "Iteration 467350 | Loss: 0.262856\n",
      "Iteration 467375 | Loss: 0.262856\n",
      "Iteration 467400 | Loss: 0.262856\n",
      "Iteration 467425 | Loss: 0.262856\n",
      "Iteration 467450 | Loss: 0.262856\n",
      "Iteration 467475 | Loss: 0.262856\n",
      "Iteration 467500 | Loss: 0.262856\n",
      "Iteration 467525 | Loss: 0.262856\n",
      "Iteration 467550 | Loss: 0.262856\n",
      "Iteration 467575 | Loss: 0.262856\n",
      "Iteration 467600 | Loss: 0.262856\n",
      "Iteration 467625 | Loss: 0.262856\n",
      "Iteration 467650 | Loss: 0.262856\n",
      "Iteration 467675 | Loss: 0.262856\n",
      "Iteration 467700 | Loss: 0.262856\n",
      "Iteration 467725 | Loss: 0.262856\n",
      "Iteration 467750 | Loss: 0.262856\n",
      "Iteration 467775 | Loss: 0.262856\n",
      "Iteration 467800 | Loss: 0.262856\n",
      "Iteration 467825 | Loss: 0.262856\n",
      "Iteration 467850 | Loss: 0.262856\n",
      "Iteration 467875 | Loss: 0.262856\n",
      "Iteration 467900 | Loss: 0.262856\n",
      "Iteration 467925 | Loss: 0.262856\n",
      "Iteration 467950 | Loss: 0.262856\n",
      "Iteration 467975 | Loss: 0.262856\n",
      "Iteration 468000 | Loss: 0.262856\n",
      "Iteration 468025 | Loss: 0.262856\n",
      "Iteration 468050 | Loss: 0.262856\n",
      "Iteration 468075 | Loss: 0.262856\n",
      "Iteration 468100 | Loss: 0.262856\n",
      "Iteration 468125 | Loss: 0.262856\n",
      "Iteration 468150 | Loss: 0.262856\n",
      "Iteration 468175 | Loss: 0.262856\n",
      "Iteration 468200 | Loss: 0.262856\n",
      "Iteration 468225 | Loss: 0.262856\n",
      "Iteration 468250 | Loss: 0.262856\n",
      "Iteration 468275 | Loss: 0.262856\n",
      "Iteration 468300 | Loss: 0.262856\n",
      "Iteration 468325 | Loss: 0.262856\n",
      "Iteration 468350 | Loss: 0.262856\n",
      "Iteration 468375 | Loss: 0.262856\n",
      "Iteration 468400 | Loss: 0.262856\n",
      "Iteration 468425 | Loss: 0.262856\n",
      "Iteration 468450 | Loss: 0.262856\n",
      "Iteration 468475 | Loss: 0.262856\n",
      "Iteration 468500 | Loss: 0.262856\n",
      "Iteration 468525 | Loss: 0.262856\n",
      "Iteration 468550 | Loss: 0.262856\n",
      "Iteration 468575 | Loss: 0.262856\n",
      "Iteration 468600 | Loss: 0.262856\n",
      "Iteration 468625 | Loss: 0.262856\n",
      "Iteration 468650 | Loss: 0.262856\n",
      "Iteration 468675 | Loss: 0.262856\n",
      "Iteration 468700 | Loss: 0.262856\n",
      "Iteration 468725 | Loss: 0.262856\n",
      "Iteration 468750 | Loss: 0.262856\n",
      "Iteration 468775 | Loss: 0.262856\n",
      "Iteration 468800 | Loss: 0.262856\n",
      "Iteration 468825 | Loss: 0.262856\n",
      "Iteration 468850 | Loss: 0.262856\n",
      "Iteration 468875 | Loss: 0.262856\n",
      "Iteration 468900 | Loss: 0.262856\n",
      "Iteration 468925 | Loss: 0.262856\n",
      "Iteration 468950 | Loss: 0.262856\n",
      "Iteration 468975 | Loss: 0.262856\n",
      "Iteration 469000 | Loss: 0.262856\n",
      "Iteration 469025 | Loss: 0.262856\n",
      "Iteration 469050 | Loss: 0.262856\n",
      "Iteration 469075 | Loss: 0.262856\n",
      "Iteration 469100 | Loss: 0.262856\n",
      "Iteration 469125 | Loss: 0.262856\n",
      "Iteration 469150 | Loss: 0.262856\n",
      "Iteration 469175 | Loss: 0.262856\n",
      "Iteration 469200 | Loss: 0.262856\n",
      "Iteration 469225 | Loss: 0.262856\n",
      "Iteration 469250 | Loss: 0.262856\n",
      "Iteration 469275 | Loss: 0.262856\n",
      "Iteration 469300 | Loss: 0.262856\n",
      "Iteration 469325 | Loss: 0.262856\n",
      "Iteration 469350 | Loss: 0.262856\n",
      "Iteration 469375 | Loss: 0.262856\n",
      "Iteration 469400 | Loss: 0.262856\n",
      "Iteration 469425 | Loss: 0.262856\n",
      "Iteration 469450 | Loss: 0.262856\n",
      "Iteration 469475 | Loss: 0.262856\n",
      "Iteration 469500 | Loss: 0.262856\n",
      "Iteration 469525 | Loss: 0.262856\n",
      "Iteration 469550 | Loss: 0.262856\n",
      "Iteration 469575 | Loss: 0.262856\n",
      "Iteration 469600 | Loss: 0.262856\n",
      "Iteration 469625 | Loss: 0.262856\n",
      "Iteration 469650 | Loss: 0.262856\n",
      "Iteration 469675 | Loss: 0.262856\n",
      "Iteration 469700 | Loss: 0.262856\n",
      "Iteration 469725 | Loss: 0.262856\n",
      "Iteration 469750 | Loss: 0.262856\n",
      "Iteration 469775 | Loss: 0.262856\n",
      "Iteration 469800 | Loss: 0.262856\n",
      "Iteration 469825 | Loss: 0.262856\n",
      "Iteration 469850 | Loss: 0.262856\n",
      "Iteration 469875 | Loss: 0.262856\n",
      "Iteration 469900 | Loss: 0.262856\n",
      "Iteration 469925 | Loss: 0.262856\n",
      "Iteration 469950 | Loss: 0.262856\n",
      "Iteration 469975 | Loss: 0.262856\n",
      "Iteration 470000 | Loss: 0.262856\n",
      "Iteration 470025 | Loss: 0.262856\n",
      "Iteration 470050 | Loss: 0.262856\n",
      "Iteration 470075 | Loss: 0.262856\n",
      "Iteration 470100 | Loss: 0.262856\n",
      "Iteration 470125 | Loss: 0.262856\n",
      "Iteration 470150 | Loss: 0.262856\n",
      "Iteration 470175 | Loss: 0.262856\n",
      "Iteration 470200 | Loss: 0.262856\n",
      "Iteration 470225 | Loss: 0.262856\n",
      "Iteration 470250 | Loss: 0.262856\n",
      "Iteration 470275 | Loss: 0.262856\n",
      "Iteration 470300 | Loss: 0.262856\n",
      "Iteration 470325 | Loss: 0.262856\n",
      "Iteration 470350 | Loss: 0.262856\n",
      "Iteration 470375 | Loss: 0.262856\n",
      "Iteration 470400 | Loss: 0.262856\n",
      "Iteration 470425 | Loss: 0.262856\n",
      "Iteration 470450 | Loss: 0.262856\n",
      "Iteration 470475 | Loss: 0.262856\n",
      "Iteration 470500 | Loss: 0.262856\n",
      "Iteration 470525 | Loss: 0.262856\n",
      "Iteration 470550 | Loss: 0.262856\n",
      "Iteration 470575 | Loss: 0.262856\n",
      "Iteration 470600 | Loss: 0.262856\n",
      "Iteration 470625 | Loss: 0.262856\n",
      "Iteration 470650 | Loss: 0.262856\n",
      "Iteration 470675 | Loss: 0.262856\n",
      "Iteration 470700 | Loss: 0.262856\n",
      "Iteration 470725 | Loss: 0.262856\n",
      "Iteration 470750 | Loss: 0.262856\n",
      "Iteration 470775 | Loss: 0.262856\n",
      "Iteration 470800 | Loss: 0.262856\n",
      "Iteration 470825 | Loss: 0.262856\n",
      "Iteration 470850 | Loss: 0.262856\n",
      "Iteration 470875 | Loss: 0.262856\n",
      "Iteration 470900 | Loss: 0.262856\n",
      "Iteration 470925 | Loss: 0.262856\n",
      "Iteration 470950 | Loss: 0.262856\n",
      "Iteration 470975 | Loss: 0.262856\n",
      "Iteration 471000 | Loss: 0.262856\n",
      "Iteration 471025 | Loss: 0.262856\n",
      "Iteration 471050 | Loss: 0.262856\n",
      "Iteration 471075 | Loss: 0.262856\n",
      "Iteration 471100 | Loss: 0.262856\n",
      "Iteration 471125 | Loss: 0.262856\n",
      "Iteration 471150 | Loss: 0.262856\n",
      "Iteration 471175 | Loss: 0.262856\n",
      "Iteration 471200 | Loss: 0.262856\n",
      "Iteration 471225 | Loss: 0.262856\n",
      "Iteration 471250 | Loss: 0.262856\n",
      "Iteration 471275 | Loss: 0.262856\n",
      "Iteration 471300 | Loss: 0.262856\n",
      "Iteration 471325 | Loss: 0.262856\n",
      "Iteration 471350 | Loss: 0.262856\n",
      "Iteration 471375 | Loss: 0.262856\n",
      "Iteration 471400 | Loss: 0.262856\n",
      "Iteration 471425 | Loss: 0.262856\n",
      "Iteration 471450 | Loss: 0.262856\n",
      "Iteration 471475 | Loss: 0.262856\n",
      "Iteration 471500 | Loss: 0.262856\n",
      "Iteration 471525 | Loss: 0.262856\n",
      "Iteration 471550 | Loss: 0.262856\n",
      "Iteration 471575 | Loss: 0.262856\n",
      "Iteration 471600 | Loss: 0.262856\n",
      "Iteration 471625 | Loss: 0.262856\n",
      "Iteration 471650 | Loss: 0.262856\n",
      "Iteration 471675 | Loss: 0.262856\n",
      "Iteration 471700 | Loss: 0.262856\n",
      "Iteration 471725 | Loss: 0.262856\n",
      "Iteration 471750 | Loss: 0.262856\n",
      "Iteration 471775 | Loss: 0.262856\n",
      "Iteration 471800 | Loss: 0.262856\n",
      "Iteration 471825 | Loss: 0.262856\n",
      "Iteration 471850 | Loss: 0.262856\n",
      "Iteration 471875 | Loss: 0.262856\n",
      "Iteration 471900 | Loss: 0.262856\n",
      "Iteration 471925 | Loss: 0.262856\n",
      "Iteration 471950 | Loss: 0.262856\n",
      "Iteration 471975 | Loss: 0.262856\n",
      "Iteration 472000 | Loss: 0.262856\n",
      "Iteration 472025 | Loss: 0.262856\n",
      "Iteration 472050 | Loss: 0.262856\n",
      "Iteration 472075 | Loss: 0.262856\n",
      "Iteration 472100 | Loss: 0.262856\n",
      "Iteration 472125 | Loss: 0.262856\n",
      "Iteration 472150 | Loss: 0.262856\n",
      "Iteration 472175 | Loss: 0.262856\n",
      "Iteration 472200 | Loss: 0.262856\n",
      "Iteration 472225 | Loss: 0.262856\n",
      "Iteration 472250 | Loss: 0.262856\n",
      "Iteration 472275 | Loss: 0.262856\n",
      "Iteration 472300 | Loss: 0.262856\n",
      "Iteration 472325 | Loss: 0.262856\n",
      "Iteration 472350 | Loss: 0.262856\n",
      "Iteration 472375 | Loss: 0.262856\n",
      "Iteration 472400 | Loss: 0.262856\n",
      "Iteration 472425 | Loss: 0.262856\n",
      "Iteration 472450 | Loss: 0.262856\n",
      "Iteration 472475 | Loss: 0.262856\n",
      "Iteration 472500 | Loss: 0.262856\n",
      "Iteration 472525 | Loss: 0.262856\n",
      "Iteration 472550 | Loss: 0.262856\n",
      "Iteration 472575 | Loss: 0.262856\n",
      "Iteration 472600 | Loss: 0.262856\n",
      "Iteration 472625 | Loss: 0.262856\n",
      "Iteration 472650 | Loss: 0.262856\n",
      "Iteration 472675 | Loss: 0.262856\n",
      "Iteration 472700 | Loss: 0.262856\n",
      "Iteration 472725 | Loss: 0.262856\n",
      "Iteration 472750 | Loss: 0.262856\n",
      "Iteration 472775 | Loss: 0.262856\n",
      "Iteration 472800 | Loss: 0.262856\n",
      "Iteration 472825 | Loss: 0.262856\n",
      "Iteration 472850 | Loss: 0.262856\n",
      "Iteration 472875 | Loss: 0.262856\n",
      "Iteration 472900 | Loss: 0.262856\n",
      "Iteration 472925 | Loss: 0.262856\n",
      "Iteration 472950 | Loss: 0.262856\n",
      "Iteration 472975 | Loss: 0.262856\n",
      "Iteration 473000 | Loss: 0.262856\n",
      "Iteration 473025 | Loss: 0.262856\n",
      "Iteration 473050 | Loss: 0.262856\n",
      "Iteration 473075 | Loss: 0.262856\n",
      "Iteration 473100 | Loss: 0.262856\n",
      "Iteration 473125 | Loss: 0.262856\n",
      "Iteration 473150 | Loss: 0.262856\n",
      "Iteration 473175 | Loss: 0.262856\n",
      "Iteration 473200 | Loss: 0.262856\n",
      "Iteration 473225 | Loss: 0.262856\n",
      "Iteration 473250 | Loss: 0.262856\n",
      "Iteration 473275 | Loss: 0.262856\n",
      "Iteration 473300 | Loss: 0.262856\n",
      "Iteration 473325 | Loss: 0.262856\n",
      "Iteration 473350 | Loss: 0.262856\n",
      "Iteration 473375 | Loss: 0.262856\n",
      "Iteration 473400 | Loss: 0.262856\n",
      "Iteration 473425 | Loss: 0.262856\n",
      "Iteration 473450 | Loss: 0.262856\n",
      "Iteration 473475 | Loss: 0.262856\n",
      "Iteration 473500 | Loss: 0.262856\n",
      "Iteration 473525 | Loss: 0.262856\n",
      "Iteration 473550 | Loss: 0.262856\n",
      "Iteration 473575 | Loss: 0.262856\n",
      "Iteration 473600 | Loss: 0.262856\n",
      "Iteration 473625 | Loss: 0.262856\n",
      "Iteration 473650 | Loss: 0.262856\n",
      "Iteration 473675 | Loss: 0.262856\n",
      "Iteration 473700 | Loss: 0.262856\n",
      "Iteration 473725 | Loss: 0.262856\n",
      "Iteration 473750 | Loss: 0.262856\n",
      "Iteration 473775 | Loss: 0.262856\n",
      "Iteration 473800 | Loss: 0.262856\n",
      "Iteration 473825 | Loss: 0.262856\n",
      "Iteration 473850 | Loss: 0.262856\n",
      "Iteration 473875 | Loss: 0.262856\n",
      "Iteration 473900 | Loss: 0.262856\n",
      "Iteration 473925 | Loss: 0.262856\n",
      "Iteration 473950 | Loss: 0.262856\n",
      "Iteration 473975 | Loss: 0.262856\n",
      "Iteration 474000 | Loss: 0.262856\n",
      "Iteration 474025 | Loss: 0.262856\n",
      "Iteration 474050 | Loss: 0.262856\n",
      "Iteration 474075 | Loss: 0.262856\n",
      "Iteration 474100 | Loss: 0.262856\n",
      "Iteration 474125 | Loss: 0.262856\n",
      "Iteration 474150 | Loss: 0.262856\n",
      "Iteration 474175 | Loss: 0.262856\n",
      "Iteration 474200 | Loss: 0.262856\n",
      "Iteration 474225 | Loss: 0.262856\n",
      "Iteration 474250 | Loss: 0.262856\n",
      "Iteration 474275 | Loss: 0.262856\n",
      "Iteration 474300 | Loss: 0.262856\n",
      "Iteration 474325 | Loss: 0.262856\n",
      "Iteration 474350 | Loss: 0.262856\n",
      "Iteration 474375 | Loss: 0.262856\n",
      "Iteration 474400 | Loss: 0.262856\n",
      "Iteration 474425 | Loss: 0.262856\n",
      "Iteration 474450 | Loss: 0.262856\n",
      "Iteration 474475 | Loss: 0.262856\n",
      "Iteration 474500 | Loss: 0.262856\n",
      "Iteration 474525 | Loss: 0.262856\n",
      "Iteration 474550 | Loss: 0.262856\n",
      "Iteration 474575 | Loss: 0.262856\n",
      "Iteration 474600 | Loss: 0.262856\n",
      "Iteration 474625 | Loss: 0.262856\n",
      "Iteration 474650 | Loss: 0.262856\n",
      "Iteration 474675 | Loss: 0.262856\n",
      "Iteration 474700 | Loss: 0.262856\n",
      "Iteration 474725 | Loss: 0.262856\n",
      "Iteration 474750 | Loss: 0.262856\n",
      "Iteration 474775 | Loss: 0.262856\n",
      "Iteration 474800 | Loss: 0.262856\n",
      "Iteration 474825 | Loss: 0.262856\n",
      "Iteration 474850 | Loss: 0.262856\n",
      "Iteration 474875 | Loss: 0.262856\n",
      "Iteration 474900 | Loss: 0.262856\n",
      "Iteration 474925 | Loss: 0.262856\n",
      "Iteration 474950 | Loss: 0.262856\n",
      "Iteration 474975 | Loss: 0.262856\n",
      "Iteration 475000 | Loss: 0.262856\n",
      "Iteration 475025 | Loss: 0.262856\n",
      "Iteration 475050 | Loss: 0.262856\n",
      "Iteration 475075 | Loss: 0.262856\n",
      "Iteration 475100 | Loss: 0.262856\n",
      "Iteration 475125 | Loss: 0.262856\n",
      "Iteration 475150 | Loss: 0.262856\n",
      "Iteration 475175 | Loss: 0.262856\n",
      "Iteration 475200 | Loss: 0.262856\n",
      "Iteration 475225 | Loss: 0.262856\n",
      "Iteration 475250 | Loss: 0.262856\n",
      "Iteration 475275 | Loss: 0.262856\n",
      "Iteration 475300 | Loss: 0.262856\n",
      "Iteration 475325 | Loss: 0.262856\n",
      "Iteration 475350 | Loss: 0.262856\n",
      "Iteration 475375 | Loss: 0.262856\n",
      "Iteration 475400 | Loss: 0.262856\n",
      "Iteration 475425 | Loss: 0.262856\n",
      "Iteration 475450 | Loss: 0.262856\n",
      "Iteration 475475 | Loss: 0.262856\n",
      "Iteration 475500 | Loss: 0.262856\n",
      "Iteration 475525 | Loss: 0.262856\n",
      "Iteration 475550 | Loss: 0.262856\n",
      "Iteration 475575 | Loss: 0.262856\n",
      "Iteration 475600 | Loss: 0.262856\n",
      "Iteration 475625 | Loss: 0.262856\n",
      "Iteration 475650 | Loss: 0.262856\n",
      "Iteration 475675 | Loss: 0.262856\n",
      "Iteration 475700 | Loss: 0.262856\n",
      "Iteration 475725 | Loss: 0.262856\n",
      "Iteration 475750 | Loss: 0.262856\n",
      "Iteration 475775 | Loss: 0.262856\n",
      "Iteration 475800 | Loss: 0.262856\n",
      "Iteration 475825 | Loss: 0.262856\n",
      "Iteration 475850 | Loss: 0.262856\n",
      "Iteration 475875 | Loss: 0.262856\n",
      "Iteration 475900 | Loss: 0.262856\n",
      "Iteration 475925 | Loss: 0.262856\n",
      "Iteration 475950 | Loss: 0.262856\n",
      "Iteration 475975 | Loss: 0.262856\n",
      "Iteration 476000 | Loss: 0.262856\n",
      "Iteration 476025 | Loss: 0.262856\n",
      "Iteration 476050 | Loss: 0.262856\n",
      "Iteration 476075 | Loss: 0.262856\n",
      "Iteration 476100 | Loss: 0.262856\n",
      "Iteration 476125 | Loss: 0.262856\n",
      "Iteration 476150 | Loss: 0.262856\n",
      "Iteration 476175 | Loss: 0.262856\n",
      "Iteration 476200 | Loss: 0.262856\n",
      "Iteration 476225 | Loss: 0.262856\n",
      "Iteration 476250 | Loss: 0.262856\n",
      "Iteration 476275 | Loss: 0.262856\n",
      "Iteration 476300 | Loss: 0.262856\n",
      "Iteration 476325 | Loss: 0.262856\n",
      "Iteration 476350 | Loss: 0.262856\n",
      "Iteration 476375 | Loss: 0.262856\n",
      "Iteration 476400 | Loss: 0.262856\n",
      "Iteration 476425 | Loss: 0.262856\n",
      "Iteration 476450 | Loss: 0.262856\n",
      "Iteration 476475 | Loss: 0.262856\n",
      "Iteration 476500 | Loss: 0.262856\n",
      "Iteration 476525 | Loss: 0.262856\n",
      "Iteration 476550 | Loss: 0.262856\n",
      "Iteration 476575 | Loss: 0.262856\n",
      "Iteration 476600 | Loss: 0.262856\n",
      "Iteration 476625 | Loss: 0.262856\n",
      "Iteration 476650 | Loss: 0.262856\n",
      "Iteration 476675 | Loss: 0.262856\n",
      "Iteration 476700 | Loss: 0.262856\n",
      "Iteration 476725 | Loss: 0.262856\n",
      "Iteration 476750 | Loss: 0.262856\n",
      "Iteration 476775 | Loss: 0.262856\n",
      "Iteration 476800 | Loss: 0.262856\n",
      "Iteration 476825 | Loss: 0.262856\n",
      "Iteration 476850 | Loss: 0.262856\n",
      "Iteration 476875 | Loss: 0.262856\n",
      "Iteration 476900 | Loss: 0.262856\n",
      "Iteration 476925 | Loss: 0.262856\n",
      "Iteration 476950 | Loss: 0.262856\n",
      "Iteration 476975 | Loss: 0.262856\n",
      "Iteration 477000 | Loss: 0.262856\n",
      "Iteration 477025 | Loss: 0.262856\n",
      "Iteration 477050 | Loss: 0.262856\n",
      "Iteration 477075 | Loss: 0.262856\n",
      "Iteration 477100 | Loss: 0.262856\n",
      "Iteration 477125 | Loss: 0.262856\n",
      "Iteration 477150 | Loss: 0.262856\n",
      "Iteration 477175 | Loss: 0.262856\n",
      "Iteration 477200 | Loss: 0.262856\n",
      "Iteration 477225 | Loss: 0.262856\n",
      "Iteration 477250 | Loss: 0.262856\n",
      "Iteration 477275 | Loss: 0.262856\n",
      "Iteration 477300 | Loss: 0.262856\n",
      "Iteration 477325 | Loss: 0.262856\n",
      "Iteration 477350 | Loss: 0.262856\n",
      "Iteration 477375 | Loss: 0.262856\n",
      "Iteration 477400 | Loss: 0.262856\n",
      "Iteration 477425 | Loss: 0.262856\n",
      "Iteration 477450 | Loss: 0.262856\n",
      "Iteration 477475 | Loss: 0.262856\n",
      "Iteration 477500 | Loss: 0.262856\n",
      "Iteration 477525 | Loss: 0.262856\n",
      "Iteration 477550 | Loss: 0.262856\n",
      "Iteration 477575 | Loss: 0.262856\n",
      "Iteration 477600 | Loss: 0.262856\n",
      "Iteration 477625 | Loss: 0.262856\n",
      "Iteration 477650 | Loss: 0.262856\n",
      "Iteration 477675 | Loss: 0.262856\n",
      "Iteration 477700 | Loss: 0.262856\n",
      "Iteration 477725 | Loss: 0.262856\n",
      "Iteration 477750 | Loss: 0.262856\n",
      "Iteration 477775 | Loss: 0.262856\n",
      "Iteration 477800 | Loss: 0.262856\n",
      "Iteration 477825 | Loss: 0.262856\n",
      "Iteration 477850 | Loss: 0.262856\n",
      "Iteration 477875 | Loss: 0.262856\n",
      "Iteration 477900 | Loss: 0.262856\n",
      "Iteration 477925 | Loss: 0.262856\n",
      "Iteration 477950 | Loss: 0.262856\n",
      "Iteration 477975 | Loss: 0.262856\n",
      "Iteration 478000 | Loss: 0.262856\n",
      "Iteration 478025 | Loss: 0.262856\n",
      "Iteration 478050 | Loss: 0.262856\n",
      "Iteration 478075 | Loss: 0.262856\n",
      "Iteration 478100 | Loss: 0.262856\n",
      "Iteration 478125 | Loss: 0.262856\n",
      "Iteration 478150 | Loss: 0.262856\n",
      "Iteration 478175 | Loss: 0.262856\n",
      "Iteration 478200 | Loss: 0.262856\n",
      "Iteration 478225 | Loss: 0.262856\n",
      "Iteration 478250 | Loss: 0.262856\n",
      "Iteration 478275 | Loss: 0.262856\n",
      "Iteration 478300 | Loss: 0.262856\n",
      "Iteration 478325 | Loss: 0.262856\n",
      "Iteration 478350 | Loss: 0.262856\n",
      "Iteration 478375 | Loss: 0.262856\n",
      "Iteration 478400 | Loss: 0.262856\n",
      "Iteration 478425 | Loss: 0.262856\n",
      "Iteration 478450 | Loss: 0.262856\n",
      "Iteration 478475 | Loss: 0.262856\n",
      "Iteration 478500 | Loss: 0.262856\n",
      "Iteration 478525 | Loss: 0.262856\n",
      "Iteration 478550 | Loss: 0.262856\n",
      "Iteration 478575 | Loss: 0.262856\n",
      "Iteration 478600 | Loss: 0.262856\n",
      "Iteration 478625 | Loss: 0.262856\n",
      "Iteration 478650 | Loss: 0.262856\n",
      "Iteration 478675 | Loss: 0.262856\n",
      "Iteration 478700 | Loss: 0.262856\n",
      "Iteration 478725 | Loss: 0.262856\n",
      "Iteration 478750 | Loss: 0.262856\n",
      "Iteration 478775 | Loss: 0.262856\n",
      "Iteration 478800 | Loss: 0.262856\n",
      "Iteration 478825 | Loss: 0.262856\n",
      "Iteration 478850 | Loss: 0.262856\n",
      "Iteration 478875 | Loss: 0.262856\n",
      "Iteration 478900 | Loss: 0.262856\n",
      "Iteration 478925 | Loss: 0.262856\n",
      "Iteration 478950 | Loss: 0.262856\n",
      "Iteration 478975 | Loss: 0.262856\n",
      "Iteration 479000 | Loss: 0.262856\n",
      "Iteration 479025 | Loss: 0.262856\n",
      "Iteration 479050 | Loss: 0.262856\n",
      "Iteration 479075 | Loss: 0.262856\n",
      "Iteration 479100 | Loss: 0.262856\n",
      "Iteration 479125 | Loss: 0.262856\n",
      "Iteration 479150 | Loss: 0.262856\n",
      "Iteration 479175 | Loss: 0.262856\n",
      "Iteration 479200 | Loss: 0.262856\n",
      "Iteration 479225 | Loss: 0.262856\n",
      "Iteration 479250 | Loss: 0.262856\n",
      "Iteration 479275 | Loss: 0.262856\n",
      "Iteration 479300 | Loss: 0.262856\n",
      "Iteration 479325 | Loss: 0.262856\n",
      "Iteration 479350 | Loss: 0.262856\n",
      "Iteration 479375 | Loss: 0.262856\n",
      "Iteration 479400 | Loss: 0.262856\n",
      "Iteration 479425 | Loss: 0.262856\n",
      "Iteration 479450 | Loss: 0.262856\n",
      "Iteration 479475 | Loss: 0.262856\n",
      "Iteration 479500 | Loss: 0.262856\n",
      "Iteration 479525 | Loss: 0.262856\n",
      "Iteration 479550 | Loss: 0.262856\n",
      "Iteration 479575 | Loss: 0.262856\n",
      "Iteration 479600 | Loss: 0.262856\n",
      "Iteration 479625 | Loss: 0.262856\n",
      "Iteration 479650 | Loss: 0.262856\n",
      "Iteration 479675 | Loss: 0.262856\n",
      "Iteration 479700 | Loss: 0.262856\n",
      "Iteration 479725 | Loss: 0.262856\n",
      "Iteration 479750 | Loss: 0.262856\n",
      "Iteration 479775 | Loss: 0.262856\n",
      "Iteration 479800 | Loss: 0.262856\n",
      "Iteration 479825 | Loss: 0.262856\n",
      "Iteration 479850 | Loss: 0.262856\n",
      "Iteration 479875 | Loss: 0.262856\n",
      "Iteration 479900 | Loss: 0.262856\n",
      "Iteration 479925 | Loss: 0.262856\n",
      "Iteration 479950 | Loss: 0.262856\n",
      "Iteration 479975 | Loss: 0.262856\n",
      "Iteration 480000 | Loss: 0.262856\n",
      "Iteration 480025 | Loss: 0.262856\n",
      "Iteration 480050 | Loss: 0.262856\n",
      "Iteration 480075 | Loss: 0.262856\n",
      "Iteration 480100 | Loss: 0.262856\n",
      "Iteration 480125 | Loss: 0.262856\n",
      "Iteration 480150 | Loss: 0.262856\n",
      "Iteration 480175 | Loss: 0.262856\n",
      "Iteration 480200 | Loss: 0.262856\n",
      "Iteration 480225 | Loss: 0.262856\n",
      "Iteration 480250 | Loss: 0.262856\n",
      "Iteration 480275 | Loss: 0.262856\n",
      "Iteration 480300 | Loss: 0.262856\n",
      "Iteration 480325 | Loss: 0.262856\n",
      "Iteration 480350 | Loss: 0.262856\n",
      "Iteration 480375 | Loss: 0.262856\n",
      "Iteration 480400 | Loss: 0.262856\n",
      "Iteration 480425 | Loss: 0.262856\n",
      "Iteration 480450 | Loss: 0.262856\n",
      "Iteration 480475 | Loss: 0.262856\n",
      "Iteration 480500 | Loss: 0.262856\n",
      "Iteration 480525 | Loss: 0.262856\n",
      "Iteration 480550 | Loss: 0.262856\n",
      "Iteration 480575 | Loss: 0.262856\n",
      "Iteration 480600 | Loss: 0.262856\n",
      "Iteration 480625 | Loss: 0.262856\n",
      "Iteration 480650 | Loss: 0.262856\n",
      "Iteration 480675 | Loss: 0.262856\n",
      "Iteration 480700 | Loss: 0.262856\n",
      "Iteration 480725 | Loss: 0.262856\n",
      "Iteration 480750 | Loss: 0.262856\n",
      "Iteration 480775 | Loss: 0.262856\n",
      "Iteration 480800 | Loss: 0.262856\n",
      "Iteration 480825 | Loss: 0.262856\n",
      "Iteration 480850 | Loss: 0.262856\n",
      "Iteration 480875 | Loss: 0.262856\n",
      "Iteration 480900 | Loss: 0.262856\n",
      "Iteration 480925 | Loss: 0.262856\n",
      "Iteration 480950 | Loss: 0.262856\n",
      "Iteration 480975 | Loss: 0.262856\n",
      "Iteration 481000 | Loss: 0.262856\n",
      "Iteration 481025 | Loss: 0.262856\n",
      "Iteration 481050 | Loss: 0.262856\n",
      "Iteration 481075 | Loss: 0.262856\n",
      "Iteration 481100 | Loss: 0.262856\n",
      "Iteration 481125 | Loss: 0.262856\n",
      "Iteration 481150 | Loss: 0.262856\n",
      "Iteration 481175 | Loss: 0.262856\n",
      "Iteration 481200 | Loss: 0.262856\n",
      "Iteration 481225 | Loss: 0.262856\n",
      "Iteration 481250 | Loss: 0.262856\n",
      "Iteration 481275 | Loss: 0.262856\n",
      "Iteration 481300 | Loss: 0.262856\n",
      "Iteration 481325 | Loss: 0.262856\n",
      "Iteration 481350 | Loss: 0.262856\n",
      "Iteration 481375 | Loss: 0.262856\n",
      "Iteration 481400 | Loss: 0.262856\n",
      "Iteration 481425 | Loss: 0.262856\n",
      "Iteration 481450 | Loss: 0.262856\n",
      "Iteration 481475 | Loss: 0.262856\n",
      "Iteration 481500 | Loss: 0.262856\n",
      "Iteration 481525 | Loss: 0.262856\n",
      "Iteration 481550 | Loss: 0.262856\n",
      "Iteration 481575 | Loss: 0.262856\n",
      "Iteration 481600 | Loss: 0.262856\n",
      "Iteration 481625 | Loss: 0.262856\n",
      "Iteration 481650 | Loss: 0.262856\n",
      "Iteration 481675 | Loss: 0.262856\n",
      "Iteration 481700 | Loss: 0.262856\n",
      "Iteration 481725 | Loss: 0.262856\n",
      "Iteration 481750 | Loss: 0.262856\n",
      "Iteration 481775 | Loss: 0.262856\n",
      "Iteration 481800 | Loss: 0.262856\n",
      "Iteration 481825 | Loss: 0.262856\n",
      "Iteration 481850 | Loss: 0.262856\n",
      "Iteration 481875 | Loss: 0.262856\n",
      "Iteration 481900 | Loss: 0.262856\n",
      "Iteration 481925 | Loss: 0.262856\n",
      "Iteration 481950 | Loss: 0.262856\n",
      "Iteration 481975 | Loss: 0.262856\n",
      "Iteration 482000 | Loss: 0.262856\n",
      "Iteration 482025 | Loss: 0.262856\n",
      "Iteration 482050 | Loss: 0.262856\n",
      "Iteration 482075 | Loss: 0.262856\n",
      "Iteration 482100 | Loss: 0.262856\n",
      "Iteration 482125 | Loss: 0.262856\n",
      "Iteration 482150 | Loss: 0.262856\n",
      "Iteration 482175 | Loss: 0.262856\n",
      "Iteration 482200 | Loss: 0.262856\n",
      "Iteration 482225 | Loss: 0.262856\n",
      "Iteration 482250 | Loss: 0.262856\n",
      "Iteration 482275 | Loss: 0.262855\n",
      "Iteration 482300 | Loss: 0.262855\n",
      "Iteration 482325 | Loss: 0.262855\n",
      "Iteration 482350 | Loss: 0.262855\n",
      "Iteration 482375 | Loss: 0.262855\n",
      "Iteration 482400 | Loss: 0.262855\n",
      "Iteration 482425 | Loss: 0.262855\n",
      "Iteration 482450 | Loss: 0.262855\n",
      "Iteration 482475 | Loss: 0.262855\n",
      "Iteration 482500 | Loss: 0.262855\n",
      "Iteration 482525 | Loss: 0.262855\n",
      "Iteration 482550 | Loss: 0.262855\n",
      "Iteration 482575 | Loss: 0.262855\n",
      "Iteration 482600 | Loss: 0.262855\n",
      "Iteration 482625 | Loss: 0.262855\n",
      "Iteration 482650 | Loss: 0.262855\n",
      "Iteration 482675 | Loss: 0.262855\n",
      "Iteration 482700 | Loss: 0.262855\n",
      "Iteration 482725 | Loss: 0.262855\n",
      "Iteration 482750 | Loss: 0.262855\n",
      "Iteration 482775 | Loss: 0.262855\n",
      "Iteration 482800 | Loss: 0.262855\n",
      "Iteration 482825 | Loss: 0.262855\n",
      "Iteration 482850 | Loss: 0.262855\n",
      "Iteration 482875 | Loss: 0.262855\n",
      "Iteration 482900 | Loss: 0.262855\n",
      "Iteration 482925 | Loss: 0.262855\n",
      "Iteration 482950 | Loss: 0.262855\n",
      "Iteration 482975 | Loss: 0.262855\n",
      "Iteration 483000 | Loss: 0.262855\n",
      "Iteration 483025 | Loss: 0.262855\n",
      "Iteration 483050 | Loss: 0.262855\n",
      "Iteration 483075 | Loss: 0.262855\n",
      "Iteration 483100 | Loss: 0.262855\n",
      "Iteration 483125 | Loss: 0.262855\n",
      "Iteration 483150 | Loss: 0.262855\n",
      "Iteration 483175 | Loss: 0.262855\n",
      "Iteration 483200 | Loss: 0.262855\n",
      "Iteration 483225 | Loss: 0.262855\n",
      "Iteration 483250 | Loss: 0.262855\n",
      "Iteration 483275 | Loss: 0.262855\n",
      "Iteration 483300 | Loss: 0.262855\n",
      "Iteration 483325 | Loss: 0.262855\n",
      "Iteration 483350 | Loss: 0.262855\n",
      "Iteration 483375 | Loss: 0.262855\n",
      "Iteration 483400 | Loss: 0.262855\n",
      "Iteration 483425 | Loss: 0.262855\n",
      "Iteration 483450 | Loss: 0.262855\n",
      "Iteration 483475 | Loss: 0.262855\n",
      "Iteration 483500 | Loss: 0.262855\n",
      "Iteration 483525 | Loss: 0.262855\n",
      "Iteration 483550 | Loss: 0.262855\n",
      "Iteration 483575 | Loss: 0.262855\n",
      "Iteration 483600 | Loss: 0.262855\n",
      "Iteration 483625 | Loss: 0.262855\n",
      "Iteration 483650 | Loss: 0.262855\n",
      "Iteration 483675 | Loss: 0.262855\n",
      "Iteration 483700 | Loss: 0.262855\n",
      "Iteration 483725 | Loss: 0.262855\n",
      "Iteration 483750 | Loss: 0.262855\n",
      "Iteration 483775 | Loss: 0.262855\n",
      "Iteration 483800 | Loss: 0.262855\n",
      "Iteration 483825 | Loss: 0.262855\n",
      "Iteration 483850 | Loss: 0.262855\n",
      "Iteration 483875 | Loss: 0.262855\n",
      "Iteration 483900 | Loss: 0.262855\n",
      "Iteration 483925 | Loss: 0.262855\n",
      "Iteration 483950 | Loss: 0.262855\n",
      "Iteration 483975 | Loss: 0.262855\n",
      "Iteration 484000 | Loss: 0.262855\n",
      "Iteration 484025 | Loss: 0.262855\n",
      "Iteration 484050 | Loss: 0.262855\n",
      "Iteration 484075 | Loss: 0.262855\n",
      "Iteration 484100 | Loss: 0.262855\n",
      "Iteration 484125 | Loss: 0.262855\n",
      "Iteration 484150 | Loss: 0.262855\n",
      "Iteration 484175 | Loss: 0.262855\n",
      "Iteration 484200 | Loss: 0.262855\n",
      "Iteration 484225 | Loss: 0.262855\n",
      "Iteration 484250 | Loss: 0.262855\n",
      "Iteration 484275 | Loss: 0.262855\n",
      "Iteration 484300 | Loss: 0.262855\n",
      "Iteration 484325 | Loss: 0.262855\n",
      "Iteration 484350 | Loss: 0.262855\n",
      "Iteration 484375 | Loss: 0.262855\n",
      "Iteration 484400 | Loss: 0.262855\n",
      "Iteration 484425 | Loss: 0.262855\n",
      "Iteration 484450 | Loss: 0.262855\n",
      "Iteration 484475 | Loss: 0.262855\n",
      "Iteration 484500 | Loss: 0.262855\n",
      "Iteration 484525 | Loss: 0.262855\n",
      "Iteration 484550 | Loss: 0.262855\n",
      "Iteration 484575 | Loss: 0.262855\n",
      "Iteration 484600 | Loss: 0.262855\n",
      "Iteration 484625 | Loss: 0.262855\n",
      "Iteration 484650 | Loss: 0.262855\n",
      "Iteration 484675 | Loss: 0.262855\n",
      "Iteration 484700 | Loss: 0.262855\n",
      "Iteration 484725 | Loss: 0.262855\n",
      "Iteration 484750 | Loss: 0.262855\n",
      "Iteration 484775 | Loss: 0.262855\n",
      "Iteration 484800 | Loss: 0.262855\n",
      "Iteration 484825 | Loss: 0.262855\n",
      "Iteration 484850 | Loss: 0.262855\n",
      "Iteration 484875 | Loss: 0.262855\n",
      "Iteration 484900 | Loss: 0.262855\n",
      "Iteration 484925 | Loss: 0.262855\n",
      "Iteration 484950 | Loss: 0.262855\n",
      "Iteration 484975 | Loss: 0.262855\n",
      "Iteration 485000 | Loss: 0.262855\n",
      "Iteration 485025 | Loss: 0.262855\n",
      "Iteration 485050 | Loss: 0.262855\n",
      "Iteration 485075 | Loss: 0.262855\n",
      "Iteration 485100 | Loss: 0.262855\n",
      "Iteration 485125 | Loss: 0.262855\n",
      "Iteration 485150 | Loss: 0.262855\n",
      "Iteration 485175 | Loss: 0.262855\n",
      "Iteration 485200 | Loss: 0.262855\n",
      "Iteration 485225 | Loss: 0.262855\n",
      "Iteration 485250 | Loss: 0.262855\n",
      "Iteration 485275 | Loss: 0.262855\n",
      "Iteration 485300 | Loss: 0.262855\n",
      "Iteration 485325 | Loss: 0.262855\n",
      "Iteration 485350 | Loss: 0.262855\n",
      "Iteration 485375 | Loss: 0.262855\n",
      "Iteration 485400 | Loss: 0.262855\n",
      "Iteration 485425 | Loss: 0.262855\n",
      "Iteration 485450 | Loss: 0.262855\n",
      "Iteration 485475 | Loss: 0.262855\n",
      "Iteration 485500 | Loss: 0.262855\n",
      "Iteration 485525 | Loss: 0.262855\n",
      "Iteration 485550 | Loss: 0.262855\n",
      "Iteration 485575 | Loss: 0.262855\n",
      "Iteration 485600 | Loss: 0.262855\n",
      "Iteration 485625 | Loss: 0.262855\n",
      "Iteration 485650 | Loss: 0.262855\n",
      "Iteration 485675 | Loss: 0.262855\n",
      "Iteration 485700 | Loss: 0.262855\n",
      "Iteration 485725 | Loss: 0.262855\n",
      "Iteration 485750 | Loss: 0.262855\n",
      "Iteration 485775 | Loss: 0.262855\n",
      "Iteration 485800 | Loss: 0.262855\n",
      "Iteration 485825 | Loss: 0.262855\n",
      "Iteration 485850 | Loss: 0.262855\n",
      "Iteration 485875 | Loss: 0.262855\n",
      "Iteration 485900 | Loss: 0.262855\n",
      "Iteration 485925 | Loss: 0.262855\n",
      "Iteration 485950 | Loss: 0.262855\n",
      "Iteration 485975 | Loss: 0.262855\n",
      "Iteration 486000 | Loss: 0.262855\n",
      "Iteration 486025 | Loss: 0.262855\n",
      "Iteration 486050 | Loss: 0.262855\n",
      "Iteration 486075 | Loss: 0.262855\n",
      "Iteration 486100 | Loss: 0.262855\n",
      "Iteration 486125 | Loss: 0.262855\n",
      "Iteration 486150 | Loss: 0.262855\n",
      "Iteration 486175 | Loss: 0.262855\n",
      "Iteration 486200 | Loss: 0.262855\n",
      "Iteration 486225 | Loss: 0.262855\n",
      "Iteration 486250 | Loss: 0.262855\n",
      "Iteration 486275 | Loss: 0.262855\n",
      "Iteration 486300 | Loss: 0.262855\n",
      "Iteration 486325 | Loss: 0.262855\n",
      "Iteration 486350 | Loss: 0.262855\n",
      "Iteration 486375 | Loss: 0.262855\n",
      "Iteration 486400 | Loss: 0.262855\n",
      "Iteration 486425 | Loss: 0.262855\n",
      "Iteration 486450 | Loss: 0.262855\n",
      "Iteration 486475 | Loss: 0.262855\n",
      "Iteration 486500 | Loss: 0.262855\n",
      "Iteration 486525 | Loss: 0.262855\n",
      "Iteration 486550 | Loss: 0.262855\n",
      "Iteration 486575 | Loss: 0.262855\n",
      "Iteration 486600 | Loss: 0.262855\n",
      "Iteration 486625 | Loss: 0.262855\n",
      "Iteration 486650 | Loss: 0.262855\n",
      "Iteration 486675 | Loss: 0.262855\n",
      "Iteration 486700 | Loss: 0.262855\n",
      "Iteration 486725 | Loss: 0.262855\n",
      "Iteration 486750 | Loss: 0.262855\n",
      "Iteration 486775 | Loss: 0.262855\n",
      "Iteration 486800 | Loss: 0.262855\n",
      "Iteration 486825 | Loss: 0.262855\n",
      "Iteration 486850 | Loss: 0.262855\n",
      "Iteration 486875 | Loss: 0.262855\n",
      "Iteration 486900 | Loss: 0.262855\n",
      "Iteration 486925 | Loss: 0.262855\n",
      "Iteration 486950 | Loss: 0.262855\n",
      "Iteration 486975 | Loss: 0.262855\n",
      "Iteration 487000 | Loss: 0.262855\n",
      "Iteration 487025 | Loss: 0.262855\n",
      "Iteration 487050 | Loss: 0.262855\n",
      "Iteration 487075 | Loss: 0.262855\n",
      "Iteration 487100 | Loss: 0.262855\n",
      "Iteration 487125 | Loss: 0.262855\n",
      "Iteration 487150 | Loss: 0.262855\n",
      "Iteration 487175 | Loss: 0.262855\n",
      "Iteration 487200 | Loss: 0.262855\n",
      "Iteration 487225 | Loss: 0.262855\n",
      "Iteration 487250 | Loss: 0.262855\n",
      "Iteration 487275 | Loss: 0.262855\n",
      "Iteration 487300 | Loss: 0.262855\n",
      "Iteration 487325 | Loss: 0.262855\n",
      "Iteration 487350 | Loss: 0.262855\n",
      "Iteration 487375 | Loss: 0.262855\n",
      "Iteration 487400 | Loss: 0.262855\n",
      "Iteration 487425 | Loss: 0.262855\n",
      "Iteration 487450 | Loss: 0.262855\n",
      "Iteration 487475 | Loss: 0.262855\n",
      "Iteration 487500 | Loss: 0.262855\n",
      "Iteration 487525 | Loss: 0.262855\n",
      "Iteration 487550 | Loss: 0.262855\n",
      "Iteration 487575 | Loss: 0.262855\n",
      "Iteration 487600 | Loss: 0.262855\n",
      "Iteration 487625 | Loss: 0.262855\n",
      "Iteration 487650 | Loss: 0.262855\n",
      "Iteration 487675 | Loss: 0.262855\n",
      "Iteration 487700 | Loss: 0.262855\n",
      "Iteration 487725 | Loss: 0.262855\n",
      "Iteration 487750 | Loss: 0.262855\n",
      "Iteration 487775 | Loss: 0.262855\n",
      "Iteration 487800 | Loss: 0.262855\n",
      "Iteration 487825 | Loss: 0.262855\n",
      "Iteration 487850 | Loss: 0.262855\n",
      "Iteration 487875 | Loss: 0.262855\n",
      "Iteration 487900 | Loss: 0.262855\n",
      "Iteration 487925 | Loss: 0.262855\n",
      "Iteration 487950 | Loss: 0.262855\n",
      "Iteration 487975 | Loss: 0.262855\n",
      "Iteration 488000 | Loss: 0.262855\n",
      "Iteration 488025 | Loss: 0.262855\n",
      "Iteration 488050 | Loss: 0.262855\n",
      "Iteration 488075 | Loss: 0.262855\n",
      "Iteration 488100 | Loss: 0.262855\n",
      "Iteration 488125 | Loss: 0.262855\n",
      "Iteration 488150 | Loss: 0.262855\n",
      "Iteration 488175 | Loss: 0.262855\n",
      "Iteration 488200 | Loss: 0.262855\n",
      "Iteration 488225 | Loss: 0.262855\n",
      "Iteration 488250 | Loss: 0.262855\n",
      "Iteration 488275 | Loss: 0.262855\n",
      "Iteration 488300 | Loss: 0.262855\n",
      "Iteration 488325 | Loss: 0.262855\n",
      "Iteration 488350 | Loss: 0.262855\n",
      "Iteration 488375 | Loss: 0.262855\n",
      "Iteration 488400 | Loss: 0.262855\n",
      "Iteration 488425 | Loss: 0.262855\n",
      "Iteration 488450 | Loss: 0.262855\n",
      "Iteration 488475 | Loss: 0.262855\n",
      "Iteration 488500 | Loss: 0.262855\n",
      "Iteration 488525 | Loss: 0.262855\n",
      "Iteration 488550 | Loss: 0.262855\n",
      "Iteration 488575 | Loss: 0.262855\n",
      "Iteration 488600 | Loss: 0.262855\n",
      "Iteration 488625 | Loss: 0.262855\n",
      "Iteration 488650 | Loss: 0.262855\n",
      "Iteration 488675 | Loss: 0.262855\n",
      "Iteration 488700 | Loss: 0.262855\n",
      "Iteration 488725 | Loss: 0.262855\n",
      "Iteration 488750 | Loss: 0.262855\n",
      "Iteration 488775 | Loss: 0.262855\n",
      "Iteration 488800 | Loss: 0.262855\n",
      "Iteration 488825 | Loss: 0.262855\n",
      "Iteration 488850 | Loss: 0.262855\n",
      "Iteration 488875 | Loss: 0.262855\n",
      "Iteration 488900 | Loss: 0.262855\n",
      "Iteration 488925 | Loss: 0.262855\n",
      "Iteration 488950 | Loss: 0.262855\n",
      "Iteration 488975 | Loss: 0.262855\n",
      "Iteration 489000 | Loss: 0.262855\n",
      "Iteration 489025 | Loss: 0.262855\n",
      "Iteration 489050 | Loss: 0.262855\n",
      "Iteration 489075 | Loss: 0.262855\n",
      "Iteration 489100 | Loss: 0.262855\n",
      "Iteration 489125 | Loss: 0.262855\n",
      "Iteration 489150 | Loss: 0.262855\n",
      "Iteration 489175 | Loss: 0.262855\n",
      "Iteration 489200 | Loss: 0.262855\n",
      "Iteration 489225 | Loss: 0.262855\n",
      "Iteration 489250 | Loss: 0.262855\n",
      "Iteration 489275 | Loss: 0.262855\n",
      "Iteration 489300 | Loss: 0.262855\n",
      "Iteration 489325 | Loss: 0.262855\n",
      "Iteration 489350 | Loss: 0.262855\n",
      "Iteration 489375 | Loss: 0.262855\n",
      "Iteration 489400 | Loss: 0.262855\n",
      "Iteration 489425 | Loss: 0.262855\n",
      "Iteration 489450 | Loss: 0.262855\n",
      "Iteration 489475 | Loss: 0.262855\n",
      "Iteration 489500 | Loss: 0.262855\n",
      "Iteration 489525 | Loss: 0.262855\n",
      "Iteration 489550 | Loss: 0.262855\n",
      "Iteration 489575 | Loss: 0.262855\n",
      "Iteration 489600 | Loss: 0.262855\n",
      "Iteration 489625 | Loss: 0.262855\n",
      "Iteration 489650 | Loss: 0.262855\n",
      "Iteration 489675 | Loss: 0.262855\n",
      "Iteration 489700 | Loss: 0.262855\n",
      "Iteration 489725 | Loss: 0.262855\n",
      "Iteration 489750 | Loss: 0.262855\n",
      "Iteration 489775 | Loss: 0.262855\n",
      "Iteration 489800 | Loss: 0.262855\n",
      "Iteration 489825 | Loss: 0.262855\n",
      "Iteration 489850 | Loss: 0.262855\n",
      "Iteration 489875 | Loss: 0.262855\n",
      "Iteration 489900 | Loss: 0.262855\n",
      "Iteration 489925 | Loss: 0.262855\n",
      "Iteration 489950 | Loss: 0.262855\n",
      "Iteration 489975 | Loss: 0.262855\n",
      "Iteration 490000 | Loss: 0.262855\n",
      "Iteration 490025 | Loss: 0.262855\n",
      "Iteration 490050 | Loss: 0.262855\n",
      "Iteration 490075 | Loss: 0.262855\n",
      "Iteration 490100 | Loss: 0.262855\n",
      "Iteration 490125 | Loss: 0.262855\n",
      "Iteration 490150 | Loss: 0.262855\n",
      "Iteration 490175 | Loss: 0.262855\n",
      "Iteration 490200 | Loss: 0.262855\n",
      "Iteration 490225 | Loss: 0.262855\n",
      "Iteration 490250 | Loss: 0.262855\n",
      "Iteration 490275 | Loss: 0.262855\n",
      "Iteration 490300 | Loss: 0.262855\n",
      "Iteration 490325 | Loss: 0.262855\n",
      "Iteration 490350 | Loss: 0.262855\n",
      "Iteration 490375 | Loss: 0.262855\n",
      "Iteration 490400 | Loss: 0.262855\n",
      "Iteration 490425 | Loss: 0.262855\n",
      "Iteration 490450 | Loss: 0.262855\n",
      "Iteration 490475 | Loss: 0.262855\n",
      "Iteration 490500 | Loss: 0.262855\n",
      "Iteration 490525 | Loss: 0.262855\n",
      "Iteration 490550 | Loss: 0.262855\n",
      "Iteration 490575 | Loss: 0.262855\n",
      "Iteration 490600 | Loss: 0.262855\n",
      "Iteration 490625 | Loss: 0.262855\n",
      "Iteration 490650 | Loss: 0.262855\n",
      "Iteration 490675 | Loss: 0.262855\n",
      "Iteration 490700 | Loss: 0.262855\n",
      "Iteration 490725 | Loss: 0.262855\n",
      "Iteration 490750 | Loss: 0.262855\n",
      "Iteration 490775 | Loss: 0.262855\n",
      "Iteration 490800 | Loss: 0.262855\n",
      "Iteration 490825 | Loss: 0.262855\n",
      "Iteration 490850 | Loss: 0.262855\n",
      "Iteration 490875 | Loss: 0.262855\n",
      "Iteration 490900 | Loss: 0.262855\n",
      "Iteration 490925 | Loss: 0.262855\n",
      "Iteration 490950 | Loss: 0.262855\n",
      "Iteration 490975 | Loss: 0.262855\n",
      "Iteration 491000 | Loss: 0.262855\n",
      "Iteration 491025 | Loss: 0.262855\n",
      "Iteration 491050 | Loss: 0.262855\n",
      "Iteration 491075 | Loss: 0.262855\n",
      "Iteration 491100 | Loss: 0.262855\n",
      "Iteration 491125 | Loss: 0.262855\n",
      "Iteration 491150 | Loss: 0.262855\n",
      "Iteration 491175 | Loss: 0.262855\n",
      "Iteration 491200 | Loss: 0.262855\n",
      "Iteration 491225 | Loss: 0.262855\n",
      "Iteration 491250 | Loss: 0.262855\n",
      "Iteration 491275 | Loss: 0.262855\n",
      "Iteration 491300 | Loss: 0.262855\n",
      "Iteration 491325 | Loss: 0.262855\n",
      "Iteration 491350 | Loss: 0.262855\n",
      "Iteration 491375 | Loss: 0.262855\n",
      "Iteration 491400 | Loss: 0.262855\n",
      "Iteration 491425 | Loss: 0.262855\n",
      "Iteration 491450 | Loss: 0.262855\n",
      "Iteration 491475 | Loss: 0.262855\n",
      "Iteration 491500 | Loss: 0.262855\n",
      "Iteration 491525 | Loss: 0.262855\n",
      "Iteration 491550 | Loss: 0.262855\n",
      "Iteration 491575 | Loss: 0.262855\n",
      "Iteration 491600 | Loss: 0.262855\n",
      "Iteration 491625 | Loss: 0.262855\n",
      "Iteration 491650 | Loss: 0.262855\n",
      "Iteration 491675 | Loss: 0.262855\n",
      "Iteration 491700 | Loss: 0.262855\n",
      "Iteration 491725 | Loss: 0.262855\n",
      "Iteration 491750 | Loss: 0.262855\n",
      "Iteration 491775 | Loss: 0.262855\n",
      "Iteration 491800 | Loss: 0.262855\n",
      "Iteration 491825 | Loss: 0.262855\n",
      "Iteration 491850 | Loss: 0.262855\n",
      "Iteration 491875 | Loss: 0.262855\n",
      "Iteration 491900 | Loss: 0.262855\n",
      "Iteration 491925 | Loss: 0.262855\n",
      "Iteration 491950 | Loss: 0.262855\n",
      "Iteration 491975 | Loss: 0.262855\n",
      "Iteration 492000 | Loss: 0.262855\n",
      "Iteration 492025 | Loss: 0.262855\n",
      "Iteration 492050 | Loss: 0.262855\n",
      "Iteration 492075 | Loss: 0.262855\n",
      "Iteration 492100 | Loss: 0.262855\n",
      "Iteration 492125 | Loss: 0.262855\n",
      "Iteration 492150 | Loss: 0.262855\n",
      "Iteration 492175 | Loss: 0.262855\n",
      "Iteration 492200 | Loss: 0.262855\n",
      "Iteration 492225 | Loss: 0.262855\n",
      "Iteration 492250 | Loss: 0.262855\n",
      "Iteration 492275 | Loss: 0.262855\n",
      "Iteration 492300 | Loss: 0.262855\n",
      "Iteration 492325 | Loss: 0.262855\n",
      "Iteration 492350 | Loss: 0.262855\n",
      "Iteration 492375 | Loss: 0.262855\n",
      "Iteration 492400 | Loss: 0.262855\n",
      "Iteration 492425 | Loss: 0.262855\n",
      "Iteration 492450 | Loss: 0.262855\n",
      "Iteration 492475 | Loss: 0.262855\n",
      "Iteration 492500 | Loss: 0.262855\n",
      "Iteration 492525 | Loss: 0.262855\n",
      "Iteration 492550 | Loss: 0.262855\n",
      "Iteration 492575 | Loss: 0.262855\n",
      "Iteration 492600 | Loss: 0.262855\n",
      "Iteration 492625 | Loss: 0.262855\n",
      "Iteration 492650 | Loss: 0.262855\n",
      "Iteration 492675 | Loss: 0.262855\n",
      "Iteration 492700 | Loss: 0.262855\n",
      "Iteration 492725 | Loss: 0.262855\n",
      "Iteration 492750 | Loss: 0.262855\n",
      "Iteration 492775 | Loss: 0.262855\n",
      "Iteration 492800 | Loss: 0.262855\n",
      "Iteration 492825 | Loss: 0.262855\n",
      "Iteration 492850 | Loss: 0.262855\n",
      "Iteration 492875 | Loss: 0.262855\n",
      "Iteration 492900 | Loss: 0.262855\n",
      "Iteration 492925 | Loss: 0.262855\n",
      "Iteration 492950 | Loss: 0.262855\n",
      "Iteration 492975 | Loss: 0.262855\n",
      "Iteration 493000 | Loss: 0.262855\n",
      "Iteration 493025 | Loss: 0.262855\n",
      "Iteration 493050 | Loss: 0.262855\n",
      "Iteration 493075 | Loss: 0.262855\n",
      "Iteration 493100 | Loss: 0.262855\n",
      "Iteration 493125 | Loss: 0.262855\n",
      "Iteration 493150 | Loss: 0.262855\n",
      "Iteration 493175 | Loss: 0.262855\n",
      "Iteration 493200 | Loss: 0.262855\n",
      "Iteration 493225 | Loss: 0.262855\n",
      "Iteration 493250 | Loss: 0.262855\n",
      "Iteration 493275 | Loss: 0.262855\n",
      "Iteration 493300 | Loss: 0.262855\n",
      "Iteration 493325 | Loss: 0.262855\n",
      "Iteration 493350 | Loss: 0.262855\n",
      "Iteration 493375 | Loss: 0.262855\n",
      "Iteration 493400 | Loss: 0.262855\n",
      "Iteration 493425 | Loss: 0.262855\n",
      "Iteration 493450 | Loss: 0.262855\n",
      "Iteration 493475 | Loss: 0.262855\n",
      "Iteration 493500 | Loss: 0.262855\n",
      "Iteration 493525 | Loss: 0.262855\n",
      "Iteration 493550 | Loss: 0.262855\n",
      "Iteration 493575 | Loss: 0.262855\n",
      "Iteration 493600 | Loss: 0.262855\n",
      "Iteration 493625 | Loss: 0.262855\n",
      "Iteration 493650 | Loss: 0.262855\n",
      "Iteration 493675 | Loss: 0.262855\n",
      "Iteration 493700 | Loss: 0.262855\n",
      "Iteration 493725 | Loss: 0.262855\n",
      "Iteration 493750 | Loss: 0.262855\n",
      "Iteration 493775 | Loss: 0.262855\n",
      "Iteration 493800 | Loss: 0.262855\n",
      "Iteration 493825 | Loss: 0.262855\n",
      "Iteration 493850 | Loss: 0.262855\n",
      "Iteration 493875 | Loss: 0.262855\n",
      "Iteration 493900 | Loss: 0.262855\n",
      "Iteration 493925 | Loss: 0.262855\n",
      "Iteration 493950 | Loss: 0.262855\n",
      "Iteration 493975 | Loss: 0.262855\n",
      "Iteration 494000 | Loss: 0.262855\n",
      "Iteration 494025 | Loss: 0.262855\n",
      "Iteration 494050 | Loss: 0.262855\n",
      "Iteration 494075 | Loss: 0.262855\n",
      "Iteration 494100 | Loss: 0.262855\n",
      "Iteration 494125 | Loss: 0.262855\n",
      "Iteration 494150 | Loss: 0.262855\n",
      "Iteration 494175 | Loss: 0.262855\n",
      "Iteration 494200 | Loss: 0.262855\n",
      "Iteration 494225 | Loss: 0.262855\n",
      "Iteration 494250 | Loss: 0.262855\n",
      "Iteration 494275 | Loss: 0.262855\n",
      "Iteration 494300 | Loss: 0.262855\n",
      "Iteration 494325 | Loss: 0.262855\n",
      "Iteration 494350 | Loss: 0.262855\n",
      "Iteration 494375 | Loss: 0.262855\n",
      "Iteration 494400 | Loss: 0.262855\n",
      "Iteration 494425 | Loss: 0.262855\n",
      "Iteration 494450 | Loss: 0.262855\n",
      "Iteration 494475 | Loss: 0.262855\n",
      "Iteration 494500 | Loss: 0.262855\n",
      "Iteration 494525 | Loss: 0.262855\n",
      "Iteration 494550 | Loss: 0.262855\n",
      "Iteration 494575 | Loss: 0.262855\n",
      "Iteration 494600 | Loss: 0.262855\n",
      "Iteration 494625 | Loss: 0.262855\n",
      "Iteration 494650 | Loss: 0.262855\n",
      "Iteration 494675 | Loss: 0.262855\n",
      "Iteration 494700 | Loss: 0.262855\n",
      "Iteration 494725 | Loss: 0.262855\n",
      "Iteration 494750 | Loss: 0.262855\n",
      "Iteration 494775 | Loss: 0.262855\n",
      "Iteration 494800 | Loss: 0.262855\n",
      "Iteration 494825 | Loss: 0.262855\n",
      "Iteration 494850 | Loss: 0.262855\n",
      "Iteration 494875 | Loss: 0.262855\n",
      "Iteration 494900 | Loss: 0.262855\n",
      "Iteration 494925 | Loss: 0.262855\n",
      "Iteration 494950 | Loss: 0.262855\n",
      "Iteration 494975 | Loss: 0.262855\n",
      "Iteration 495000 | Loss: 0.262855\n",
      "Iteration 495025 | Loss: 0.262855\n",
      "Iteration 495050 | Loss: 0.262855\n",
      "Iteration 495075 | Loss: 0.262855\n",
      "Iteration 495100 | Loss: 0.262855\n",
      "Iteration 495125 | Loss: 0.262855\n",
      "Iteration 495150 | Loss: 0.262855\n",
      "Iteration 495175 | Loss: 0.262855\n",
      "Iteration 495200 | Loss: 0.262855\n",
      "Iteration 495225 | Loss: 0.262855\n",
      "Iteration 495250 | Loss: 0.262855\n",
      "Iteration 495275 | Loss: 0.262855\n",
      "Iteration 495300 | Loss: 0.262855\n",
      "Iteration 495325 | Loss: 0.262855\n",
      "Iteration 495350 | Loss: 0.262855\n",
      "Iteration 495375 | Loss: 0.262855\n",
      "Iteration 495400 | Loss: 0.262855\n",
      "Iteration 495425 | Loss: 0.262855\n",
      "Iteration 495450 | Loss: 0.262855\n",
      "Iteration 495475 | Loss: 0.262855\n",
      "Iteration 495500 | Loss: 0.262855\n",
      "Iteration 495525 | Loss: 0.262855\n",
      "Iteration 495550 | Loss: 0.262855\n",
      "Iteration 495575 | Loss: 0.262855\n",
      "Iteration 495600 | Loss: 0.262855\n",
      "Iteration 495625 | Loss: 0.262855\n",
      "Iteration 495650 | Loss: 0.262855\n",
      "Iteration 495675 | Loss: 0.262855\n",
      "Iteration 495700 | Loss: 0.262855\n",
      "Iteration 495725 | Loss: 0.262855\n",
      "Iteration 495750 | Loss: 0.262855\n",
      "Iteration 495775 | Loss: 0.262855\n",
      "Iteration 495800 | Loss: 0.262855\n",
      "Iteration 495825 | Loss: 0.262855\n",
      "Iteration 495850 | Loss: 0.262855\n",
      "Iteration 495875 | Loss: 0.262855\n",
      "Iteration 495900 | Loss: 0.262855\n",
      "Iteration 495925 | Loss: 0.262855\n",
      "Iteration 495950 | Loss: 0.262855\n",
      "Iteration 495975 | Loss: 0.262855\n",
      "Iteration 496000 | Loss: 0.262855\n",
      "Iteration 496025 | Loss: 0.262855\n",
      "Iteration 496050 | Loss: 0.262855\n",
      "Iteration 496075 | Loss: 0.262855\n",
      "Iteration 496100 | Loss: 0.262855\n",
      "Iteration 496125 | Loss: 0.262855\n",
      "Iteration 496150 | Loss: 0.262855\n",
      "Iteration 496175 | Loss: 0.262855\n",
      "Iteration 496200 | Loss: 0.262855\n",
      "Iteration 496225 | Loss: 0.262855\n",
      "Iteration 496250 | Loss: 0.262855\n",
      "Iteration 496275 | Loss: 0.262855\n",
      "Iteration 496300 | Loss: 0.262855\n",
      "Iteration 496325 | Loss: 0.262855\n",
      "Iteration 496350 | Loss: 0.262855\n",
      "Iteration 496375 | Loss: 0.262855\n",
      "Iteration 496400 | Loss: 0.262855\n",
      "Iteration 496425 | Loss: 0.262855\n",
      "Iteration 496450 | Loss: 0.262855\n",
      "Iteration 496475 | Loss: 0.262855\n",
      "Iteration 496500 | Loss: 0.262855\n",
      "Iteration 496525 | Loss: 0.262855\n",
      "Iteration 496550 | Loss: 0.262855\n",
      "Iteration 496575 | Loss: 0.262855\n",
      "Iteration 496600 | Loss: 0.262855\n",
      "Iteration 496625 | Loss: 0.262855\n",
      "Iteration 496650 | Loss: 0.262855\n",
      "Iteration 496675 | Loss: 0.262855\n",
      "Iteration 496700 | Loss: 0.262855\n",
      "Iteration 496725 | Loss: 0.262855\n",
      "Iteration 496750 | Loss: 0.262855\n",
      "Iteration 496775 | Loss: 0.262855\n",
      "Iteration 496800 | Loss: 0.262855\n",
      "Iteration 496825 | Loss: 0.262855\n",
      "Iteration 496850 | Loss: 0.262855\n",
      "Iteration 496875 | Loss: 0.262855\n",
      "Iteration 496900 | Loss: 0.262855\n",
      "Iteration 496925 | Loss: 0.262855\n",
      "Iteration 496950 | Loss: 0.262855\n",
      "Iteration 496975 | Loss: 0.262855\n",
      "Iteration 497000 | Loss: 0.262855\n",
      "Iteration 497025 | Loss: 0.262855\n",
      "Iteration 497050 | Loss: 0.262855\n",
      "Iteration 497075 | Loss: 0.262855\n",
      "Iteration 497100 | Loss: 0.262855\n",
      "Iteration 497125 | Loss: 0.262855\n",
      "Iteration 497150 | Loss: 0.262855\n",
      "Iteration 497175 | Loss: 0.262855\n",
      "Iteration 497200 | Loss: 0.262855\n",
      "Iteration 497225 | Loss: 0.262855\n",
      "Iteration 497250 | Loss: 0.262855\n",
      "Iteration 497275 | Loss: 0.262855\n",
      "Iteration 497300 | Loss: 0.262855\n",
      "Iteration 497325 | Loss: 0.262855\n",
      "Iteration 497350 | Loss: 0.262855\n",
      "Iteration 497375 | Loss: 0.262855\n",
      "Iteration 497400 | Loss: 0.262855\n",
      "Iteration 497425 | Loss: 0.262855\n",
      "Iteration 497450 | Loss: 0.262855\n",
      "Iteration 497475 | Loss: 0.262855\n",
      "Iteration 497500 | Loss: 0.262855\n",
      "Iteration 497525 | Loss: 0.262855\n",
      "Iteration 497550 | Loss: 0.262855\n",
      "Iteration 497575 | Loss: 0.262855\n",
      "Iteration 497600 | Loss: 0.262855\n",
      "Iteration 497625 | Loss: 0.262855\n",
      "Iteration 497650 | Loss: 0.262855\n",
      "Iteration 497675 | Loss: 0.262855\n",
      "Iteration 497700 | Loss: 0.262855\n",
      "Iteration 497725 | Loss: 0.262855\n",
      "Iteration 497750 | Loss: 0.262855\n",
      "Iteration 497775 | Loss: 0.262855\n",
      "Iteration 497800 | Loss: 0.262855\n",
      "Iteration 497825 | Loss: 0.262855\n",
      "Iteration 497850 | Loss: 0.262855\n",
      "Iteration 497875 | Loss: 0.262855\n",
      "Iteration 497900 | Loss: 0.262855\n",
      "Iteration 497925 | Loss: 0.262855\n",
      "Iteration 497950 | Loss: 0.262855\n",
      "Iteration 497975 | Loss: 0.262855\n",
      "Iteration 498000 | Loss: 0.262855\n",
      "Iteration 498025 | Loss: 0.262855\n",
      "Iteration 498050 | Loss: 0.262855\n",
      "Iteration 498075 | Loss: 0.262855\n",
      "Iteration 498100 | Loss: 0.262855\n",
      "Iteration 498125 | Loss: 0.262855\n",
      "Iteration 498150 | Loss: 0.262855\n",
      "Iteration 498175 | Loss: 0.262855\n",
      "Iteration 498200 | Loss: 0.262855\n",
      "Iteration 498225 | Loss: 0.262855\n",
      "Iteration 498250 | Loss: 0.262855\n",
      "Iteration 498275 | Loss: 0.262855\n",
      "Iteration 498300 | Loss: 0.262855\n",
      "Iteration 498325 | Loss: 0.262855\n",
      "Iteration 498350 | Loss: 0.262855\n",
      "Iteration 498375 | Loss: 0.262855\n",
      "Iteration 498400 | Loss: 0.262855\n",
      "Iteration 498425 | Loss: 0.262855\n",
      "Iteration 498450 | Loss: 0.262855\n",
      "Iteration 498475 | Loss: 0.262855\n",
      "Iteration 498500 | Loss: 0.262855\n",
      "Iteration 498525 | Loss: 0.262855\n",
      "Iteration 498550 | Loss: 0.262855\n",
      "Iteration 498575 | Loss: 0.262855\n",
      "Iteration 498600 | Loss: 0.262855\n",
      "Iteration 498625 | Loss: 0.262855\n",
      "Iteration 498650 | Loss: 0.262855\n",
      "Iteration 498675 | Loss: 0.262855\n",
      "Iteration 498700 | Loss: 0.262855\n",
      "Iteration 498725 | Loss: 0.262855\n",
      "Iteration 498750 | Loss: 0.262855\n",
      "Iteration 498775 | Loss: 0.262855\n",
      "Iteration 498800 | Loss: 0.262855\n",
      "Iteration 498825 | Loss: 0.262855\n",
      "Iteration 498850 | Loss: 0.262855\n",
      "Iteration 498875 | Loss: 0.262855\n",
      "Iteration 498900 | Loss: 0.262855\n",
      "Iteration 498925 | Loss: 0.262855\n",
      "Iteration 498950 | Loss: 0.262855\n",
      "Iteration 498975 | Loss: 0.262855\n",
      "Iteration 499000 | Loss: 0.262855\n",
      "Iteration 499025 | Loss: 0.262855\n",
      "Iteration 499050 | Loss: 0.262855\n",
      "Iteration 499075 | Loss: 0.262855\n",
      "Iteration 499100 | Loss: 0.262855\n",
      "Iteration 499125 | Loss: 0.262855\n",
      "Iteration 499150 | Loss: 0.262855\n",
      "Iteration 499175 | Loss: 0.262855\n",
      "Iteration 499200 | Loss: 0.262855\n",
      "Iteration 499225 | Loss: 0.262855\n",
      "Iteration 499250 | Loss: 0.262855\n",
      "Iteration 499275 | Loss: 0.262855\n",
      "Iteration 499300 | Loss: 0.262855\n",
      "Iteration 499325 | Loss: 0.262855\n",
      "Iteration 499350 | Loss: 0.262855\n",
      "Iteration 499375 | Loss: 0.262855\n",
      "Iteration 499400 | Loss: 0.262855\n",
      "Iteration 499425 | Loss: 0.262855\n",
      "Iteration 499450 | Loss: 0.262855\n",
      "Iteration 499475 | Loss: 0.262855\n",
      "Iteration 499500 | Loss: 0.262855\n",
      "Iteration 499525 | Loss: 0.262855\n",
      "Iteration 499550 | Loss: 0.262855\n",
      "Iteration 499575 | Loss: 0.262855\n",
      "Iteration 499600 | Loss: 0.262855\n",
      "Iteration 499625 | Loss: 0.262855\n",
      "Iteration 499650 | Loss: 0.262855\n",
      "Iteration 499675 | Loss: 0.262855\n",
      "Iteration 499700 | Loss: 0.262855\n",
      "Iteration 499725 | Loss: 0.262855\n",
      "Iteration 499750 | Loss: 0.262855\n",
      "Iteration 499775 | Loss: 0.262855\n",
      "Iteration 499800 | Loss: 0.262855\n",
      "Iteration 499825 | Loss: 0.262855\n",
      "Iteration 499850 | Loss: 0.262855\n",
      "Iteration 499875 | Loss: 0.262855\n",
      "Iteration 499900 | Loss: 0.262855\n",
      "Iteration 499925 | Loss: 0.262855\n",
      "Iteration 499950 | Loss: 0.262855\n",
      "Iteration 499975 | Loss: 0.262855\n",
      "Iteration 499999 | Loss: 0.262855\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "learned_w, learned_b, losses = train_linear_regression(\n",
    "    X, y, \n",
    "    learning_rate= 0.00001, \n",
    "    n_iterations=500000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results ===\n",
      "True weights:    [ 2.  -3.5  1.5]\n",
      "Learned weights: [ 2.00857172 -3.51316002  1.48090014]\n",
      "\n",
      "True bias:    5.0\n",
      "Learned bias: 4.9908\n"
     ]
    }
   ],
   "source": [
    "# Compare learned parameters with true parameters\n",
    "print(\"\\n=== Results ===\")\n",
    "print(f\"True weights:    {true_weights}\")\n",
    "print(f\"Learned weights: {learned_w}\")\n",
    "print(f\"\\nTrue bias:    {true_bias}\")\n",
    "print(f\"Learned bias: {learned_b:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the loss curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Feature Scaling\n",
    "\n",
    "Gradient descent works much better when features are on similar scales. Let's see why and how to fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is statistical way to standarize any sameple of feature $i$ to a normal distribution:\n",
    "$$X_i = \\frac{x_i-\\mu_i}{\\sigma_i}$$\n",
    "where $X_i$ is the column vector of raw data feature $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranges:\n",
      "  Feature 1: [-3241.27, 3852.73]\n",
      "  Feature 2: [-0.00, 0.00]\n",
      "  Feature 3: [-2.90, 2.60]\n"
     ]
    }
   ],
   "source": [
    "# Create data with very different scales\n",
    "np.random.seed(42)\n",
    "X_unscaled = np.column_stack([\n",
    "    np.random.randn(500) * 1000,      # Feature 1: scale ~1000\n",
    "    np.random.randn(500) * 0.001,     # Feature 2: scale ~0.001\n",
    "    np.random.randn(500)              # Feature 3: scale ~1\n",
    "])\n",
    "y_unscaled = X_unscaled @ np.array([0.001, 1000, 1]) + 5 + np.random.randn(500) * 0.5\n",
    "\n",
    "print(f\"Feature ranges:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Feature {i+1}: [{X_unscaled[:, i].min():.2f}, {X_unscaled[:, i].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 | Loss: 85.696115\n",
      "Iteration   25 | Loss: 871155159202080805340365040358143631879229664798494767381715394830750039814045862849735767705550771108526879477364251653809771512373826652642481443263610550402662104844181197413872993691833211914127192672145978163200.000000\n",
      "Iteration   50 | Loss: inf\n",
      "Iteration   75 | Loss: nan\n",
      "Iteration   99 | Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheetah\\AppData\\Local\\Temp\\ipykernel_19700\\342151707.py:13: RuntimeWarning: overflow encountered in square\n",
      "  return np.mean((y_pred - y_true) ** 2)\n",
      "C:\\Users\\cheetah\\AppData\\Local\\Temp\\ipykernel_19700\\3547732573.py:14: RuntimeWarning: overflow encountered in matmul\n",
      "  return (2*((y_pred - y) @ X)/y.shape[0], 2*(np.mean(y_pred - y)))\n",
      "C:\\Users\\cheetah\\AppData\\Local\\Temp\\ipykernel_19700\\3547732573.py:14: RuntimeWarning: invalid value encountered in matmul\n",
      "  return (2*((y_pred - y) @ X)/y.shape[0], 2*(np.mean(y_pred - y)))\n",
      "d:\\CodeFile\\miniconda\\Lib\\site-packages\\numpy\\_core\\_methods.py:134: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "C:\\Users\\cheetah\\AppData\\Local\\Temp\\ipykernel_19700\\2670533896.py:36: RuntimeWarning: invalid value encountered in subtract\n",
      "  w = w - (learning_rate * gd_w)\n"
     ]
    }
   ],
   "source": [
    "# This will likely fail or converge very slowly!\n",
    "try:\n",
    "    w_bad, b_bad, losses_bad = train_linear_regression(\n",
    "        X_unscaled, y_unscaled, learning_rate=0.01, n_iterations=100\n",
    "    )\n",
    "except:\n",
    "    print(\"Training failed due to numerical instability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Implement standardize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Standardization (Z-score normalization)\n",
    "def standardize(X: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Standardize features to have mean=0 and std=1.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (X_standardized, mean, std)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    X_standardized = (X - mean) / std\n",
    "    return X_standardized, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled feature ranges:\n",
      "  Feature 1: [-3.31, 3.92]\n",
      "  Feature 2: [-2.79, 2.66]\n",
      "  Feature 3: [-2.98, 2.47]\n",
      "Iteration    0 | Loss: 29.787905\n",
      "Iteration   25 | Loss: 0.240939\n",
      "Iteration   50 | Loss: 0.240479\n",
      "Iteration   75 | Loss: 0.240479\n",
      "Iteration  100 | Loss: 0.240479\n",
      "Iteration  125 | Loss: 0.240479\n",
      "Iteration  150 | Loss: 0.240479\n",
      "Iteration  175 | Loss: 0.240479\n",
      "Iteration  200 | Loss: 0.240479\n",
      "Iteration  225 | Loss: 0.240479\n",
      "Iteration  250 | Loss: 0.240479\n",
      "Iteration  275 | Loss: 0.240479\n",
      "Iteration  300 | Loss: 0.240479\n",
      "Iteration  325 | Loss: 0.240479\n",
      "Iteration  350 | Loss: 0.240479\n",
      "Iteration  375 | Loss: 0.240479\n",
      "Iteration  400 | Loss: 0.240479\n",
      "Iteration  425 | Loss: 0.240479\n",
      "Iteration  450 | Loss: 0.240479\n",
      "Iteration  475 | Loss: 0.240479\n",
      "Iteration  499 | Loss: 0.240479\n"
     ]
    }
   ],
   "source": [
    "# Standardize and train\n",
    "X_scaled, X_mean, X_std = standardize(X_unscaled)\n",
    "\n",
    "print(f\"Scaled feature ranges:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Feature {i+1}: [{X_scaled[:, i].min():.2f}, {X_scaled[:, i].max():.2f}]\")\n",
    "\n",
    "w_good, b_good, losses_good = train_linear_regression(\n",
    "    X_scaled, y_unscaled, learning_rate=0.1, n_iterations=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Tasks (Deadline: Sunday 30th Nov 2025)\n",
    "\n",
    "Complete the following tasks to practice implementing gradient descent for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Implement Mini-Batch Gradient Descent\n",
    "\n",
    "Instead of using all samples in each iteration (batch gradient descent), implement **mini-batch gradient descent** which uses a random subset of samples.\n",
    "\n",
    "Formally said, choose $X_b$ and its corresponding $y_b$ which is a subset of $row(X), row(y)$ to be trained for each iteration.\n",
    "\n",
    "\n",
    "Benefits of mini-batch:\n",
    "- Faster iterations\n",
    "- Can escape local minima\n",
    "- Better generalization\n",
    "\n",
    "```python\n",
    "# Expected usage:\n",
    "w, b, losses = train_minibatch_gd(X, y, batch_size=32, learning_rate=0.01, n_iterations=1000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_minibatch_gd(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    batch_size: int = 32,\n",
    "    learning_rate: float = 0.01,\n",
    "    n_iterations: int = 1000,\n",
    "    verbose: bool = True,\n",
    "    log_every_n_step: int = 20,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression using mini-batch gradient descent.\n",
    "    \n",
    "    Hints:\n",
    "    - Use np.random.choice to select random indices\n",
    "    - Compute gradients using only the selected samples\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize parameters randomly\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Your code here\n",
    "        batch_index = np.random.choice(n_samples, size= batch_size, replace= False)\n",
    "\n",
    "        X_batch = X[batch_index]\n",
    "        y_batch = y[batch_index]\n",
    "\n",
    "        y_pred = predict(X_batch, w, b)\n",
    "        gd_w, gd_b = compute_gradients(X_batch, y_batch, y_pred)\n",
    "        \n",
    "        w = w - (learning_rate * gd_w)\n",
    "        b = b - (learning_rate * gd_b)\n",
    "        if verbose and (i % log_every_n_step == 0 or i == n_iterations - 1):\n",
    "            y_true_pred = predict(X, w, b)\n",
    "            loss = compute_mse(y, y_true_pred)\n",
    "            loss_history.append(loss)\n",
    "            print(f\"Iteration {i:4d} | Loss: {loss:.8f}\")\n",
    "    \n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 | Loss: 45.65667789\n",
      "Iteration   50 | Loss: 41.15168638\n",
      "Iteration  100 | Loss: 37.06600561\n",
      "Iteration  150 | Loss: 33.21211844\n",
      "Iteration  200 | Loss: 29.89126673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  250 | Loss: 26.94478713\n",
      "Iteration  300 | Loss: 24.30921277\n",
      "Iteration  350 | Loss: 21.91481915\n",
      "Iteration  400 | Loss: 19.81458390\n",
      "Iteration  450 | Loss: 17.88327783\n",
      "Iteration  500 | Loss: 16.11471385\n",
      "Iteration  550 | Loss: 14.55474491\n",
      "Iteration  600 | Loss: 13.12922128\n",
      "Iteration  650 | Loss: 11.84293116\n",
      "Iteration  700 | Loss: 10.67326419\n",
      "Iteration  750 | Loss: 9.63844795\n",
      "Iteration  800 | Loss: 8.71797702\n",
      "Iteration  850 | Loss: 7.89626162\n",
      "Iteration  900 | Loss: 7.13103581\n",
      "Iteration  950 | Loss: 6.45471432\n",
      "Iteration 1000 | Loss: 5.85261120\n",
      "Iteration 1050 | Loss: 5.29617885\n",
      "Iteration 1100 | Loss: 4.79292111\n",
      "Iteration 1150 | Loss: 4.33930279\n",
      "Iteration 1200 | Loss: 3.93839640\n",
      "Iteration 1250 | Loss: 3.58426863\n",
      "Iteration 1300 | Loss: 3.25962732\n",
      "Iteration 1350 | Loss: 2.96141684\n",
      "Iteration 1400 | Loss: 2.69228790\n",
      "Iteration 1450 | Loss: 2.44787975\n",
      "Iteration 1500 | Loss: 2.22852758\n",
      "Iteration 1550 | Loss: 2.03353351\n",
      "Iteration 1600 | Loss: 1.85801629\n",
      "Iteration 1650 | Loss: 1.70180460\n",
      "Iteration 1700 | Loss: 1.55617147\n",
      "Iteration 1750 | Loss: 1.42351925\n",
      "Iteration 1800 | Loss: 1.30922924\n",
      "Iteration 1850 | Loss: 1.20436108\n",
      "Iteration 1900 | Loss: 1.11212843\n",
      "Iteration 1950 | Loss: 1.03004063\n",
      "Iteration 2000 | Loss: 0.95584345\n",
      "Iteration 2050 | Loss: 0.88811539\n",
      "Iteration 2100 | Loss: 0.82482657\n",
      "Iteration 2150 | Loss: 0.77102932\n",
      "Iteration 2200 | Loss: 0.71927724\n",
      "Iteration 2250 | Loss: 0.67460597\n",
      "Iteration 2300 | Loss: 0.63479740\n",
      "Iteration 2350 | Loss: 0.59804168\n",
      "Iteration 2400 | Loss: 0.56635422\n",
      "Iteration 2450 | Loss: 0.53509965\n",
      "Iteration 2500 | Loss: 0.50786029\n",
      "Iteration 2550 | Loss: 0.48497657\n",
      "Iteration 2600 | Loss: 0.46295071\n",
      "Iteration 2650 | Loss: 0.44386287\n",
      "Iteration 2700 | Loss: 0.42544684\n",
      "Iteration 2750 | Loss: 0.40942296\n",
      "Iteration 2800 | Loss: 0.39515114\n",
      "Iteration 2850 | Loss: 0.38147598\n",
      "Iteration 2900 | Loss: 0.36976016\n",
      "Iteration 2950 | Loss: 0.35908398\n",
      "Iteration 3000 | Loss: 0.35014716\n",
      "Iteration 3050 | Loss: 0.34182806\n",
      "Iteration 3100 | Loss: 0.33347369\n",
      "Iteration 3150 | Loss: 0.32636277\n",
      "Iteration 3200 | Loss: 0.31965735\n",
      "Iteration 3250 | Loss: 0.31440720\n",
      "Iteration 3300 | Loss: 0.30928411\n",
      "Iteration 3350 | Loss: 0.30525225\n",
      "Iteration 3400 | Loss: 0.30109260\n",
      "Iteration 3450 | Loss: 0.29716783\n",
      "Iteration 3500 | Loss: 0.29406623\n",
      "Iteration 3550 | Loss: 0.29074921\n",
      "Iteration 3600 | Loss: 0.28810597\n",
      "Iteration 3650 | Loss: 0.28578990\n",
      "Iteration 3700 | Loss: 0.28330697\n",
      "Iteration 3750 | Loss: 0.28123541\n",
      "Iteration 3800 | Loss: 0.27935104\n",
      "Iteration 3850 | Loss: 0.27772405\n",
      "Iteration 3900 | Loss: 0.27609358\n",
      "Iteration 3950 | Loss: 0.27489943\n",
      "Iteration 4000 | Loss: 0.27361375\n",
      "Iteration 4050 | Loss: 0.27232544\n",
      "Iteration 4100 | Loss: 0.27137347\n",
      "Iteration 4150 | Loss: 0.27051019\n",
      "Iteration 4200 | Loss: 0.26981586\n",
      "Iteration 4250 | Loss: 0.26901304\n",
      "Iteration 4300 | Loss: 0.26838430\n",
      "Iteration 4350 | Loss: 0.26768504\n",
      "Iteration 4400 | Loss: 0.26723931\n",
      "Iteration 4450 | Loss: 0.26677174\n",
      "Iteration 4500 | Loss: 0.26640287\n",
      "Iteration 4550 | Loss: 0.26615226\n",
      "Iteration 4600 | Loss: 0.26582468\n",
      "Iteration 4650 | Loss: 0.26560698\n",
      "Iteration 4700 | Loss: 0.26543789\n",
      "Iteration 4750 | Loss: 0.26510139\n",
      "Iteration 4800 | Loss: 0.26485265\n",
      "Iteration 4850 | Loss: 0.26463715\n",
      "Iteration 4900 | Loss: 0.26447914\n",
      "Iteration 4950 | Loss: 0.26431692\n",
      "Iteration 5000 | Loss: 0.26419275\n",
      "Iteration 5050 | Loss: 0.26402231\n",
      "Iteration 5100 | Loss: 0.26390361\n",
      "Iteration 5150 | Loss: 0.26377994\n",
      "Iteration 5200 | Loss: 0.26368737\n",
      "Iteration 5250 | Loss: 0.26358368\n",
      "Iteration 5300 | Loss: 0.26350625\n",
      "Iteration 5350 | Loss: 0.26342886\n",
      "Iteration 5400 | Loss: 0.26334429\n",
      "Iteration 5450 | Loss: 0.26329185\n",
      "Iteration 5500 | Loss: 0.26323845\n",
      "Iteration 5550 | Loss: 0.26322144\n",
      "Iteration 5600 | Loss: 0.26318034\n",
      "Iteration 5650 | Loss: 0.26316701\n",
      "Iteration 5700 | Loss: 0.26313807\n",
      "Iteration 5750 | Loss: 0.26311922\n",
      "Iteration 5800 | Loss: 0.26309929\n",
      "Iteration 5850 | Loss: 0.26307058\n",
      "Iteration 5900 | Loss: 0.26306134\n",
      "Iteration 5950 | Loss: 0.26303973\n",
      "Iteration 6000 | Loss: 0.26304231\n",
      "Iteration 6050 | Loss: 0.26301769\n",
      "Iteration 6100 | Loss: 0.26302352\n",
      "Iteration 6150 | Loss: 0.26300616\n",
      "Iteration 6200 | Loss: 0.26299891\n",
      "Iteration 6250 | Loss: 0.26299560\n",
      "Iteration 6300 | Loss: 0.26299283\n",
      "Iteration 6350 | Loss: 0.26298194\n",
      "Iteration 6400 | Loss: 0.26297667\n",
      "Iteration 6450 | Loss: 0.26296591\n",
      "Iteration 6500 | Loss: 0.26295012\n",
      "Iteration 6550 | Loss: 0.26293947\n",
      "Iteration 6600 | Loss: 0.26292758\n",
      "Iteration 6650 | Loss: 0.26290874\n",
      "Iteration 6700 | Loss: 0.26289699\n",
      "Iteration 6750 | Loss: 0.26290201\n",
      "Iteration 6800 | Loss: 0.26290133\n",
      "Iteration 6850 | Loss: 0.26290142\n",
      "Iteration 6900 | Loss: 0.26290455\n",
      "Iteration 6950 | Loss: 0.26289906\n",
      "Iteration 7000 | Loss: 0.26288984\n",
      "Iteration 7050 | Loss: 0.26288411\n",
      "Iteration 7100 | Loss: 0.26288111\n",
      "Iteration 7150 | Loss: 0.26287279\n",
      "Iteration 7200 | Loss: 0.26287409\n",
      "Iteration 7250 | Loss: 0.26287522\n",
      "Iteration 7300 | Loss: 0.26287868\n",
      "Iteration 7350 | Loss: 0.26287828\n",
      "Iteration 7400 | Loss: 0.26287766\n",
      "Iteration 7450 | Loss: 0.26287374\n",
      "Iteration 7500 | Loss: 0.26287501\n",
      "Iteration 7550 | Loss: 0.26287495\n",
      "Iteration 7600 | Loss: 0.26287444\n",
      "Iteration 7650 | Loss: 0.26287239\n",
      "Iteration 7700 | Loss: 0.26286737\n",
      "Iteration 7750 | Loss: 0.26286414\n",
      "Iteration 7800 | Loss: 0.26286535\n",
      "Iteration 7850 | Loss: 0.26286419\n",
      "Iteration 7900 | Loss: 0.26286649\n",
      "Iteration 7950 | Loss: 0.26286966\n",
      "Iteration 8000 | Loss: 0.26287442\n",
      "Iteration 8050 | Loss: 0.26287624\n",
      "Iteration 8100 | Loss: 0.26287262\n",
      "Iteration 8150 | Loss: 0.26287167\n",
      "Iteration 8200 | Loss: 0.26287048\n",
      "Iteration 8250 | Loss: 0.26287321\n",
      "Iteration 8300 | Loss: 0.26287455\n",
      "Iteration 8350 | Loss: 0.26287438\n",
      "Iteration 8400 | Loss: 0.26286861\n",
      "Iteration 8450 | Loss: 0.26286661\n",
      "Iteration 8500 | Loss: 0.26286661\n",
      "Iteration 8550 | Loss: 0.26286371\n",
      "Iteration 8600 | Loss: 0.26286389\n",
      "Iteration 8650 | Loss: 0.26286436\n",
      "Iteration 8700 | Loss: 0.26286320\n",
      "Iteration 8750 | Loss: 0.26286273\n",
      "Iteration 8800 | Loss: 0.26286260\n",
      "Iteration 8850 | Loss: 0.26286642\n",
      "Iteration 8900 | Loss: 0.26286877\n",
      "Iteration 8950 | Loss: 0.26287052\n",
      "Iteration 9000 | Loss: 0.26287306\n",
      "Iteration 9050 | Loss: 0.26287140\n",
      "Iteration 9100 | Loss: 0.26287059\n",
      "Iteration 9150 | Loss: 0.26286611\n",
      "Iteration 9200 | Loss: 0.26285935\n",
      "Iteration 9250 | Loss: 0.26286103\n",
      "Iteration 9300 | Loss: 0.26286785\n",
      "Iteration 9350 | Loss: 0.26286463\n",
      "Iteration 9400 | Loss: 0.26286323\n",
      "Iteration 9450 | Loss: 0.26286523\n",
      "Iteration 9500 | Loss: 0.26287012\n",
      "Iteration 9550 | Loss: 0.26286771\n",
      "Iteration 9600 | Loss: 0.26286469\n",
      "Iteration 9650 | Loss: 0.26286122\n",
      "Iteration 9700 | Loss: 0.26286027\n",
      "Iteration 9750 | Loss: 0.26285845\n",
      "Iteration 9800 | Loss: 0.26285960\n",
      "Iteration 9850 | Loss: 0.26285956\n",
      "Iteration 9900 | Loss: 0.26286157\n",
      "Iteration 9950 | Loss: 0.26285961\n",
      "Iteration 10000 | Loss: 0.26285863\n",
      "Iteration 10050 | Loss: 0.26285763\n",
      "Iteration 10100 | Loss: 0.26286024\n",
      "Iteration 10150 | Loss: 0.26286048\n",
      "Iteration 10200 | Loss: 0.26286270\n",
      "Iteration 10250 | Loss: 0.26286315\n",
      "Iteration 10300 | Loss: 0.26286385\n",
      "Iteration 10350 | Loss: 0.26286212\n",
      "Iteration 10400 | Loss: 0.26285958\n",
      "Iteration 10450 | Loss: 0.26285726\n",
      "Iteration 10500 | Loss: 0.26285903\n",
      "Iteration 10550 | Loss: 0.26285957\n",
      "Iteration 10600 | Loss: 0.26285837\n",
      "Iteration 10650 | Loss: 0.26285860\n",
      "Iteration 10700 | Loss: 0.26285959\n",
      "Iteration 10750 | Loss: 0.26286217\n",
      "Iteration 10800 | Loss: 0.26286297\n",
      "Iteration 10850 | Loss: 0.26286250\n",
      "Iteration 10900 | Loss: 0.26286313\n",
      "Iteration 10950 | Loss: 0.26286538\n",
      "Iteration 11000 | Loss: 0.26286353\n",
      "Iteration 11050 | Loss: 0.26286408\n",
      "Iteration 11100 | Loss: 0.26286227\n",
      "Iteration 11150 | Loss: 0.26286184\n",
      "Iteration 11200 | Loss: 0.26286079\n",
      "Iteration 11250 | Loss: 0.26286224\n",
      "Iteration 11300 | Loss: 0.26286007\n",
      "Iteration 11350 | Loss: 0.26285991\n",
      "Iteration 11400 | Loss: 0.26286218\n",
      "Iteration 11450 | Loss: 0.26286593\n",
      "Iteration 11500 | Loss: 0.26286688\n",
      "Iteration 11550 | Loss: 0.26286891\n",
      "Iteration 11600 | Loss: 0.26286886\n",
      "Iteration 11650 | Loss: 0.26287215\n",
      "Iteration 11700 | Loss: 0.26287493\n",
      "Iteration 11750 | Loss: 0.26287843\n",
      "Iteration 11800 | Loss: 0.26287356\n",
      "Iteration 11850 | Loss: 0.26288090\n",
      "Iteration 11900 | Loss: 0.26287915\n",
      "Iteration 11950 | Loss: 0.26288085\n",
      "Iteration 12000 | Loss: 0.26287601\n",
      "Iteration 12050 | Loss: 0.26287175\n",
      "Iteration 12100 | Loss: 0.26287722\n",
      "Iteration 12150 | Loss: 0.26287241\n",
      "Iteration 12200 | Loss: 0.26287279\n",
      "Iteration 12250 | Loss: 0.26287133\n",
      "Iteration 12300 | Loss: 0.26287233\n",
      "Iteration 12350 | Loss: 0.26287514\n",
      "Iteration 12400 | Loss: 0.26287147\n",
      "Iteration 12450 | Loss: 0.26287318\n",
      "Iteration 12500 | Loss: 0.26287681\n",
      "Iteration 12550 | Loss: 0.26287990\n",
      "Iteration 12600 | Loss: 0.26287642\n",
      "Iteration 12650 | Loss: 0.26287531\n",
      "Iteration 12700 | Loss: 0.26287608\n",
      "Iteration 12750 | Loss: 0.26288001\n",
      "Iteration 12800 | Loss: 0.26287544\n",
      "Iteration 12850 | Loss: 0.26287699\n",
      "Iteration 12900 | Loss: 0.26287242\n",
      "Iteration 12950 | Loss: 0.26287086\n",
      "Iteration 13000 | Loss: 0.26287068\n",
      "Iteration 13050 | Loss: 0.26287073\n",
      "Iteration 13100 | Loss: 0.26286530\n",
      "Iteration 13150 | Loss: 0.26287027\n",
      "Iteration 13200 | Loss: 0.26286945\n",
      "Iteration 13250 | Loss: 0.26287119\n",
      "Iteration 13300 | Loss: 0.26287074\n",
      "Iteration 13350 | Loss: 0.26286644\n",
      "Iteration 13400 | Loss: 0.26286491\n",
      "Iteration 13450 | Loss: 0.26286340\n",
      "Iteration 13500 | Loss: 0.26286239\n",
      "Iteration 13550 | Loss: 0.26286532\n",
      "Iteration 13600 | Loss: 0.26286737\n",
      "Iteration 13650 | Loss: 0.26286766\n",
      "Iteration 13700 | Loss: 0.26286471\n",
      "Iteration 13750 | Loss: 0.26286336\n",
      "Iteration 13800 | Loss: 0.26286260\n",
      "Iteration 13850 | Loss: 0.26286302\n",
      "Iteration 13900 | Loss: 0.26286210\n",
      "Iteration 13950 | Loss: 0.26285873\n",
      "Iteration 14000 | Loss: 0.26285898\n",
      "Iteration 14050 | Loss: 0.26285927\n",
      "Iteration 14100 | Loss: 0.26286107\n",
      "Iteration 14150 | Loss: 0.26285939\n",
      "Iteration 14200 | Loss: 0.26285809\n",
      "Iteration 14250 | Loss: 0.26285719\n",
      "Iteration 14300 | Loss: 0.26285629\n",
      "Iteration 14350 | Loss: 0.26285715\n",
      "Iteration 14400 | Loss: 0.26285693\n",
      "Iteration 14450 | Loss: 0.26285768\n",
      "Iteration 14500 | Loss: 0.26285735\n",
      "Iteration 14550 | Loss: 0.26285821\n",
      "Iteration 14600 | Loss: 0.26285811\n",
      "Iteration 14650 | Loss: 0.26285761\n",
      "Iteration 14700 | Loss: 0.26285682\n",
      "Iteration 14750 | Loss: 0.26285634\n",
      "Iteration 14800 | Loss: 0.26285717\n",
      "Iteration 14850 | Loss: 0.26285746\n",
      "Iteration 14900 | Loss: 0.26285782\n",
      "Iteration 14950 | Loss: 0.26285945\n",
      "Iteration 15000 | Loss: 0.26286239\n",
      "Iteration 15050 | Loss: 0.26286325\n",
      "Iteration 15100 | Loss: 0.26286415\n",
      "Iteration 15150 | Loss: 0.26285916\n",
      "Iteration 15200 | Loss: 0.26285979\n",
      "Iteration 15250 | Loss: 0.26285892\n",
      "Iteration 15300 | Loss: 0.26285785\n",
      "Iteration 15350 | Loss: 0.26285625\n",
      "Iteration 15400 | Loss: 0.26285851\n",
      "Iteration 15450 | Loss: 0.26285984\n",
      "Iteration 15500 | Loss: 0.26285834\n",
      "Iteration 15550 | Loss: 0.26285900\n",
      "Iteration 15600 | Loss: 0.26285940\n",
      "Iteration 15650 | Loss: 0.26286098\n",
      "Iteration 15700 | Loss: 0.26285888\n",
      "Iteration 15750 | Loss: 0.26285913\n",
      "Iteration 15800 | Loss: 0.26285739\n",
      "Iteration 15850 | Loss: 0.26285836\n",
      "Iteration 15900 | Loss: 0.26285794\n",
      "Iteration 15950 | Loss: 0.26285753\n",
      "Iteration 16000 | Loss: 0.26285964\n",
      "Iteration 16050 | Loss: 0.26286020\n",
      "Iteration 16100 | Loss: 0.26285910\n",
      "Iteration 16150 | Loss: 0.26286087\n",
      "Iteration 16200 | Loss: 0.26285830\n",
      "Iteration 16250 | Loss: 0.26285930\n",
      "Iteration 16300 | Loss: 0.26285874\n",
      "Iteration 16350 | Loss: 0.26285907\n",
      "Iteration 16400 | Loss: 0.26285915\n",
      "Iteration 16450 | Loss: 0.26285891\n",
      "Iteration 16500 | Loss: 0.26285797\n",
      "Iteration 16550 | Loss: 0.26285866\n",
      "Iteration 16600 | Loss: 0.26285896\n",
      "Iteration 16650 | Loss: 0.26285867\n",
      "Iteration 16700 | Loss: 0.26285961\n",
      "Iteration 16750 | Loss: 0.26285977\n",
      "Iteration 16800 | Loss: 0.26286238\n",
      "Iteration 16850 | Loss: 0.26286319\n",
      "Iteration 16900 | Loss: 0.26286789\n",
      "Iteration 16950 | Loss: 0.26286978\n",
      "Iteration 17000 | Loss: 0.26287467\n",
      "Iteration 17050 | Loss: 0.26287395\n",
      "Iteration 17100 | Loss: 0.26287474\n",
      "Iteration 17150 | Loss: 0.26287873\n",
      "Iteration 17200 | Loss: 0.26287570\n",
      "Iteration 17250 | Loss: 0.26287350\n",
      "Iteration 17300 | Loss: 0.26286968\n",
      "Iteration 17350 | Loss: 0.26287113\n",
      "Iteration 17400 | Loss: 0.26286433\n",
      "Iteration 17450 | Loss: 0.26286344\n",
      "Iteration 17500 | Loss: 0.26286214\n",
      "Iteration 17550 | Loss: 0.26286340\n",
      "Iteration 17600 | Loss: 0.26286190\n",
      "Iteration 17650 | Loss: 0.26285865\n",
      "Iteration 17700 | Loss: 0.26285712\n",
      "Iteration 17750 | Loss: 0.26285905\n",
      "Iteration 17800 | Loss: 0.26286016\n",
      "Iteration 17850 | Loss: 0.26285862\n",
      "Iteration 17900 | Loss: 0.26286011\n",
      "Iteration 17950 | Loss: 0.26285986\n",
      "Iteration 18000 | Loss: 0.26286293\n",
      "Iteration 18050 | Loss: 0.26286551\n",
      "Iteration 18100 | Loss: 0.26286430\n",
      "Iteration 18150 | Loss: 0.26286494\n",
      "Iteration 18200 | Loss: 0.26286634\n",
      "Iteration 18250 | Loss: 0.26287063\n",
      "Iteration 18300 | Loss: 0.26286724\n",
      "Iteration 18350 | Loss: 0.26287673\n",
      "Iteration 18400 | Loss: 0.26287723\n",
      "Iteration 18450 | Loss: 0.26287794\n",
      "Iteration 18500 | Loss: 0.26287341\n",
      "Iteration 18550 | Loss: 0.26287044\n",
      "Iteration 18600 | Loss: 0.26287107\n",
      "Iteration 18650 | Loss: 0.26286715\n",
      "Iteration 18700 | Loss: 0.26286578\n",
      "Iteration 18750 | Loss: 0.26286354\n",
      "Iteration 18800 | Loss: 0.26286174\n",
      "Iteration 18850 | Loss: 0.26286082\n",
      "Iteration 18900 | Loss: 0.26286382\n",
      "Iteration 18950 | Loss: 0.26286468\n",
      "Iteration 19000 | Loss: 0.26286614\n",
      "Iteration 19050 | Loss: 0.26286556\n",
      "Iteration 19100 | Loss: 0.26286214\n",
      "Iteration 19150 | Loss: 0.26285866\n",
      "Iteration 19200 | Loss: 0.26285607\n",
      "Iteration 19250 | Loss: 0.26285578\n",
      "Iteration 19300 | Loss: 0.26285594\n",
      "Iteration 19350 | Loss: 0.26285697\n",
      "Iteration 19400 | Loss: 0.26285714\n",
      "Iteration 19450 | Loss: 0.26286181\n",
      "Iteration 19500 | Loss: 0.26286675\n",
      "Iteration 19550 | Loss: 0.26287405\n",
      "Iteration 19600 | Loss: 0.26287336\n",
      "Iteration 19650 | Loss: 0.26287035\n",
      "Iteration 19700 | Loss: 0.26287193\n",
      "Iteration 19750 | Loss: 0.26287143\n",
      "Iteration 19800 | Loss: 0.26286737\n",
      "Iteration 19850 | Loss: 0.26286735\n",
      "Iteration 19900 | Loss: 0.26286468\n",
      "Iteration 19950 | Loss: 0.26286389\n",
      "Iteration 19999 | Loss: 0.26286445\n"
     ]
    }
   ],
   "source": [
    "_, _, loss_history = train_minibatch_gd(\n",
    "    X, y,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.0005,\n",
    "    n_iterations=20000,\n",
    "    log_every_n_step=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement Learning Rate Scheduling\n",
    "\n",
    "Implement a training function that **decreases the learning rate** over time. This helps converge more precisely at the end of training.\n",
    "\n",
    "Common schedules:\n",
    "- Step decay: $\\alpha_t = \\alpha_0 \\cdot 0.9^{\\lfloor t/100 \\rfloor}$\n",
    "- Exponential decay: $\\alpha_t = \\alpha_0 \\cdot e^{-kt}$\n",
    "- Inverse time: $\\alpha_t = \\frac{\\alpha_0}{1 + k \\cdot t}$\n",
    "\n",
    "where $t$ is number of current step/iteration and $k$ is the decay constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_lr_schedule(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    initial_lr: float = 0.1,\n",
    "    schedule: str = 'exponential',  # 'step', 'exponential', or 'inverse'\n",
    "    n_iterations: int = 1000,\n",
    "    decay_constant: float = 0.0001,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train with learning rate scheduling.\n",
    "    \n",
    "    Implement at least one scheduling strategy.\n",
    "    \"\"\"\n",
    "\n",
    "    e=2.71828182845904523536028747135266249775724709369995957496696762772407663;n_samples,n_features=X.shape;w=np.random.randn(n_features)*0.01;b=0.0;learning_rate=initial_lr;loss_history=[];step=lambda i:initial_lr*pow(0.9,i//100);exponential=lambda i:initial_lr*pow(e,-decay_constant*i);inverse=lambda i:initial_lr/(1+decay_constant*i);lr_func=step if(schedule=='step')else(exponential if(schedule=='exponential')else(inverse if(schedule=='inverse')else None))\n",
    "    for _ in range(n_iterations):y_pred=predict(X, w, b);gd_w,gd_b=compute_gradients(X, y, y_pred);w=w-(learning_rate*gd_w);b=b-(learning_rate*gd_b);loss_history.append(compute_mse(y,y_pred))\n",
    "    \n",
    "    \n",
    "    #jk this is the real readable code\n",
    "    \"\"\"\n",
    "    e = 2.71828182845904523536028747135266249775724709369995957496696762772407663\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    learning_rate = initial_lr\n",
    "    loss_history = []\n",
    "\n",
    "    step = lambda i : initial_lr*pow(0.9,i//100)\n",
    "    exponential = lambda i : initial_lr*pow(e,-decay_constant*i)\n",
    "    inverse=lambda i : initial_lr/(1+decay_constant*i)\n",
    "    \n",
    "    if schedule == 'step':\n",
    "        lr_func = step\n",
    "    elif schedule == 'exponential':\n",
    "        lr_func = exponential\n",
    "    elif schedule == 'inverse':\n",
    "        lr_func = inverse\n",
    "    else:\n",
    "        lr_func = None\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Your code here\n",
    "        y_pred = predict(X, w, b)\n",
    "        gd_w, gd_b = compute_gradients(X, y, y_pred)\n",
    "        w = w - (learning_rate * gd_w)\n",
    "        b = b - (learning_rate * gd_b)\n",
    "        loss_history.append(compute_mse(y,y_pred))\n",
    "    \"\"\"\n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step decay:\n",
      "Exponential decay:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse time decay:\n",
      "Normal linear regression:\n",
      "Iteration    0 | Loss: 45.797114\n",
      "Iteration    1 | Loss: 43.906487\n",
      "Iteration    2 | Loss: 42.094426\n",
      "Iteration    3 | Loss: 40.357663\n",
      "Iteration    4 | Loss: 38.693067\n",
      "Iteration    5 | Loss: 37.097637\n",
      "Iteration    6 | Loss: 35.568496\n",
      "Iteration    7 | Loss: 34.102890\n",
      "Iteration    8 | Loss: 32.698175\n",
      "Iteration    9 | Loss: 31.351820\n",
      "Iteration   10 | Loss: 30.061399\n",
      "Iteration   11 | Loss: 28.824586\n",
      "Iteration   12 | Loss: 27.639153\n",
      "Iteration   13 | Loss: 26.502963\n",
      "Iteration   14 | Loss: 25.413969\n",
      "Iteration   15 | Loss: 24.370209\n",
      "Iteration   16 | Loss: 23.369804\n",
      "Iteration   17 | Loss: 22.410950\n",
      "Iteration   18 | Loss: 21.491921\n",
      "Iteration   19 | Loss: 20.611061\n",
      "Iteration   20 | Loss: 19.766783\n",
      "Iteration   21 | Loss: 18.957568\n",
      "Iteration   22 | Loss: 18.181957\n",
      "Iteration   23 | Loss: 17.438555\n",
      "Iteration   24 | Loss: 16.726023\n",
      "Iteration   25 | Loss: 16.043077\n",
      "Iteration   26 | Loss: 15.388489\n",
      "Iteration   27 | Loss: 14.761079\n",
      "Iteration   28 | Loss: 14.159719\n",
      "Iteration   29 | Loss: 13.583326\n",
      "Iteration   30 | Loss: 13.030862\n",
      "Iteration   31 | Loss: 12.501333\n",
      "Iteration   32 | Loss: 11.993787\n",
      "Iteration   33 | Loss: 11.507310\n",
      "Iteration   34 | Loss: 11.041027\n",
      "Iteration   35 | Loss: 10.594098\n",
      "Iteration   36 | Loss: 10.165720\n",
      "Iteration   37 | Loss: 9.755122\n",
      "Iteration   38 | Loss: 9.361565\n",
      "Iteration   39 | Loss: 8.984341\n",
      "Iteration   40 | Loss: 8.622773\n",
      "Iteration   41 | Loss: 8.276209\n",
      "Iteration   42 | Loss: 7.944027\n",
      "Iteration   43 | Loss: 7.625629\n",
      "Iteration   44 | Loss: 7.320443\n",
      "Iteration   45 | Loss: 7.027920\n",
      "Iteration   46 | Loss: 6.747533\n",
      "Iteration   47 | Loss: 6.478780\n",
      "Iteration   48 | Loss: 6.221177\n",
      "Iteration   49 | Loss: 5.974261\n",
      "Iteration   50 | Loss: 5.737587\n",
      "Iteration   51 | Loss: 5.510732\n",
      "Iteration   52 | Loss: 5.293287\n",
      "Iteration   53 | Loss: 5.084861\n",
      "Iteration   54 | Loss: 4.885081\n",
      "Iteration   55 | Loss: 4.693586\n",
      "Iteration   56 | Loss: 4.510034\n",
      "Iteration   57 | Loss: 4.334094\n",
      "Iteration   58 | Loss: 4.165450\n",
      "Iteration   59 | Loss: 4.003800\n",
      "Iteration   60 | Loss: 3.848853\n",
      "Iteration   61 | Loss: 3.700331\n",
      "Iteration   62 | Loss: 3.557968\n",
      "Iteration   63 | Loss: 3.421507\n",
      "Iteration   64 | Loss: 3.290703\n",
      "Iteration   65 | Loss: 3.165323\n",
      "Iteration   66 | Loss: 3.045140\n",
      "Iteration   67 | Loss: 2.929940\n",
      "Iteration   68 | Loss: 2.819515\n",
      "Iteration   69 | Loss: 2.713667\n",
      "Iteration   70 | Loss: 2.612207\n",
      "Iteration   71 | Loss: 2.514952\n",
      "Iteration   72 | Loss: 2.421727\n",
      "Iteration   73 | Loss: 2.332367\n",
      "Iteration   74 | Loss: 2.246709\n",
      "Iteration   75 | Loss: 2.164601\n",
      "Iteration   76 | Loss: 2.085896\n",
      "Iteration   77 | Loss: 2.010452\n",
      "Iteration   78 | Loss: 1.938133\n",
      "Iteration   79 | Loss: 1.868811\n",
      "Iteration   80 | Loss: 1.802362\n",
      "Iteration   81 | Loss: 1.738665\n",
      "Iteration   82 | Loss: 1.677606\n",
      "Iteration   83 | Loss: 1.619078\n",
      "Iteration   84 | Loss: 1.562973\n",
      "Iteration   85 | Loss: 1.509193\n",
      "Iteration   86 | Loss: 1.457639\n",
      "Iteration   87 | Loss: 1.408222\n",
      "Iteration   88 | Loss: 1.360850\n",
      "Iteration   89 | Loss: 1.315441\n",
      "Iteration   90 | Loss: 1.271912\n",
      "Iteration   91 | Loss: 1.230185\n",
      "Iteration   92 | Loss: 1.190186\n",
      "Iteration   93 | Loss: 1.151843\n",
      "Iteration   94 | Loss: 1.115088\n",
      "Iteration   95 | Loss: 1.079855\n",
      "Iteration   96 | Loss: 1.046080\n",
      "Iteration   97 | Loss: 1.013703\n",
      "Iteration   98 | Loss: 0.982666\n",
      "Iteration   99 | Loss: 0.952914\n"
     ]
    }
   ],
   "source": [
    "# Test them all:\n",
    "print(\"Step decay:\")\n",
    "_, _, loss_history_step_decay = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='step',\n",
    "    n_iterations=100,\n",
    "    decay_constant=0.0001\n",
    ")\n",
    "\n",
    "print(\"Exponential decay:\")\n",
    "_, _, loss_history_expo_decay = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='exponential',\n",
    "    n_iterations=100,\n",
    "    decay_constant=0.0001\n",
    ")\n",
    "\n",
    "print(\"Inverse time decay:\")\n",
    "_, _, loss_history_inve_decay = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='inverse',\n",
    "    n_iterations=100,\n",
    "    decay_constant=0.0001\n",
    ")\n",
    "\n",
    "print(\"Normal linear regression:\")\n",
    "_, _, loss_history_normal = train_linear_regression(\n",
    "    X, y,\n",
    "    learning_rate=0.01,\n",
    "    n_iterations=100,\n",
    "    verbose= True,\n",
    "    log_every_n_step=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 4000x2000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(40, 20))\n",
    "plt.plot(loss_history_step_decay, label='step_decay')\n",
    "plt.plot(loss_history_expo_decay, label='expo_decay')\n",
    "plt.plot(loss_history_inve_decay, label='inve_decay')\n",
    "plt.plot(loss_history_normal, label='normal')\n",
    "plt.xlim(95,99)\n",
    "plt.ylim(0.951, 1.085)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss over Time')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Add Regularization (Ridge Regression)\n",
    "\n",
    "Implement **L2 regularization** (Ridge regression) to prevent overfitting.\n",
    "\n",
    "The loss function becomes:\n",
    "$$\\mathcal{L} = \\mathcal{L}_{MSE} + \\lambda \\sum w_i^2$$\n",
    "\n",
    "The gradient for weights becomes:\n",
    "$$\\frac{\\partial Loss}{\\partial w} = \\frac{\\partial MSE}{\\partial w} + 2\\lambda w$$\n",
    "\n",
    "where $\\lambda$ is the regularization constant and $w_i$ is the weight value of corresponding feature $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ridge_loss(y_true: np.ndarray, y_pred: np.ndarray, w: np.ndarray, reg_lambda: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute Ridge regression loss (MSE + L2 regularization).\n",
    "    \n",
    "    Args:\n",
    "        y_true: Actual target values\n",
    "        y_pred: Predicted values\n",
    "        w: Weight vector\n",
    "        reg_lambda: Regularization strength\n",
    "    \n",
    "    Returns:\n",
    "        Ridge loss value\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    mse_loss = compute_mse(y_true, y_pred)\n",
    "    return mse_loss + (reg_lambda * np.sum(w ** 2))\n",
    "\n",
    "def calculate_ridge_gradients(X: np.ndarray, y: np.ndarray, y_pred: np.ndarray, w: np.ndarray, reg_lambda: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute gradients for Ridge regression.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: True target values\n",
    "        y_pred: Predicted values\n",
    "        w: Weight vector\n",
    "        reg_lambda: Regularization strength\n",
    "        \"\"\"\n",
    "    # Your code here\n",
    "    return compute_gradients(X, y, y_pred)[0] + (2 * reg_lambda * w), compute_gradients(X, y, y_pred)[1]\n",
    "\n",
    "def train_ridge_regression(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    learning_rate: float = 0.01,\n",
    "    reg_lambda: float = 0.1,  # Regularization strength\n",
    "    n_iterations: int = 1000\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression with L2 regularization.\n",
    "    \n",
    "    Hints:\n",
    "    - Modify the loss calculation to include regularization term\n",
    "    - Modify the gradient calculation for weights\n",
    "    - Note: We typically don't regularize the bias term\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize parameters randomly\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    loss_history = []\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        y_pred = predict(X, w, b)\n",
    "        gd_w, gd_b = calculate_ridge_gradients(X, y, y_pred, w, reg_lambda)\n",
    "\n",
    "        w -= learning_rate * gd_w\n",
    "        b -= learning_rate * gd_b\n",
    "        loss_history.append(compute_mse(y, y_pred))\n",
    "\n",
    "    y_pred = predict(X, w, b)\n",
    "    loss_history.append(compute_mse(y, y_pred))\n",
    "\n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b, loss_history_ridge =train_ridge_regression(\n",
    "    X, y,\n",
    "    learning_rate=0.01,\n",
    "    reg_lambda=0.1,\n",
    "    n_iterations=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 4000x2000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(40, 20))\n",
    "plt.plot(loss_history_step_decay, label='step_decay')\n",
    "plt.plot(loss_history_expo_decay, label='expo_decay')\n",
    "plt.plot(loss_history_inve_decay, label='inve_decay')\n",
    "plt.plot(loss_history_normal, label='normal')\n",
    "plt.plot(loss_history_ridge, label='ridge')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss over Time')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Task: Implement Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Implement pure SGD where you update weights after **each individual sample** (batch_size=1).\n",
    "\n",
    "Compare the convergence behavior of:\n",
    "1. Batch GD (all samples)\n",
    "2. Mini-batch GD (e.g., 32 samples)\n",
    "3. SGD (1 sample)\n",
    "\n",
    "Plot the loss curves for all three on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
