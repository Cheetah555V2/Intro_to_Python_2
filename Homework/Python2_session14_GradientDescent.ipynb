{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 14: Gradient Descent for Multivariate Linear Regression\n",
    "\n",
    "In this session, we will:\n",
    "1. Learn the basics of **NumPy** for numerical computing\n",
    "2. Understand **gradient descent** optimization\n",
    "3. Implement **multivariate linear regression** from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Introduction to NumPy\n",
    "\n",
    "NumPy is the fundamental package for numerical computing in Python. It provides:\n",
    "- N-dimensional arrays (`ndarray`)\n",
    "- Broadcasting for element-wise operations\n",
    "- Linear algebra operations\n",
    "- Mathematical functions optimized for arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Creating Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D array: [-1  2  3  4  5]\n",
      "Shape: (5,)\n",
      "Dtype: int8\n"
     ]
    }
   ],
   "source": [
    "# From Python lists\n",
    "arr1 = np.array([-1, 2, 3, 4, 5], dtype=np.int8)\n",
    "print(f\"1D array: {arr1}\")\n",
    "print(f\"Shape: {arr1.shape}\")\n",
    "print(f\"Dtype: {arr1.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D array:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "Shape: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "# 2D array (matrix)\n",
    "arr2 = np.array([[1, 2, 3], \n",
    "                 [4, 5, 6]])\n",
    "print(f\"2D array:\\n{arr2}\")\n",
    "print(f\"Shape: {arr2.shape}\")  # (rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeros:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "ones:\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "\n",
      "random:\n",
      "[[-0.71975298  0.67775704 -2.15175801]\n",
      " [-0.98528405  0.85324736 -0.04521529]\n",
      " [-0.11288205  1.27712606 -0.00579066]]\n",
      "\n",
      "arange: [0 2 4 6 8]\n",
      "linspace: [0.   0.25 0.5  0.75 1.  ]\n"
     ]
    }
   ],
   "source": [
    "# Common array creation functions\n",
    "zeros = np.zeros((3, 4))       # 3x4 matrix of zeros\n",
    "ones = np.ones((2, 3))         # 2x3 matrix of ones\n",
    "random = np.random.randn(3, 3) # 3x3 matrix of random values (normal distribution)\n",
    "arange = np.arange(0, 10, 2)   # [0, 2, 4, 6, 8]\n",
    "linspace = np.linspace(0, 1, 5) # 5 evenly spaced values from 0 to 1\n",
    "\n",
    "print(f\"zeros:\\n{zeros}\\n\")\n",
    "print(f\"ones:\\n{ones}\\n\")\n",
    "print(f\"random:\\n{random}\\n\")\n",
    "print(f\"arange: {arange}\")\n",
    "print(f\"linspace: {linspace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Array Operations and Broadcasting\n",
    "\n",
    "NumPy operations are **element-wise** by default. Broadcasting allows operations between arrays of different shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b = [5 7 9]\n",
      "a * b = [ 4 10 18]\n",
      "a ** 2 = [1 4 9]\n",
      "a * 10 = [10 20 30]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "print(f\"a + b = {a + b}\")      # Element-wise addition\n",
    "print(f\"a * b = {a * b}\")      # Element-wise multiplication\n",
    "print(f\"a ** 2 = {a ** 2}\")    # Element-wise power\n",
    "print(f\"a * 10 = {a * 10}\")    # Broadcasting: scalar applied to all elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix + row_vector:\n",
      "[[11 22 33]\n",
      " [14 25 36]]\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting with 2D arrays\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6]])\n",
    "row_vector = np.array([10, 20, 30])\n",
    "\n",
    "\"\"\"\n",
    "[[10, 20, 30],\n",
    "[10, 20 ,30]]\n",
    "\"\"\"\n",
    "\n",
    "# The row vector is \"broadcast\" to each row of the matrix\n",
    "result = matrix + row_vector\n",
    "print(f\"Matrix + row_vector:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Matrix Operations\n",
    "\n",
    "For linear regression, we need matrix multiplication and transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A @ B (matrix multiplication):\n",
      "[[19 22]\n",
      " [43 50]]\n",
      "\n",
      "np.dot(A, B):\n",
      "[[19 22]\n",
      " [43 50]]\n",
      "\n",
      "A.T (transpose):\n",
      "[[1 3]\n",
      " [2 4]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2], \n",
    "              [3, 4]])\n",
    "B = np.array([[5, 6], \n",
    "              [7, 8]])\n",
    "\n",
    "# Matrix multiplication (dot product)\n",
    "print(f\"A @ B (matrix multiplication):\\n{A @ B}\")\n",
    "print(f\"\\nnp.dot(A, B):\\n{np.dot(A, B)}\")\n",
    "\n",
    "# Transpose\n",
    "print(f\"\\nA.T (transpose):\\n{A.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Useful Aggregation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum all: 21\n",
      "Sum along rows (axis=1): [ 6 15]\n",
      "Sum along columns (axis=0): [5 7 9]\n",
      "Mean: 3.5\n",
      "Std: 1.707825127659933\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2, 3],\n",
    "                [4, 5, 6]])\n",
    "\n",
    "print(f\"Sum all: {np.sum(arr)}\")\n",
    "print(f\"Sum along rows (axis=1): {np.sum(arr, axis=1)}\")\n",
    "print(f\"Sum along columns (axis=0): {np.sum(arr, axis=0)}\")\n",
    "print(f\"Mean: {np.mean(arr)}\")\n",
    "print(f\"Std: {np.std(arr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Linear Regression Review\n",
    "\n",
    "### What is Linear Regression?\n",
    "\n",
    "Linear regression models the relationship between features $X$ and target $y$ as:\n",
    "\n",
    "$$\\hat{y} = X \\cdot w + b$$\n",
    "\n",
    "Where:\n",
    "- $X$ is the feature matrix of shape `(n_samples, n_features)`\n",
    "- $w$ is the weight vector of shape `(n_features,)`\n",
    "- $b$ is the bias (intercept) scalar\n",
    "- $\\hat{y}$ is the predicted values\n",
    "\n",
    "### The Goal\n",
    "\n",
    "Find $w$ and $b$ that minimize the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\\mathcal{L} = MSE = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients for Linear Regression\n",
    "\n",
    "For MSE loss with linear regression, the gradients are:\n",
    "\n",
    "$$\\nabla_w \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{\\partial MSE}{\\partial w} = \\frac{2}{n}  (\\hat{y} - y) X$$\n",
    "\n",
    "$$\\nabla_b \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{\\partial MSE}{\\partial b} = \\frac{2}{n} \\sum(\\hat{y} - y)$$\n",
    "\n",
    "Let's derive this step by step:\n",
    "\n",
    "1. $MSE = \\frac{1}{n}\\sum(\\hat{y} - y)^2 = \\frac{1}{n}\\sum(Xw + b - y)^2$\n",
    "\n",
    "2. Using chain rule: $\\frac{\\partial MSE}{\\partial w} = \\frac{2}{n} \\cdot (Xw + b - y) \\cdot X$\n",
    "\n",
    "3. In matrix form: $\\frac{\\partial MSE}{\\partial w} = \\frac{2}{n} (\\hat{y} - y) X$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Gradient Descent\n",
    "\n",
    "### What is Gradient Descent?\n",
    "\n",
    "Gradient descent is an optimization algorithm that iteratively updates parameters to minimize a loss function.\n",
    "\n",
    "Think of it like descending a mountain in fog - you can only feel the slope at your current position, so you take small steps in the steepest downward direction.\n",
    "\n",
    "### The Update Rule\n",
    "\n",
    "$$w_{new} = w_{old} - \\alpha \\cdot \\nabla_w \\mathcal{L} = w_{old} - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial w} $$\n",
    "\n",
    "Where $\\alpha$ is the **learning rate** - how big of a step we take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Implementation from Scratch\n",
    "\n",
    "Let's build our linear regression step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1000, 3)\n",
      "y shape: (1000,)\n",
      "True weights: [ 2.  -3.5  1.5]\n",
      "True bias: 5.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.49671415, -0.1382643 ,  0.64768854],\n",
       "       [ 1.52302986, -0.23415337, -0.23413696],\n",
       "       [ 1.57921282,  0.76743473, -0.46947439],\n",
       "       ...,\n",
       "       [ 0.60211832,  0.07203686, -0.21220897],\n",
       "       [-0.95191846,  0.07748052,  0.25775254],\n",
       "       [-1.24176058,  0.33417642, -0.15525905]], shape=(1000, 3))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's create some synthetic data\n",
    "np.random.seed(42)\n",
    "\n",
    "# True parameters we want to learn\n",
    "true_weights = np.array([2.0, -3.5, 1.5])\n",
    "true_bias = 5.0\n",
    "\n",
    "# Generate random features\n",
    "n_samples = 1000\n",
    "n_features = 3\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Generate target with some noise\n",
    "noise = np.random.randn(n_samples) * 0.5\n",
    "y = X @ true_weights + true_bias + noise\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"True weights: {true_weights}\")\n",
    "print(f\"True bias: {true_bias}\")\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Core Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write `predict()`, `compute_mse()`, `compute_gradients()`, which perform neccessary operations for gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: np.ndarray, w: np.ndarray, b: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute predictions for linear regression.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        w: Weight vector of shape (n_features,)\n",
    "        b: Bias scalar\n",
    "    \n",
    "    Returns:\n",
    "        Predictions of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    return (X @ w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Mean Squared Error.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Actual target values\n",
    "        y_pred: Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        MSE loss value\n",
    "    \"\"\"\n",
    "   # Your code here\n",
    "    return np.mean(np.sum((y_pred - y_true) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X: np.ndarray, y: np.ndarray, y_pred: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute gradients for weights and bias.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: True target values\n",
    "        y_pred: Predicted values\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (gradient_w, gradient_b)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    return (2*((y_pred - y) @ X)/y.shape[0], 2*(np.mean(y_pred - y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute first gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 1.]), 0.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.ones((3))\n",
    "b = 0.0\n",
    "w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 3), (3,), 0.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, w.shape, b # Should be ((1000, 3), (3,), (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-2.85186442,  9.29304439, -1.0522734 ]), np.float64(-10.19197986236907))\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X, w, b) # Initial predictions\n",
    "print(compute_gradients(X, y, y_pred)) # Should print gradients (grad_w, grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you observe what happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    learning_rate: float = 0.01, \n",
    "    n_iterations: int = 1000,\n",
    "    verbose: bool = True,\n",
    "    log_every_n_step = 25,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression using gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: Target values of shape (n_samples,)\n",
    "        learning_rate: Step size for gradient descent\n",
    "        n_iterations: Number of training iterations\n",
    "        verbose: Whether to print progress\n",
    "        log_every_n_step: Number of steps to log the result\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (final_weights, final_bias, loss_history)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize parameters randomly\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Your code here\n",
    "        y_pred = predict(X, w, b)\n",
    "        gd_w, gd_b = compute_gradients(X, y, y_pred)\n",
    "        \n",
    "        w = w - (learning_rate * gd_w)\n",
    "        b = b - (learning_rate * gd_b)\n",
    "        if verbose and (i % log_every_n_step == 0 or i == n_iterations - 1):\n",
    "            loss = compute_mse(y, y_pred)\n",
    "            loss_history.append(loss)\n",
    "            print(f\"Iteration {i:4d} | Loss: {loss:.6f}\")\n",
    "    \n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 | Loss: 45807.265849\n",
      "Iteration   25 | Loss: 44861.540275\n",
      "Iteration   50 | Loss: 43935.468910\n",
      "Iteration   75 | Loss: 43028.643001\n",
      "Iteration  100 | Loss: 42140.662300\n",
      "Iteration  125 | Loss: 41271.134887\n",
      "Iteration  150 | Loss: 40419.677000\n",
      "Iteration  175 | Loss: 39585.912862\n",
      "Iteration  200 | Loss: 38769.474516\n",
      "Iteration  225 | Loss: 37970.001661\n",
      "Iteration  250 | Loss: 37187.141495\n",
      "Iteration  275 | Loss: 36420.548557\n",
      "Iteration  300 | Loss: 35669.884573\n",
      "Iteration  325 | Loss: 34934.818311\n",
      "Iteration  350 | Loss: 34215.025429\n",
      "Iteration  375 | Loss: 33510.188334\n",
      "Iteration  400 | Loss: 32819.996043\n",
      "Iteration  425 | Loss: 32144.144041\n",
      "Iteration  450 | Loss: 31482.334151\n",
      "Iteration  475 | Loss: 30834.274401\n",
      "Iteration  500 | Loss: 30199.678891\n",
      "Iteration  525 | Loss: 29578.267672\n",
      "Iteration  550 | Loss: 28969.766619\n",
      "Iteration  575 | Loss: 28373.907309\n",
      "Iteration  600 | Loss: 27790.426906\n",
      "Iteration  625 | Loss: 27219.068042\n",
      "Iteration  650 | Loss: 26659.578701\n",
      "Iteration  675 | Loss: 26111.712113\n",
      "Iteration  700 | Loss: 25575.226640\n",
      "Iteration  725 | Loss: 25049.885673\n",
      "Iteration  750 | Loss: 24535.457524\n",
      "Iteration  775 | Loss: 24031.715325\n",
      "Iteration  800 | Loss: 23538.436929\n",
      "Iteration  825 | Loss: 23055.404807\n",
      "Iteration  850 | Loss: 22582.405960\n",
      "Iteration  875 | Loss: 22119.231815\n",
      "Iteration  900 | Loss: 21665.678143\n",
      "Iteration  925 | Loss: 21221.544958\n",
      "Iteration  950 | Loss: 20786.636439\n",
      "Iteration  975 | Loss: 20360.760835\n",
      "Iteration 1000 | Loss: 19943.730384\n",
      "Iteration 1025 | Loss: 19535.361231\n",
      "Iteration 1050 | Loss: 19135.473346\n",
      "Iteration 1075 | Loss: 18743.890440\n",
      "Iteration 1100 | Loss: 18360.439895\n",
      "Iteration 1125 | Loss: 17984.952681\n",
      "Iteration 1150 | Loss: 17617.263285\n",
      "Iteration 1175 | Loss: 17257.209635\n",
      "Iteration 1200 | Loss: 16904.633031\n",
      "Iteration 1225 | Loss: 16559.378075\n",
      "Iteration 1250 | Loss: 16221.292599\n",
      "Iteration 1275 | Loss: 15890.227600\n",
      "Iteration 1300 | Loss: 15566.037175\n",
      "Iteration 1325 | Loss: 15248.578455\n",
      "Iteration 1350 | Loss: 14937.711542\n",
      "Iteration 1375 | Loss: 14633.299448\n",
      "Iteration 1400 | Loss: 14335.208034\n",
      "Iteration 1425 | Loss: 14043.305949\n",
      "Iteration 1450 | Loss: 13757.464576\n",
      "Iteration 1475 | Loss: 13477.557972\n",
      "Iteration 1500 | Loss: 13203.462811\n",
      "Iteration 1525 | Loss: 12935.058335\n",
      "Iteration 1550 | Loss: 12672.226294\n",
      "Iteration 1575 | Loss: 12414.850899\n",
      "Iteration 1600 | Loss: 12162.818768\n",
      "Iteration 1625 | Loss: 11916.018878\n",
      "Iteration 1650 | Loss: 11674.342512\n",
      "Iteration 1675 | Loss: 11437.683216\n",
      "Iteration 1700 | Loss: 11205.936748\n",
      "Iteration 1725 | Loss: 10979.001035\n",
      "Iteration 1750 | Loss: 10756.776125\n",
      "Iteration 1775 | Loss: 10539.164144\n",
      "Iteration 1800 | Loss: 10326.069256\n",
      "Iteration 1825 | Loss: 10117.397613\n",
      "Iteration 1850 | Loss: 9913.057323\n",
      "Iteration 1875 | Loss: 9712.958400\n",
      "Iteration 1900 | Loss: 9517.012732\n",
      "Iteration 1925 | Loss: 9325.134038\n",
      "Iteration 1950 | Loss: 9137.237832\n",
      "Iteration 1975 | Loss: 8953.241381\n",
      "Iteration 2000 | Loss: 8773.063676\n",
      "Iteration 2025 | Loss: 8596.625390\n",
      "Iteration 2050 | Loss: 8423.848846\n",
      "Iteration 2075 | Loss: 8254.657981\n",
      "Iteration 2100 | Loss: 8088.978313\n",
      "Iteration 2125 | Loss: 7926.736910\n",
      "Iteration 2150 | Loss: 7767.862354\n",
      "Iteration 2175 | Loss: 7612.284714\n",
      "Iteration 2200 | Loss: 7459.935511\n",
      "Iteration 2225 | Loss: 7310.747690\n",
      "Iteration 2250 | Loss: 7164.655589\n",
      "Iteration 2275 | Loss: 7021.594912\n",
      "Iteration 2300 | Loss: 6881.502699\n",
      "Iteration 2325 | Loss: 6744.317299\n",
      "Iteration 2350 | Loss: 6609.978342\n",
      "Iteration 2375 | Loss: 6478.426713\n",
      "Iteration 2400 | Loss: 6349.604526\n",
      "Iteration 2425 | Loss: 6223.455098\n",
      "Iteration 2450 | Loss: 6099.922923\n",
      "Iteration 2475 | Loss: 5978.953650\n",
      "Iteration 2500 | Loss: 5860.494058\n",
      "Iteration 2525 | Loss: 5744.492029\n",
      "Iteration 2550 | Loss: 5630.896533\n",
      "Iteration 2575 | Loss: 5519.657597\n",
      "Iteration 2600 | Loss: 5410.726288\n",
      "Iteration 2625 | Loss: 5304.054688\n",
      "Iteration 2650 | Loss: 5199.595878\n",
      "Iteration 2675 | Loss: 5097.303912\n",
      "Iteration 2700 | Loss: 4997.133798\n",
      "Iteration 2725 | Loss: 4899.041481\n",
      "Iteration 2750 | Loss: 4802.983821\n",
      "Iteration 2775 | Loss: 4708.918573\n",
      "Iteration 2800 | Loss: 4616.804371\n",
      "Iteration 2825 | Loss: 4526.600709\n",
      "Iteration 2850 | Loss: 4438.267923\n",
      "Iteration 2875 | Loss: 4351.767170\n",
      "Iteration 2900 | Loss: 4267.060418\n",
      "Iteration 2925 | Loss: 4184.110423\n",
      "Iteration 2950 | Loss: 4102.880716\n",
      "Iteration 2975 | Loss: 4023.335585\n",
      "Iteration 3000 | Loss: 3945.440060\n",
      "Iteration 3025 | Loss: 3869.159897\n",
      "Iteration 3050 | Loss: 3794.461566\n",
      "Iteration 3075 | Loss: 3721.312230\n",
      "Iteration 3100 | Loss: 3649.679737\n",
      "Iteration 3125 | Loss: 3579.532602\n",
      "Iteration 3150 | Loss: 3510.839995\n",
      "Iteration 3175 | Loss: 3443.571724\n",
      "Iteration 3200 | Loss: 3377.698226\n",
      "Iteration 3225 | Loss: 3313.190553\n",
      "Iteration 3250 | Loss: 3250.020356\n",
      "Iteration 3275 | Loss: 3188.159878\n",
      "Iteration 3300 | Loss: 3127.581935\n",
      "Iteration 3325 | Loss: 3068.259911\n",
      "Iteration 3350 | Loss: 3010.167741\n",
      "Iteration 3375 | Loss: 2953.279901\n",
      "Iteration 3400 | Loss: 2897.571399\n",
      "Iteration 3425 | Loss: 2843.017760\n",
      "Iteration 3450 | Loss: 2789.595020\n",
      "Iteration 3475 | Loss: 2737.279710\n",
      "Iteration 3500 | Loss: 2686.048850\n",
      "Iteration 3525 | Loss: 2635.879938\n",
      "Iteration 3550 | Loss: 2586.750936\n",
      "Iteration 3575 | Loss: 2538.640269\n",
      "Iteration 3600 | Loss: 2491.526804\n",
      "Iteration 3625 | Loss: 2445.389853\n",
      "Iteration 3650 | Loss: 2400.209152\n",
      "Iteration 3675 | Loss: 2355.964863\n",
      "Iteration 3700 | Loss: 2312.637556\n",
      "Iteration 3725 | Loss: 2270.208206\n",
      "Iteration 3750 | Loss: 2228.658184\n",
      "Iteration 3775 | Loss: 2187.969246\n",
      "Iteration 3800 | Loss: 2148.123528\n",
      "Iteration 3825 | Loss: 2109.103537\n",
      "Iteration 3850 | Loss: 2070.892144\n",
      "Iteration 3875 | Loss: 2033.472573\n",
      "Iteration 3900 | Loss: 1996.828398\n",
      "Iteration 3925 | Loss: 1960.943536\n",
      "Iteration 3950 | Loss: 1925.802234\n",
      "Iteration 3975 | Loss: 1891.389070\n",
      "Iteration 4000 | Loss: 1857.688939\n",
      "Iteration 4025 | Loss: 1824.687051\n",
      "Iteration 4050 | Loss: 1792.368924\n",
      "Iteration 4075 | Loss: 1760.720375\n",
      "Iteration 4100 | Loss: 1729.727516\n",
      "Iteration 4125 | Loss: 1699.376748\n",
      "Iteration 4150 | Loss: 1669.654754\n",
      "Iteration 4175 | Loss: 1640.548493\n",
      "Iteration 4200 | Loss: 1612.045194\n",
      "Iteration 4225 | Loss: 1584.132354\n",
      "Iteration 4250 | Loss: 1556.797727\n",
      "Iteration 4275 | Loss: 1530.029322\n",
      "Iteration 4300 | Loss: 1503.815397\n",
      "Iteration 4325 | Loss: 1478.144452\n",
      "Iteration 4350 | Loss: 1453.005230\n",
      "Iteration 4375 | Loss: 1428.386703\n",
      "Iteration 4400 | Loss: 1404.278074\n",
      "Iteration 4425 | Loss: 1380.668771\n",
      "Iteration 4450 | Loss: 1357.548439\n",
      "Iteration 4475 | Loss: 1334.906941\n",
      "Iteration 4500 | Loss: 1312.734348\n",
      "Iteration 4525 | Loss: 1291.020938\n",
      "Iteration 4550 | Loss: 1269.757190\n",
      "Iteration 4575 | Loss: 1248.933781\n",
      "Iteration 4600 | Loss: 1228.541583\n",
      "Iteration 4625 | Loss: 1208.571655\n",
      "Iteration 4650 | Loss: 1189.015244\n",
      "Iteration 4675 | Loss: 1169.863776\n",
      "Iteration 4700 | Loss: 1151.108856\n",
      "Iteration 4725 | Loss: 1132.742265\n",
      "Iteration 4750 | Loss: 1114.755952\n",
      "Iteration 4775 | Loss: 1097.142035\n",
      "Iteration 4800 | Loss: 1079.892793\n",
      "Iteration 4825 | Loss: 1063.000669\n",
      "Iteration 4850 | Loss: 1046.458259\n",
      "Iteration 4875 | Loss: 1030.258315\n",
      "Iteration 4900 | Loss: 1014.393738\n",
      "Iteration 4925 | Loss: 998.857578\n",
      "Iteration 4950 | Loss: 983.643028\n",
      "Iteration 4975 | Loss: 968.743422\n",
      "Iteration 5000 | Loss: 954.152233\n",
      "Iteration 5025 | Loss: 939.863069\n",
      "Iteration 5050 | Loss: 925.869671\n",
      "Iteration 5075 | Loss: 912.165910\n",
      "Iteration 5100 | Loss: 898.745783\n",
      "Iteration 5125 | Loss: 885.603412\n",
      "Iteration 5150 | Loss: 872.733042\n",
      "Iteration 5175 | Loss: 860.129037\n",
      "Iteration 5200 | Loss: 847.785877\n",
      "Iteration 5225 | Loss: 835.698156\n",
      "Iteration 5250 | Loss: 823.860583\n",
      "Iteration 5275 | Loss: 812.267974\n",
      "Iteration 5300 | Loss: 800.915254\n",
      "Iteration 5325 | Loss: 789.797452\n",
      "Iteration 5350 | Loss: 778.909701\n",
      "Iteration 5375 | Loss: 768.247236\n",
      "Iteration 5400 | Loss: 757.805388\n",
      "Iteration 5425 | Loss: 747.579587\n",
      "Iteration 5450 | Loss: 737.565357\n",
      "Iteration 5475 | Loss: 727.758315\n",
      "Iteration 5500 | Loss: 718.154170\n",
      "Iteration 5525 | Loss: 708.748719\n",
      "Iteration 5550 | Loss: 699.537844\n",
      "Iteration 5575 | Loss: 690.517517\n",
      "Iteration 5600 | Loss: 681.683790\n",
      "Iteration 5625 | Loss: 673.032797\n",
      "Iteration 5650 | Loss: 664.560755\n",
      "Iteration 5675 | Loss: 656.263956\n",
      "Iteration 5700 | Loss: 648.138771\n",
      "Iteration 5725 | Loss: 640.181646\n",
      "Iteration 5750 | Loss: 632.389100\n",
      "Iteration 5775 | Loss: 624.757724\n",
      "Iteration 5800 | Loss: 617.284181\n",
      "Iteration 5825 | Loss: 609.965202\n",
      "Iteration 5850 | Loss: 602.797587\n",
      "Iteration 5875 | Loss: 595.778200\n",
      "Iteration 5900 | Loss: 588.903973\n",
      "Iteration 5925 | Loss: 582.171899\n",
      "Iteration 5950 | Loss: 575.579036\n",
      "Iteration 5975 | Loss: 569.122500\n",
      "Iteration 6000 | Loss: 562.799469\n",
      "Iteration 6025 | Loss: 556.607178\n",
      "Iteration 6050 | Loss: 550.542921\n",
      "Iteration 6075 | Loss: 544.604047\n",
      "Iteration 6100 | Loss: 538.787959\n",
      "Iteration 6125 | Loss: 533.092117\n",
      "Iteration 6150 | Loss: 527.514030\n",
      "Iteration 6175 | Loss: 522.051261\n",
      "Iteration 6200 | Loss: 516.701423\n",
      "Iteration 6225 | Loss: 511.462177\n",
      "Iteration 6250 | Loss: 506.331234\n",
      "Iteration 6275 | Loss: 501.306353\n",
      "Iteration 6300 | Loss: 496.385338\n",
      "Iteration 6325 | Loss: 491.566040\n",
      "Iteration 6350 | Loss: 486.846352\n",
      "Iteration 6375 | Loss: 482.224213\n",
      "Iteration 6400 | Loss: 477.697605\n",
      "Iteration 6425 | Loss: 473.264549\n",
      "Iteration 6450 | Loss: 468.923111\n",
      "Iteration 6475 | Loss: 464.671393\n",
      "Iteration 6500 | Loss: 460.507539\n",
      "Iteration 6525 | Loss: 456.429730\n",
      "Iteration 6550 | Loss: 452.436187\n",
      "Iteration 6575 | Loss: 448.525165\n",
      "Iteration 6600 | Loss: 444.694956\n",
      "Iteration 6625 | Loss: 440.943889\n",
      "Iteration 6650 | Loss: 437.270326\n",
      "Iteration 6675 | Loss: 433.672663\n",
      "Iteration 6700 | Loss: 430.149330\n",
      "Iteration 6725 | Loss: 426.698789\n",
      "Iteration 6750 | Loss: 423.319533\n",
      "Iteration 6775 | Loss: 420.010089\n",
      "Iteration 6800 | Loss: 416.769011\n",
      "Iteration 6825 | Loss: 413.594885\n",
      "Iteration 6850 | Loss: 410.486327\n",
      "Iteration 6875 | Loss: 407.441980\n",
      "Iteration 6900 | Loss: 404.460515\n",
      "Iteration 6925 | Loss: 401.540632\n",
      "Iteration 6950 | Loss: 398.681057\n",
      "Iteration 6975 | Loss: 395.880543\n",
      "Iteration 7000 | Loss: 393.137868\n",
      "Iteration 7025 | Loss: 390.451835\n",
      "Iteration 7050 | Loss: 387.821273\n",
      "Iteration 7075 | Loss: 385.245035\n",
      "Iteration 7100 | Loss: 382.721998\n",
      "Iteration 7125 | Loss: 380.251060\n",
      "Iteration 7150 | Loss: 377.831145\n",
      "Iteration 7175 | Loss: 375.461198\n",
      "Iteration 7200 | Loss: 373.140185\n",
      "Iteration 7225 | Loss: 370.867094\n",
      "Iteration 7250 | Loss: 368.640934\n",
      "Iteration 7275 | Loss: 366.460735\n",
      "Iteration 7300 | Loss: 364.325547\n",
      "Iteration 7325 | Loss: 362.234438\n",
      "Iteration 7350 | Loss: 360.186498\n",
      "Iteration 7375 | Loss: 358.180834\n",
      "Iteration 7400 | Loss: 356.216572\n",
      "Iteration 7425 | Loss: 354.292855\n",
      "Iteration 7450 | Loss: 352.408847\n",
      "Iteration 7475 | Loss: 350.563725\n",
      "Iteration 7500 | Loss: 348.756686\n",
      "Iteration 7525 | Loss: 346.986943\n",
      "Iteration 7550 | Loss: 345.253725\n",
      "Iteration 7575 | Loss: 343.556276\n",
      "Iteration 7600 | Loss: 341.893858\n",
      "Iteration 7625 | Loss: 340.265746\n",
      "Iteration 7650 | Loss: 338.671232\n",
      "Iteration 7675 | Loss: 337.109620\n",
      "Iteration 7700 | Loss: 335.580231\n",
      "Iteration 7725 | Loss: 334.082399\n",
      "Iteration 7750 | Loss: 332.615472\n",
      "Iteration 7775 | Loss: 331.178811\n",
      "Iteration 7800 | Loss: 329.771790\n",
      "Iteration 7825 | Loss: 328.393797\n",
      "Iteration 7850 | Loss: 327.044233\n",
      "Iteration 7875 | Loss: 325.722510\n",
      "Iteration 7900 | Loss: 324.428052\n",
      "Iteration 7925 | Loss: 323.160296\n",
      "Iteration 7950 | Loss: 321.918691\n",
      "Iteration 7975 | Loss: 320.702696\n",
      "Iteration 8000 | Loss: 319.511782\n",
      "Iteration 8025 | Loss: 318.345432\n",
      "Iteration 8050 | Loss: 317.203136\n",
      "Iteration 8075 | Loss: 316.084399\n",
      "Iteration 8100 | Loss: 314.988734\n",
      "Iteration 8125 | Loss: 313.915664\n",
      "Iteration 8150 | Loss: 312.864723\n",
      "Iteration 8175 | Loss: 311.835453\n",
      "Iteration 8200 | Loss: 310.827407\n",
      "Iteration 8225 | Loss: 309.840147\n",
      "Iteration 8250 | Loss: 308.873242\n",
      "Iteration 8275 | Loss: 307.926274\n",
      "Iteration 8300 | Loss: 306.998829\n",
      "Iteration 8325 | Loss: 306.090505\n",
      "Iteration 8350 | Loss: 305.200907\n",
      "Iteration 8375 | Loss: 304.329649\n",
      "Iteration 8400 | Loss: 303.476351\n",
      "Iteration 8425 | Loss: 302.640642\n",
      "Iteration 8450 | Loss: 301.822160\n",
      "Iteration 8475 | Loss: 301.020548\n",
      "Iteration 8500 | Loss: 300.235459\n",
      "Iteration 8525 | Loss: 299.466552\n",
      "Iteration 8550 | Loss: 298.713491\n",
      "Iteration 8575 | Loss: 297.975951\n",
      "Iteration 8600 | Loss: 297.253610\n",
      "Iteration 8625 | Loss: 296.546155\n",
      "Iteration 8650 | Loss: 295.853278\n",
      "Iteration 8675 | Loss: 295.174679\n",
      "Iteration 8700 | Loss: 294.510063\n",
      "Iteration 8725 | Loss: 293.859141\n",
      "Iteration 8750 | Loss: 293.221631\n",
      "Iteration 8775 | Loss: 292.597256\n",
      "Iteration 8800 | Loss: 291.985744\n",
      "Iteration 8825 | Loss: 291.386830\n",
      "Iteration 8850 | Loss: 290.800255\n",
      "Iteration 8875 | Loss: 290.225763\n",
      "Iteration 8900 | Loss: 289.663105\n",
      "Iteration 8925 | Loss: 289.112037\n",
      "Iteration 8950 | Loss: 288.572320\n",
      "Iteration 8975 | Loss: 288.043720\n",
      "Iteration 9000 | Loss: 287.526007\n",
      "Iteration 9025 | Loss: 287.018956\n",
      "Iteration 9050 | Loss: 286.522348\n",
      "Iteration 9075 | Loss: 286.035967\n",
      "Iteration 9100 | Loss: 285.559603\n",
      "Iteration 9125 | Loss: 285.093047\n",
      "Iteration 9150 | Loss: 284.636099\n",
      "Iteration 9175 | Loss: 284.188560\n",
      "Iteration 9200 | Loss: 283.750236\n",
      "Iteration 9225 | Loss: 283.320936\n",
      "Iteration 9250 | Loss: 282.900475\n",
      "Iteration 9275 | Loss: 282.488670\n",
      "Iteration 9300 | Loss: 282.085343\n",
      "Iteration 9325 | Loss: 281.690319\n",
      "Iteration 9350 | Loss: 281.303427\n",
      "Iteration 9375 | Loss: 280.924498\n",
      "Iteration 9400 | Loss: 280.553370\n",
      "Iteration 9425 | Loss: 280.189880\n",
      "Iteration 9450 | Loss: 279.833871\n",
      "Iteration 9475 | Loss: 279.485190\n",
      "Iteration 9500 | Loss: 279.143684\n",
      "Iteration 9525 | Loss: 278.809207\n",
      "Iteration 9550 | Loss: 278.481613\n",
      "Iteration 9575 | Loss: 278.160760\n",
      "Iteration 9600 | Loss: 277.846509\n",
      "Iteration 9625 | Loss: 277.538724\n",
      "Iteration 9650 | Loss: 277.237272\n",
      "Iteration 9675 | Loss: 276.942022\n",
      "Iteration 9700 | Loss: 276.652847\n",
      "Iteration 9725 | Loss: 276.369621\n",
      "Iteration 9750 | Loss: 276.092221\n",
      "Iteration 9775 | Loss: 275.820528\n",
      "Iteration 9800 | Loss: 275.554424\n",
      "Iteration 9825 | Loss: 275.293794\n",
      "Iteration 9850 | Loss: 275.038524\n",
      "Iteration 9875 | Loss: 274.788504\n",
      "Iteration 9900 | Loss: 274.543627\n",
      "Iteration 9925 | Loss: 274.303786\n",
      "Iteration 9950 | Loss: 274.068878\n",
      "Iteration 9975 | Loss: 273.838800\n",
      "Iteration 10000 | Loss: 273.613454\n",
      "Iteration 10025 | Loss: 273.392741\n",
      "Iteration 10050 | Loss: 273.176566\n",
      "Iteration 10075 | Loss: 272.964837\n",
      "Iteration 10100 | Loss: 272.757460\n",
      "Iteration 10125 | Loss: 272.554347\n",
      "Iteration 10150 | Loss: 272.355410\n",
      "Iteration 10175 | Loss: 272.160563\n",
      "Iteration 10200 | Loss: 271.969721\n",
      "Iteration 10225 | Loss: 271.782802\n",
      "Iteration 10250 | Loss: 271.599725\n",
      "Iteration 10275 | Loss: 271.420411\n",
      "Iteration 10300 | Loss: 271.244782\n",
      "Iteration 10325 | Loss: 271.072763\n",
      "Iteration 10350 | Loss: 270.904279\n",
      "Iteration 10375 | Loss: 270.739258\n",
      "Iteration 10400 | Loss: 270.577628\n",
      "Iteration 10425 | Loss: 270.419319\n",
      "Iteration 10450 | Loss: 270.264264\n",
      "Iteration 10475 | Loss: 270.112394\n",
      "Iteration 10500 | Loss: 269.963644\n",
      "Iteration 10525 | Loss: 269.817951\n",
      "Iteration 10550 | Loss: 269.675251\n",
      "Iteration 10575 | Loss: 269.535482\n",
      "Iteration 10600 | Loss: 269.398585\n",
      "Iteration 10625 | Loss: 269.264499\n",
      "Iteration 10650 | Loss: 269.133168\n",
      "Iteration 10675 | Loss: 269.004535\n",
      "Iteration 10700 | Loss: 268.878543\n",
      "Iteration 10725 | Loss: 268.755139\n",
      "Iteration 10750 | Loss: 268.634270\n",
      "Iteration 10775 | Loss: 268.515882\n",
      "Iteration 10800 | Loss: 268.399926\n",
      "Iteration 10825 | Loss: 268.286351\n",
      "Iteration 10850 | Loss: 268.175108\n",
      "Iteration 10875 | Loss: 268.066149\n",
      "Iteration 10900 | Loss: 267.959427\n",
      "Iteration 10925 | Loss: 267.854896\n",
      "Iteration 10950 | Loss: 267.752512\n",
      "Iteration 10975 | Loss: 267.652229\n",
      "Iteration 11000 | Loss: 267.554005\n",
      "Iteration 11025 | Loss: 267.457796\n",
      "Iteration 11050 | Loss: 267.363563\n",
      "Iteration 11075 | Loss: 267.271264\n",
      "Iteration 11100 | Loss: 267.180859\n",
      "Iteration 11125 | Loss: 267.092310\n",
      "Iteration 11150 | Loss: 267.005578\n",
      "Iteration 11175 | Loss: 266.920626\n",
      "Iteration 11200 | Loss: 266.837417\n",
      "Iteration 11225 | Loss: 266.755915\n",
      "Iteration 11250 | Loss: 266.676086\n",
      "Iteration 11275 | Loss: 266.597894\n",
      "Iteration 11300 | Loss: 266.521307\n",
      "Iteration 11325 | Loss: 266.446291\n",
      "Iteration 11350 | Loss: 266.372814\n",
      "Iteration 11375 | Loss: 266.300845\n",
      "Iteration 11400 | Loss: 266.230351\n",
      "Iteration 11425 | Loss: 266.161304\n",
      "Iteration 11450 | Loss: 266.093672\n",
      "Iteration 11475 | Loss: 266.027428\n",
      "Iteration 11500 | Loss: 265.962543\n",
      "Iteration 11525 | Loss: 265.898988\n",
      "Iteration 11550 | Loss: 265.836736\n",
      "Iteration 11575 | Loss: 265.775761\n",
      "Iteration 11600 | Loss: 265.716036\n",
      "Iteration 11625 | Loss: 265.657536\n",
      "Iteration 11650 | Loss: 265.600235\n",
      "Iteration 11675 | Loss: 265.544109\n",
      "Iteration 11700 | Loss: 265.489134\n",
      "Iteration 11725 | Loss: 265.435286\n",
      "Iteration 11750 | Loss: 265.382541\n",
      "Iteration 11775 | Loss: 265.330878\n",
      "Iteration 11800 | Loss: 265.280273\n",
      "Iteration 11825 | Loss: 265.230706\n",
      "Iteration 11850 | Loss: 265.182155\n",
      "Iteration 11875 | Loss: 265.134598\n",
      "Iteration 11900 | Loss: 265.088017\n",
      "Iteration 11925 | Loss: 265.042389\n",
      "Iteration 11950 | Loss: 264.997697\n",
      "Iteration 11975 | Loss: 264.953921\n",
      "Iteration 12000 | Loss: 264.911041\n",
      "Iteration 12025 | Loss: 264.869040\n",
      "Iteration 12050 | Loss: 264.827899\n",
      "Iteration 12075 | Loss: 264.787601\n",
      "Iteration 12100 | Loss: 264.748129\n",
      "Iteration 12125 | Loss: 264.709465\n",
      "Iteration 12150 | Loss: 264.671593\n",
      "Iteration 12175 | Loss: 264.634497\n",
      "Iteration 12200 | Loss: 264.598161\n",
      "Iteration 12225 | Loss: 264.562569\n",
      "Iteration 12250 | Loss: 264.527705\n",
      "Iteration 12275 | Loss: 264.493556\n",
      "Iteration 12300 | Loss: 264.460105\n",
      "Iteration 12325 | Loss: 264.427340\n",
      "Iteration 12350 | Loss: 264.395246\n",
      "Iteration 12375 | Loss: 264.363808\n",
      "Iteration 12400 | Loss: 264.333014\n",
      "Iteration 12425 | Loss: 264.302851\n",
      "Iteration 12450 | Loss: 264.273305\n",
      "Iteration 12475 | Loss: 264.244364\n",
      "Iteration 12500 | Loss: 264.216015\n",
      "Iteration 12525 | Loss: 264.188247\n",
      "Iteration 12550 | Loss: 264.161046\n",
      "Iteration 12575 | Loss: 264.134403\n",
      "Iteration 12600 | Loss: 264.108304\n",
      "Iteration 12625 | Loss: 264.082740\n",
      "Iteration 12650 | Loss: 264.057699\n",
      "Iteration 12675 | Loss: 264.033170\n",
      "Iteration 12700 | Loss: 264.009143\n",
      "Iteration 12725 | Loss: 263.985607\n",
      "Iteration 12750 | Loss: 263.962553\n",
      "Iteration 12775 | Loss: 263.939971\n",
      "Iteration 12800 | Loss: 263.917850\n",
      "Iteration 12825 | Loss: 263.896182\n",
      "Iteration 12850 | Loss: 263.874957\n",
      "Iteration 12875 | Loss: 263.854166\n",
      "Iteration 12900 | Loss: 263.833801\n",
      "Iteration 12925 | Loss: 263.813851\n",
      "Iteration 12950 | Loss: 263.794310\n",
      "Iteration 12975 | Loss: 263.775169\n",
      "Iteration 13000 | Loss: 263.756418\n",
      "Iteration 13025 | Loss: 263.738051\n",
      "Iteration 13050 | Loss: 263.720060\n",
      "Iteration 13075 | Loss: 263.702436\n",
      "Iteration 13100 | Loss: 263.685173\n",
      "Iteration 13125 | Loss: 263.668262\n",
      "Iteration 13150 | Loss: 263.651697\n",
      "Iteration 13175 | Loss: 263.635471\n",
      "Iteration 13200 | Loss: 263.619576\n",
      "Iteration 13225 | Loss: 263.604007\n",
      "Iteration 13250 | Loss: 263.588755\n",
      "Iteration 13275 | Loss: 263.573815\n",
      "Iteration 13300 | Loss: 263.559180\n",
      "Iteration 13325 | Loss: 263.544844\n",
      "Iteration 13350 | Loss: 263.530801\n",
      "Iteration 13375 | Loss: 263.517045\n",
      "Iteration 13400 | Loss: 263.503570\n",
      "Iteration 13425 | Loss: 263.490371\n",
      "Iteration 13450 | Loss: 263.477441\n",
      "Iteration 13475 | Loss: 263.464775\n",
      "Iteration 13500 | Loss: 263.452367\n",
      "Iteration 13525 | Loss: 263.440213\n",
      "Iteration 13550 | Loss: 263.428308\n",
      "Iteration 13575 | Loss: 263.416645\n",
      "Iteration 13600 | Loss: 263.405220\n",
      "Iteration 13625 | Loss: 263.394029\n",
      "Iteration 13650 | Loss: 263.383066\n",
      "Iteration 13675 | Loss: 263.372328\n",
      "Iteration 13700 | Loss: 263.361808\n",
      "Iteration 13725 | Loss: 263.351503\n",
      "Iteration 13750 | Loss: 263.341408\n",
      "Iteration 13775 | Loss: 263.331520\n",
      "Iteration 13800 | Loss: 263.321833\n",
      "Iteration 13825 | Loss: 263.312344\n",
      "Iteration 13850 | Loss: 263.303048\n",
      "Iteration 13875 | Loss: 263.293943\n",
      "Iteration 13900 | Loss: 263.285023\n",
      "Iteration 13925 | Loss: 263.276285\n",
      "Iteration 13950 | Loss: 263.267725\n",
      "Iteration 13975 | Loss: 263.259340\n",
      "Iteration 14000 | Loss: 263.251126\n",
      "Iteration 14025 | Loss: 263.243079\n",
      "Iteration 14050 | Loss: 263.235197\n",
      "Iteration 14075 | Loss: 263.227476\n",
      "Iteration 14100 | Loss: 263.219912\n",
      "Iteration 14125 | Loss: 263.212502\n",
      "Iteration 14150 | Loss: 263.205243\n",
      "Iteration 14175 | Loss: 263.198132\n",
      "Iteration 14200 | Loss: 263.191167\n",
      "Iteration 14225 | Loss: 263.184343\n",
      "Iteration 14250 | Loss: 263.177658\n",
      "Iteration 14275 | Loss: 263.171110\n",
      "Iteration 14300 | Loss: 263.164695\n",
      "Iteration 14325 | Loss: 263.158411\n",
      "Iteration 14350 | Loss: 263.152255\n",
      "Iteration 14375 | Loss: 263.146225\n",
      "Iteration 14400 | Loss: 263.140317\n",
      "Iteration 14425 | Loss: 263.134530\n",
      "Iteration 14450 | Loss: 263.128861\n",
      "Iteration 14475 | Loss: 263.123307\n",
      "Iteration 14500 | Loss: 263.117866\n",
      "Iteration 14525 | Loss: 263.112537\n",
      "Iteration 14550 | Loss: 263.107316\n",
      "Iteration 14575 | Loss: 263.102201\n",
      "Iteration 14600 | Loss: 263.097190\n",
      "Iteration 14625 | Loss: 263.092282\n",
      "Iteration 14650 | Loss: 263.087473\n",
      "Iteration 14675 | Loss: 263.082762\n",
      "Iteration 14700 | Loss: 263.078148\n",
      "Iteration 14725 | Loss: 263.073627\n",
      "Iteration 14750 | Loss: 263.069198\n",
      "Iteration 14775 | Loss: 263.064860\n",
      "Iteration 14800 | Loss: 263.060610\n",
      "Iteration 14825 | Loss: 263.056446\n",
      "Iteration 14850 | Loss: 263.052367\n",
      "Iteration 14875 | Loss: 263.048371\n",
      "Iteration 14900 | Loss: 263.044457\n",
      "Iteration 14925 | Loss: 263.040622\n",
      "Iteration 14950 | Loss: 263.036865\n",
      "Iteration 14975 | Loss: 263.033185\n",
      "Iteration 15000 | Loss: 263.029579\n",
      "Iteration 15025 | Loss: 263.026047\n",
      "Iteration 15050 | Loss: 263.022587\n",
      "Iteration 15075 | Loss: 263.019197\n",
      "Iteration 15100 | Loss: 263.015876\n",
      "Iteration 15125 | Loss: 263.012623\n",
      "Iteration 15150 | Loss: 263.009436\n",
      "Iteration 15175 | Loss: 263.006314\n",
      "Iteration 15200 | Loss: 263.003255\n",
      "Iteration 15225 | Loss: 263.000258\n",
      "Iteration 15250 | Loss: 262.997323\n",
      "Iteration 15275 | Loss: 262.994447\n",
      "Iteration 15300 | Loss: 262.991629\n",
      "Iteration 15325 | Loss: 262.988869\n",
      "Iteration 15350 | Loss: 262.986165\n",
      "Iteration 15375 | Loss: 262.983516\n",
      "Iteration 15400 | Loss: 262.980921\n",
      "Iteration 15425 | Loss: 262.978378\n",
      "Iteration 15450 | Loss: 262.975887\n",
      "Iteration 15475 | Loss: 262.973447\n",
      "Iteration 15500 | Loss: 262.971056\n",
      "Iteration 15525 | Loss: 262.968714\n",
      "Iteration 15550 | Loss: 262.966420\n",
      "Iteration 15575 | Loss: 262.964172\n",
      "Iteration 15600 | Loss: 262.961970\n",
      "Iteration 15625 | Loss: 262.959813\n",
      "Iteration 15650 | Loss: 262.957699\n",
      "Iteration 15675 | Loss: 262.955629\n",
      "Iteration 15700 | Loss: 262.953600\n",
      "Iteration 15725 | Loss: 262.951613\n",
      "Iteration 15750 | Loss: 262.949666\n",
      "Iteration 15775 | Loss: 262.947758\n",
      "Iteration 15800 | Loss: 262.945889\n",
      "Iteration 15825 | Loss: 262.944059\n",
      "Iteration 15850 | Loss: 262.942265\n",
      "Iteration 15875 | Loss: 262.940508\n",
      "Iteration 15900 | Loss: 262.938786\n",
      "Iteration 15925 | Loss: 262.937100\n",
      "Iteration 15950 | Loss: 262.935447\n",
      "Iteration 15975 | Loss: 262.933828\n",
      "Iteration 16000 | Loss: 262.932242\n",
      "Iteration 16025 | Loss: 262.930689\n",
      "Iteration 16050 | Loss: 262.929166\n",
      "Iteration 16075 | Loss: 262.927675\n",
      "Iteration 16100 | Loss: 262.926214\n",
      "Iteration 16125 | Loss: 262.924783\n",
      "Iteration 16150 | Loss: 262.923380\n",
      "Iteration 16175 | Loss: 262.922006\n",
      "Iteration 16200 | Loss: 262.920660\n",
      "Iteration 16225 | Loss: 262.919341\n",
      "Iteration 16250 | Loss: 262.918049\n",
      "Iteration 16275 | Loss: 262.916783\n",
      "Iteration 16300 | Loss: 262.915543\n",
      "Iteration 16325 | Loss: 262.914328\n",
      "Iteration 16350 | Loss: 262.913138\n",
      "Iteration 16375 | Loss: 262.911972\n",
      "Iteration 16400 | Loss: 262.910829\n",
      "Iteration 16425 | Loss: 262.909709\n",
      "Iteration 16450 | Loss: 262.908613\n",
      "Iteration 16475 | Loss: 262.907538\n",
      "Iteration 16500 | Loss: 262.906485\n",
      "Iteration 16525 | Loss: 262.905454\n",
      "Iteration 16550 | Loss: 262.904443\n",
      "Iteration 16575 | Loss: 262.903453\n",
      "Iteration 16600 | Loss: 262.902483\n",
      "Iteration 16625 | Loss: 262.901533\n",
      "Iteration 16650 | Loss: 262.900602\n",
      "Iteration 16675 | Loss: 262.899690\n",
      "Iteration 16700 | Loss: 262.898796\n",
      "Iteration 16725 | Loss: 262.897920\n",
      "Iteration 16750 | Loss: 262.897062\n",
      "Iteration 16775 | Loss: 262.896222\n",
      "Iteration 16800 | Loss: 262.895398\n",
      "Iteration 16825 | Loss: 262.894591\n",
      "Iteration 16850 | Loss: 262.893801\n",
      "Iteration 16875 | Loss: 262.893026\n",
      "Iteration 16900 | Loss: 262.892268\n",
      "Iteration 16925 | Loss: 262.891524\n",
      "Iteration 16950 | Loss: 262.890796\n",
      "Iteration 16975 | Loss: 262.890082\n",
      "Iteration 17000 | Loss: 262.889383\n",
      "Iteration 17025 | Loss: 262.888698\n",
      "Iteration 17050 | Loss: 262.888026\n",
      "Iteration 17075 | Loss: 262.887369\n",
      "Iteration 17100 | Loss: 262.886724\n",
      "Iteration 17125 | Loss: 262.886093\n",
      "Iteration 17150 | Loss: 262.885475\n",
      "Iteration 17175 | Loss: 262.884869\n",
      "Iteration 17200 | Loss: 262.884275\n",
      "Iteration 17225 | Loss: 262.883693\n",
      "Iteration 17250 | Loss: 262.883123\n",
      "Iteration 17275 | Loss: 262.882565\n",
      "Iteration 17300 | Loss: 262.882017\n",
      "Iteration 17325 | Loss: 262.881481\n",
      "Iteration 17350 | Loss: 262.880956\n",
      "Iteration 17375 | Loss: 262.880441\n",
      "Iteration 17400 | Loss: 262.879937\n",
      "Iteration 17425 | Loss: 262.879443\n",
      "Iteration 17450 | Loss: 262.878959\n",
      "Iteration 17475 | Loss: 262.878485\n",
      "Iteration 17500 | Loss: 262.878020\n",
      "Iteration 17525 | Loss: 262.877565\n",
      "Iteration 17550 | Loss: 262.877118\n",
      "Iteration 17575 | Loss: 262.876681\n",
      "Iteration 17600 | Loss: 262.876253\n",
      "Iteration 17625 | Loss: 262.875833\n",
      "Iteration 17650 | Loss: 262.875422\n",
      "Iteration 17675 | Loss: 262.875019\n",
      "Iteration 17700 | Loss: 262.874625\n",
      "Iteration 17725 | Loss: 262.874238\n",
      "Iteration 17750 | Loss: 262.873859\n",
      "Iteration 17775 | Loss: 262.873488\n",
      "Iteration 17800 | Loss: 262.873124\n",
      "Iteration 17825 | Loss: 262.872767\n",
      "Iteration 17850 | Loss: 262.872418\n",
      "Iteration 17875 | Loss: 262.872076\n",
      "Iteration 17900 | Loss: 262.871741\n",
      "Iteration 17925 | Loss: 262.871412\n",
      "Iteration 17950 | Loss: 262.871090\n",
      "Iteration 17975 | Loss: 262.870775\n",
      "Iteration 18000 | Loss: 262.870466\n",
      "Iteration 18025 | Loss: 262.870163\n",
      "Iteration 18050 | Loss: 262.869866\n",
      "Iteration 18075 | Loss: 262.869575\n",
      "Iteration 18100 | Loss: 262.869291\n",
      "Iteration 18125 | Loss: 262.869011\n",
      "Iteration 18150 | Loss: 262.868738\n",
      "Iteration 18175 | Loss: 262.868470\n",
      "Iteration 18200 | Loss: 262.868207\n",
      "Iteration 18225 | Loss: 262.867950\n",
      "Iteration 18250 | Loss: 262.867698\n",
      "Iteration 18275 | Loss: 262.867451\n",
      "Iteration 18300 | Loss: 262.867209\n",
      "Iteration 18325 | Loss: 262.866972\n",
      "Iteration 18350 | Loss: 262.866739\n",
      "Iteration 18375 | Loss: 262.866512\n",
      "Iteration 18400 | Loss: 262.866289\n",
      "Iteration 18425 | Loss: 262.866070\n",
      "Iteration 18450 | Loss: 262.865856\n",
      "Iteration 18475 | Loss: 262.865646\n",
      "Iteration 18500 | Loss: 262.865440\n",
      "Iteration 18525 | Loss: 262.865239\n",
      "Iteration 18550 | Loss: 262.865041\n",
      "Iteration 18575 | Loss: 262.864848\n",
      "Iteration 18600 | Loss: 262.864658\n",
      "Iteration 18625 | Loss: 262.864472\n",
      "Iteration 18650 | Loss: 262.864290\n",
      "Iteration 18675 | Loss: 262.864112\n",
      "Iteration 18700 | Loss: 262.863937\n",
      "Iteration 18725 | Loss: 262.863766\n",
      "Iteration 18750 | Loss: 262.863598\n",
      "Iteration 18775 | Loss: 262.863434\n",
      "Iteration 18800 | Loss: 262.863272\n",
      "Iteration 18825 | Loss: 262.863115\n",
      "Iteration 18850 | Loss: 262.862960\n",
      "Iteration 18875 | Loss: 262.862808\n",
      "Iteration 18900 | Loss: 262.862660\n",
      "Iteration 18925 | Loss: 262.862514\n",
      "Iteration 18950 | Loss: 262.862372\n",
      "Iteration 18975 | Loss: 262.862232\n",
      "Iteration 19000 | Loss: 262.862095\n",
      "Iteration 19025 | Loss: 262.861961\n",
      "Iteration 19050 | Loss: 262.861829\n",
      "Iteration 19075 | Loss: 262.861700\n",
      "Iteration 19100 | Loss: 262.861574\n",
      "Iteration 19125 | Loss: 262.861450\n",
      "Iteration 19150 | Loss: 262.861329\n",
      "Iteration 19175 | Loss: 262.861210\n",
      "Iteration 19200 | Loss: 262.861094\n",
      "Iteration 19225 | Loss: 262.860980\n",
      "Iteration 19250 | Loss: 262.860868\n",
      "Iteration 19275 | Loss: 262.860758\n",
      "Iteration 19300 | Loss: 262.860651\n",
      "Iteration 19325 | Loss: 262.860546\n",
      "Iteration 19350 | Loss: 262.860443\n",
      "Iteration 19375 | Loss: 262.860342\n",
      "Iteration 19400 | Loss: 262.860243\n",
      "Iteration 19425 | Loss: 262.860146\n",
      "Iteration 19450 | Loss: 262.860051\n",
      "Iteration 19475 | Loss: 262.859957\n",
      "Iteration 19500 | Loss: 262.859866\n",
      "Iteration 19525 | Loss: 262.859777\n",
      "Iteration 19550 | Loss: 262.859689\n",
      "Iteration 19575 | Loss: 262.859603\n",
      "Iteration 19600 | Loss: 262.859519\n",
      "Iteration 19625 | Loss: 262.859437\n",
      "Iteration 19650 | Loss: 262.859356\n",
      "Iteration 19675 | Loss: 262.859277\n",
      "Iteration 19700 | Loss: 262.859199\n",
      "Iteration 19725 | Loss: 262.859123\n",
      "Iteration 19750 | Loss: 262.859048\n",
      "Iteration 19775 | Loss: 262.858975\n",
      "Iteration 19800 | Loss: 262.858904\n",
      "Iteration 19825 | Loss: 262.858834\n",
      "Iteration 19850 | Loss: 262.858765\n",
      "Iteration 19875 | Loss: 262.858698\n",
      "Iteration 19900 | Loss: 262.858632\n",
      "Iteration 19925 | Loss: 262.858567\n",
      "Iteration 19950 | Loss: 262.858504\n",
      "Iteration 19975 | Loss: 262.858441\n",
      "Iteration 20000 | Loss: 262.858381\n",
      "Iteration 20025 | Loss: 262.858321\n",
      "Iteration 20050 | Loss: 262.858262\n",
      "Iteration 20075 | Loss: 262.858205\n",
      "Iteration 20100 | Loss: 262.858149\n",
      "Iteration 20125 | Loss: 262.858094\n",
      "Iteration 20150 | Loss: 262.858040\n",
      "Iteration 20175 | Loss: 262.857987\n",
      "Iteration 20200 | Loss: 262.857936\n",
      "Iteration 20225 | Loss: 262.857885\n",
      "Iteration 20250 | Loss: 262.857835\n",
      "Iteration 20275 | Loss: 262.857786\n",
      "Iteration 20300 | Loss: 262.857739\n",
      "Iteration 20325 | Loss: 262.857692\n",
      "Iteration 20350 | Loss: 262.857646\n",
      "Iteration 20375 | Loss: 262.857601\n",
      "Iteration 20400 | Loss: 262.857557\n",
      "Iteration 20425 | Loss: 262.857514\n",
      "Iteration 20450 | Loss: 262.857471\n",
      "Iteration 20475 | Loss: 262.857430\n",
      "Iteration 20500 | Loss: 262.857389\n",
      "Iteration 20525 | Loss: 262.857350\n",
      "Iteration 20550 | Loss: 262.857311\n",
      "Iteration 20575 | Loss: 262.857272\n",
      "Iteration 20600 | Loss: 262.857235\n",
      "Iteration 20625 | Loss: 262.857198\n",
      "Iteration 20650 | Loss: 262.857162\n",
      "Iteration 20675 | Loss: 262.857127\n",
      "Iteration 20700 | Loss: 262.857092\n",
      "Iteration 20725 | Loss: 262.857059\n",
      "Iteration 20750 | Loss: 262.857025\n",
      "Iteration 20775 | Loss: 262.856993\n",
      "Iteration 20800 | Loss: 262.856961\n",
      "Iteration 20825 | Loss: 262.856930\n",
      "Iteration 20850 | Loss: 262.856899\n",
      "Iteration 20875 | Loss: 262.856869\n",
      "Iteration 20900 | Loss: 262.856840\n",
      "Iteration 20925 | Loss: 262.856811\n",
      "Iteration 20950 | Loss: 262.856783\n",
      "Iteration 20975 | Loss: 262.856755\n",
      "Iteration 21000 | Loss: 262.856728\n",
      "Iteration 21025 | Loss: 262.856701\n",
      "Iteration 21050 | Loss: 262.856675\n",
      "Iteration 21075 | Loss: 262.856650\n",
      "Iteration 21100 | Loss: 262.856625\n",
      "Iteration 21125 | Loss: 262.856600\n",
      "Iteration 21150 | Loss: 262.856576\n",
      "Iteration 21175 | Loss: 262.856552\n",
      "Iteration 21200 | Loss: 262.856529\n",
      "Iteration 21225 | Loss: 262.856507\n",
      "Iteration 21250 | Loss: 262.856484\n",
      "Iteration 21275 | Loss: 262.856463\n",
      "Iteration 21300 | Loss: 262.856441\n",
      "Iteration 21325 | Loss: 262.856421\n",
      "Iteration 21350 | Loss: 262.856400\n",
      "Iteration 21375 | Loss: 262.856380\n",
      "Iteration 21400 | Loss: 262.856360\n",
      "Iteration 21425 | Loss: 262.856341\n",
      "Iteration 21450 | Loss: 262.856322\n",
      "Iteration 21475 | Loss: 262.856304\n",
      "Iteration 21500 | Loss: 262.856286\n",
      "Iteration 21525 | Loss: 262.856268\n",
      "Iteration 21550 | Loss: 262.856250\n",
      "Iteration 21575 | Loss: 262.856233\n",
      "Iteration 21600 | Loss: 262.856217\n",
      "Iteration 21625 | Loss: 262.856200\n",
      "Iteration 21650 | Loss: 262.856184\n",
      "Iteration 21675 | Loss: 262.856168\n",
      "Iteration 21700 | Loss: 262.856153\n",
      "Iteration 21725 | Loss: 262.856138\n",
      "Iteration 21750 | Loss: 262.856123\n",
      "Iteration 21775 | Loss: 262.856108\n",
      "Iteration 21800 | Loss: 262.856094\n",
      "Iteration 21825 | Loss: 262.856080\n",
      "Iteration 21850 | Loss: 262.856067\n",
      "Iteration 21875 | Loss: 262.856053\n",
      "Iteration 21900 | Loss: 262.856040\n",
      "Iteration 21925 | Loss: 262.856027\n",
      "Iteration 21950 | Loss: 262.856014\n",
      "Iteration 21975 | Loss: 262.856002\n",
      "Iteration 22000 | Loss: 262.855990\n",
      "Iteration 22025 | Loss: 262.855978\n",
      "Iteration 22050 | Loss: 262.855966\n",
      "Iteration 22075 | Loss: 262.855955\n",
      "Iteration 22100 | Loss: 262.855944\n",
      "Iteration 22125 | Loss: 262.855933\n",
      "Iteration 22150 | Loss: 262.855922\n",
      "Iteration 22175 | Loss: 262.855912\n",
      "Iteration 22200 | Loss: 262.855901\n",
      "Iteration 22225 | Loss: 262.855891\n",
      "Iteration 22250 | Loss: 262.855881\n",
      "Iteration 22275 | Loss: 262.855871\n",
      "Iteration 22300 | Loss: 262.855862\n",
      "Iteration 22325 | Loss: 262.855853\n",
      "Iteration 22350 | Loss: 262.855843\n",
      "Iteration 22375 | Loss: 262.855834\n",
      "Iteration 22400 | Loss: 262.855826\n",
      "Iteration 22425 | Loss: 262.855817\n",
      "Iteration 22450 | Loss: 262.855809\n",
      "Iteration 22475 | Loss: 262.855800\n",
      "Iteration 22500 | Loss: 262.855792\n",
      "Iteration 22525 | Loss: 262.855784\n",
      "Iteration 22550 | Loss: 262.855776\n",
      "Iteration 22575 | Loss: 262.855769\n",
      "Iteration 22600 | Loss: 262.855761\n",
      "Iteration 22625 | Loss: 262.855754\n",
      "Iteration 22650 | Loss: 262.855747\n",
      "Iteration 22675 | Loss: 262.855740\n",
      "Iteration 22700 | Loss: 262.855733\n",
      "Iteration 22725 | Loss: 262.855726\n",
      "Iteration 22750 | Loss: 262.855719\n",
      "Iteration 22775 | Loss: 262.855713\n",
      "Iteration 22800 | Loss: 262.855706\n",
      "Iteration 22825 | Loss: 262.855700\n",
      "Iteration 22850 | Loss: 262.855694\n",
      "Iteration 22875 | Loss: 262.855688\n",
      "Iteration 22900 | Loss: 262.855682\n",
      "Iteration 22925 | Loss: 262.855676\n",
      "Iteration 22950 | Loss: 262.855671\n",
      "Iteration 22975 | Loss: 262.855665\n",
      "Iteration 23000 | Loss: 262.855660\n",
      "Iteration 23025 | Loss: 262.855654\n",
      "Iteration 23050 | Loss: 262.855649\n",
      "Iteration 23075 | Loss: 262.855644\n",
      "Iteration 23100 | Loss: 262.855639\n",
      "Iteration 23125 | Loss: 262.855634\n",
      "Iteration 23150 | Loss: 262.855629\n",
      "Iteration 23175 | Loss: 262.855624\n",
      "Iteration 23200 | Loss: 262.855620\n",
      "Iteration 23225 | Loss: 262.855615\n",
      "Iteration 23250 | Loss: 262.855611\n",
      "Iteration 23275 | Loss: 262.855606\n",
      "Iteration 23300 | Loss: 262.855602\n",
      "Iteration 23325 | Loss: 262.855598\n",
      "Iteration 23350 | Loss: 262.855594\n",
      "Iteration 23375 | Loss: 262.855590\n",
      "Iteration 23400 | Loss: 262.855586\n",
      "Iteration 23425 | Loss: 262.855582\n",
      "Iteration 23450 | Loss: 262.855578\n",
      "Iteration 23475 | Loss: 262.855574\n",
      "Iteration 23500 | Loss: 262.855571\n",
      "Iteration 23525 | Loss: 262.855567\n",
      "Iteration 23550 | Loss: 262.855564\n",
      "Iteration 23575 | Loss: 262.855560\n",
      "Iteration 23600 | Loss: 262.855557\n",
      "Iteration 23625 | Loss: 262.855554\n",
      "Iteration 23650 | Loss: 262.855550\n",
      "Iteration 23675 | Loss: 262.855547\n",
      "Iteration 23700 | Loss: 262.855544\n",
      "Iteration 23725 | Loss: 262.855541\n",
      "Iteration 23750 | Loss: 262.855538\n",
      "Iteration 23775 | Loss: 262.855535\n",
      "Iteration 23800 | Loss: 262.855532\n",
      "Iteration 23825 | Loss: 262.855529\n",
      "Iteration 23850 | Loss: 262.855527\n",
      "Iteration 23875 | Loss: 262.855524\n",
      "Iteration 23900 | Loss: 262.855521\n",
      "Iteration 23925 | Loss: 262.855519\n",
      "Iteration 23950 | Loss: 262.855516\n",
      "Iteration 23975 | Loss: 262.855514\n",
      "Iteration 24000 | Loss: 262.855511\n",
      "Iteration 24025 | Loss: 262.855509\n",
      "Iteration 24050 | Loss: 262.855506\n",
      "Iteration 24075 | Loss: 262.855504\n",
      "Iteration 24100 | Loss: 262.855502\n",
      "Iteration 24125 | Loss: 262.855500\n",
      "Iteration 24150 | Loss: 262.855497\n",
      "Iteration 24175 | Loss: 262.855495\n",
      "Iteration 24200 | Loss: 262.855493\n",
      "Iteration 24225 | Loss: 262.855491\n",
      "Iteration 24250 | Loss: 262.855489\n",
      "Iteration 24275 | Loss: 262.855487\n",
      "Iteration 24300 | Loss: 262.855485\n",
      "Iteration 24325 | Loss: 262.855483\n",
      "Iteration 24350 | Loss: 262.855482\n",
      "Iteration 24375 | Loss: 262.855480\n",
      "Iteration 24400 | Loss: 262.855478\n",
      "Iteration 24425 | Loss: 262.855476\n",
      "Iteration 24450 | Loss: 262.855475\n",
      "Iteration 24475 | Loss: 262.855473\n",
      "Iteration 24500 | Loss: 262.855471\n",
      "Iteration 24525 | Loss: 262.855470\n",
      "Iteration 24550 | Loss: 262.855468\n",
      "Iteration 24575 | Loss: 262.855466\n",
      "Iteration 24600 | Loss: 262.855465\n",
      "Iteration 24625 | Loss: 262.855463\n",
      "Iteration 24650 | Loss: 262.855462\n",
      "Iteration 24675 | Loss: 262.855461\n",
      "Iteration 24700 | Loss: 262.855459\n",
      "Iteration 24725 | Loss: 262.855458\n",
      "Iteration 24750 | Loss: 262.855456\n",
      "Iteration 24775 | Loss: 262.855455\n",
      "Iteration 24800 | Loss: 262.855454\n",
      "Iteration 24825 | Loss: 262.855453\n",
      "Iteration 24850 | Loss: 262.855451\n",
      "Iteration 24875 | Loss: 262.855450\n",
      "Iteration 24900 | Loss: 262.855449\n",
      "Iteration 24925 | Loss: 262.855448\n",
      "Iteration 24950 | Loss: 262.855447\n",
      "Iteration 24975 | Loss: 262.855445\n",
      "Iteration 25000 | Loss: 262.855444\n",
      "Iteration 25025 | Loss: 262.855443\n",
      "Iteration 25050 | Loss: 262.855442\n",
      "Iteration 25075 | Loss: 262.855441\n",
      "Iteration 25100 | Loss: 262.855440\n",
      "Iteration 25125 | Loss: 262.855439\n",
      "Iteration 25150 | Loss: 262.855438\n",
      "Iteration 25175 | Loss: 262.855437\n",
      "Iteration 25200 | Loss: 262.855436\n",
      "Iteration 25225 | Loss: 262.855435\n",
      "Iteration 25250 | Loss: 262.855434\n",
      "Iteration 25275 | Loss: 262.855433\n",
      "Iteration 25300 | Loss: 262.855433\n",
      "Iteration 25325 | Loss: 262.855432\n",
      "Iteration 25350 | Loss: 262.855431\n",
      "Iteration 25375 | Loss: 262.855430\n",
      "Iteration 25400 | Loss: 262.855429\n",
      "Iteration 25425 | Loss: 262.855428\n",
      "Iteration 25450 | Loss: 262.855428\n",
      "Iteration 25475 | Loss: 262.855427\n",
      "Iteration 25500 | Loss: 262.855426\n",
      "Iteration 25525 | Loss: 262.855425\n",
      "Iteration 25550 | Loss: 262.855425\n",
      "Iteration 25575 | Loss: 262.855424\n",
      "Iteration 25600 | Loss: 262.855423\n",
      "Iteration 25625 | Loss: 262.855423\n",
      "Iteration 25650 | Loss: 262.855422\n",
      "Iteration 25675 | Loss: 262.855421\n",
      "Iteration 25700 | Loss: 262.855421\n",
      "Iteration 25725 | Loss: 262.855420\n",
      "Iteration 25750 | Loss: 262.855419\n",
      "Iteration 25775 | Loss: 262.855419\n",
      "Iteration 25800 | Loss: 262.855418\n",
      "Iteration 25825 | Loss: 262.855418\n",
      "Iteration 25850 | Loss: 262.855417\n",
      "Iteration 25875 | Loss: 262.855417\n",
      "Iteration 25900 | Loss: 262.855416\n",
      "Iteration 25925 | Loss: 262.855416\n",
      "Iteration 25950 | Loss: 262.855415\n",
      "Iteration 25975 | Loss: 262.855415\n",
      "Iteration 26000 | Loss: 262.855414\n",
      "Iteration 26025 | Loss: 262.855414\n",
      "Iteration 26050 | Loss: 262.855413\n",
      "Iteration 26075 | Loss: 262.855413\n",
      "Iteration 26100 | Loss: 262.855412\n",
      "Iteration 26125 | Loss: 262.855412\n",
      "Iteration 26150 | Loss: 262.855411\n",
      "Iteration 26175 | Loss: 262.855411\n",
      "Iteration 26200 | Loss: 262.855410\n",
      "Iteration 26225 | Loss: 262.855410\n",
      "Iteration 26250 | Loss: 262.855409\n",
      "Iteration 26275 | Loss: 262.855409\n",
      "Iteration 26300 | Loss: 262.855409\n",
      "Iteration 26325 | Loss: 262.855408\n",
      "Iteration 26350 | Loss: 262.855408\n",
      "Iteration 26375 | Loss: 262.855408\n",
      "Iteration 26400 | Loss: 262.855407\n",
      "Iteration 26425 | Loss: 262.855407\n",
      "Iteration 26450 | Loss: 262.855406\n",
      "Iteration 26475 | Loss: 262.855406\n",
      "Iteration 26500 | Loss: 262.855406\n",
      "Iteration 26525 | Loss: 262.855405\n",
      "Iteration 26550 | Loss: 262.855405\n",
      "Iteration 26575 | Loss: 262.855405\n",
      "Iteration 26600 | Loss: 262.855405\n",
      "Iteration 26625 | Loss: 262.855404\n",
      "Iteration 26650 | Loss: 262.855404\n",
      "Iteration 26675 | Loss: 262.855404\n",
      "Iteration 26700 | Loss: 262.855403\n",
      "Iteration 26725 | Loss: 262.855403\n",
      "Iteration 26750 | Loss: 262.855403\n",
      "Iteration 26775 | Loss: 262.855402\n",
      "Iteration 26800 | Loss: 262.855402\n",
      "Iteration 26825 | Loss: 262.855402\n",
      "Iteration 26850 | Loss: 262.855402\n",
      "Iteration 26875 | Loss: 262.855401\n",
      "Iteration 26900 | Loss: 262.855401\n",
      "Iteration 26925 | Loss: 262.855401\n",
      "Iteration 26950 | Loss: 262.855401\n",
      "Iteration 26975 | Loss: 262.855400\n",
      "Iteration 27000 | Loss: 262.855400\n",
      "Iteration 27025 | Loss: 262.855400\n",
      "Iteration 27050 | Loss: 262.855400\n",
      "Iteration 27075 | Loss: 262.855400\n",
      "Iteration 27100 | Loss: 262.855399\n",
      "Iteration 27125 | Loss: 262.855399\n",
      "Iteration 27150 | Loss: 262.855399\n",
      "Iteration 27175 | Loss: 262.855399\n",
      "Iteration 27200 | Loss: 262.855399\n",
      "Iteration 27225 | Loss: 262.855398\n",
      "Iteration 27250 | Loss: 262.855398\n",
      "Iteration 27275 | Loss: 262.855398\n",
      "Iteration 27300 | Loss: 262.855398\n",
      "Iteration 27325 | Loss: 262.855398\n",
      "Iteration 27350 | Loss: 262.855397\n",
      "Iteration 27375 | Loss: 262.855397\n",
      "Iteration 27400 | Loss: 262.855397\n",
      "Iteration 27425 | Loss: 262.855397\n",
      "Iteration 27450 | Loss: 262.855397\n",
      "Iteration 27475 | Loss: 262.855397\n",
      "Iteration 27500 | Loss: 262.855397\n",
      "Iteration 27525 | Loss: 262.855396\n",
      "Iteration 27550 | Loss: 262.855396\n",
      "Iteration 27575 | Loss: 262.855396\n",
      "Iteration 27600 | Loss: 262.855396\n",
      "Iteration 27625 | Loss: 262.855396\n",
      "Iteration 27650 | Loss: 262.855396\n",
      "Iteration 27675 | Loss: 262.855396\n",
      "Iteration 27700 | Loss: 262.855395\n",
      "Iteration 27725 | Loss: 262.855395\n",
      "Iteration 27750 | Loss: 262.855395\n",
      "Iteration 27775 | Loss: 262.855395\n",
      "Iteration 27800 | Loss: 262.855395\n",
      "Iteration 27825 | Loss: 262.855395\n",
      "Iteration 27850 | Loss: 262.855395\n",
      "Iteration 27875 | Loss: 262.855395\n",
      "Iteration 27900 | Loss: 262.855394\n",
      "Iteration 27925 | Loss: 262.855394\n",
      "Iteration 27950 | Loss: 262.855394\n",
      "Iteration 27975 | Loss: 262.855394\n",
      "Iteration 28000 | Loss: 262.855394\n",
      "Iteration 28025 | Loss: 262.855394\n",
      "Iteration 28050 | Loss: 262.855394\n",
      "Iteration 28075 | Loss: 262.855394\n",
      "Iteration 28100 | Loss: 262.855394\n",
      "Iteration 28125 | Loss: 262.855393\n",
      "Iteration 28150 | Loss: 262.855393\n",
      "Iteration 28175 | Loss: 262.855393\n",
      "Iteration 28200 | Loss: 262.855393\n",
      "Iteration 28225 | Loss: 262.855393\n",
      "Iteration 28250 | Loss: 262.855393\n",
      "Iteration 28275 | Loss: 262.855393\n",
      "Iteration 28300 | Loss: 262.855393\n",
      "Iteration 28325 | Loss: 262.855393\n",
      "Iteration 28350 | Loss: 262.855393\n",
      "Iteration 28375 | Loss: 262.855393\n",
      "Iteration 28400 | Loss: 262.855393\n",
      "Iteration 28425 | Loss: 262.855392\n",
      "Iteration 28450 | Loss: 262.855392\n",
      "Iteration 28475 | Loss: 262.855392\n",
      "Iteration 28500 | Loss: 262.855392\n",
      "Iteration 28525 | Loss: 262.855392\n",
      "Iteration 28550 | Loss: 262.855392\n",
      "Iteration 28575 | Loss: 262.855392\n",
      "Iteration 28600 | Loss: 262.855392\n",
      "Iteration 28625 | Loss: 262.855392\n",
      "Iteration 28650 | Loss: 262.855392\n",
      "Iteration 28675 | Loss: 262.855392\n",
      "Iteration 28700 | Loss: 262.855392\n",
      "Iteration 28725 | Loss: 262.855392\n",
      "Iteration 28750 | Loss: 262.855392\n",
      "Iteration 28775 | Loss: 262.855392\n",
      "Iteration 28800 | Loss: 262.855392\n",
      "Iteration 28825 | Loss: 262.855391\n",
      "Iteration 28850 | Loss: 262.855391\n",
      "Iteration 28875 | Loss: 262.855391\n",
      "Iteration 28900 | Loss: 262.855391\n",
      "Iteration 28925 | Loss: 262.855391\n",
      "Iteration 28950 | Loss: 262.855391\n",
      "Iteration 28975 | Loss: 262.855391\n",
      "Iteration 29000 | Loss: 262.855391\n",
      "Iteration 29025 | Loss: 262.855391\n",
      "Iteration 29050 | Loss: 262.855391\n",
      "Iteration 29075 | Loss: 262.855391\n",
      "Iteration 29100 | Loss: 262.855391\n",
      "Iteration 29125 | Loss: 262.855391\n",
      "Iteration 29150 | Loss: 262.855391\n",
      "Iteration 29175 | Loss: 262.855391\n",
      "Iteration 29200 | Loss: 262.855391\n",
      "Iteration 29225 | Loss: 262.855391\n",
      "Iteration 29250 | Loss: 262.855391\n",
      "Iteration 29275 | Loss: 262.855391\n",
      "Iteration 29300 | Loss: 262.855391\n",
      "Iteration 29325 | Loss: 262.855391\n",
      "Iteration 29350 | Loss: 262.855391\n",
      "Iteration 29375 | Loss: 262.855390\n",
      "Iteration 29400 | Loss: 262.855390\n",
      "Iteration 29425 | Loss: 262.855390\n",
      "Iteration 29450 | Loss: 262.855390\n",
      "Iteration 29475 | Loss: 262.855390\n",
      "Iteration 29500 | Loss: 262.855390\n",
      "Iteration 29525 | Loss: 262.855390\n",
      "Iteration 29550 | Loss: 262.855390\n",
      "Iteration 29575 | Loss: 262.855390\n",
      "Iteration 29600 | Loss: 262.855390\n",
      "Iteration 29625 | Loss: 262.855390\n",
      "Iteration 29650 | Loss: 262.855390\n",
      "Iteration 29675 | Loss: 262.855390\n",
      "Iteration 29700 | Loss: 262.855390\n",
      "Iteration 29725 | Loss: 262.855390\n",
      "Iteration 29750 | Loss: 262.855390\n",
      "Iteration 29775 | Loss: 262.855390\n",
      "Iteration 29800 | Loss: 262.855390\n",
      "Iteration 29825 | Loss: 262.855390\n",
      "Iteration 29850 | Loss: 262.855390\n",
      "Iteration 29875 | Loss: 262.855390\n",
      "Iteration 29900 | Loss: 262.855390\n",
      "Iteration 29925 | Loss: 262.855390\n",
      "Iteration 29950 | Loss: 262.855390\n",
      "Iteration 29975 | Loss: 262.855390\n",
      "Iteration 30000 | Loss: 262.855390\n",
      "Iteration 30025 | Loss: 262.855390\n",
      "Iteration 30050 | Loss: 262.855390\n",
      "Iteration 30075 | Loss: 262.855390\n",
      "Iteration 30100 | Loss: 262.855390\n",
      "Iteration 30125 | Loss: 262.855390\n",
      "Iteration 30150 | Loss: 262.855390\n",
      "Iteration 30175 | Loss: 262.855390\n",
      "Iteration 30200 | Loss: 262.855390\n",
      "Iteration 30225 | Loss: 262.855390\n",
      "Iteration 30250 | Loss: 262.855390\n",
      "Iteration 30275 | Loss: 262.855390\n",
      "Iteration 30300 | Loss: 262.855390\n",
      "Iteration 30325 | Loss: 262.855390\n",
      "Iteration 30350 | Loss: 262.855390\n",
      "Iteration 30375 | Loss: 262.855390\n",
      "Iteration 30400 | Loss: 262.855389\n",
      "Iteration 30425 | Loss: 262.855389\n",
      "Iteration 30450 | Loss: 262.855389\n",
      "Iteration 30475 | Loss: 262.855389\n",
      "Iteration 30500 | Loss: 262.855389\n",
      "Iteration 30525 | Loss: 262.855389\n",
      "Iteration 30550 | Loss: 262.855389\n",
      "Iteration 30575 | Loss: 262.855389\n",
      "Iteration 30600 | Loss: 262.855389\n",
      "Iteration 30625 | Loss: 262.855389\n",
      "Iteration 30650 | Loss: 262.855389\n",
      "Iteration 30675 | Loss: 262.855389\n",
      "Iteration 30700 | Loss: 262.855389\n",
      "Iteration 30725 | Loss: 262.855389\n",
      "Iteration 30750 | Loss: 262.855389\n",
      "Iteration 30775 | Loss: 262.855389\n",
      "Iteration 30800 | Loss: 262.855389\n",
      "Iteration 30825 | Loss: 262.855389\n",
      "Iteration 30850 | Loss: 262.855389\n",
      "Iteration 30875 | Loss: 262.855389\n",
      "Iteration 30900 | Loss: 262.855389\n",
      "Iteration 30925 | Loss: 262.855389\n",
      "Iteration 30950 | Loss: 262.855389\n",
      "Iteration 30975 | Loss: 262.855389\n",
      "Iteration 31000 | Loss: 262.855389\n",
      "Iteration 31025 | Loss: 262.855389\n",
      "Iteration 31050 | Loss: 262.855389\n",
      "Iteration 31075 | Loss: 262.855389\n",
      "Iteration 31100 | Loss: 262.855389\n",
      "Iteration 31125 | Loss: 262.855389\n",
      "Iteration 31150 | Loss: 262.855389\n",
      "Iteration 31175 | Loss: 262.855389\n",
      "Iteration 31200 | Loss: 262.855389\n",
      "Iteration 31225 | Loss: 262.855389\n",
      "Iteration 31250 | Loss: 262.855389\n",
      "Iteration 31275 | Loss: 262.855389\n",
      "Iteration 31300 | Loss: 262.855389\n",
      "Iteration 31325 | Loss: 262.855389\n",
      "Iteration 31350 | Loss: 262.855389\n",
      "Iteration 31375 | Loss: 262.855389\n",
      "Iteration 31400 | Loss: 262.855389\n",
      "Iteration 31425 | Loss: 262.855389\n",
      "Iteration 31450 | Loss: 262.855389\n",
      "Iteration 31475 | Loss: 262.855389\n",
      "Iteration 31500 | Loss: 262.855389\n",
      "Iteration 31525 | Loss: 262.855389\n",
      "Iteration 31550 | Loss: 262.855389\n",
      "Iteration 31575 | Loss: 262.855389\n",
      "Iteration 31600 | Loss: 262.855389\n",
      "Iteration 31625 | Loss: 262.855389\n",
      "Iteration 31650 | Loss: 262.855389\n",
      "Iteration 31675 | Loss: 262.855389\n",
      "Iteration 31700 | Loss: 262.855389\n",
      "Iteration 31725 | Loss: 262.855389\n",
      "Iteration 31750 | Loss: 262.855389\n",
      "Iteration 31775 | Loss: 262.855389\n",
      "Iteration 31800 | Loss: 262.855389\n",
      "Iteration 31825 | Loss: 262.855389\n",
      "Iteration 31850 | Loss: 262.855389\n",
      "Iteration 31875 | Loss: 262.855389\n",
      "Iteration 31900 | Loss: 262.855389\n",
      "Iteration 31925 | Loss: 262.855389\n",
      "Iteration 31950 | Loss: 262.855389\n",
      "Iteration 31975 | Loss: 262.855389\n",
      "Iteration 32000 | Loss: 262.855389\n",
      "Iteration 32025 | Loss: 262.855389\n",
      "Iteration 32050 | Loss: 262.855389\n",
      "Iteration 32075 | Loss: 262.855389\n",
      "Iteration 32100 | Loss: 262.855389\n",
      "Iteration 32125 | Loss: 262.855389\n",
      "Iteration 32150 | Loss: 262.855389\n",
      "Iteration 32175 | Loss: 262.855389\n",
      "Iteration 32200 | Loss: 262.855389\n",
      "Iteration 32225 | Loss: 262.855389\n",
      "Iteration 32250 | Loss: 262.855389\n",
      "Iteration 32275 | Loss: 262.855389\n",
      "Iteration 32300 | Loss: 262.855389\n",
      "Iteration 32325 | Loss: 262.855389\n",
      "Iteration 32350 | Loss: 262.855389\n",
      "Iteration 32375 | Loss: 262.855389\n",
      "Iteration 32400 | Loss: 262.855389\n",
      "Iteration 32425 | Loss: 262.855389\n",
      "Iteration 32450 | Loss: 262.855389\n",
      "Iteration 32475 | Loss: 262.855389\n",
      "Iteration 32500 | Loss: 262.855389\n",
      "Iteration 32525 | Loss: 262.855389\n",
      "Iteration 32550 | Loss: 262.855389\n",
      "Iteration 32575 | Loss: 262.855389\n",
      "Iteration 32600 | Loss: 262.855389\n",
      "Iteration 32625 | Loss: 262.855389\n",
      "Iteration 32650 | Loss: 262.855389\n",
      "Iteration 32675 | Loss: 262.855389\n",
      "Iteration 32700 | Loss: 262.855389\n",
      "Iteration 32725 | Loss: 262.855389\n",
      "Iteration 32750 | Loss: 262.855389\n",
      "Iteration 32775 | Loss: 262.855389\n",
      "Iteration 32800 | Loss: 262.855389\n",
      "Iteration 32825 | Loss: 262.855389\n",
      "Iteration 32850 | Loss: 262.855389\n",
      "Iteration 32875 | Loss: 262.855389\n",
      "Iteration 32900 | Loss: 262.855389\n",
      "Iteration 32925 | Loss: 262.855389\n",
      "Iteration 32950 | Loss: 262.855389\n",
      "Iteration 32975 | Loss: 262.855389\n",
      "Iteration 33000 | Loss: 262.855389\n",
      "Iteration 33025 | Loss: 262.855389\n",
      "Iteration 33050 | Loss: 262.855389\n",
      "Iteration 33075 | Loss: 262.855389\n",
      "Iteration 33100 | Loss: 262.855389\n",
      "Iteration 33125 | Loss: 262.855389\n",
      "Iteration 33150 | Loss: 262.855389\n",
      "Iteration 33175 | Loss: 262.855389\n",
      "Iteration 33200 | Loss: 262.855389\n",
      "Iteration 33225 | Loss: 262.855389\n",
      "Iteration 33250 | Loss: 262.855389\n",
      "Iteration 33275 | Loss: 262.855389\n",
      "Iteration 33300 | Loss: 262.855389\n",
      "Iteration 33325 | Loss: 262.855389\n",
      "Iteration 33350 | Loss: 262.855389\n",
      "Iteration 33375 | Loss: 262.855389\n",
      "Iteration 33400 | Loss: 262.855389\n",
      "Iteration 33425 | Loss: 262.855389\n",
      "Iteration 33450 | Loss: 262.855389\n",
      "Iteration 33475 | Loss: 262.855389\n",
      "Iteration 33500 | Loss: 262.855389\n",
      "Iteration 33525 | Loss: 262.855389\n",
      "Iteration 33550 | Loss: 262.855389\n",
      "Iteration 33575 | Loss: 262.855389\n",
      "Iteration 33600 | Loss: 262.855389\n",
      "Iteration 33625 | Loss: 262.855389\n",
      "Iteration 33650 | Loss: 262.855389\n",
      "Iteration 33675 | Loss: 262.855389\n",
      "Iteration 33700 | Loss: 262.855389\n",
      "Iteration 33725 | Loss: 262.855389\n",
      "Iteration 33750 | Loss: 262.855389\n",
      "Iteration 33775 | Loss: 262.855389\n",
      "Iteration 33800 | Loss: 262.855389\n",
      "Iteration 33825 | Loss: 262.855389\n",
      "Iteration 33850 | Loss: 262.855389\n",
      "Iteration 33875 | Loss: 262.855389\n",
      "Iteration 33900 | Loss: 262.855389\n",
      "Iteration 33925 | Loss: 262.855389\n",
      "Iteration 33950 | Loss: 262.855389\n",
      "Iteration 33975 | Loss: 262.855389\n",
      "Iteration 34000 | Loss: 262.855389\n",
      "Iteration 34025 | Loss: 262.855389\n",
      "Iteration 34050 | Loss: 262.855389\n",
      "Iteration 34075 | Loss: 262.855389\n",
      "Iteration 34100 | Loss: 262.855389\n",
      "Iteration 34125 | Loss: 262.855389\n",
      "Iteration 34150 | Loss: 262.855389\n",
      "Iteration 34175 | Loss: 262.855389\n",
      "Iteration 34200 | Loss: 262.855389\n",
      "Iteration 34225 | Loss: 262.855389\n",
      "Iteration 34250 | Loss: 262.855389\n",
      "Iteration 34275 | Loss: 262.855389\n",
      "Iteration 34300 | Loss: 262.855389\n",
      "Iteration 34325 | Loss: 262.855389\n",
      "Iteration 34350 | Loss: 262.855389\n",
      "Iteration 34375 | Loss: 262.855389\n",
      "Iteration 34400 | Loss: 262.855389\n",
      "Iteration 34425 | Loss: 262.855389\n",
      "Iteration 34450 | Loss: 262.855389\n",
      "Iteration 34475 | Loss: 262.855389\n",
      "Iteration 34500 | Loss: 262.855389\n",
      "Iteration 34525 | Loss: 262.855389\n",
      "Iteration 34550 | Loss: 262.855389\n",
      "Iteration 34575 | Loss: 262.855389\n",
      "Iteration 34600 | Loss: 262.855389\n",
      "Iteration 34625 | Loss: 262.855389\n",
      "Iteration 34650 | Loss: 262.855389\n",
      "Iteration 34675 | Loss: 262.855389\n",
      "Iteration 34700 | Loss: 262.855389\n",
      "Iteration 34725 | Loss: 262.855389\n",
      "Iteration 34750 | Loss: 262.855389\n",
      "Iteration 34775 | Loss: 262.855389\n",
      "Iteration 34800 | Loss: 262.855389\n",
      "Iteration 34825 | Loss: 262.855389\n",
      "Iteration 34850 | Loss: 262.855389\n",
      "Iteration 34875 | Loss: 262.855389\n",
      "Iteration 34900 | Loss: 262.855389\n",
      "Iteration 34925 | Loss: 262.855389\n",
      "Iteration 34950 | Loss: 262.855389\n",
      "Iteration 34975 | Loss: 262.855389\n",
      "Iteration 35000 | Loss: 262.855389\n",
      "Iteration 35025 | Loss: 262.855389\n",
      "Iteration 35050 | Loss: 262.855389\n",
      "Iteration 35075 | Loss: 262.855389\n",
      "Iteration 35100 | Loss: 262.855389\n",
      "Iteration 35125 | Loss: 262.855389\n",
      "Iteration 35150 | Loss: 262.855389\n",
      "Iteration 35175 | Loss: 262.855389\n",
      "Iteration 35200 | Loss: 262.855389\n",
      "Iteration 35225 | Loss: 262.855389\n",
      "Iteration 35250 | Loss: 262.855389\n",
      "Iteration 35275 | Loss: 262.855389\n",
      "Iteration 35300 | Loss: 262.855389\n",
      "Iteration 35325 | Loss: 262.855389\n",
      "Iteration 35350 | Loss: 262.855389\n",
      "Iteration 35375 | Loss: 262.855389\n",
      "Iteration 35400 | Loss: 262.855389\n",
      "Iteration 35425 | Loss: 262.855389\n",
      "Iteration 35450 | Loss: 262.855389\n",
      "Iteration 35475 | Loss: 262.855389\n",
      "Iteration 35500 | Loss: 262.855389\n",
      "Iteration 35525 | Loss: 262.855389\n",
      "Iteration 35550 | Loss: 262.855389\n",
      "Iteration 35575 | Loss: 262.855389\n",
      "Iteration 35600 | Loss: 262.855389\n",
      "Iteration 35625 | Loss: 262.855389\n",
      "Iteration 35650 | Loss: 262.855389\n",
      "Iteration 35675 | Loss: 262.855389\n",
      "Iteration 35700 | Loss: 262.855389\n",
      "Iteration 35725 | Loss: 262.855389\n",
      "Iteration 35750 | Loss: 262.855389\n",
      "Iteration 35775 | Loss: 262.855389\n",
      "Iteration 35800 | Loss: 262.855389\n",
      "Iteration 35825 | Loss: 262.855389\n",
      "Iteration 35850 | Loss: 262.855389\n",
      "Iteration 35875 | Loss: 262.855389\n",
      "Iteration 35900 | Loss: 262.855389\n",
      "Iteration 35925 | Loss: 262.855389\n",
      "Iteration 35950 | Loss: 262.855389\n",
      "Iteration 35975 | Loss: 262.855389\n",
      "Iteration 36000 | Loss: 262.855389\n",
      "Iteration 36025 | Loss: 262.855389\n",
      "Iteration 36050 | Loss: 262.855389\n",
      "Iteration 36075 | Loss: 262.855389\n",
      "Iteration 36100 | Loss: 262.855389\n",
      "Iteration 36125 | Loss: 262.855389\n",
      "Iteration 36150 | Loss: 262.855389\n",
      "Iteration 36175 | Loss: 262.855389\n",
      "Iteration 36200 | Loss: 262.855389\n",
      "Iteration 36225 | Loss: 262.855389\n",
      "Iteration 36250 | Loss: 262.855389\n",
      "Iteration 36275 | Loss: 262.855389\n",
      "Iteration 36300 | Loss: 262.855389\n",
      "Iteration 36325 | Loss: 262.855389\n",
      "Iteration 36350 | Loss: 262.855389\n",
      "Iteration 36375 | Loss: 262.855389\n",
      "Iteration 36400 | Loss: 262.855389\n",
      "Iteration 36425 | Loss: 262.855389\n",
      "Iteration 36450 | Loss: 262.855389\n",
      "Iteration 36475 | Loss: 262.855389\n",
      "Iteration 36500 | Loss: 262.855389\n",
      "Iteration 36525 | Loss: 262.855389\n",
      "Iteration 36550 | Loss: 262.855389\n",
      "Iteration 36575 | Loss: 262.855389\n",
      "Iteration 36600 | Loss: 262.855389\n",
      "Iteration 36625 | Loss: 262.855389\n",
      "Iteration 36650 | Loss: 262.855389\n",
      "Iteration 36675 | Loss: 262.855389\n",
      "Iteration 36700 | Loss: 262.855389\n",
      "Iteration 36725 | Loss: 262.855389\n",
      "Iteration 36750 | Loss: 262.855389\n",
      "Iteration 36775 | Loss: 262.855389\n",
      "Iteration 36800 | Loss: 262.855389\n",
      "Iteration 36825 | Loss: 262.855389\n",
      "Iteration 36850 | Loss: 262.855389\n",
      "Iteration 36875 | Loss: 262.855389\n",
      "Iteration 36900 | Loss: 262.855389\n",
      "Iteration 36925 | Loss: 262.855389\n",
      "Iteration 36950 | Loss: 262.855389\n",
      "Iteration 36975 | Loss: 262.855389\n",
      "Iteration 37000 | Loss: 262.855389\n",
      "Iteration 37025 | Loss: 262.855389\n",
      "Iteration 37050 | Loss: 262.855389\n",
      "Iteration 37075 | Loss: 262.855389\n",
      "Iteration 37100 | Loss: 262.855389\n",
      "Iteration 37125 | Loss: 262.855389\n",
      "Iteration 37150 | Loss: 262.855389\n",
      "Iteration 37175 | Loss: 262.855389\n",
      "Iteration 37200 | Loss: 262.855389\n",
      "Iteration 37225 | Loss: 262.855389\n",
      "Iteration 37250 | Loss: 262.855389\n",
      "Iteration 37275 | Loss: 262.855389\n",
      "Iteration 37300 | Loss: 262.855389\n",
      "Iteration 37325 | Loss: 262.855389\n",
      "Iteration 37350 | Loss: 262.855389\n",
      "Iteration 37375 | Loss: 262.855389\n",
      "Iteration 37400 | Loss: 262.855389\n",
      "Iteration 37425 | Loss: 262.855389\n",
      "Iteration 37450 | Loss: 262.855389\n",
      "Iteration 37475 | Loss: 262.855389\n",
      "Iteration 37500 | Loss: 262.855389\n",
      "Iteration 37525 | Loss: 262.855389\n",
      "Iteration 37550 | Loss: 262.855389\n",
      "Iteration 37575 | Loss: 262.855389\n",
      "Iteration 37600 | Loss: 262.855389\n",
      "Iteration 37625 | Loss: 262.855389\n",
      "Iteration 37650 | Loss: 262.855389\n",
      "Iteration 37675 | Loss: 262.855389\n",
      "Iteration 37700 | Loss: 262.855389\n",
      "Iteration 37725 | Loss: 262.855389\n",
      "Iteration 37750 | Loss: 262.855389\n",
      "Iteration 37775 | Loss: 262.855389\n",
      "Iteration 37800 | Loss: 262.855389\n",
      "Iteration 37825 | Loss: 262.855389\n",
      "Iteration 37850 | Loss: 262.855389\n",
      "Iteration 37875 | Loss: 262.855389\n",
      "Iteration 37900 | Loss: 262.855389\n",
      "Iteration 37925 | Loss: 262.855389\n",
      "Iteration 37950 | Loss: 262.855389\n",
      "Iteration 37975 | Loss: 262.855389\n",
      "Iteration 38000 | Loss: 262.855389\n",
      "Iteration 38025 | Loss: 262.855389\n",
      "Iteration 38050 | Loss: 262.855389\n",
      "Iteration 38075 | Loss: 262.855389\n",
      "Iteration 38100 | Loss: 262.855389\n",
      "Iteration 38125 | Loss: 262.855389\n",
      "Iteration 38150 | Loss: 262.855389\n",
      "Iteration 38175 | Loss: 262.855389\n",
      "Iteration 38200 | Loss: 262.855389\n",
      "Iteration 38225 | Loss: 262.855389\n",
      "Iteration 38250 | Loss: 262.855389\n",
      "Iteration 38275 | Loss: 262.855389\n",
      "Iteration 38300 | Loss: 262.855389\n",
      "Iteration 38325 | Loss: 262.855389\n",
      "Iteration 38350 | Loss: 262.855389\n",
      "Iteration 38375 | Loss: 262.855389\n",
      "Iteration 38400 | Loss: 262.855389\n",
      "Iteration 38425 | Loss: 262.855389\n",
      "Iteration 38450 | Loss: 262.855389\n",
      "Iteration 38475 | Loss: 262.855389\n",
      "Iteration 38500 | Loss: 262.855389\n",
      "Iteration 38525 | Loss: 262.855389\n",
      "Iteration 38550 | Loss: 262.855389\n",
      "Iteration 38575 | Loss: 262.855389\n",
      "Iteration 38600 | Loss: 262.855389\n",
      "Iteration 38625 | Loss: 262.855389\n",
      "Iteration 38650 | Loss: 262.855389\n",
      "Iteration 38675 | Loss: 262.855389\n",
      "Iteration 38700 | Loss: 262.855389\n",
      "Iteration 38725 | Loss: 262.855389\n",
      "Iteration 38750 | Loss: 262.855389\n",
      "Iteration 38775 | Loss: 262.855389\n",
      "Iteration 38800 | Loss: 262.855389\n",
      "Iteration 38825 | Loss: 262.855389\n",
      "Iteration 38850 | Loss: 262.855389\n",
      "Iteration 38875 | Loss: 262.855389\n",
      "Iteration 38900 | Loss: 262.855389\n",
      "Iteration 38925 | Loss: 262.855389\n",
      "Iteration 38950 | Loss: 262.855389\n",
      "Iteration 38975 | Loss: 262.855389\n",
      "Iteration 39000 | Loss: 262.855389\n",
      "Iteration 39025 | Loss: 262.855389\n",
      "Iteration 39050 | Loss: 262.855389\n",
      "Iteration 39075 | Loss: 262.855389\n",
      "Iteration 39100 | Loss: 262.855389\n",
      "Iteration 39125 | Loss: 262.855389\n",
      "Iteration 39150 | Loss: 262.855389\n",
      "Iteration 39175 | Loss: 262.855389\n",
      "Iteration 39200 | Loss: 262.855389\n",
      "Iteration 39225 | Loss: 262.855389\n",
      "Iteration 39250 | Loss: 262.855389\n",
      "Iteration 39275 | Loss: 262.855389\n",
      "Iteration 39300 | Loss: 262.855389\n",
      "Iteration 39325 | Loss: 262.855389\n",
      "Iteration 39350 | Loss: 262.855389\n",
      "Iteration 39375 | Loss: 262.855389\n",
      "Iteration 39400 | Loss: 262.855389\n",
      "Iteration 39425 | Loss: 262.855389\n",
      "Iteration 39450 | Loss: 262.855389\n",
      "Iteration 39475 | Loss: 262.855389\n",
      "Iteration 39500 | Loss: 262.855389\n",
      "Iteration 39525 | Loss: 262.855389\n",
      "Iteration 39550 | Loss: 262.855389\n",
      "Iteration 39575 | Loss: 262.855389\n",
      "Iteration 39600 | Loss: 262.855389\n",
      "Iteration 39625 | Loss: 262.855389\n",
      "Iteration 39650 | Loss: 262.855389\n",
      "Iteration 39675 | Loss: 262.855389\n",
      "Iteration 39700 | Loss: 262.855389\n",
      "Iteration 39725 | Loss: 262.855389\n",
      "Iteration 39750 | Loss: 262.855389\n",
      "Iteration 39775 | Loss: 262.855389\n",
      "Iteration 39800 | Loss: 262.855389\n",
      "Iteration 39825 | Loss: 262.855389\n",
      "Iteration 39850 | Loss: 262.855389\n",
      "Iteration 39875 | Loss: 262.855389\n",
      "Iteration 39900 | Loss: 262.855389\n",
      "Iteration 39925 | Loss: 262.855389\n",
      "Iteration 39950 | Loss: 262.855389\n",
      "Iteration 39975 | Loss: 262.855389\n",
      "Iteration 40000 | Loss: 262.855389\n",
      "Iteration 40025 | Loss: 262.855389\n",
      "Iteration 40050 | Loss: 262.855389\n",
      "Iteration 40075 | Loss: 262.855389\n",
      "Iteration 40100 | Loss: 262.855389\n",
      "Iteration 40125 | Loss: 262.855389\n",
      "Iteration 40150 | Loss: 262.855389\n",
      "Iteration 40175 | Loss: 262.855389\n",
      "Iteration 40200 | Loss: 262.855389\n",
      "Iteration 40225 | Loss: 262.855389\n",
      "Iteration 40250 | Loss: 262.855389\n",
      "Iteration 40275 | Loss: 262.855389\n",
      "Iteration 40300 | Loss: 262.855389\n",
      "Iteration 40325 | Loss: 262.855389\n",
      "Iteration 40350 | Loss: 262.855389\n",
      "Iteration 40375 | Loss: 262.855389\n",
      "Iteration 40400 | Loss: 262.855389\n",
      "Iteration 40425 | Loss: 262.855389\n",
      "Iteration 40450 | Loss: 262.855389\n",
      "Iteration 40475 | Loss: 262.855389\n",
      "Iteration 40500 | Loss: 262.855389\n",
      "Iteration 40525 | Loss: 262.855389\n",
      "Iteration 40550 | Loss: 262.855389\n",
      "Iteration 40575 | Loss: 262.855389\n",
      "Iteration 40600 | Loss: 262.855389\n",
      "Iteration 40625 | Loss: 262.855389\n",
      "Iteration 40650 | Loss: 262.855389\n",
      "Iteration 40675 | Loss: 262.855389\n",
      "Iteration 40700 | Loss: 262.855389\n",
      "Iteration 40725 | Loss: 262.855389\n",
      "Iteration 40750 | Loss: 262.855389\n",
      "Iteration 40775 | Loss: 262.855389\n",
      "Iteration 40800 | Loss: 262.855389\n",
      "Iteration 40825 | Loss: 262.855389\n",
      "Iteration 40850 | Loss: 262.855389\n",
      "Iteration 40875 | Loss: 262.855389\n",
      "Iteration 40900 | Loss: 262.855389\n",
      "Iteration 40925 | Loss: 262.855389\n",
      "Iteration 40950 | Loss: 262.855389\n",
      "Iteration 40975 | Loss: 262.855389\n",
      "Iteration 41000 | Loss: 262.855389\n",
      "Iteration 41025 | Loss: 262.855389\n",
      "Iteration 41050 | Loss: 262.855389\n",
      "Iteration 41075 | Loss: 262.855389\n",
      "Iteration 41100 | Loss: 262.855389\n",
      "Iteration 41125 | Loss: 262.855389\n",
      "Iteration 41150 | Loss: 262.855389\n",
      "Iteration 41175 | Loss: 262.855389\n",
      "Iteration 41200 | Loss: 262.855389\n",
      "Iteration 41225 | Loss: 262.855389\n",
      "Iteration 41250 | Loss: 262.855389\n",
      "Iteration 41275 | Loss: 262.855389\n",
      "Iteration 41300 | Loss: 262.855389\n",
      "Iteration 41325 | Loss: 262.855389\n",
      "Iteration 41350 | Loss: 262.855389\n",
      "Iteration 41375 | Loss: 262.855389\n",
      "Iteration 41400 | Loss: 262.855389\n",
      "Iteration 41425 | Loss: 262.855389\n",
      "Iteration 41450 | Loss: 262.855389\n",
      "Iteration 41475 | Loss: 262.855389\n",
      "Iteration 41500 | Loss: 262.855389\n",
      "Iteration 41525 | Loss: 262.855389\n",
      "Iteration 41550 | Loss: 262.855389\n",
      "Iteration 41575 | Loss: 262.855389\n",
      "Iteration 41600 | Loss: 262.855389\n",
      "Iteration 41625 | Loss: 262.855389\n",
      "Iteration 41650 | Loss: 262.855389\n",
      "Iteration 41675 | Loss: 262.855389\n",
      "Iteration 41700 | Loss: 262.855389\n",
      "Iteration 41725 | Loss: 262.855389\n",
      "Iteration 41750 | Loss: 262.855389\n",
      "Iteration 41775 | Loss: 262.855389\n",
      "Iteration 41800 | Loss: 262.855389\n",
      "Iteration 41825 | Loss: 262.855389\n",
      "Iteration 41850 | Loss: 262.855389\n",
      "Iteration 41875 | Loss: 262.855389\n",
      "Iteration 41900 | Loss: 262.855389\n",
      "Iteration 41925 | Loss: 262.855389\n",
      "Iteration 41950 | Loss: 262.855389\n",
      "Iteration 41975 | Loss: 262.855389\n",
      "Iteration 42000 | Loss: 262.855389\n",
      "Iteration 42025 | Loss: 262.855389\n",
      "Iteration 42050 | Loss: 262.855389\n",
      "Iteration 42075 | Loss: 262.855389\n",
      "Iteration 42100 | Loss: 262.855389\n",
      "Iteration 42125 | Loss: 262.855389\n",
      "Iteration 42150 | Loss: 262.855389\n",
      "Iteration 42175 | Loss: 262.855389\n",
      "Iteration 42200 | Loss: 262.855389\n",
      "Iteration 42225 | Loss: 262.855389\n",
      "Iteration 42250 | Loss: 262.855389\n",
      "Iteration 42275 | Loss: 262.855389\n",
      "Iteration 42300 | Loss: 262.855389\n",
      "Iteration 42325 | Loss: 262.855389\n",
      "Iteration 42350 | Loss: 262.855389\n",
      "Iteration 42375 | Loss: 262.855389\n",
      "Iteration 42400 | Loss: 262.855389\n",
      "Iteration 42425 | Loss: 262.855389\n",
      "Iteration 42450 | Loss: 262.855389\n",
      "Iteration 42475 | Loss: 262.855389\n",
      "Iteration 42500 | Loss: 262.855389\n",
      "Iteration 42525 | Loss: 262.855389\n",
      "Iteration 42550 | Loss: 262.855389\n",
      "Iteration 42575 | Loss: 262.855389\n",
      "Iteration 42600 | Loss: 262.855389\n",
      "Iteration 42625 | Loss: 262.855389\n",
      "Iteration 42650 | Loss: 262.855389\n",
      "Iteration 42675 | Loss: 262.855389\n",
      "Iteration 42700 | Loss: 262.855389\n",
      "Iteration 42725 | Loss: 262.855389\n",
      "Iteration 42750 | Loss: 262.855389\n",
      "Iteration 42775 | Loss: 262.855389\n",
      "Iteration 42800 | Loss: 262.855389\n",
      "Iteration 42825 | Loss: 262.855389\n",
      "Iteration 42850 | Loss: 262.855389\n",
      "Iteration 42875 | Loss: 262.855389\n",
      "Iteration 42900 | Loss: 262.855389\n",
      "Iteration 42925 | Loss: 262.855389\n",
      "Iteration 42950 | Loss: 262.855389\n",
      "Iteration 42975 | Loss: 262.855389\n",
      "Iteration 43000 | Loss: 262.855389\n",
      "Iteration 43025 | Loss: 262.855389\n",
      "Iteration 43050 | Loss: 262.855389\n",
      "Iteration 43075 | Loss: 262.855389\n",
      "Iteration 43100 | Loss: 262.855389\n",
      "Iteration 43125 | Loss: 262.855389\n",
      "Iteration 43150 | Loss: 262.855389\n",
      "Iteration 43175 | Loss: 262.855389\n",
      "Iteration 43200 | Loss: 262.855389\n",
      "Iteration 43225 | Loss: 262.855389\n",
      "Iteration 43250 | Loss: 262.855389\n",
      "Iteration 43275 | Loss: 262.855389\n",
      "Iteration 43300 | Loss: 262.855389\n",
      "Iteration 43325 | Loss: 262.855389\n",
      "Iteration 43350 | Loss: 262.855389\n",
      "Iteration 43375 | Loss: 262.855389\n",
      "Iteration 43400 | Loss: 262.855389\n",
      "Iteration 43425 | Loss: 262.855389\n",
      "Iteration 43450 | Loss: 262.855389\n",
      "Iteration 43475 | Loss: 262.855389\n",
      "Iteration 43500 | Loss: 262.855389\n",
      "Iteration 43525 | Loss: 262.855389\n",
      "Iteration 43550 | Loss: 262.855389\n",
      "Iteration 43575 | Loss: 262.855389\n",
      "Iteration 43600 | Loss: 262.855389\n",
      "Iteration 43625 | Loss: 262.855389\n",
      "Iteration 43650 | Loss: 262.855389\n",
      "Iteration 43675 | Loss: 262.855389\n",
      "Iteration 43700 | Loss: 262.855389\n",
      "Iteration 43725 | Loss: 262.855389\n",
      "Iteration 43750 | Loss: 262.855389\n",
      "Iteration 43775 | Loss: 262.855389\n",
      "Iteration 43800 | Loss: 262.855389\n",
      "Iteration 43825 | Loss: 262.855389\n",
      "Iteration 43850 | Loss: 262.855389\n",
      "Iteration 43875 | Loss: 262.855389\n",
      "Iteration 43900 | Loss: 262.855389\n",
      "Iteration 43925 | Loss: 262.855389\n",
      "Iteration 43950 | Loss: 262.855389\n",
      "Iteration 43975 | Loss: 262.855389\n",
      "Iteration 44000 | Loss: 262.855389\n",
      "Iteration 44025 | Loss: 262.855389\n",
      "Iteration 44050 | Loss: 262.855389\n",
      "Iteration 44075 | Loss: 262.855389\n",
      "Iteration 44100 | Loss: 262.855389\n",
      "Iteration 44125 | Loss: 262.855389\n",
      "Iteration 44150 | Loss: 262.855389\n",
      "Iteration 44175 | Loss: 262.855389\n",
      "Iteration 44200 | Loss: 262.855389\n",
      "Iteration 44225 | Loss: 262.855389\n",
      "Iteration 44250 | Loss: 262.855389\n",
      "Iteration 44275 | Loss: 262.855389\n",
      "Iteration 44300 | Loss: 262.855389\n",
      "Iteration 44325 | Loss: 262.855389\n",
      "Iteration 44350 | Loss: 262.855389\n",
      "Iteration 44375 | Loss: 262.855389\n",
      "Iteration 44400 | Loss: 262.855389\n",
      "Iteration 44425 | Loss: 262.855389\n",
      "Iteration 44450 | Loss: 262.855389\n",
      "Iteration 44475 | Loss: 262.855389\n",
      "Iteration 44500 | Loss: 262.855389\n",
      "Iteration 44525 | Loss: 262.855389\n",
      "Iteration 44550 | Loss: 262.855389\n",
      "Iteration 44575 | Loss: 262.855389\n",
      "Iteration 44600 | Loss: 262.855389\n",
      "Iteration 44625 | Loss: 262.855389\n",
      "Iteration 44650 | Loss: 262.855389\n",
      "Iteration 44675 | Loss: 262.855389\n",
      "Iteration 44700 | Loss: 262.855389\n",
      "Iteration 44725 | Loss: 262.855389\n",
      "Iteration 44750 | Loss: 262.855389\n",
      "Iteration 44775 | Loss: 262.855389\n",
      "Iteration 44800 | Loss: 262.855389\n",
      "Iteration 44825 | Loss: 262.855389\n",
      "Iteration 44850 | Loss: 262.855389\n",
      "Iteration 44875 | Loss: 262.855389\n",
      "Iteration 44900 | Loss: 262.855389\n",
      "Iteration 44925 | Loss: 262.855389\n",
      "Iteration 44950 | Loss: 262.855389\n",
      "Iteration 44975 | Loss: 262.855389\n",
      "Iteration 45000 | Loss: 262.855389\n",
      "Iteration 45025 | Loss: 262.855389\n",
      "Iteration 45050 | Loss: 262.855389\n",
      "Iteration 45075 | Loss: 262.855389\n",
      "Iteration 45100 | Loss: 262.855389\n",
      "Iteration 45125 | Loss: 262.855389\n",
      "Iteration 45150 | Loss: 262.855389\n",
      "Iteration 45175 | Loss: 262.855389\n",
      "Iteration 45200 | Loss: 262.855389\n",
      "Iteration 45225 | Loss: 262.855389\n",
      "Iteration 45250 | Loss: 262.855389\n",
      "Iteration 45275 | Loss: 262.855389\n",
      "Iteration 45300 | Loss: 262.855389\n",
      "Iteration 45325 | Loss: 262.855389\n",
      "Iteration 45350 | Loss: 262.855389\n",
      "Iteration 45375 | Loss: 262.855389\n",
      "Iteration 45400 | Loss: 262.855389\n",
      "Iteration 45425 | Loss: 262.855389\n",
      "Iteration 45450 | Loss: 262.855389\n",
      "Iteration 45475 | Loss: 262.855389\n",
      "Iteration 45500 | Loss: 262.855389\n",
      "Iteration 45525 | Loss: 262.855389\n",
      "Iteration 45550 | Loss: 262.855389\n",
      "Iteration 45575 | Loss: 262.855389\n",
      "Iteration 45600 | Loss: 262.855389\n",
      "Iteration 45625 | Loss: 262.855389\n",
      "Iteration 45650 | Loss: 262.855389\n",
      "Iteration 45675 | Loss: 262.855389\n",
      "Iteration 45700 | Loss: 262.855389\n",
      "Iteration 45725 | Loss: 262.855389\n",
      "Iteration 45750 | Loss: 262.855389\n",
      "Iteration 45775 | Loss: 262.855389\n",
      "Iteration 45800 | Loss: 262.855389\n",
      "Iteration 45825 | Loss: 262.855389\n",
      "Iteration 45850 | Loss: 262.855389\n",
      "Iteration 45875 | Loss: 262.855389\n",
      "Iteration 45900 | Loss: 262.855389\n",
      "Iteration 45925 | Loss: 262.855389\n",
      "Iteration 45950 | Loss: 262.855389\n",
      "Iteration 45975 | Loss: 262.855389\n",
      "Iteration 46000 | Loss: 262.855389\n",
      "Iteration 46025 | Loss: 262.855389\n",
      "Iteration 46050 | Loss: 262.855389\n",
      "Iteration 46075 | Loss: 262.855389\n",
      "Iteration 46100 | Loss: 262.855389\n",
      "Iteration 46125 | Loss: 262.855389\n",
      "Iteration 46150 | Loss: 262.855389\n",
      "Iteration 46175 | Loss: 262.855389\n",
      "Iteration 46200 | Loss: 262.855389\n",
      "Iteration 46225 | Loss: 262.855389\n",
      "Iteration 46250 | Loss: 262.855389\n",
      "Iteration 46275 | Loss: 262.855389\n",
      "Iteration 46300 | Loss: 262.855389\n",
      "Iteration 46325 | Loss: 262.855389\n",
      "Iteration 46350 | Loss: 262.855389\n",
      "Iteration 46375 | Loss: 262.855389\n",
      "Iteration 46400 | Loss: 262.855389\n",
      "Iteration 46425 | Loss: 262.855389\n",
      "Iteration 46450 | Loss: 262.855389\n",
      "Iteration 46475 | Loss: 262.855389\n",
      "Iteration 46500 | Loss: 262.855389\n",
      "Iteration 46525 | Loss: 262.855389\n",
      "Iteration 46550 | Loss: 262.855389\n",
      "Iteration 46575 | Loss: 262.855389\n",
      "Iteration 46600 | Loss: 262.855389\n",
      "Iteration 46625 | Loss: 262.855389\n",
      "Iteration 46650 | Loss: 262.855389\n",
      "Iteration 46675 | Loss: 262.855389\n",
      "Iteration 46700 | Loss: 262.855389\n",
      "Iteration 46725 | Loss: 262.855389\n",
      "Iteration 46750 | Loss: 262.855389\n",
      "Iteration 46775 | Loss: 262.855389\n",
      "Iteration 46800 | Loss: 262.855389\n",
      "Iteration 46825 | Loss: 262.855389\n",
      "Iteration 46850 | Loss: 262.855389\n",
      "Iteration 46875 | Loss: 262.855389\n",
      "Iteration 46900 | Loss: 262.855389\n",
      "Iteration 46925 | Loss: 262.855389\n",
      "Iteration 46950 | Loss: 262.855389\n",
      "Iteration 46975 | Loss: 262.855389\n",
      "Iteration 47000 | Loss: 262.855389\n",
      "Iteration 47025 | Loss: 262.855389\n",
      "Iteration 47050 | Loss: 262.855389\n",
      "Iteration 47075 | Loss: 262.855389\n",
      "Iteration 47100 | Loss: 262.855389\n",
      "Iteration 47125 | Loss: 262.855389\n",
      "Iteration 47150 | Loss: 262.855389\n",
      "Iteration 47175 | Loss: 262.855389\n",
      "Iteration 47200 | Loss: 262.855389\n",
      "Iteration 47225 | Loss: 262.855389\n",
      "Iteration 47250 | Loss: 262.855389\n",
      "Iteration 47275 | Loss: 262.855389\n",
      "Iteration 47300 | Loss: 262.855389\n",
      "Iteration 47325 | Loss: 262.855389\n",
      "Iteration 47350 | Loss: 262.855389\n",
      "Iteration 47375 | Loss: 262.855389\n",
      "Iteration 47400 | Loss: 262.855389\n",
      "Iteration 47425 | Loss: 262.855389\n",
      "Iteration 47450 | Loss: 262.855389\n",
      "Iteration 47475 | Loss: 262.855389\n",
      "Iteration 47500 | Loss: 262.855389\n",
      "Iteration 47525 | Loss: 262.855389\n",
      "Iteration 47550 | Loss: 262.855389\n",
      "Iteration 47575 | Loss: 262.855389\n",
      "Iteration 47600 | Loss: 262.855389\n",
      "Iteration 47625 | Loss: 262.855389\n",
      "Iteration 47650 | Loss: 262.855389\n",
      "Iteration 47675 | Loss: 262.855389\n",
      "Iteration 47700 | Loss: 262.855389\n",
      "Iteration 47725 | Loss: 262.855389\n",
      "Iteration 47750 | Loss: 262.855389\n",
      "Iteration 47775 | Loss: 262.855389\n",
      "Iteration 47800 | Loss: 262.855389\n",
      "Iteration 47825 | Loss: 262.855389\n",
      "Iteration 47850 | Loss: 262.855389\n",
      "Iteration 47875 | Loss: 262.855389\n",
      "Iteration 47900 | Loss: 262.855389\n",
      "Iteration 47925 | Loss: 262.855389\n",
      "Iteration 47950 | Loss: 262.855389\n",
      "Iteration 47975 | Loss: 262.855389\n",
      "Iteration 48000 | Loss: 262.855389\n",
      "Iteration 48025 | Loss: 262.855389\n",
      "Iteration 48050 | Loss: 262.855389\n",
      "Iteration 48075 | Loss: 262.855389\n",
      "Iteration 48100 | Loss: 262.855389\n",
      "Iteration 48125 | Loss: 262.855389\n",
      "Iteration 48150 | Loss: 262.855389\n",
      "Iteration 48175 | Loss: 262.855389\n",
      "Iteration 48200 | Loss: 262.855389\n",
      "Iteration 48225 | Loss: 262.855389\n",
      "Iteration 48250 | Loss: 262.855389\n",
      "Iteration 48275 | Loss: 262.855389\n",
      "Iteration 48300 | Loss: 262.855389\n",
      "Iteration 48325 | Loss: 262.855389\n",
      "Iteration 48350 | Loss: 262.855389\n",
      "Iteration 48375 | Loss: 262.855389\n",
      "Iteration 48400 | Loss: 262.855389\n",
      "Iteration 48425 | Loss: 262.855389\n",
      "Iteration 48450 | Loss: 262.855389\n",
      "Iteration 48475 | Loss: 262.855389\n",
      "Iteration 48500 | Loss: 262.855389\n",
      "Iteration 48525 | Loss: 262.855389\n",
      "Iteration 48550 | Loss: 262.855389\n",
      "Iteration 48575 | Loss: 262.855389\n",
      "Iteration 48600 | Loss: 262.855389\n",
      "Iteration 48625 | Loss: 262.855389\n",
      "Iteration 48650 | Loss: 262.855389\n",
      "Iteration 48675 | Loss: 262.855389\n",
      "Iteration 48700 | Loss: 262.855389\n",
      "Iteration 48725 | Loss: 262.855389\n",
      "Iteration 48750 | Loss: 262.855389\n",
      "Iteration 48775 | Loss: 262.855389\n",
      "Iteration 48800 | Loss: 262.855389\n",
      "Iteration 48825 | Loss: 262.855389\n",
      "Iteration 48850 | Loss: 262.855389\n",
      "Iteration 48875 | Loss: 262.855389\n",
      "Iteration 48900 | Loss: 262.855389\n",
      "Iteration 48925 | Loss: 262.855389\n",
      "Iteration 48950 | Loss: 262.855389\n",
      "Iteration 48975 | Loss: 262.855389\n",
      "Iteration 49000 | Loss: 262.855389\n",
      "Iteration 49025 | Loss: 262.855389\n",
      "Iteration 49050 | Loss: 262.855389\n",
      "Iteration 49075 | Loss: 262.855389\n",
      "Iteration 49100 | Loss: 262.855389\n",
      "Iteration 49125 | Loss: 262.855389\n",
      "Iteration 49150 | Loss: 262.855389\n",
      "Iteration 49175 | Loss: 262.855389\n",
      "Iteration 49200 | Loss: 262.855389\n",
      "Iteration 49225 | Loss: 262.855389\n",
      "Iteration 49250 | Loss: 262.855389\n",
      "Iteration 49275 | Loss: 262.855389\n",
      "Iteration 49300 | Loss: 262.855389\n",
      "Iteration 49325 | Loss: 262.855389\n",
      "Iteration 49350 | Loss: 262.855389\n",
      "Iteration 49375 | Loss: 262.855389\n",
      "Iteration 49400 | Loss: 262.855389\n",
      "Iteration 49425 | Loss: 262.855389\n",
      "Iteration 49450 | Loss: 262.855389\n",
      "Iteration 49475 | Loss: 262.855389\n",
      "Iteration 49500 | Loss: 262.855389\n",
      "Iteration 49525 | Loss: 262.855389\n",
      "Iteration 49550 | Loss: 262.855389\n",
      "Iteration 49575 | Loss: 262.855389\n",
      "Iteration 49600 | Loss: 262.855389\n",
      "Iteration 49625 | Loss: 262.855389\n",
      "Iteration 49650 | Loss: 262.855389\n",
      "Iteration 49675 | Loss: 262.855389\n",
      "Iteration 49700 | Loss: 262.855389\n",
      "Iteration 49725 | Loss: 262.855389\n",
      "Iteration 49750 | Loss: 262.855389\n",
      "Iteration 49775 | Loss: 262.855389\n",
      "Iteration 49800 | Loss: 262.855389\n",
      "Iteration 49825 | Loss: 262.855389\n",
      "Iteration 49850 | Loss: 262.855389\n",
      "Iteration 49875 | Loss: 262.855389\n",
      "Iteration 49900 | Loss: 262.855389\n",
      "Iteration 49925 | Loss: 262.855389\n",
      "Iteration 49950 | Loss: 262.855389\n",
      "Iteration 49975 | Loss: 262.855389\n",
      "Iteration 49999 | Loss: 262.855389\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "learned_w, learned_b, losses = train_linear_regression(\n",
    "    X, y, \n",
    "    learning_rate= 0.0002, \n",
    "    n_iterations=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results ===\n",
      "True weights:    [ 2.  -3.5  1.5]\n",
      "Learned weights: [ 2.00851256 -3.51329578  1.48094932]\n",
      "\n",
      "True bias:    5.0\n",
      "Learned bias: 4.9910\n"
     ]
    }
   ],
   "source": [
    "# Compare learned parameters with true parameters\n",
    "print(\"\\n=== Results ===\")\n",
    "print(f\"True weights:    {true_weights}\")\n",
    "print(f\"Learned weights: {learned_w}\")\n",
    "print(f\"\\nTrue bias:    {true_bias}\")\n",
    "print(f\"Learned bias: {learned_b:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the loss curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Feature Scaling\n",
    "\n",
    "Gradient descent works much better when features are on similar scales. Let's see why and how to fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is statistical way to standarize any sameple of feature $i$ to a normal distribution:\n",
    "$$X_i = \\frac{x_i-\\mu_i}{\\sigma_i}$$\n",
    "where $X_i$ is the column vector of raw data feature $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranges:\n",
      "  Feature 1: [-3241.27, 3852.73]\n",
      "  Feature 2: [-0.00, 0.00]\n",
      "  Feature 3: [-2.90, 2.60]\n"
     ]
    }
   ],
   "source": [
    "# Create data with very different scales\n",
    "np.random.seed(42)\n",
    "X_unscaled = np.column_stack([\n",
    "    np.random.randn(500) * 1000,      # Feature 1: scale ~1000\n",
    "    np.random.randn(500) * 0.001,     # Feature 2: scale ~0.001\n",
    "    np.random.randn(500)              # Feature 3: scale ~1\n",
    "])\n",
    "y_unscaled = X_unscaled @ np.array([0.001, 1000, 1]) + 5 + np.random.randn(500) * 0.5\n",
    "\n",
    "print(f\"Feature ranges:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Feature {i+1}: [{X_unscaled[:, i].min():.2f}, {X_unscaled[:, i].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0 | Loss: 42848.057748\n",
      "Iteration   25 | Loss: 435577579601040436350005280948301069956086342035519664781201202940703503386754841912070402279058324153344625299747317402703940116413871177418784184755443047370040528184935469917022413186320435255686814960663588283351040.000000\n",
      "Iteration   50 | Loss: inf\n",
      "Iteration   75 | Loss: nan\n",
      "Iteration   99 | Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheetah\\AppData\\Local\\Temp\\ipykernel_14236\\870493944.py:13: RuntimeWarning: overflow encountered in square\n",
      "  return np.mean(np.sum((y_pred - y_true) ** 2))\n",
      "C:\\Users\\cheetah\\AppData\\Local\\Temp\\ipykernel_14236\\3547732573.py:14: RuntimeWarning: overflow encountered in matmul\n",
      "  return (2*((y_pred - y) @ X)/y.shape[0], 2*(np.mean(y_pred - y)))\n",
      "C:\\Users\\cheetah\\AppData\\Local\\Temp\\ipykernel_14236\\3547732573.py:14: RuntimeWarning: invalid value encountered in matmul\n",
      "  return (2*((y_pred - y) @ X)/y.shape[0], 2*(np.mean(y_pred - y)))\n",
      "d:\\CodeFile\\miniconda\\Lib\\site-packages\\numpy\\_core\\_methods.py:134: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "C:\\Users\\cheetah\\AppData\\Local\\Temp\\ipykernel_14236\\2670533896.py:36: RuntimeWarning: invalid value encountered in subtract\n",
      "  w = w - (learning_rate * gd_w)\n"
     ]
    }
   ],
   "source": [
    "# This will likely fail or converge very slowly!\n",
    "try:\n",
    "    w_bad, b_bad, losses_bad = train_linear_regression(\n",
    "        X_unscaled, y_unscaled, learning_rate=0.01, n_iterations=100\n",
    "    )\n",
    "except:\n",
    "    print(\"Training failed due to numerical instability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Implement standardize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Standardization (Z-score normalization)\n",
    "def standardize(X: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Standardize features to have mean=0 and std=1.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (X_standardized, mean, std)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Standardize and train\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m X_scaled, X_mean, X_std = standardize(X_unscaled)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mScaled feature ranges:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n",
      "\u001b[31mTypeError\u001b[39m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# Standardize and train\n",
    "X_scaled, X_mean, X_std = standardize(X_unscaled)\n",
    "\n",
    "print(f\"Scaled feature ranges:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Feature {i+1}: [{X_scaled[:, i].min():.2f}, {X_scaled[:, i].max():.2f}]\")\n",
    "\n",
    "w_good, b_good, losses_good = train_linear_regression(\n",
    "    X_scaled, y_unscaled, learning_rate=0.1, n_iterations=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Tasks (Deadline: Sunday 30th Nov 2025)\n",
    "\n",
    "Complete the following tasks to practice implementing gradient descent for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Implement Mini-Batch Gradient Descent\n",
    "\n",
    "Instead of using all samples in each iteration (batch gradient descent), implement **mini-batch gradient descent** which uses a random subset of samples.\n",
    "\n",
    "Formally said, choose $X_b$ and its corresponding $y_b$ which is a subset of $row(X), row(y)$ to be trained for each iteration.\n",
    "\n",
    "\n",
    "Benefits of mini-batch:\n",
    "- Faster iterations\n",
    "- Can escape local minima\n",
    "- Better generalization\n",
    "\n",
    "```python\n",
    "# Expected usage:\n",
    "w, b, losses = train_minibatch_gd(X, y, batch_size=32, learning_rate=0.01, n_iterations=1000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_minibatch_gd(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    batch_size: int = 32,\n",
    "    learning_rate: float = 0.01,\n",
    "    n_iterations: int = 1000,\n",
    "    verbose: bool = True,\n",
    "    log_every_n_step: int = 20,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression using mini-batch gradient descent.\n",
    "    \n",
    "    Hints:\n",
    "    - Use np.random.choice to select random indices\n",
    "    - Compute gradients using only the selected samples\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize parameters randomly\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Your code here\n",
    "        pass\n",
    "        \n",
    "        if verbose and (i % log_every_n_step == 0 or i == n_iterations - 1):\n",
    "            print(f\"Iteration {i:4d} | Loss: {loss:.6f}\")\n",
    "    \n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, loss_history = train_minibatch_gd(\n",
    "    X, y,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.01,\n",
    "    n_iterations=200,\n",
    "    log_every_n_step=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement Learning Rate Scheduling\n",
    "\n",
    "Implement a training function that **decreases the learning rate** over time. This helps converge more precisely at the end of training.\n",
    "\n",
    "Common schedules:\n",
    "- Step decay: $\\alpha_t = \\alpha_0 \\cdot 0.9^{\\lfloor t/100 \\rfloor}$\n",
    "- Exponential decay: $\\alpha_t = \\alpha_0 \\cdot e^{-kt}$\n",
    "- Inverse time: $\\alpha_t = \\frac{\\alpha_0}{1 + k \\cdot t}$\n",
    "\n",
    "where $t$ is number of current step/iteration and $k$ is the decay constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_lr_schedule(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    initial_lr: float = 0.1,\n",
    "    schedule: str = 'exponential',  # 'step', 'exponential', or 'inverse'\n",
    "    n_iterations: int = 1000,\n",
    "    decay_constant: float = 0.0001,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train with learning rate scheduling.\n",
    "    \n",
    "    Implement at least one scheduling strategy.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.random.randn(n_features) * 0.01\n",
    "    b = 0.0\n",
    "    \n",
    "    learning_rate = initial_lr\n",
    "    loss_history = []\n",
    "    for i in range(n_iterations):\n",
    "        # Your code here\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test them all:\n",
    "print(\"Step decay:\")\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='step',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.0001\n",
    ")\n",
    "\n",
    "print(\"Exponential decay:\")\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='exponential',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.0001\n",
    ")\n",
    "\n",
    "print(\"Inverse time decay:\")\n",
    "_, _, loss_history = train_with_lr_schedule(\n",
    "    X, y,\n",
    "    initial_lr=0.01,\n",
    "    schedule='inverse',\n",
    "    n_iterations=500,\n",
    "    decay_constant=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Add Regularization (Ridge Regression)\n",
    "\n",
    "Implement **L2 regularization** (Ridge regression) to prevent overfitting.\n",
    "\n",
    "The loss function becomes:\n",
    "$$\\mathcal{L} = \\mathcal{L}_{MSE} + \\lambda \\sum w_i^2$$\n",
    "\n",
    "The gradient for weights becomes:\n",
    "$$\\frac{\\partial Loss}{\\partial w} = \\frac{\\partial MSE}{\\partial w} + 2\\lambda w$$\n",
    "\n",
    "where $\\lambda$ is the regularization constant and $w_i$ is the weight value of corresponding feature $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ridge_loss(y_true: np.ndarray, y_pred: np.ndarray, w: np.ndarray, reg_lambda: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute Ridge regression loss (MSE + L2 regularization).\n",
    "    \n",
    "    Args:\n",
    "        y_true: Actual target values\n",
    "        y_pred: Predicted values\n",
    "        w: Weight vector\n",
    "        reg_lambda: Regularization strength\n",
    "    \n",
    "    Returns:\n",
    "        Ridge loss value\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "def calculate_ridge_gradients(X: np.ndarray, y: np.ndarray, y_pred: np.ndarray, w: np.ndarray, reg_lambda: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Compute gradients for Ridge regression.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: True target values\n",
    "        y_pred: Predicted values\n",
    "        w: Weight vector\n",
    "        reg_lambda: Regularization strength\n",
    "        \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "def train_ridge_regression(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    learning_rate: float = 0.01,\n",
    "    reg_lambda: float = 0.1,  # Regularization strength\n",
    "    n_iterations: int = 1000\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Train linear regression with L2 regularization.\n",
    "    \n",
    "    Hints:\n",
    "    - Modify the loss calculation to include regularization term\n",
    "    - Modify the gradient calculation for weights\n",
    "    - Note: We typically don't regularize the bias term\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _ =train_ridge_regression(\n",
    "    X, y,\n",
    "    learning_rate=0.01,\n",
    "    reg_lambda=0.1,\n",
    "    n_iterations=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Task: Implement Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Implement pure SGD where you update weights after **each individual sample** (batch_size=1).\n",
    "\n",
    "Compare the convergence behavior of:\n",
    "1. Batch GD (all samples)\n",
    "2. Mini-batch GD (e.g., 32 samples)\n",
    "3. SGD (1 sample)\n",
    "\n",
    "Plot the loss curves for all three on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
